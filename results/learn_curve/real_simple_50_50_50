/home/duc/Documents/Research/SSVI-TF/venv/bin/python3 /home/duc/Documents/Research/SSVI-TF/learning_curve.py
Generating synthetic  real valued data ...
Generating synthetic  real valued data took:  3.642090320587158
Using  0.2  of training data
Tensor dimensions:  [50, 50, 50]
Optimization metrics:
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |
    50       38.27      0.1362     0.136      3.1623     4.4719
   100       82.07      0.0707     0.0705     3.1619     0.229
   150       125.99     0.0564     0.0565     3.1093     0.0892
   200       169.73     0.0532     0.0524     2.3906     0.0424
   250       213.44     0.0493     0.0489     2.3044     0.0351
   300       257.05     0.0478     0.0472     2.1806     0.0341
   350       300.58     0.046      0.0456     1.7777     0.0324
   400       344.23     0.0455     0.0451     1.5204     0.026
   450       387.91     0.0446     0.0442     1.637      0.0188
   500       431.49     0.0446     0.044      1.3243     0.0183
   550       475.06     0.0444     0.0441     1.2178     0.0137
   600       518.87     0.0441     0.0436     0.968      0.0142
   650       562.38     0.0439     0.0434     0.8347     0.0132
   700       605.91     0.0443     0.0435     0.9782     0.0073
   750       649.41     0.044      0.0435     1.1163     0.0102
   800       692.98     0.0442     0.0437     1.136      0.009
   850       736.59     0.044      0.0435     0.8671     0.0058
   900       780.12     0.0443     0.0438     0.9209     0.0057
   950       823.66     0.0441     0.0438     1.0959     0.0075
   1000      867.23     0.0436     0.0431     1.0665     0.0066
   1050      910.81     0.0436     0.0431     0.7677     0.0054
   1100      954.45     0.0435     0.043      0.5827     0.004
   1150      998.11     0.0435     0.0431     0.6308     0.0046
   1200     1041.75     0.0436     0.0433     0.7779     0.0039
   1250     1085.28     0.0435     0.043      0.9498     0.0029
   1300     1128.95     0.0437     0.0433     0.9782     0.0036
   1350     1172.41     0.0434     0.0431     0.9502     0.0033
   1400     1215.99     0.0438     0.0431     0.7213     0.0034
   1450      1259.6     0.0437     0.0432     0.8558     0.0028
   1500     1303.08     0.0435     0.0431     0.8096     0.0024
   1550     1346.53     0.0439     0.0436     0.9256     0.0023
   1600     1389.98     0.0439     0.0434     0.8516     0.0023
   1650     1433.56     0.0437     0.0434     0.9378     0.0019
   1700      1477.2     0.0435     0.043      0.7049     0.0021
   1750     1520.81     0.0439     0.0433     0.9616     0.0016
   1800     1564.57     0.0437     0.0432     0.7685     0.0017
   1850     1608.06     0.0436     0.043      0.959      0.0015
   1900     1651.69     0.0436     0.0431     0.6547     0.0014
   1950     1695.45     0.0435     0.043      0.8102     0.0014
   2000     1739.37     0.0435     0.0431     0.7414     0.0013
Using  0.4  of training data
Tensor dimensions:  [50, 50, 50]
Optimization metrics:
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |
    50       38.29      0.1055     0.1055     3.0535     4.4678
   100       82.71      0.0588     0.0592     2.7519     0.1998
   150       127.06     0.0536     0.0532     2.4482     0.0989
   200       171.42     0.0499     0.0499     2.5086     0.0475
   250       215.98     0.0478     0.0473     2.1435     0.031
   300       260.7      0.0463     0.0459     1.6243     0.0212
   350       305.45     0.0452     0.0449     1.3892     0.0159
   400       350.14     0.0457     0.0453     1.303      0.0141
   450       394.62     0.045      0.0446     1.3581     0.0111
   500       439.27     0.0444     0.044      1.2779     0.009
   550       483.8      0.0444     0.0443     1.2542     0.0077
   600       528.39     0.0444     0.0439      1.19      0.0067
   650       572.96     0.0438     0.0437     1.0485     0.0057
   700       617.5      0.0437     0.0431     0.9025     0.005
   750       662.15     0.0437     0.0435     0.7893     0.0042
   800       706.73     0.0436     0.0432     0.7245     0.0036
   850       751.2      0.0436     0.0433     0.8639     0.0034
   900       796.06     0.0441     0.0437     1.0978     0.0031
   950       840.73     0.0438     0.0435      1.25      0.0027
   1000      885.28     0.0442     0.0437     0.9251     0.0025
   1050      929.95     0.0435     0.0433     0.7696     0.0027
   1100      974.39     0.044      0.0436     0.7359     0.002
   1150     1018.87     0.0438     0.0436     0.8163     0.0018
   1200     1063.56     0.0441     0.0438     1.1123     0.0018
   1250     1108.14     0.0434     0.0431     1.0101     0.0015
   1300     1152.66     0.0438     0.0434     0.8277     0.0016
   1350     1197.06     0.0436     0.0432     0.7858     0.0014
   1400      1241.4     0.0441     0.0436     0.7475     0.0012
   1450     1285.86     0.0436     0.0433     0.9762     0.0014
   1500     1330.26     0.0436     0.0433     0.9759     0.0012
   1550      1374.6     0.0432     0.043      0.7105     0.001
   1600     1419.15     0.0434     0.0432     0.607      0.0012
   1650     1463.66     0.0434     0.0431     0.7326     0.0011
   1700     1508.36     0.0436     0.0433     0.5728     0.0009
   1750     1552.97     0.0436     0.0432     0.7979     0.0009
   1800     1597.42     0.0434     0.0432     0.8333     0.0008
   1850     1641.96     0.0438     0.0433     0.9209     0.0008
   1900      1686.3     0.0437     0.0433     0.7279     0.0009
   1950      1730.8     0.0438     0.0435     0.8118     0.0007
   2000      1775.2     0.0435     0.0432     0.9146     0.0007
Using  0.6  of training data
Tensor dimensions:  [50, 50, 50]
Optimization metrics:
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |
    50       38.43      0.1018     0.1017     2.8935     4.4664
   100       83.19      0.057      0.057      2.9465     0.2968
   150       127.89     0.0512     0.0511     1.8348     0.1588
   200       172.77     0.0469     0.0466     1.9644     0.072
   250       217.63     0.0456     0.0453     1.3206     0.0411
   300       262.44     0.0449     0.0446     1.2063     0.0329
   350       307.22     0.0442     0.0438     1.4416     0.0216
   400       352.0      0.0437     0.0436     1.5023     0.0167
   450       396.89     0.0439     0.0437     1.1397     0.0129
   500       441.77     0.044      0.0437     1.1223     0.0097
   550       486.6      0.0442     0.0439     1.3883     0.0091
   600       531.64     0.0442     0.0439     1.1674     0.0081
   650       576.78     0.0438     0.0436     0.9963     0.0059
   700       621.77     0.0439     0.0436     1.2427     0.0048
   750       666.86     0.0439     0.0437     1.3082     0.0052
   800       712.05     0.0441     0.0438     1.1904     0.0045
   850       757.08     0.0437     0.0434     1.097      0.0041
   900       801.98     0.0435     0.0433     1.0954     0.0035
   950       846.87     0.0434     0.0432     1.0532     0.0032
   1000      891.74     0.0435     0.0434     1.0524     0.0026
   1050      936.72     0.0437     0.0435     0.9511     0.0027
   1100      981.65     0.0436     0.0434     0.8272     0.0024
   1150     1026.41     0.0434     0.0433     0.9779     0.0021
   1200     1071.38     0.0436     0.0433     0.8806     0.0019
   1250     1116.41     0.0439     0.0436     0.8237     0.0017
   1300     1161.34     0.0436     0.0433     0.7985     0.0019
   1350     1206.25     0.0434     0.0431     0.771      0.0016
   1400     1251.28     0.0434     0.0431     0.8222     0.0014
   1450     1296.25     0.0435     0.0433     0.7989     0.0013
   1500     1341.08     0.0436     0.0435     0.7926     0.0013
   1550     1385.82     0.0435     0.043      0.8781     0.0012
   1600     1430.63     0.0432     0.0431     0.6833     0.0012
   1650     1475.36     0.0437     0.0435     1.0033     0.0011
   1700     1520.17     0.0435     0.0432     1.2084     0.001
   1750     1564.97     0.0435     0.0433     0.7441     0.0009
   1800     1609.89     0.0436     0.0434     0.7704     0.0009
   1850     1654.65     0.0432     0.0429     0.9207     0.0009
   1900     1699.41     0.0432     0.043      0.5514     0.0009
   1950     1744.21     0.0435     0.0433     0.768      0.0008
   2000     1788.96     0.0434     0.0431     0.8835     0.0008
Using  0.8  of training data
Tensor dimensions:  [50, 50, 50]
Optimization metrics:
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |
    50       38.44      0.0986     0.0989     2.8184     4.4657
   100       83.75      0.0579     0.0581     2.7055     0.4674
   150       129.1      0.0511     0.0511     1.6507     0.1849
   200       174.37     0.047      0.0467     1.5212     0.0854
   250       219.56     0.0453     0.045      1.196      0.0506
   300       264.77     0.0448     0.0445     1.0869     0.034
   350       310.0      0.044      0.0437     1.1573     0.0235
   400       355.25     0.0437     0.0434     0.777      0.0208
   450       400.53     0.044      0.0438     0.8848     0.0165
   500       446.01     0.0437     0.0434     1.196      0.0122
   550       491.36     0.0436     0.0434     1.0964     0.0114
   600       536.57     0.0435     0.0433     1.0111     0.0087
   650       581.94     0.0436     0.0433     1.2361     0.0081
   700       627.16     0.0433     0.0431     0.9084     0.0067
   750       672.52     0.0433     0.0429     0.6434     0.0058
   800       717.9      0.0435     0.0431     0.6997     0.005
   850       763.28     0.0433     0.043      0.661      0.0046
   900       808.56     0.0435     0.0431     0.5465     0.0042
   950       853.85     0.0434     0.043      0.5867     0.0034
   1000      899.35     0.0437     0.0435     1.013      0.0029
   1050      944.84     0.0434     0.0432     1.0815     0.0029
   1100      990.23     0.0434     0.0432     0.9479     0.0027
   1150     1035.49     0.0432     0.043      1.0258     0.0025
   1200     1080.82     0.0435     0.0432     0.9365     0.0024
   1250     1126.29     0.0432     0.043      0.9669     0.0021
   1300     1171.73     0.0436     0.0434     0.9891     0.0019
   1350     1216.87     0.0434     0.0431     1.1075     0.0017
   1400     1262.07     0.0438     0.0434     1.1097     0.0016
   1450      1307.3     0.0433     0.0429     0.8655     0.0017
   1500     1352.54     0.0434     0.0432     0.935      0.0014
   1550     1397.78     0.0434     0.0431      0.94      0.0014
   1600      1443.1     0.0433     0.043      0.6855     0.0014
   1650     1488.48     0.0434     0.0432     0.7891     0.0014
   1700     1533.85     0.0434     0.0431     0.8317     0.0013
   1750     1579.29     0.0435     0.0432     0.8809     0.0012
   1800     1624.63     0.0434     0.0431     1.0039     0.0011
   1850     1669.81     0.0432     0.0431     0.9151     0.001
   1900     1715.18     0.0433     0.0429     0.7205     0.0009
   1950     1760.39     0.0431     0.0428     0.4973     0.001
   2000     1805.74     0.0432     0.043      0.6208     0.0008
Using  1  of training data
Tensor dimensions:  [50, 50, 50]
Optimization metrics:
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |
    50        38.6      0.0979     0.0983     2.673      4.4652
   100       84.24      0.0585     0.0589     2.5536     0.4062
   150       129.99     0.0507     0.0506     1.3033     0.1325
   200       175.62     0.0475     0.0473     1.2624     0.0681
   250       221.37     0.0458     0.0457     1.2839     0.0422
   300       267.0      0.0446     0.0444      0.7       0.0311
   350       312.76     0.0445     0.0443     0.9488     0.0204
   400       358.4      0.0442     0.044      1.0925     0.0177
   450       404.13     0.0435     0.0434     0.9074     0.0136
   500       449.89     0.0436     0.0434     0.7173     0.0129
   550       495.62     0.0431     0.043      1.1473     0.0102
   600       541.35     0.0434     0.0432     0.6528     0.0091
   650       587.01     0.0432     0.0432     0.8765     0.0074
   700       632.85     0.0434     0.0431     1.0167     0.0064
   750       678.65     0.0433     0.0432     0.8367     0.0058
   800       724.49     0.0432     0.043      0.8406     0.0044
   850       770.2      0.0433     0.043      0.8235     0.0044
   900       815.95     0.0434     0.0431     0.7453     0.0043
   950       861.68     0.0434     0.0433     0.7746     0.0037
   1000      907.35     0.0433     0.043      0.9428     0.003
   1050      953.08     0.0433     0.0431     0.7465     0.0028
   1100      998.78     0.0434     0.0431     0.8379     0.0026
   1150     1044.56     0.043      0.0429     0.8486     0.0024
   1200     1090.25     0.0434     0.0431     0.6837     0.0023
   1250     1135.91     0.0433     0.0431     1.0866     0.002
   1300     1181.87     0.0433     0.0432     0.9172     0.002
   1350     1227.79     0.0431     0.043      0.7703     0.0018
   1400     1273.52     0.0432     0.043      0.8688     0.0017
   1450     1319.35     0.0432     0.043      0.8369     0.0016
   1500     1365.12     0.0432     0.043      0.8662     0.0015
   1550      1410.8     0.0433     0.043      0.845      0.0014
   1600      1456.5     0.0433     0.0431     1.0706     0.0012
   1650     1502.27     0.0432     0.043      0.818      0.0012
   1700     1547.87     0.043      0.0428     0.6594     0.0011
   1750     1593.54     0.0432     0.043      0.7311     0.001
   1800     1639.24     0.0432     0.0429     0.7307     0.001
   1850      1684.9     0.0433     0.0431     0.6139     0.001
   1900     1730.62     0.0431     0.0429     0.5128     0.001
   1950     1776.35     0.0432     0.0429     0.5036     0.001
   2000      1822.0     0.0434     0.0433     0.8484     0.0009

Process finished with exit code 0
