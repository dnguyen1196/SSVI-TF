Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  3.179922580718994
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50        60.0     374.8893   374.1494    3.1623     4.4719     0.0001  
   100       126.75    207.3954   208.3424    3.1618     0.2999      0.0    
   150       192.74    173.9325   172.8011    3.0743     0.1891      0.0    
   200       258.93    161.7229   160.6905    2.8246     0.1458      0.0    
   250       324.45    155.8759   156.062     2.1492     0.1106      0.0    
   300       390.39    153.8566   153.0646    1.9318     0.0464      0.0    
   350       457.15    149.2416   149.8137    1.7477     0.0263      0.0    
   400       522.77    151.1041   150.4574    1.6571     0.0178      0.0    
   450       588.91    150.2248   149.678     2.2933     0.022       0.0    
   500       654.91    149.5318   149.2468    1.7262     0.0197      0.0    
   550       721.02    147.9836   147.8651    1.4984     0.0123      0.0    
   600       786.64    147.7799   147.1693    1.126      0.0104      0.0    
   650       852.4     148.0814   148.2594    1.2416     0.0076      0.0    
   700       919.23    149.3226   148.5246    1.3989     0.0092      0.0    
   750       984.67    147.3281   147.3341    1.3695     0.0096      0.0    
   800      1050.75    148.0899   147.4652    1.3607     0.0067      0.0    
   850      1116.81    147.1434   147.0046    1.2277     0.0055      0.0    
   900      1182.56    149.1828   148.4084    1.3344     0.0058      0.0    
   950      1248.57    147.4825   146.8717    1.1779     0.0064      0.0    
   1000     1314.59    147.245    146.8204    1.0825     0.0043      0.0    
   1050     1379.99    147.193    147.1399     1.19      0.0044      0.0    
   1100     1445.38    147.4192   146.5301    1.3156     0.004       0.0    
   1150     1511.43    146.1325   146.1363    0.8216     0.0047      0.0    
   1200      1577.4    146.6395   146.0416    1.0936     0.0035      0.0    
   1250     1642.58    147.5833   147.0841    1.114      0.0039      0.0    
   1300     1708.78    146.5908   146.2438    0.9835     0.0027      0.0    
   1350     1774.19    146.4321   146.3231    0.9199     0.0025      0.0    
   1400     1839.88    146.9902   146.6882    1.1316     0.0025      0.0    
   1450      1905.3    146.964    146.9671    1.2379     0.0023      0.0    
   1500     1971.22    146.8007   146.4118    0.9998     0.002       0.0    
   1550     2036.68    146.6701   146.1898    0.9734     0.0027      0.0    
   1600     2103.58    146.4158   146.1305    0.8531     0.0018      0.0    
   1650     2169.69    146.5738   145.9535    0.8685     0.0019      0.0    
   1700     2235.83    146.3207   145.9936    1.1225     0.0019      0.0    
   1750     2301.84    146.247    145.9559    0.9276     0.0014      0.0    
   1800     2368.51    146.1963   145.6858    1.1296     0.0016      0.0    
   1850     2435.32    146.6665   146.1055    0.9905     0.0014      0.0    
   1900     2501.67     147.36    146.8946    0.8628     0.0015      0.0    
   1950     2567.61    146.3284   145.9486    0.8875     0.0015      0.0    
   2000     2633.68    145.7367   145.4198    0.6177     0.0013      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       61.17     298.4202   298.5483    3.0672     4.4679      0.0    
   100       128.2     177.827    177.8951    2.8261     0.371       0.0    
   150       194.55    159.951    159.6047    2.3428     0.1414      0.0    
   200       260.32    154.0881   153.7305    1.9909     0.0625      0.0    
   250       326.78    152.9147   152.2539    2.1469     0.0382      0.0    
   300       392.62    150.5216   150.4603    2.0312     0.0277      0.0    
   350       459.34    150.803    149.6717    1.7596     0.0192      0.0    
   400       527.09    148.6098   148.7665    1.414      0.0165      0.0    
   450       594.1     150.1542   149.4151    1.7932     0.0126      0.0    
   500       661.37    148.5511   148.4465    1.532      0.0098      0.0    
   550       727.17    147.2069   147.5437    1.2543     0.0082      0.0    
   600       794.52    148.6446   148.1234    1.561      0.0077      0.0    
   650       861.87    147.6845   147.3102    1.3917     0.0061      0.0    
   700       928.46    146.7211   146.3044    0.9949     0.0057      0.0    
   750       995.37    147.724    147.1317    0.9523     0.004       0.0    
   800      1063.08    147.3874   147.1258    1.0008     0.0041      0.0    
   850      1129.49    145.7618   146.0154    1.0996     0.0039      0.0    
   900      1196.63    147.3135   146.8804    1.1549     0.0034      0.0    
   950      1263.19    147.777    147.2833    1.2841     0.0028      0.0    
   1000     1328.87    147.7249   147.4596    1.8438     0.0029      0.0    
   1050     1395.85    147.2905   146.7066    1.3048     0.0025      0.0    
   1100     1462.47    146.5668   145.9194    0.9409     0.0021      0.0    
   1150     1528.98    146.0425   145.7485    0.8452     0.0021      0.0    
   1200     1596.03    146.5005   146.4964    0.9399     0.0021      0.0    
   1250     1663.17    147.4631   147.2794    1.0364     0.0018      0.0    
   1300     1729.36    147.5422   147.2179    1.1796     0.0016      0.0    
   1350     1796.42    147.0624   146.8239    0.9799     0.0014      0.0    
   1400     1862.87    146.6296   146.3505    1.0801     0.0015      0.0    
   1450     1929.72    147.1353   146.6116    0.9847     0.0012      0.0    
   1500     1996.38    146.7967   146.5139    0.993      0.0013      0.0    
   1550     2062.94    146.6279   146.2771    1.0684     0.0013      0.0    
   1600     2129.18    147.4129   147.0377    0.9004     0.0012      0.0    
   1650     2195.06    146.7999   146.5808    1.4783     0.0011      0.0    
   1700     2260.13    146.7674    146.54     0.9586     0.001       0.0    
   1750     2325.73    146.8766   146.3067    0.978      0.0012      0.0    
   1800     2392.45    146.2881   146.1113    0.8442     0.0009      0.0    
   1850     2459.19    146.3398   146.0991    0.9446     0.0009      0.0    
   1900     2526.88    146.0586   145.7971    1.1953     0.0008      0.0    
   1950     2593.18    145.8185   145.906     0.8303     0.0007      0.0    
   2000     2659.82    146.3984   146.2077    1.0762     0.0007      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       60.24     288.3371   286.6733    2.9108     4.4666      0.0    
   100       127.17    176.8201   175.5887    2.6711     0.3791      0.0    
   150       193.17    158.0327   156.8837    2.1449     0.1206      0.0    
   200       259.82    150.9865   150.3532    1.5685     0.0719      0.0    
   250       326.55    149.2303   148.6321    1.336      0.0489      0.0    
   300       393.7     147.2499   146.9469    1.0639     0.0309      0.0    
   350       460.87    147.7315   147.1326    1.2147     0.022       0.0    
   400       527.97    146.6014   146.4193    1.095      0.0163      0.0    
   450       595.53    147.0409   146.3292    1.1805     0.0141      0.0    
   500       663.2     147.2113   146.9726    1.3107     0.0118      0.0    
   550       729.62    146.6634   146.3196    1.1539     0.0099      0.0    
   600       796.19    147.7509   147.0968    1.4011     0.0087      0.0    
   650       863.36    145.7032   145.4653    0.9929     0.0065      0.0    
   700       930.57    146.0938   145.4734    0.6928     0.0065      0.0    
   750       996.77    146.2704   145.8819    0.945      0.0051      0.0    
   800      1063.71    146.6632   146.2035    0.8664     0.0048      0.0    
   850      1130.21    147.6919   147.0975    1.4334     0.0037      0.0    
   900      1197.07    146.2923   146.2489    1.2151     0.0044      0.0    
   950      1264.08    146.5043   146.0258    1.2391     0.0035      0.0    
   1000     1331.25    146.0486   145.8563    1.1534     0.003       0.0    
   1050     1398.61    146.5955   146.2753    1.1232     0.0028      0.0    
   1100     1465.98    146.5778   146.1254    1.3814     0.0026      0.0    
   1150     1533.26    147.2667   146.6711    1.1439     0.0023      0.0    
   1200     1600.41    146.4591   145.8948    1.3877     0.0022      0.0    
   1250     1667.05    146.0723   145.7786    0.9415     0.0022      0.0    
   1300     1734.05    145.5581   145.1012    0.8134     0.0018      0.0    
   1350     1802.16    145.5842   145.1191    0.9614     0.0016      0.0    
   1400     1869.23    146.2137   145.9135    0.7115     0.0017      0.0    
   1450     1935.53    145.7686   145.6758    0.954      0.0016      0.0    
   1500     2003.34    146.7531   146.0732    1.3666     0.0014      0.0    
   1550     2069.88    146.3988   146.0734    0.983      0.0014      0.0    
   1600     2136.89    145.9008   145.6788    0.9252     0.0012      0.0    
   1650     2202.97    145.6183   145.1036    0.8752     0.0012      0.0    
   1700     2269.92    145.4818   145.3009    0.7549     0.0011      0.0    
   1750     2336.88    145.5456   144.9721    0.7375     0.001       0.0    
   1800     2403.09    146.2191   145.6017    0.9537     0.0011      0.0    
   1850     2470.02    145.8835   145.4551    1.0091     0.0009      0.0    
   1900     2536.48    145.9692   145.6432    0.6657     0.001       0.0    
   1950     2602.89    146.117    145.4842    0.7376     0.0009      0.0    
   2000     2669.73    146.0002   145.484     0.7786     0.0008      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       60.97     280.9531   280.0357    2.7719     4.4658      0.0    
   100       128.12    181.4177   180.3946    2.3456     0.3835      0.0    
   150       195.1     159.8532   159.5272    1.7232     0.1384      0.0    
   200       262.62    152.8144   152.8484    1.0773     0.076       0.0    
   250       329.66    150.204    150.0228    1.1602     0.0509      0.0    
   300       397.14    147.263    147.1934    1.4188     0.0368      0.0    
   350       465.06    147.0023   147.3778    1.3142     0.0277      0.0    
   400       531.69    146.8465   147.0476    1.508      0.0198      0.0    
   450       598.4     146.885    146.9259    1.5022     0.0148      0.0    
   500       665.17    146.7768   146.9087    1.364      0.0134      0.0    
   550       732.63    146.3213   146.618     1.3187     0.0101      0.0    
   600       799.32    145.3767   145.5741    1.1802     0.009       0.0    
   650       865.55    145.8366   146.1797    0.8511     0.008       0.0    
   700       932.44    145.8651   145.9981    1.0663     0.0056      0.0    
   750       999.53    145.4191   145.4578    0.9289     0.0059      0.0    
   800      1067.02    145.6398   145.6955    1.025      0.005       0.0    
   850      1135.11    145.9773   146.3557    0.858      0.005       0.0    
   900      1201.81    146.2861   146.1659    0.8136     0.0037      0.0    
   950      1269.69    145.7548   146.1086    1.2632     0.0035      0.0    
   1000     1337.64    145.7264   145.7527    0.8715     0.0035      0.0    
   1050     1405.24    144.9739   145.3148    0.8634     0.0031      0.0    
   1100     1472.06    145.8764   146.1579    1.0338     0.003       0.0    
   1150     1538.89    146.8338   146.709     1.0331     0.0026      0.0    
   1200     1606.43    145.4585   145.7997    0.9782     0.0024      0.0    
   1250     1674.49    145.9071   146.0791    0.7927     0.002       0.0    
   1300     1741.38    145.7419   145.6787    0.7333     0.0019      0.0    
   1350     1808.51    145.6758   145.6896    0.7865     0.0018      0.0    
   1400     1876.88    145.8183   145.7744    0.7368     0.0018      0.0    
   1450     1943.99    145.2873   145.6481    0.8348     0.0017      0.0    
   1500     2011.21    146.2823   145.9304    0.8248     0.0016      0.0    
   1550     2078.35    145.8927   146.006     0.7404     0.0014      0.0    
   1600     2145.14    145.5276   145.6896    0.7946     0.0013      0.0    
   1650     2213.01    145.4835   145.5036    0.8819     0.0013      0.0    
   1700     2279.87    146.3303   146.0082    0.7462     0.0012      0.0    
   1750      2346.4    145.4049   145.6708    0.8904     0.0011      0.0    
   1800      2414.4    145.7394   145.6549    0.8954     0.0011      0.0    
   1850      2481.6    145.5963   145.5482    0.6436     0.0011      0.0    
   1900     2548.67    145.7656   146.0262    0.8149     0.001       0.0    
   1950     2616.17    145.4425   145.4996    0.9897     0.0009      0.0    
   2000     2683.32    145.764    145.8955    0.8651     0.0009      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       60.45     288.3825   286.8047    2.6518     4.4652      0.0    
   100       128.2     188.5561   187.2665    2.629      0.3434      0.0    
   150       195.99    163.4831   162.526     1.3255     0.1479      0.0    
   200       263.53    153.1011   152.8583    1.2042     0.067       0.0    
   250       331.14    149.8225   149.4933    0.8838     0.0519      0.0    
   300       399.35    148.0264   147.855     0.9741     0.0285      0.0    
   350       466.51    147.2058   146.8563    0.975      0.0204      0.0    
   400       533.66    146.2515   146.3699    0.9534     0.0171      0.0    
   450       601.0     146.7455   146.509     1.365      0.0136      0.0    
   500       669.7     145.8123   145.7819    1.4047     0.0101      0.0    
   550       736.88    146.0932   145.8641    0.9686     0.0088      0.0    
   600       804.55    146.5834   146.4727    1.1483     0.0083      0.0    
   650       872.27    145.7397   145.6546    1.2036     0.0063      0.0    
   700       939.34    145.5267   145.5109    0.9149     0.0063      0.0    
   750      1007.73    145.2779   145.3591    0.9616     0.0054      0.0    
   800      1076.45    145.1846   145.0753    0.7703     0.0048      0.0    
   850      1143.93    145.3629   145.273     1.0512     0.0041      0.0    
   900      1210.84    145.0031   145.2199    0.7374     0.0037      0.0    
   950      1278.64    145.5366   145.3793    0.7211     0.0035      0.0    
   1000     1345.88    145.4518   145.6125    0.6759     0.0029      0.0    
   1050     1414.08    145.1455   145.3908    0.6842     0.0031      0.0    
   1100     1481.68    145.5911   145.5544    0.713      0.0023      0.0    
   1150     1549.08    144.7547   144.9084    0.7851     0.0023      0.0    
   1200     1616.35    145.2149   145.2569    0.6464     0.0022      0.0    
   1250     1684.21    145.3346   145.3866    0.6041     0.002       0.0    
   1300     1752.06    145.7476   145.6592    0.5678     0.0019      0.0    
   1350     1818.77    145.2276   145.2567    0.697      0.0018      0.0    
   1400     1886.33    144.9945   145.181     0.5465     0.0018      0.0    
   1450      1953.9    145.2511   145.2403    0.6598     0.0015      0.0    
   1500     2020.81    145.2106   145.2504    0.9212     0.0014      0.0    
   1550     2087.84    145.3643   145.2142    0.8702     0.0015      0.0    
   1600     2155.86    145.5283   145.7135    0.9541     0.0012      0.0    
   1650     2224.05    145.4822   145.3846    0.8182     0.0012      0.0    
   1700     2290.81    145.9972   146.0712    0.9944     0.0011      0.0    
   1750     2358.63    145.9109   145.7099    1.2959     0.0012      0.0    
   1800     2426.48    144.9157   144.9376    0.7535     0.0011      0.0    
   1850     2494.08    145.577    145.5778    0.6315     0.0009      0.0    
   1900     2561.59    145.2869   145.2985    0.838      0.0009      0.0    
   1950      2629.2    145.4368   145.4449    0.8519     0.0009      0.0    
   2000     2696.04    145.4703   145.4139    1.0701     0.0008      0.0    
