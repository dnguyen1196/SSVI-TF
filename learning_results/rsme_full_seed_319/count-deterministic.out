Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  3.8867568969726562
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       75.84      4.2449     4.2785     0.3162     4.4199   128892.15  104489.81  
   100       285.19     4.2405     4.2741     0.3126     0.2422   127982.77  103750.97  
   150       494.74     4.2244     4.2577     0.3036     0.0488   124989.37  101315.24  
   200       703.02     4.1857     4.2186     0.294      0.0246   118840.61   96310.63  
   250       911.82     4.1109     4.1439     0.2861     0.0254   109165.93   88457.28  
   300      1120.29     3.9889      4.02      0.2816     0.0217    96900.02   78501.32  
   350      1329.71     3.8103     3.8407     0.2756     0.0186    83533.04   67658.51  
   400      1538.62     3.5827     3.6118     0.2601     0.016     70840.17   57375.37  
   450      1747.66     3.3258     3.3537     0.2535     0.0132    59976.31   48577.31  
   500      1956.62     3.0659     3.0914     0.2183     0.0106    51553.07   41739.66  
   550      2165.24     2.8134     2.8359     0.2344     0.011     45138.62   36539.59  
   600      2374.31     2.5881     2.6076      0.18      0.0128    40568.12   32823.67  
   650      2583.19     2.3866     2.4025     0.1935     0.0115    37237.34   30118.68  
   700      2792.19     2.2067     2.2204     0.1596     0.0127    34773.86   28116.7   
   750      3001.04     2.049      2.0604     0.1219     0.0121    32961.44   26649.05  
   800      3210.36     1.9134     1.9238     0.1754     0.0115    31630.78   25561.06  
   850      3418.81     1.7961     1.8022     0.2037     0.011     30608.13   24725.43  
   900       3627.1     1.6885     1.6944     0.2577     0.0106    29764.1    24048.69  
   950      3836.47     1.596      1.5997     0.1183     0.0105    29130.97   23534.59  
   1000     4045.71     1.5152     1.5165     0.1882      0.01     28613.88   23108.51  
   1050     4254.63     1.4437     1.4453     0.1935     0.0099    28199.49   22770.4   
   1100      4462.6     1.3825     1.3832     0.1018     0.0096    27871.19   22506.23  
   1150      4670.3     1.3295     1.3288     0.0783     0.0094    27615.88   22300.15  
   1200     4879.02     1.2848     1.2813     0.0912     0.0091    27401.89   22126.47  
   1250     5088.14     1.2436     1.2401     0.1109     0.0089    27235.95   21984.57  
   1300     5296.38     1.2102     1.2053     0.1946     0.0088    27095.0    21877.53  
   1350     5504.49     1.1793     1.1746     0.1593     0.0082    26976.58   21781.31  
   1400     5713.83     1.1532     1.1469     0.1562     0.008     26878.25   21692.06  
   1450     5922.47     1.1308     1.1219     0.1396     0.0076    26801.19   21630.03  
   1500     6131.06     1.1098     1.0995     0.1731     0.0075    26700.74   21556.72  
   1550     6339.95     1.0914     1.0837     0.1178     0.0072    26647.77   21516.3   
   1600     6548.77     1.0778     1.0676     0.2051     0.0069    26598.8    21471.77  
   1650     6757.61     1.0637     1.0526     0.1671     0.0067    26547.78   21432.19  
   1700     6966.05     1.0504     1.0383     0.1405     0.0065    26509.43   21395.16  
   1750      7175.4     1.0397     1.0284     0.1053     0.0062    26473.93   21372.08  
   1800     7384.23     1.0304     1.0191     0.112      0.0061    26465.44   21355.08  
   1850     7592.99     1.0238     1.0104     0.1088     0.0058    26437.08   21334.1   
   1900     7801.64     1.0151     1.0033     0.1269     0.0057    26402.23   21314.17  
   1950     8009.83     1.0073     0.9956     0.0847     0.0056    26380.25   21293.7   
   2000     8217.15     1.0019     0.9874     0.1179     0.0053    26376.06   21288.24  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       75.69      4.2003     4.2211     0.0613     3.6181    87716.97   141765.6  
   100       343.21     4.2344     4.2574     0.0324     0.8456   110757.94  179054.73  
   150       610.33     4.2393     4.2623     0.0249     0.3995   117928.76  190606.73  
   200       875.46     4.2418     4.2642     0.0259     0.2464   121139.45  195777.17  
   250      1140.52     4.2426     4.2651     0.0263     0.1786   122915.38  198636.79  
   300      1405.56     4.2428     4.2656     0.0279     0.1344   124044.61  200453.14  
   350       1670.6     4.2433     4.2658     0.0273     0.1062   124809.85  201684.72  
   400      1935.24     4.2436     4.2661     0.0313     0.0879   125366.82  202580.39  
   450      2200.04     4.2438     4.2663      0.03      0.0723   125786.89  203256.06  
   500      2465.85     4.2438     4.2665     0.0297     0.0633   126115.06  203783.81  
   550      2730.71     4.2437     4.2666     0.0306     0.0547   126377.26  204205.31  
   600      2996.39     4.2441     4.2666     0.0343     0.0487   126590.83  204548.64  
   650      3262.54     4.2441     4.2666     0.0325     0.0425   126767.28  204832.31  
   700      3528.51     4.2441     4.2668     0.0316     0.0386   126912.68  205065.98  
   750      3784.79     4.244      4.2667     0.0312     0.0349   127034.49  205261.72  
   800      4047.25     4.2439     4.2669     0.0347     0.0322   127135.42  205423.81  
   850      4311.07     4.244      4.2667     0.0339     0.029    127218.44   205557.1  
   900      4575.48     4.2439     4.2666     0.0366     0.0265    127285.6  205664.84  
   950      4840.05     4.2439     4.2667     0.0349     0.0243   127337.29  205747.69  
   1000     5104.63     4.2438     4.2665     0.0388     0.0227   127373.79  205806.11  
   1050     5369.21     4.2437     4.2663     0.0432     0.0214   127393.96  205838.12  
   1100      5633.6     4.2435     4.2661     0.0467     0.0197   127398.18  205844.45  
   1150     5898.28     4.2432     4.2658     0.0438     0.0185   127381.47  205817.03  
   1200     6162.65     4.2428     4.2655     0.0436     0.0171    127342.1  205753.03  
   1250     6427.22     4.2423     4.2651     0.0496     0.0161   127273.17  205641.07  
   1300     6690.72     4.2414     4.2643     0.0524     0.0155    127169.5  205473.11  
   1350     6956.16     4.2408     4.2633     0.056      0.0145   127014.85   205222.8  
   1400     7221.93     4.2393     4.2622     0.0598     0.0137   126794.28  204865.85  
   1450     7487.79     4.2377     4.2603     0.0677     0.0129   126488.63   204371.4  
   1500     7752.75     4.2353     4.2579     0.0761     0.0125   126059.16   203676.7  
   1550     8016.92     4.232      4.2546     0.0878     0.012    125453.65   202698.2  
   1600      8279.8     4.227      4.2495     0.0935     0.0118   124580.07  201284.06  
   1650     8542.44     4.2194     4.2421     0.1073     0.0117   123317.89  199239.61  
   1700     8805.08     4.2081     4.2308     0.1181     0.0114   121470.47  196248.48  
   1750     9068.17     4.191      4.2134     0.1344     0.0118   118787.56  191906.62  
   1800     9331.67     4.1651     4.187      0.1705     0.0127   114998.91  185778.11  
   1850     9595.96     4.1261     4.1478     0.1748     0.0141   109881.12  177516.14  
   1900     9861.35     4.0682     4.0904     0.1822     0.0161   103261.66  166821.26  
   1950     10127.21    3.9875     4.0092     0.1988     0.019     95363.11  154075.78  
   2000     10392.65    3.8804     3.9013     0.1932     0.0201    86694.53  140078.98  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       74.28      4.1784     4.1933     0.0613     3.4553    80192.79  193725.18  
   100       396.48     4.2282     4.244      0.0339     0.9198   104473.11  252399.35  
   150       717.74     4.2373     4.2525     0.0295     0.4199   113998.13  275332.43  
   200      1038.49     4.2405     4.2552     0.0256     0.268    118422.31  285972.86  
   250      1360.42     4.2416     4.2567     0.0222     0.1897   120901.14  291928.24  
   300      1685.55     4.2422     4.2576     0.0197     0.1439   122465.34   295685.8  
   350      2012.91     4.2428     4.258      0.0249     0.1153   123535.84  298256.26  
   400      2337.55     4.2432     4.2582     0.0228     0.0961   124308.55  300111.15  
   450       2661.0     4.2433     4.2586     0.0243     0.0789   124893.69  301515.87  
   500      2983.43     4.2437     4.2586     0.0229     0.0674   125352.06  302615.81  
   550      3303.83     4.2437     4.259      0.0228     0.0588   125718.63  303495.93  
   600      3625.23     4.2437     4.2589     0.0233     0.0523   126019.12  304217.02  
   650      3948.11     4.244      4.259      0.0241     0.0458   126268.12  304814.53  
   700      4271.44     4.244      4.2593     0.0231     0.0419   126478.66   305319.6  
   750      4593.66     4.244      4.2593     0.0255     0.0374    126659.0  305752.15  
   800      4913.36     4.244      4.2593     0.0236     0.0343   126814.48   306125.2  
   850      5235.13     4.2442     4.2593     0.0256     0.0315   126949.22  306448.56  
   900      5557.77     4.2441     4.2594     0.025      0.0289   127067.45  306732.29  
   950      5882.33     4.2442     4.2594     0.0247     0.0268   127170.93   306980.5  
   1000     6204.96     4.2442     4.2594     0.0236     0.025     127261.8  307198.47  
   1050     6527.33     4.2442     4.2593     0.0273     0.0235   127341.72  307390.21  
   1100     6848.95     4.2442     4.2595     0.0264     0.0217    127411.3  307557.13  
   1150      7171.8     4.2442     4.2595     0.0279     0.0206   127472.92  307704.82  
   1200     7493.26     4.244      4.2594     0.0287     0.0194   127525.46   307830.7  
   1250     7815.81     4.2443     4.2593     0.0251     0.0183   127569.37  307935.74  
   1300     8138.34     4.2442     4.2593     0.0297     0.0173   127605.89   308023.1  
   1350     8460.55     4.244      4.2592     0.0276     0.0163   127634.45  308091.38  
   1400     8783.28     4.2439     4.2592     0.0353     0.0153   127653.99  308137.93  
   1450      9105.3     4.2438     4.259      0.0308     0.0149   127663.34  308159.92  
   1500     9425.52     4.2436     4.2589     0.0312     0.014    127660.85   308153.4  
   1550     9746.32     4.2434     4.2586     0.0359     0.0133   127644.56  308113.59  
   1600     10066.55    4.2432     4.2583     0.0362     0.0127   127613.07  308037.02  
   1650     10387.45    4.2427     4.2579     0.0361     0.0123   127559.38  307907.27  
   1700     10708.4     4.2422     4.2574     0.0405     0.0118   127476.31  307706.45  
   1750     11028.24    4.2414     4.2566     0.0537     0.0114   127353.47  307409.82  
   1800     11351.17    4.2405     4.2556     0.0536     0.011    127179.01  306988.82  
   1850     11676.45    4.2391     4.2542     0.0552     0.0106   126920.85  306366.01  
   1900     11996.74    4.2368     4.2522     0.0612     0.0105   126545.89  305460.89  
   1950     12318.03    4.2338     4.249      0.0804     0.0101    125993.4  304129.55  
   2000     12638.34    4.2289     4.2441     0.0828     0.0102   125144.05  302082.75  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       74.74      4.1687     4.1817     0.0931     3.3584    77436.83  249361.52  
   100       453.09     4.2254     4.2385     0.0513     1.0341   101683.41  327433.28  
   150       830.38     4.2362     4.2491     0.0345     0.4199   112120.71  360957.93  
   200      1208.62     4.2392     4.2523     0.0251     0.2792   117084.76  376875.91  
   250      1589.05     4.2409     4.254      0.0234     0.1948   119896.55  385889.75  
   300      1967.82     4.2415     4.2547     0.0258     0.1507   121674.27  391590.24  
   350      2349.37     4.2425     4.2554     0.0234     0.1175   122893.14  395496.19  
   400      2727.55     4.2429     4.2559     0.0229     0.0973   123777.15  398330.42  
   450       3105.6     4.243      4.2561     0.0234     0.0816   124447.09  400477.24  
   500      3484.98     4.2433     4.2565     0.0229     0.0707    124968.6  402148.72  
   550      3864.09     4.2435     4.2564     0.0216     0.0609   125386.12  403486.08  
   600      4244.92     4.2437     4.2567     0.0238     0.0539   125726.46  404576.17  
   650      4626.12     4.2439     4.2567     0.0223     0.0475   126009.88  405483.86  
   700      5006.66     4.2438     4.2569     0.0237     0.0423   126249.51  406251.55  
   750      5386.47     4.2441     4.2569     0.0266     0.0388    126454.1  406906.83  
   800      5767.22     4.244      4.2569     0.0223     0.0352   126631.33   407474.6  
   850      6147.63     4.2439     4.257      0.0241     0.0327   126785.25  407967.47  
   900      6528.22     4.2443     4.2571     0.0236     0.0298   126920.66  408401.04  
   950      6909.96     4.2442     4.2571     0.0241     0.0276   127039.38  408781.16  
   1000     7291.24     4.2443     4.2572     0.0239     0.0258   127143.94  409115.82  
   1050     7671.33     4.2443     4.2572     0.0227     0.0242   127237.16  409414.08  
   1100     8052.69     4.2441     4.2572     0.024      0.0227   127319.05  409675.94  
   1150     8433.49     4.2442     4.2572     0.0246     0.0213   127391.53  409907.98  
   1200     8815.17     4.2443     4.2571     0.0238     0.0198    127455.5  410112.58  
   1250     9196.63     4.2443     4.2572     0.0266     0.019     127510.9  410289.71  
   1300     9577.62     4.2442     4.2572     0.0271     0.0179   127557.37  410438.23  
   1350     9958.43     4.2442     4.2571     0.0258     0.0171   127596.63  410563.35  
   1400     10339.87    4.244      4.257      0.027      0.0162   127626.93  410659.71  
   1450     10719.81    4.2439     4.2569     0.0315     0.0152   127647.06  410723.62  
   1500     11099.86    4.2438     4.2568     0.033      0.0148   127656.61  410753.61  
   1550     11479.26    4.2436     4.2566     0.032      0.0142   127654.26  410745.23  
   1600     11858.93    4.2435     4.2563     0.0381     0.0133    127636.8  410687.88  
   1650     12238.78    4.2429     4.256      0.0381     0.0129   127597.56  410561.06  
   1700     12618.99    4.2427     4.2556     0.0417     0.0124   127534.12  410355.89  
   1750     12998.4     4.242      4.255      0.0466     0.012    127435.02  410035.45  
   1800     13377.12    4.2411     4.254      0.0503     0.0116    127284.9  409551.25  
   1850     13755.07    4.2397     4.2527     0.0539     0.0115   127061.17  408830.78  
   1900     14131.4     4.238      4.2508     0.0738     0.0111   126715.26  407716.31  
   1950     14507.02    4.235      4.2479     0.0718     0.0111   126185.67  406010.89  
   2000     14882.55    4.2303     4.2432     0.0899     0.011    125344.59  403303.12  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       76.66      4.163      4.1808     0.0904     3.3188    76045.4   306546.94  
   100       515.48     4.2239     4.2432     0.0536     0.9615   100259.39  404260.01  
   150       952.91     4.2344     4.2543     0.0365     0.4378   111114.71  448001.31  
   200      1391.48     4.2392     4.2583     0.0303     0.2847   116401.92  469291.61  
   250      1829.65     4.2408     4.2598     0.0279     0.1991    119376.2  481270.91  
   300      2269.67     4.2417     4.2608     0.0273     0.1534   121273.87  488911.73  
   350       2709.5     4.2428     4.2616     0.0238     0.1201   122569.58  494126.73  
   400      3147.98     4.2431     4.2621     0.025      0.0984   123506.79  497899.38  
   450      3585.27     4.2431     4.2624     0.0211     0.0829   124213.21  500742.32  
   500      4023.38     4.2434     4.2626     0.0219     0.0727   124765.98  502966.39  
   550       4460.6     4.2436     4.2627     0.0228     0.0621   125210.05  504753.39  
   600      4899.76     4.2438     4.2628     0.0229     0.0547    125573.5  506216.02  
   650      5338.05     4.2437     4.2629     0.0235     0.0487   125877.04  507437.11  
   700      5775.63     4.2438     4.2631     0.0255     0.0435   126132.51  508464.82  
   750      6211.83     4.2441     4.2632     0.0215     0.0401   126351.54  509345.54  
   800      6648.58     4.2442     4.2633     0.0212     0.0358   126540.48  510105.22  
   850      7085.87     4.2442     4.2634     0.0229     0.0331   126705.32  510768.11  
   900      7523.24     4.2442     4.2634     0.023      0.031    126849.63  511348.45  
   950      7960.32     4.2441     4.2634     0.0224     0.0283   126976.94  511860.51  
   1000     8398.97     4.2443     4.2635     0.0218     0.0266    127090.1  512315.42  
   1050     8837.52     4.2443     4.2635     0.0217     0.0245   127189.79   512716.1  
   1100     9276.07     4.2442     4.2635     0.0232     0.0231    127278.8  513073.92  
   1150     9716.65     4.2442     4.2635     0.0227     0.0217   127358.91  513395.84  
   1200     10154.07    4.2444     4.2636     0.0245     0.0205   127429.68  513679.98  
   1250     10591.6     4.2443     4.2635     0.0238     0.0193   127492.14  513930.97  
   1300     11030.3     4.2443     4.2635     0.0278     0.0185   127546.08   514147.5  
   1350     11468.45    4.2444     4.2635     0.0246     0.0174   127592.48  514333.85  
   1400     11906.11    4.2442     4.2634     0.0287     0.0166   127630.86   514487.8  
   1450     12345.55    4.2441     4.2633     0.0246     0.016    127661.36  514610.18  
   1500     12785.29    4.244      4.2632     0.0291     0.0151   127682.32  514693.88  
   1550     13225.1     4.2439     4.2631     0.0352     0.0145   127692.14  514732.74  
   1600     13665.07    4.2438     4.2629     0.0324     0.0137   127689.51   514721.4  
   1650     14104.96    4.2436     4.2627     0.0319     0.0133   127671.15   514646.6  
   1700     14543.64    4.2432     4.2623     0.0376     0.0127   127633.96  514495.73  
   1750     14981.68    4.2428     4.262      0.0377     0.0123   127567.94  514228.56  
   1800     15418.09    4.2421     4.2613     0.0445     0.012    127464.95  513812.42  
   1850     15856.33    4.2412     4.2604     0.0576     0.0118   127304.65  513164.97  
   1900     16294.98    4.2398     4.259      0.0606     0.0114   127056.75  512162.98  
   1950     16732.03    4.2378     4.2567     0.0646     0.0113   126662.93  510571.12  
   2000     17170.82    4.2341     4.2534     0.0832     0.0112    126047.3  508086.02  
