Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  2.695329189300537
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       72.95      1.2336     1.1834     3.1623     3.9773     0.001   
   100       250.79     0.7709     0.7386     3.1362     0.2472     0.0001  
   150       426.9      0.7595     0.7211     2.6266     0.1663     0.0001  
   200       602.1      0.7134     0.6542     2.2909     0.1253      0.0    
   250       777.9      0.6785     0.6163     2.1544     0.1054      0.0    
   300       953.56     0.6632     0.5923     2.1384     0.0941      0.0    
   350      1129.29     0.6478     0.5664     2.2407     0.0865      0.0    
   400      1304.88     0.6409     0.557      1.9852     0.0617      0.0    
   450      1480.75     0.6579     0.5541     1.9473     0.0628      0.0    
   500      1657.17     0.6157     0.5282     1.9064     0.0841      0.0    
   550      1833.84     0.6285     0.5064     2.1532     0.0504      0.0    
   600      2010.58     0.6103     0.4976     1.9165      0.06       0.0    
   650      2187.79     0.6099     0.4876     1.7488     0.0465      0.0    
   700      2364.28     0.6106     0.4887     1.7876     0.0513      0.0    
   750      2540.77     0.5995     0.4796     1.8067     0.0448      0.0    
   800      2717.35     0.6103     0.4695     1.6867     0.041       0.0    
   850      2893.33     0.5922     0.4617     1.9761     0.0486      0.0    
   900      3069.06     0.5864     0.4494     2.0852     0.0381      0.0    
   950      3244.12     0.5903     0.4409     1.969      0.0363      0.0    
   1000     3419.01     0.5985     0.4532     1.8152     0.0374      0.0    
   1050     3594.79     0.6032     0.463      2.0426     0.037       0.0    
   1100      3769.9     0.5946     0.447      2.0608     0.034       0.0    
   1150     3944.32     0.6016     0.4762     2.2715     0.0372      0.0    
   1200     4119.18     0.6031     0.4752     2.1099     0.035       0.0    
   1250     4293.95     0.5973     0.4835     2.2912     0.0373      0.0    
   1300      4468.7     0.6057     0.4686     2.2638     0.0327      0.0    
   1350     4643.07     0.5865     0.4548     2.2454     0.0346      0.0    
   1400     4817.89     0.6103     0.494      2.3469     0.0346      0.0    
   1450     4991.64     0.5961     0.4831     2.8852     0.0379      0.0    
   1500     5165.83     0.6065     0.4794     2.4096     0.0362      0.0    
   1550     5339.12     0.6025     0.5099     2.7933     0.0354      0.0    
   1600     5512.54     0.6035     0.4808     2.6473     0.0342      0.0    
   1650     5686.28     0.6008     0.5018     2.4346     0.0395      0.0    
   1700     5859.45     0.6024     0.494      2.7765     0.0424      0.0    
   1750     6031.99     0.5934     0.4905     2.5914     0.0447      0.0    
   1800     6204.01     0.593      0.466      2.3521     0.0401      0.0    
   1850     6375.88     0.5876     0.4864     2.486      0.0411      0.0    
   1900     6547.48     0.595      0.4783     2.4702     0.0468      0.0    
   1950     6717.27     0.5842     0.4752     2.8569     0.0476      0.0    
   2000     6887.74     0.5813     0.4639     2.6213     0.0399      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       73.08      1.4116     1.413      0.2736     1.2168      0.0    
   100       297.4      1.412      1.4216     0.3709     0.7927      0.0    
   150       520.69     1.4186     1.4132     0.2368     0.4999     0.0001  
   200       744.78     1.4162     1.4105     0.1664     0.3174     0.0001  
   250       968.4      1.4155     1.4164     0.1068     0.212      0.0001  
   300      1191.21     1.421      1.4088     0.1097     0.1524     0.0001  
   350      1415.22     1.4148     1.4151     0.1079     0.1155     0.0001  
   400      1639.29     1.4078     1.4204     0.0962     0.091      0.0001  
   450      1863.51     1.4116     1.4167     0.1697     0.0727      0.0    
   500      2087.44     1.3987     1.3957     0.2099      0.06       0.0    
   550      2311.88     1.3675     1.3723     0.2594     0.0517      0.0    
   600       2536.6     1.2727     1.2796     0.4248     0.0456      0.0    
   650      2760.35     0.9436     0.9364     0.8187     0.0496      0.0    
   700      2981.88     0.7459     0.7366     0.9551     0.0876      0.0    
   750      3199.29     0.7192     0.712      0.9815     0.098       0.0    
   800      3417.01     0.731      0.7152     0.7206     0.0673      0.0    
   850      3633.75     0.7228     0.7126     0.5931     0.0358      0.0    
   900      3849.33     0.7312     0.7137     0.7258     0.026       0.0    
   950       4065.6     0.7381     0.7283     0.7032     0.0239      0.0    
   1000     4280.44     0.7307     0.7183     0.6365     0.0198      0.0    
   1050     4494.34     0.7353     0.7245     0.7119     0.0177      0.0    
   1100     4709.41     0.7174     0.7096     0.6135     0.0165      0.0    
   1150      4922.9     0.7269     0.716      0.6042     0.0161      0.0    
   1200      5137.4     0.719      0.7112     0.5579     0.0146      0.0    
   1250     5351.65     0.7359     0.7305     0.536      0.0138      0.0    
   1300     5565.34     0.7352     0.7159     0.5094     0.0125      0.0    
   1350     5781.56     0.7194     0.7129     0.5072     0.013       0.0    
   1400     5996.98     0.7271     0.7082     0.5437     0.0121      0.0    
   1450     6212.16     0.7132     0.6994     0.5262     0.0109      0.0    
   1500     6427.94     0.7188     0.704      0.7337     0.0123      0.0    
   1550     6642.24     0.7115     0.6963     0.5125     0.0111      0.0    
   1600     6857.23     0.7161     0.7046     0.3616     0.0097      0.0    
   1650     7073.14     0.7139     0.6907     0.3359     0.0094      0.0    
   1700     7289.07     0.7098     0.694      0.4095     0.0107      0.0    
   1750     7504.22     0.7063     0.6856     0.4888     0.0091      0.0    
   1800     7720.31     0.6986     0.6823     0.5294     0.0098      0.0    
   1850      7937.1     0.7026     0.6807     0.4639     0.0089      0.0    
   1900      8155.2     0.7052     0.6821     0.5753     0.0085      0.0    
   1950     8371.77     0.6962     0.6699     0.4701     0.0085      0.0    
   2000     8588.86     0.6956     0.676      0.5471     0.009       0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       73.37      1.4041     1.4146     0.189      0.9922      0.0    
   100       338.55     1.4079     1.4168     0.2505     0.7126      0.0    
   150       604.49     1.4085     1.4159     0.1546     0.5021      0.0    
   200       871.07     1.4121     1.413      0.1478     0.349       0.0    
   250      1135.67     1.4076     1.4102     0.1151     0.2449      0.0    
   300      1402.36     1.4132     1.4158     0.1241     0.1771      0.0    
   350      1668.69     1.4128     1.4173     0.0855     0.1351      0.0    
   400      1935.36     1.4219     1.4175     0.1042     0.106       0.0    
   450       2202.1     1.4139     1.4118     0.1037     0.0867      0.0    
   500      2467.54     1.4062     1.4106     0.1032     0.0719      0.0    
   550      2732.63     1.4118     1.4082     0.1105      0.06       0.0    
   600      2998.76     1.3862     1.3826     0.2066     0.0518      0.0    
   650      3266.34     1.3162     1.309      0.2938     0.046       0.0    
   700      3534.36     0.9154     0.9166     0.648      0.052       0.0    
   750      3798.97      0.73      0.7222     0.8815     0.1009      0.0    
   800      4059.85     0.718      0.7071     0.8235     0.097       0.0    
   850      4318.51     0.7314     0.7183     0.5952     0.0537      0.0    
   900      4575.89     0.7451     0.7356     0.7214     0.0357      0.0    
   950      4835.43     0.7303     0.721      0.6986     0.0279      0.0    
   1000     5095.01     0.7231     0.7111     0.5415     0.0253      0.0    
   1050     5354.86     0.7298     0.721      0.6338     0.0219      0.0    
   1100     5614.08     0.7181     0.7073     0.6721     0.0212      0.0    
   1150     5871.35     0.7258     0.7097     0.5858     0.0186      0.0    
   1200     6128.15     0.7251     0.7155     0.5882     0.0186      0.0    
   1250     6383.79     0.7167     0.7048     0.3685     0.0169      0.0    
   1300     6639.62     0.7258     0.7155     0.4845     0.0152      0.0    
   1350     6896.91     0.732      0.7221     0.5919     0.015       0.0    
   1400     7154.61     0.7126     0.7029     0.4749     0.0147      0.0    
   1450     7411.45     0.7348     0.7231     0.5151     0.0144      0.0    
   1500     7669.42     0.718      0.705      0.5905     0.0134      0.0    
   1550     7926.65     0.719      0.7031     0.403      0.0124      0.0    
   1600     8181.65     0.7269      0.71      0.6224     0.0126      0.0    
   1650     8438.23     0.7118     0.6983     0.4347     0.0123      0.0    
   1700     8694.38     0.7068     0.7036     0.4259     0.012       0.0    
   1750     8951.32     0.7036     0.6945     0.4712     0.0121      0.0    
   1800     9209.63     0.6967     0.6833     0.4043     0.0096      0.0    
   1850     9468.09     0.7028     0.6907     0.4944      0.01       0.0    
   1900     9725.11     0.6818     0.6702     0.4361     0.0107      0.0    
   1950      9983.1     0.6897     0.6775     0.456      0.0099      0.0    
   2000     10241.52    0.6788     0.6703     0.3181     0.0093      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       72.64      1.4208     1.417      0.255      0.9007      0.0    
   100       384.0      1.4154     1.4172     0.2095     0.6768      0.0    
   150       695.58     1.4238     1.4183     0.188      0.497       0.0    
   200      1006.24     1.4058     1.4114     0.2008     0.3577      0.0    
   250      1317.11     1.4165     1.4173     0.1101     0.2586      0.0    
   300      1624.89     1.4124     1.4152     0.1112     0.1896      0.0    
   350      1920.45     1.4148     1.4127     0.1176     0.1441      0.0    
   400      2213.93     1.4173     1.4157     0.1157     0.1137      0.0    
   450      2507.62     1.4129     1.4165     0.1002     0.0909      0.0    
   500      2800.65     1.4068     1.4118     0.0915     0.0753      0.0    
   550      3101.72     1.4126     1.4084     0.0871     0.0638      0.0    
   600      3399.88     1.4085     1.4002     0.1077     0.0543      0.0    
   650      3694.23     1.4001     1.3931     0.122      0.0482      0.0    
   700      3987.53     1.3652     1.3628     0.1748     0.0425      0.0    
   750      4307.99     1.2404     1.2328     0.3519     0.039       0.0    
   800      4622.68     0.816      0.8089     0.8172     0.0543      0.0    
   850      4927.66     0.7234     0.709      0.9122     0.1108      0.0    
   900      5226.61     0.724      0.7145     0.832      0.0972      0.0    
   950      5525.52     0.7251     0.7146     0.5308     0.0482      0.0    
   1000     5824.75     0.7331     0.7286     0.8166     0.0342      0.0    
   1050     6123.33     0.7306     0.7174     0.7962     0.0312      0.0    
   1100     6423.71     0.7291     0.7153     0.5828     0.0221      0.0    
   1150     6721.34     0.7339     0.7232     0.5729     0.0198      0.0    
   1200     7020.05     0.728      0.7112     0.6834     0.0194      0.0    
   1250     7318.61     0.7197     0.7074     0.5942     0.0179      0.0    
   1300     7617.32     0.7262     0.713      0.4891     0.0163      0.0    
   1350     7917.34     0.7209     0.7086     0.4447     0.016       0.0    
   1400     8217.34     0.7343     0.7252     0.4886     0.0147      0.0    
   1450     8518.17     0.7172     0.701      0.5754     0.0151      0.0    
   1500     8819.34     0.7115     0.6974     0.4769     0.0137      0.0    
   1550     9119.77     0.7238     0.7076     0.4218     0.013       0.0    
   1600     9420.76     0.7171     0.7022     0.494      0.0136      0.0    
   1650     9723.47     0.7086     0.6935     0.5156     0.0125      0.0    
   1700     10025.67    0.7163     0.7007     0.5293     0.012       0.0    
   1750     10328.99    0.7028     0.6876     0.5473     0.0122      0.0    
   1800     10632.52    0.6977     0.6843     0.6314     0.0112      0.0    
   1850     10935.46    0.6996     0.6821     0.5243     0.0106      0.0    
   1900     11238.14    0.694      0.6756     0.4211     0.0101      0.0    
   1950     11541.77    0.7055     0.6883     0.4601     0.0092      0.0    
   2000     11845.61    0.6998     0.6807     0.4463     0.0096      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       73.21      1.4084     1.4127     0.2824     0.8627      0.0    
   100       429.32     1.412      1.4185     0.2106     0.6564      0.0    
   150       783.94     1.4186     1.4132     0.1889     0.491       0.0    
   200      1140.53     1.4134     1.4175     0.1552     0.3614      0.0    
   250      1497.56     1.4103     1.4133     0.1571     0.2645      0.0    
   300      1852.14     1.4095     1.4125     0.1203     0.1972      0.0    
   350      2207.98     1.421      1.4163     0.1447     0.1499      0.0    
   400      2564.07     1.4113     1.4129     0.1286     0.1172      0.0    
   450      2919.12     1.4114     1.4137     0.0953     0.0947      0.0    
   500      3275.16     1.4108     1.4124     0.1094     0.0779      0.0    
   550      3630.32     1.4214     1.4101     0.0899     0.0658      0.0    
   600      3986.21     1.4106     1.4109     0.0909     0.0563      0.0    
   650      4343.28     1.4059     1.4063     0.1088     0.0488      0.0    
   700      4701.05     1.395      1.3985     0.1053     0.0431      0.0    
   750      5056.93     1.3835     1.3836     0.1345     0.0385      0.0    
   800      5412.74     1.3569     1.3438     0.1814     0.0352      0.0    
   850      5768.95     1.2436     1.2347     0.2939     0.033       0.0    
   900       6126.5     0.9055     0.9022     0.6078     0.0365      0.0    
   950      6484.82     0.7488     0.7432     1.0468     0.0886      0.0    
   1000     6836.72     0.7235     0.7099     1.1158     0.1025      0.0    
   1050     7187.32     0.7243     0.7071     0.7725     0.0685      0.0    
   1100     7539.93     0.724      0.7119     0.7993     0.0396      0.0    
   1150     7891.75     0.7296     0.7125     0.6109     0.0359      0.0    
   1200     8244.43     0.7197     0.7084     0.7322     0.0271      0.0    
   1250     8596.56     0.7217     0.7078     0.8969     0.0266      0.0    
   1300     8949.26     0.7196     0.7095     0.8639     0.0238      0.0    
   1350     9300.75     0.7254     0.7106     0.6736     0.0212      0.0    
   1400     9653.92     0.7172     0.706      0.7107     0.0183      0.0    
   1450     10005.93    0.7179     0.7042     0.5234     0.0179      0.0    
   1500     10358.3     0.7169     0.7044     0.5244     0.0156      0.0    
   1550     10711.11    0.7155     0.7012     0.3801     0.0149      0.0    
   1600     11061.91    0.7227     0.7087     0.4851     0.015       0.0    
   1650     11414.67    0.7115     0.6979     0.5562     0.0154      0.0    
   1700     11767.15    0.7086     0.695      0.4036     0.0127      0.0    
   1750     12120.41    0.6958     0.6861     0.4749     0.0122      0.0    
   1800     12473.48    0.7004     0.6873     0.5256     0.0124      0.0    
   1850     12826.89    0.6925     0.6896     0.6483     0.0111      0.0    
   1900     13180.29    0.6939     0.6792     0.5749     0.0109      0.0    
   1950     13533.58    0.6775     0.6686     0.458      0.0101      0.0    
   2000     13887.21    0.6816     0.669      0.6417     0.0127      0.0    
