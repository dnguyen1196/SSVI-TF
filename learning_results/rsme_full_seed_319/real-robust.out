Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  7.411949157714844
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       202.02    361.0716   362.617     3.1623     4.4721      0.0    
   100       410.14    191.3754   192.1535    3.1623     0.2068      0.0    
   150       618.87    167.6278   167.582     2.8845      0.0        0.0    
   200       827.86    160.2231   159.484     2.6523      0.0        0.0    
   250      1037.72    155.0211   154.3849    2.3185      0.0        0.0    
   300      1246.88    151.3789   151.1281    2.0788      0.0        0.0    
   350      1456.72    149.6503   149.328     1.6998      0.0        0.0    
   400      1665.55    147.8986   147.3924    1.3092      0.0        0.0    
   450      1874.57    146.9017   146.8251    1.1975      0.0        0.0    
   500      2085.61    147.0375   146.7422    1.1806      0.0        0.0    
   550      2294.79    147.352    146.9827    1.2067      0.0        0.0    
   600      2503.78    147.3521   146.9611    1.2732      0.0        0.0    
   650      2713.63    146.6537   146.6474    1.4006      0.0        0.0    
   700      2923.15    146.3807   145.9902    1.332       0.0        0.0    
   750      3132.71    146.4721   146.201     0.8164      0.0        0.0    
   800      3341.55    147.1585   146.6462    1.0483      0.0        0.0    
   850      3551.88    146.7284   146.5673    1.2278      0.0        0.0    
   900      3762.16    146.7389   146.4529    1.1919      0.0        0.0    
   950      3971.46    146.6276   146.4002    1.0322      0.0        0.0    
   1000     4181.06    146.256    145.7002    1.0394      0.0        0.0    
   1050     4390.15    145.7151   145.7061    0.9614      0.0        0.0    
   1100     4600.45    147.2506   146.5417    0.8785      0.0        0.0    
   1150     4810.63    146.543    146.3915    1.1631      0.0        0.0    
   1200     5019.81    146.5019   146.0626    1.0281      0.0        0.0    
   1250     5229.53    145.8385   145.4521    1.1164      0.0        0.0    
   1300     5440.12    146.1593   145.9264    1.0173      0.0        0.0    
   1350     5649.46    146.8611   146.1568    1.088       0.0        0.0    
   1400     5859.38    145.8995   145.6987    0.8789      0.0        0.0    
   1450     6069.38    145.8365   145.4777    0.7686      0.0        0.0    
   1500     6278.96    146.4155   146.0817    0.8638      0.0        0.0    
   1550     6487.53    146.4746   145.8128    0.9295      0.0        0.0    
   1600     6697.91    145.8275   145.614     0.9597      0.0        0.0    
   1650     6908.28    145.7297   145.6901    1.0713      0.0        0.0    
   1700     7117.06    145.8696   145.5965    0.946       0.0        0.0    
   1750     7326.87    146.3561   145.8105    0.7509      0.0        0.0    
   1800     7536.76    145.5031   145.404     0.7367      0.0        0.0    
   1850     7745.67    145.8201   145.3319    0.674       0.0        0.0    
   1900     7956.06    146.2479   145.8208    0.8181      0.0        0.0    
   1950     8166.31    145.9223   145.883     0.833       0.0        0.0    
   2000     8376.67    146.0731   145.5418    0.833       0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       204.1     303.2905   302.8966    3.0279     4.4721      0.0    
   100       414.74    177.3034   177.0036    3.0906      0.0        0.0    
   150       626.38    161.9109   160.9195    2.1479      0.0        0.0    
   200       836.9     154.5262   154.3104    2.3044      0.0        0.0    
   250      1047.57    152.2929   152.1152    2.1927      0.0        0.0    
   300      1258.61    151.5245   150.9239    2.0987      0.0        0.0    
   350      1468.79    150.3386   149.7194    1.7717      0.0        0.0    
   400      1679.87    147.8934   148.0226    1.5884      0.0        0.0    
   450      1890.13    146.8571   146.4136    1.2796      0.0        0.0    
   500      2101.61    147.3553   147.0004    1.2586      0.0        0.0    
   550      2312.28    147.2235   146.5252    1.0021      0.0        0.0    
   600      2523.53    147.2419   147.3557    1.3264      0.0        0.0    
   650      2733.95    147.1704   146.6675    1.6075      0.0        0.0    
   700      2945.32    146.4232   146.2442    1.6644      0.0        0.0    
   750      3156.36    146.8706   146.6139    1.4021      0.0        0.0    
   800      3366.62    146.3302   146.1239    0.9218      0.0        0.0    
   850      3578.31    147.149    146.6961    1.1996      0.0        0.0    
   900      3789.77    146.4909   146.4571    1.2237      0.0        0.0    
   950      4001.02    146.3968   146.1384    1.1887      0.0        0.0    
   1000     4211.41    146.2503   145.7725    1.0681      0.0        0.0    
   1050      4422.7    145.9493   145.7588    1.0213      0.0        0.0    
   1100     4633.15    146.7705   146.3047    0.8534      0.0        0.0    
   1150     4844.13    146.1808   145.6618    0.8519      0.0        0.0    
   1200     5055.46    145.8752   145.6215    0.9987      0.0        0.0    
   1250     5267.49    145.5376   145.4954    1.0011      0.0        0.0    
   1300      5478.2    145.9328   145.522     0.9672      0.0        0.0    
   1350     5689.44    146.1403   145.6187    0.7901      0.0        0.0    
   1400     5900.73    145.9013   145.5122    0.816       0.0        0.0    
   1450     6111.27    146.6447   146.0992    1.0589      0.0        0.0    
   1500     6322.46    146.3167   146.2476    1.2927      0.0        0.0    
   1550     6533.56    146.106    145.8293    0.9465      0.0        0.0    
   1600     6745.18    145.854    145.6082    1.0376      0.0        0.0    
   1650      6955.7    146.3422   146.0305    1.1872      0.0        0.0    
   1700     7166.83    146.4101   145.9072    0.9374      0.0        0.0    
   1750     7378.55    145.2698   144.9889    0.9978      0.0        0.0    
   1800     7590.34    145.3154   145.2388    0.9149      0.0        0.0    
   1850     7800.92    145.8139   145.6333    0.7366      0.0        0.0    
   1900     8012.25     145.85    145.783     1.019       0.0        0.0    
   1950     8222.53    145.9727   145.7185    0.8904      0.0        0.0    
   2000     8435.33    145.6774   145.6553    0.9769      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       202.64    290.2659   288.9922    2.8786     4.4721      0.0    
   100       413.99    175.4512   174.3703    2.6916      0.0        0.0    
   150       626.63    158.6565   157.7927    1.6801      0.0        0.0    
   200       838.58    151.124    150.5018    1.5706      0.0        0.0    
   250      1050.93    147.4749   147.2273    1.4964      0.0        0.0    
   300      1262.16    147.248    146.5481    1.1566      0.0        0.0    
   350       1473.9    146.6337   146.126     1.1165      0.0        0.0    
   400       1686.0    146.7932   146.2724    1.5835      0.0        0.0    
   450      1898.69    146.8662   146.3349    1.5065      0.0        0.0    
   500      2110.67    146.5413   146.0673     1.23       0.0        0.0    
   550      2322.89    146.401    146.1112    1.2964      0.0        0.0    
   600       2535.1    146.4639   146.2714    1.509       0.0        0.0    
   650      2746.07    146.3985   146.2098    1.5739      0.0        0.0    
   700      2957.93    146.5319   146.173     1.1311      0.0        0.0    
   750       3169.4    146.2044   145.5392    1.1997      0.0        0.0    
   800      3381.46    145.7345   145.6251    1.1205      0.0        0.0    
   850      3594.51    146.2571   145.721     0.9901      0.0        0.0    
   900      3807.14    145.955    145.4033    1.0126      0.0        0.0    
   950      4018.21    145.5426   145.4105    0.813       0.0        0.0    
   1000     4229.66    145.8629   145.473     0.9192      0.0        0.0    
   1050      4441.6    145.6279   145.3521    0.9228      0.0        0.0    
   1100     4653.62    145.9666   145.5592    1.0705      0.0        0.0    
   1150     4865.56    146.3277   146.0436    0.9075      0.0        0.0    
   1200     5077.73    145.5025   145.127     0.9519      0.0        0.0    
   1250     5288.84    145.8309   145.4139    0.7754      0.0        0.0    
   1300     5500.65    145.6484   145.4613    0.9466      0.0        0.0    
   1350     5711.82    145.6079   145.0547    0.9857      0.0        0.0    
   1400      5924.3    145.3898   145.2045    0.8877      0.0        0.0    
   1450     6135.89    146.1419   145.5983    1.0415      0.0        0.0    
   1500     6347.35    145.4684   145.2426    0.6994      0.0        0.0    
   1550     6558.79    145.858    145.2686    0.8715      0.0        0.0    
   1600     6770.35    146.2231   145.7079    0.8347      0.0        0.0    
   1650     6982.28    145.2771   145.2419    0.8139      0.0        0.0    
   1700     7194.41    145.8709   145.6087    0.7549      0.0        0.0    
   1750     7406.45    145.9965   145.5632    0.8688      0.0        0.0    
   1800     7618.09    145.396    144.9753    0.9094      0.0        0.0    
   1850     7829.95    145.5293   145.3984    0.8547      0.0        0.0    
   1900     8041.96    145.3851   144.9494    0.7355      0.0        0.0    
   1950     8254.45    145.0943   144.7074    0.6127      0.0        0.0    
   2000     8467.27    144.9845   144.9573    0.9066      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       204.62    284.5703   283.4564    2.755      4.4721      0.0    
   100       417.87    179.1812   178.1401    2.6435      0.0        0.0    
   150       632.05    159.5609   159.0838     2.01       0.0        0.0    
   200       845.02    151.9939   152.0084    1.2501      0.0        0.0    
   250      1058.95    148.7638   148.5162    1.0587      0.0        0.0    
   300       1272.3    146.6003   146.8226    0.9376      0.0        0.0    
   350      1485.48    146.7918   146.8648    0.9691      0.0        0.0    
   400       1700.2    145.9391   146.1405    0.8561      0.0        0.0    
   450      1914.63    145.8257   145.8412    1.1627      0.0        0.0    
   500       2130.3    146.2003   146.2723    1.0019      0.0        0.0    
   550      2344.62    146.2828   146.4958    1.2235      0.0        0.0    
   600      2558.42    146.1731   145.9854    1.0086      0.0        0.0    
   650      2772.37    145.9558   145.8674    0.8433      0.0        0.0    
   700      2986.07    145.7844   145.9183    1.2084      0.0        0.0    
   750      3200.63    145.9604   145.9404    1.2882      0.0        0.0    
   800      3414.22    145.2578   145.3743    0.7958      0.0        0.0    
   850      3627.57    146.3645   145.9295    0.9927      0.0        0.0    
   900      3841.54    145.8795   145.973     0.9956      0.0        0.0    
   950      4055.93    145.6981   145.6041    0.9936      0.0        0.0    
   1000     4269.64    145.6836   145.6343    1.2034      0.0        0.0    
   1050     4482.74    145.4112   145.5265    0.9362      0.0        0.0    
   1100     4696.39    145.7514   146.0486    0.9355      0.0        0.0    
   1150     4910.89    145.2682   145.6171    0.8808      0.0        0.0    
   1200     5124.53    144.9753   145.2622    0.9087      0.0        0.0    
   1250     5338.21    145.0325   144.991     0.7167      0.0        0.0    
   1300      5552.5    145.9947   146.068     0.8307      0.0        0.0    
   1350     5766.48    145.8489   146.1053    1.3649      0.0        0.0    
   1400     5980.79    145.7787   145.8319    1.1301      0.0        0.0    
   1450      6194.1    145.2528   145.3856    0.8547      0.0        0.0    
   1500     6408.29    145.6501   145.6706    0.8486      0.0        0.0    
   1550     6622.61    145.3236   145.618     0.8532      0.0        0.0    
   1600     6836.84    145.5887   145.4963    0.7098      0.0        0.0    
   1650     7050.82    145.1545   145.3537    0.5613      0.0        0.0    
   1700     7264.89    145.8908   145.5774    0.9026      0.0        0.0    
   1750     7479.13    144.9084    145.2      0.7337      0.0        0.0    
   1800     7692.11    145.5865   145.4972    0.7025      0.0        0.0    
   1850     7905.58    144.9129   145.1973    0.8388      0.0        0.0    
   1900      8120.6    145.3915   145.4726    0.9596      0.0        0.0    
   1950     8335.27    145.2528   145.5517    0.9099      0.0        0.0    
   2000     8548.38    144.8833   145.1974    0.9262      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       204.44    285.1432   284.0419    2.6398     4.4721      0.0    
   100       417.67    186.1255   184.6143    2.4241      0.0        0.0    
   150       631.91    162.6336   161.9969    1.6933      0.0        0.0    
   200       846.01    152.9247   152.3795    1.1979      0.0        0.0    
   250      1060.97    149.8185   149.3393    1.0048      0.0        0.0    
   300      1275.24     147.31    147.3515    1.2759      0.0        0.0    
   350      1489.54    146.4349   146.3623    1.0958      0.0        0.0    
   400      1703.88    146.3334   146.4244    1.1121      0.0        0.0    
   450      1918.13    145.3935   145.4729    1.1372      0.0        0.0    
   500      2132.02    145.3741   145.4624    0.708       0.0        0.0    
   550       2346.6    145.7505   145.527     0.895       0.0        0.0    
   600      2560.21    144.7529   144.9609    0.9443      0.0        0.0    
   650      2774.02    145.5704   145.5074    0.7902      0.0        0.0    
   700      2988.86    145.265    145.3435    0.8149      0.0        0.0    
   750      3202.63    145.3693   145.4785    0.7911      0.0        0.0    
   800      3417.17    145.4389   145.3744    1.0327      0.0        0.0    
   850      3632.17    145.6437   145.5473    0.9785      0.0        0.0    
   900      3846.72    145.3752   145.5078    0.9531      0.0        0.0    
   950       4061.1    145.4613   145.4712    0.7113      0.0        0.0    
   1000     4275.71    145.6575   145.4153    0.869       0.0        0.0    
   1050     4490.34    145.0246   145.3199    0.8632      0.0        0.0    
   1100     4704.46    145.0978   145.0271    0.7923      0.0        0.0    
   1150      4919.0    145.7285   145.8282    0.9542      0.0        0.0    
   1200     5134.59    144.7472   144.8683    0.8673      0.0        0.0    
   1250     5349.65    144.9623   144.9965    0.6311      0.0        0.0    
   1300     5564.68    145.4675   145.3924    0.5073      0.0        0.0    
   1350     5779.37    145.0578   144.9848    0.709       0.0        0.0    
   1400      5994.2    144.7753   145.0074    0.6298      0.0        0.0    
   1450     6207.58    145.0847   145.1118    0.7661      0.0        0.0    
   1500     6421.77    145.1572   145.0234    0.8401      0.0        0.0    
   1550     6635.32    145.4034   145.2306    0.914       0.0        0.0    
   1600     6848.35    145.0551   145.2187    0.8581      0.0        0.0    
   1650     7062.06    145.3943   145.3664    0.7774      0.0        0.0    
   1700     7276.49    145.3463   145.3636    0.782       0.0        0.0    
   1750     7490.44    144.9846   145.1331    0.7898      0.0        0.0    
   1800     7704.19    145.0866   144.9331    0.7096      0.0        0.0    
   1850     7918.25    144.8522   144.902     0.6235      0.0        0.0    
   1900     8132.26    145.2096   145.1814    0.6548      0.0        0.0    
   1950     8346.21    144.7687   144.7621     0.64       0.0        0.0    
   2000     8560.47    144.7317   144.873     0.6706      0.0        0.0    
