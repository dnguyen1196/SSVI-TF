Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.924303770065308
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       179.13     1.2881     1.2659     3.1623     1.0206     0.0172  
   100       489.95     0.8485     0.8364     3.0902     0.4771     0.0004  
   150       799.72     0.7347     0.7179     2.1079     1.3352     0.0038  
   200      1110.89     0.756      0.7418     2.1611     3.3399     0.0062  
   250      1420.98     0.7493     0.728      1.9172    16.7848     0.0042  
   300      1729.99     0.7425     0.7327     2.0371    47.3953     0.0022  
   350       2037.5     0.7428     0.7261     1.8274    13.4386     0.0106  
   400      2343.99     0.7496     0.7244     1.7973    38.6167     0.0023  
   450      2649.63     0.7601     0.7414     1.8681    355.4226    0.0097  
   500      2955.05     0.7508     0.7297     1.6506    282.4362    0.0067  
   550      3260.84     0.7666     0.7521     2.1121   2939.6954    0.0075  
   600      3566.74     0.7336     0.7217     2.2856   3957.5634    0.0073  
   650      3870.85     0.7575     0.7491     2.4043   1746.5584    0.0049  
   700      4175.56     0.7632     0.7505     2.2751    292.0852    0.0014  
   750      4479.41     0.7347     0.7196     2.3554    131.7104    0.0086  
   800      4782.23     0.7463     0.731      2.3401    1344.645    0.0064  
   850      5085.68     0.7311     0.7182     2.1039    760.1925    0.0043  
   900       5389.7     0.7259     0.7136     1.9071    159.2827    0.0022  
   950      5695.27     0.7346     0.7246     1.8867    59.6401     0.0033  
   1000     5999.27     0.7272     0.7061     1.9894    37.7246     0.0057  
   1050     6302.19     0.7306     0.7203     1.9446    18.1253     0.0015  
   1100     6606.01     0.724      0.7029     1.8103    18.5145     0.0027  
   1150     6909.73     0.7259     0.7007     1.8435    30.1185     0.0066  
   1200     7214.35     0.7316     0.7092     1.9176    58.8372     0.0035  
   1250     7517.61     0.7249     0.7054     1.6808    101.9007    0.0043  
   1300     7821.09     0.7206     0.7044     1.3504    320.2615    0.006   
   1350     8126.41     0.7187     0.6991     1.9202   6002.7015    0.0118  
   1400      8430.5     0.7592     0.7355     3.1612   5401.2465     0.0    
   1450     8733.73     0.7409     0.7259     1.5003   1981.8192    0.0063  
   1500     9037.82     0.7352     0.7167     1.8185    1157.284    0.0061  
   1550     9341.44     0.7272     0.7057     2.1196    382.7142    0.0055  
   1600     9644.84     0.7227     0.7057     1.585     431.8598     0.0    
   1650     9948.95     0.7264     0.703      2.0149    107.9345    0.011   
   1700     10252.41    0.7193     0.7053     1.3708    106.2416    0.0021  
   1750     10555.94    0.7155     0.6976     1.3859    58.8061     0.0006  
   1800     10859.17    0.7194     0.694      1.5121    69.3994     0.0014  
   1850     11162.91    0.7188     0.7009     1.3022    14.9062     0.0102  
   1900     11466.51    0.7218     0.7046     1.334     17.2566     0.0035  
   1950     11771.16    0.7129     0.7004     1.3673     8.5051     0.007   
   2000     12075.09    0.715      0.6961     1.3895     4.9691     0.0079  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       178.57     1.4116     1.4149     0.885      0.0054     0.0019  
   100       539.69     1.4128     1.4157     0.9133     0.0063     0.0007  
   150       904.02     1.3292     1.3284     0.9779     0.0068     0.0003  
   200      1266.94     0.8811     0.8753     1.0311     0.0118     0.0004  
   250      1629.73     0.7525     0.7455     1.0636     0.0417     0.0017  
   300      1992.48     0.734      0.7207     0.9747     0.0678     0.0039  
   350       2353.1     0.7515     0.7401     0.9946     0.083      0.0002  
   400      2714.91     0.7295     0.7154     0.9841     0.0901     0.0014  
   450      3074.95     0.7484     0.7353     0.9323     0.089      0.004   
   500      3435.62     0.7339     0.7221     0.9035     0.0917      0.0    
   550      3796.74     0.7375     0.7243     0.9439     0.0905     0.0046  
   600      4157.17     0.7254     0.7201     0.8478     0.1082     0.0021  
   650      4518.19     0.7305     0.7208     0.7536     0.1103     0.0012  
   700      4878.53     0.7344     0.722      0.834      0.1129     0.0011  
   750      5238.96     0.7489     0.7314     0.7538     0.1286     0.0014  
   800      5599.15     0.7328     0.7233     0.7847     0.1317     0.0015  
   850      5958.53     0.7327     0.723      0.866      0.1554     0.003   
   900      6318.51     0.7355     0.7239     0.7969     0.1522     0.0013  
   950      6678.12     0.7242     0.7194     0.692      0.1811     0.0008  
   1000     7037.16     0.7319     0.7182     0.8482     0.1819     0.0075  
   1050     7396.25     0.7302     0.7225     0.6703     0.1957     0.0026  
   1100     7754.54     0.733      0.7192     0.7596     0.2207     0.0026  
   1150      8113.0     0.725      0.7151     0.7487     0.2694     0.0007  
   1200     8470.05     0.7331     0.7215     0.6895     0.2914     0.0014  
   1250     8828.99     0.7344     0.7226     0.608      0.3015     0.0038  
   1300     9186.74     0.7378     0.7232     0.6177     0.3265     0.0015  
   1350     9545.39     0.7373     0.7271     0.6037     0.4089     0.0021  
   1400     9902.33     0.7381     0.7266     0.6592     0.4755     0.0018  
   1450     10261.16    0.7346     0.7201     0.7661     0.4971     0.0009  
   1500     10620.0     0.7294     0.7176     0.5348     0.6266     0.0004  
   1550     10978.25    0.7347     0.7265     0.628      0.7816     0.0012  
   1600     11336.56    0.7373     0.7253     0.6985     0.9423     0.0033  
   1650     11694.55    0.7328     0.7217     0.5767     0.9997     0.001   
   1700     12052.81    0.7404     0.7263     0.5951     1.1772     0.0031  
   1750     12410.67    0.7366     0.7196     0.6538     1.8827     0.0005  
   1800     12768.61    0.7422     0.7268     0.5781     2.0305     0.0023  
   1850     13127.94    0.7362     0.7208     0.6138     2.8382     0.0013  
   1900     13485.75    0.734      0.7253     0.6188     3.0209     0.0036  
   1950     13843.98    0.7411     0.7264     0.8111     4.6179     0.003   
   2000     14202.96    0.7455     0.7301     0.8586     5.9343     0.0049  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       180.1      1.4172     1.4131     0.6797     0.0036     0.0012  
   100       597.87     1.4127     1.4159     0.7809     0.0042     0.0006  
   150      1017.69     1.4155     1.4122     0.768      0.004      0.0005  
   200      1437.88     1.4124     1.412      0.6988     0.004      0.0007  
   250      1857.96     1.4125     1.4106     0.7812     0.0041     0.0004  
   300      2280.64     1.4129     1.4103     0.7675     0.0039     0.0005  
   350      2702.29     1.4121     1.4031     0.6784     0.0041      0.0    
   400      3123.84     1.3804     1.3836     0.6958     0.0043     0.0004  
   450      3546.19     1.2045     1.2017     1.0756     0.0043     0.0001  
   500      3967.68     0.7895     0.7772     1.2994     0.0099     0.0013  
   550      4387.23     0.7421     0.7302     1.332      0.0288     0.0014  
   600      4807.74     0.7363     0.7199     1.2761     0.0385     0.0043  
   650      5226.09     0.7468     0.7357     1.2865     0.036      0.0005  
   700      5642.81     0.7383     0.733      1.2031     0.0411     0.0017  
   750      6060.49     0.7381     0.7247     1.2653     0.0464     0.0016  
   800      6479.38     0.7334     0.7191     1.0911     0.0431     0.0058  
   850      6896.83     0.7335     0.7169     1.1814     0.0531     0.0002  
   900      7313.74     0.734      0.7342     0.9788     0.0521     0.0016  
   950      7730.18     0.7307     0.7202     1.046      0.051      0.0013  
   1000     8146.45     0.7338     0.718      0.9781     0.0434     0.0003  
   1050     8563.36     0.7271     0.7165     1.0761     0.0491     0.0004  
   1100     8980.45     0.7318     0.7234     0.8036     0.0479     0.0032  
   1150     9396.98     0.7295     0.7244     0.984      0.0463     0.0024  
   1200     9812.35     0.7345     0.7196     1.0672     0.0534     0.0053  
   1250     10226.7     0.7306     0.7227     0.994      0.061      0.0033  
   1300     10641.48    0.7293     0.7164     1.0776     0.0494     0.0007  
   1350     11054.98    0.7329     0.7181     0.9107     0.0632     0.0027  
   1400     11469.17    0.7263     0.7174     0.8361     0.0691     0.001   
   1450     11883.13    0.7249     0.7152     0.9842     0.0684     0.0008  
   1500     12295.22    0.7309     0.7167     0.9492     0.0733     0.0012  
   1550     12708.76    0.7265     0.7148     1.1355     0.0651     0.0038  
   1600     13121.22    0.7291     0.719      0.7701     0.0732     0.0002  
   1650     13534.57    0.7247     0.7121     0.8188     0.0688     0.0037  
   1700     13946.66    0.732      0.7208     0.896      0.0842     0.0048  
   1750     14359.3     0.7311     0.7215     1.044      0.0899     0.0077  
   1800     14771.55    0.7288     0.7214     0.8183     0.0855     0.0009  
   1850     15183.29    0.7302     0.7166     0.8436     0.0872     0.001   
   1900     15595.2     0.7223     0.7164     0.769      0.1146     0.0056  
   1950     16006.54    0.7249     0.7137     0.6714     0.0924      0.0    
   2000     16416.5     0.7298     0.7111     0.8856     0.128      0.0036  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       180.96     1.4142     1.411      0.6232     0.0028     0.0009  
   100       653.08     1.411      1.4146     0.6959     0.0031     0.0005  
   150      1126.55     1.4077     1.4187     0.6201     0.003      0.0007  
   200      1603.25     1.4217     1.4088     0.6107     0.0031     0.0005  
   250      2079.95     1.416      1.4098     0.5785     0.0031     0.0001  
   300       2556.1     1.407      1.4096     0.6215     0.003      0.0007  
   350      3033.03     1.4077     1.4104     0.5883     0.0031     0.001   
   400      3510.13     1.411      1.4068     0.6131     0.0031      0.0    
   450      3987.86     1.3767     1.3855     0.7254     0.003      0.0001  
   500      4464.81     1.1464     1.1475     1.2169     0.0033     0.0005  
   550      4941.91     0.7673     0.7577     1.264      0.0109     0.0007  
   600      5418.64     0.7447     0.7311     1.2004     0.0254     0.0014  
   650      5892.85     0.7306     0.7217     1.2148     0.0329     0.0005  
   700       6366.7     0.7248     0.7139     1.0305     0.0364     0.0003  
   750      6838.53     0.7311     0.7257     1.0683     0.0319     0.0006  
   800      7311.64     0.7237     0.7086     1.0055     0.0386     0.0124  
   850      7782.02     0.7354     0.7321     0.9272     0.0348     0.0077  
   900      8253.48     0.7317     0.7163     1.0333     0.0413     0.005   
   950      8722.98     0.7242     0.7112     0.8694     0.039      0.006   
   1000     9191.01     0.7218     0.7123     0.8979     0.0403     0.005   
   1050     9659.27     0.7324     0.7203     0.7979     0.0403     0.0065  
   1100     10128.75    0.7283     0.7173     0.9552     0.0434     0.0037  
   1150     10596.31    0.7288     0.7166     0.879      0.0413     0.0014  
   1200     11063.94    0.7338     0.7162     0.763      0.0479     0.0065  
   1250     11532.2     0.7274     0.7147     0.7996     0.0405     0.0003  
   1300     12000.29    0.725      0.7132     0.8362     0.0529      0.0    
   1350     12467.35    0.7312     0.7236     1.0155     0.0549     0.0045  
   1400     12934.69    0.7321     0.7195     0.8918     0.0583     0.0004  
   1450     13403.53    0.7298     0.717      0.8154     0.0618     0.0036  
   1500     13870.31    0.7362      0.72      0.693      0.055      0.0018  
   1550     14336.76    0.733      0.7161     0.7841     0.0655     0.0046  
   1600     14803.26    0.7273     0.7144     0.6901     0.0708     0.0013  
   1650     15271.06    0.728      0.7145     0.8076     0.0678     0.005   
   1700     15737.51    0.7252     0.7113     0.7637      0.08      0.0147  
   1750     16201.9     0.7305     0.7185     0.6874     0.0715     0.0037  
   1800     16667.01    0.7273     0.7192     0.8892     0.0681     0.0041  
   1850     17132.01    0.7216     0.7101     0.8044     0.0766     0.0012  
   1900     17597.26    0.7294     0.7147     0.7671     0.0868     0.0014  
   1950     18062.74    0.726      0.7146     0.9148     0.0831     0.0033  
   2000     18528.19    0.7324     0.717      0.6569     0.0894     0.0032  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       180.23     1.4148     1.412      0.5456     0.0024     0.0018  
   100       704.58     1.4136     1.4124     0.4479     0.0025     0.0001  
   150      1234.28     1.4214     1.4124     0.5657     0.0026     0.0007  
   200      1764.46     1.4064     1.4166     0.5511     0.0025     0.0006  
   250      2295.65     1.4108     1.4107     0.6192     0.0026     0.0001  
   300      2825.65     1.4201     1.4132     0.5516     0.0026     0.0003  
   350      3357.02     1.4209     1.4149     0.7111     0.0025     0.0002  
   400      3887.32     1.4156     1.4116     0.5873     0.0027     0.0004  
   450      4419.23     1.413      1.4158     0.5209     0.0026     0.0004  
   500      4950.35     1.4096     1.4095     0.5192     0.0024     0.0009  
   550       5483.0     1.4119     1.409      0.5109     0.0026     0.0003  
   600      6017.33     1.3995     1.3922     0.6397     0.0025     0.0003  
   650      6551.37     1.2117     1.2153     0.8933     0.003      0.0003  
   700      7084.58     0.7935     0.7882     0.9167     0.0064     0.001   
   750      7617.92     0.7418     0.7343     0.9719     0.0172     0.0021  
   800      8149.04     0.7298     0.7149     0.9494     0.0275     0.002   
   850      8677.73     0.7345     0.7296     0.9653     0.0366     0.0007  
   900      9203.42     0.7292     0.7152     0.9706     0.0351     0.0086  
   950      9730.68     0.7329     0.7187     0.9012     0.0387     0.0011  
   1000     10256.24    0.7352     0.7241     0.8831     0.0382     0.0077  
   1050     10781.99    0.7289     0.7157     0.9192     0.0399     0.0033  
   1100     11305.05    0.7345     0.7224     0.8489     0.042      0.0057  
   1150     11828.53    0.7357     0.7265     0.828      0.0387     0.0014  
   1200     12351.77    0.7338     0.7158     0.8809     0.0377     0.0021  
   1250     12871.78    0.731      0.7219     0.815      0.0502     0.0019  
   1300     13392.57    0.7404     0.7297     0.8151     0.0509     0.0015  
   1350     13912.66    0.7347     0.7179     0.7615     0.0571     0.0013  
   1400     14432.63    0.723      0.7167     0.9704     0.0465     0.0042  
   1450     14951.0     0.7242     0.7175     0.7987     0.0461     0.0084  
   1500     15469.88    0.727      0.7164     0.7943     0.047      0.0075  
   1550     15989.31    0.7269     0.7176     0.8567     0.0489     0.0003  
   1600     16509.3     0.726      0.7164     0.8775     0.0558     0.0014  
   1650     17029.56    0.7324     0.7168     0.7684     0.061      0.0018  
   1700     17547.37    0.7218     0.7132     0.7056     0.0669     0.0019  
   1750     18066.35    0.731      0.7156     0.8708     0.0683     0.005   
   1800     18585.93    0.7283     0.7153     0.8279     0.0589     0.0122  
   1850     19105.77    0.7292     0.7193     0.9213     0.0807     0.0024  
   1900     19623.88    0.723      0.7129     0.8061     0.0718     0.0025  
   1950     20142.81    0.7276     0.7146     0.8661     0.0687     0.005   
   2000     20660.54    0.7276     0.7155     0.8882     0.0775     0.0051  
