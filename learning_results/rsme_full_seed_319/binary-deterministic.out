Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  2.6777663230895996
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       70.53      1.2707     1.2042     3.1623     4.0128   
   100       146.18     0.7672     0.7284     3.1229     0.2316   
   150       220.99     0.7637     0.7306     2.5958     0.1465   
   200       295.13     0.7315     0.6941     2.6792     0.1184   
   250       369.25     0.6898     0.6425     2.4743     0.0902   
   300       443.59     0.6881     0.6076     2.0874     0.0896   
   350       517.95     0.6807     0.6109     2.2672     0.0806   
   400       592.62     0.6497     0.5682     2.2851     0.0675   
   450       667.07     0.6376     0.5568     1.9266     0.0606   
   500       741.17     0.6381     0.5378     2.0686     0.0586   
   550       816.88     0.6379     0.5455     2.0715     0.0516   
   600       891.38     0.621      0.5175     2.0999     0.0438   
   650       966.31     0.6201     0.5227     2.0225     0.0483   
   700       1041.5     0.6079     0.4946     2.0305     0.0582   
   750       1115.9     0.5988     0.4703     1.5766     0.0416   
   800      1190.28     0.5979     0.4901     1.8908     0.0378   
   850      1264.95     0.5972     0.4833     2.1091     0.0407   
   900      1339.89     0.5926     0.4895     2.1856     0.0331   
   950      1414.12     0.5961     0.4628     1.6725     0.0346   
   1000     1488.94     0.5851     0.451      1.6112     0.036    
   1050     1563.37     0.5942     0.447      1.7827     0.0337   
   1100     1638.56     0.5953     0.4609     2.0947     0.0392   
   1150     1714.01     0.5871     0.4497     1.6556     0.0305   
   1200     1789.31     0.5758     0.4465     1.9658     0.0322   
   1250     1863.87     0.5921     0.4429     1.6319     0.0296   
   1300     1938.67     0.5733     0.4486     1.978      0.0299   
   1350     2014.21     0.5772     0.4375     1.4538     0.0312   
   1400     2089.18     0.5702     0.4278     1.4873     0.0318   
   1450     2163.83     0.5985     0.462      1.8557     0.0313   
   1500     2239.17     0.5812     0.4461     1.807      0.0299   
   1550      2314.2     0.5754     0.435      1.9455     0.0297   
   1600     2389.09     0.5812     0.4352     1.8129     0.0297   
   1650      2463.6     0.583      0.4501     1.8636     0.0315   
   1700     2538.49     0.5929     0.4539     2.3749     0.0317   
   1750     2613.95     0.6064     0.4846     2.2209     0.0321   
   1800     2688.36     0.6106     0.4846     2.3738     0.028    
   1850     2763.02     0.6252     0.5188     2.4622     0.0324   
   1900     2837.05     0.6093     0.5217     2.6246     0.0381   
   1950     2912.58     0.6395     0.5492     2.4985     0.0382   
   2000     2986.64     0.6314     0.5694     2.9314     0.0466   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       69.83      1.417      1.414      1.0642     1.2926   
   100       145.29     1.3931     1.3854     1.2401     0.868    
   150       221.23     1.3679     1.359      1.0551     0.5471   
   200       296.72     1.0413     1.0357     1.0464     0.3302   
   250       371.79     0.7559     0.7523     1.6753     0.2842   
   300       447.33     0.7406     0.7245     1.7988     0.2401   
   350       522.53     0.8005     0.7959     1.3768     0.1445   
   400       597.59     0.7304     0.7137     1.4916     0.1186   
   450       672.35     0.7485     0.7451     1.202      0.0951   
   500       746.64     0.7241     0.7084     1.1781     0.0879   
   550       821.03      0.73      0.721      0.7656     0.0641   
   600       894.68     0.7292     0.7224     0.7085     0.0537   
   650       969.88     0.7244     0.7163     0.8769     0.0512   
   700      1045.47     0.721      0.7046     0.9291     0.0495   
   750      1120.15     0.7213     0.7057     1.0607     0.0412   
   800      1195.05     0.7239     0.7137     1.1349     0.0477   
   850      1269.98     0.7011     0.6837     1.1743     0.0399   
   900       1345.1     0.7026     0.6903     0.7885     0.035    
   950      1420.56     0.7053     0.6864     1.051      0.0394   
   1000     1495.23     0.6639     0.6513     0.745      0.0262   
   1050     1570.18     0.6735     0.6447     0.8129     0.0288   
   1100      1644.3     0.6508     0.6304     0.9721     0.0278   
   1150     1718.77     0.6801     0.6564     1.297      0.0273   
   1200     1792.42     0.6371     0.6116     1.0074     0.0287   
   1250     1867.08     0.6405     0.6129     0.707      0.0203   
   1300     1941.47     0.636      0.6023     0.8341     0.0231   
   1350     2016.12     0.6435     0.6093     0.8767     0.023    
   1400     2091.76     0.6208     0.588       0.92       0.02    
   1450     2167.47     0.621      0.5823     1.1822     0.0212   
   1500     2242.24     0.6199     0.5867     1.0099     0.019    
   1550     2318.44     0.6185     0.5764     0.8403     0.0187   
   1600     2393.31     0.6036     0.5695     1.002      0.019    
   1650     2468.75     0.6056     0.5683     0.9842     0.0178   
   1700     2544.31     0.6005     0.5677     1.2627     0.0186   
   1750     2618.86     0.6008     0.5619     1.0423     0.0168   
   1800     2694.46     0.6107     0.5689     1.207      0.0261   
   1850      2769.2     0.5991     0.5535     0.9073     0.016    
   1900     2844.43     0.6005     0.5639     1.1589     0.0198   
   1950     2920.46     0.5942     0.5522     0.9499     0.016    
   2000     2995.52     0.5975     0.5545     1.0447     0.0146   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       70.62      1.4112     1.4146     1.0331     1.0449   
   100       146.78     1.403      1.4064     1.0654     0.7697   
   150       223.27     1.384      1.3851     0.947      0.5494   
   200       298.18     1.3498     1.3518     0.8661     0.3726   
   250       373.58     1.112      1.1173     0.7732     0.2494   
   300       449.13     0.7409     0.7338     1.2348     0.2085   
   350       523.77     0.7228     0.7142     1.4584     0.2252   
   400       598.91     0.7657     0.7511     1.2454     0.1615   
   450       673.44     0.7373     0.7318     1.1529     0.1099   
   500       748.62     0.7265     0.7075     1.164      0.0865   
   550       824.05     0.7267     0.7189     0.8374     0.0752   
   600       898.78     0.726      0.7107     0.9808     0.0631   
   650       973.21     0.7244     0.7181     0.7429     0.0528   
   700      1048.38     0.7357     0.7219     0.8731     0.0513   
   750      1123.85     0.7184     0.7067     1.0442     0.0485   
   800      1199.53     0.717      0.7049     0.8103     0.0395   
   850      1274.06     0.6963     0.6901     0.8711     0.041    
   900      1349.39     0.692      0.6723     0.9304     0.0402   
   950      1424.72     0.6907     0.6833     0.7789     0.0339   
   1000     1499.93     0.701      0.6868     0.8542     0.0292   
   1050     1575.29     0.6793     0.6673     0.9373     0.0303   
   1100     1650.02     0.6786     0.6604     1.0528     0.0302   
   1150     1724.81     0.6674     0.652      0.9954     0.0271   
   1200     1800.69     0.6684     0.645      0.9828     0.0234   
   1250      1876.2     0.6498     0.6332     0.8638     0.0268   
   1300     1951.12     0.645      0.6294     0.7225     0.019    
   1350     2027.42     0.6523     0.628       0.94      0.0208   
   1400     2102.17     0.6465     0.6315     0.7589     0.019    
   1450     2177.67     0.6391     0.6178     1.1551     0.0238   
   1500     2253.04     0.6321     0.6162     0.8415     0.0169   
   1550     2327.19     0.6299     0.6084     0.7554     0.015    
   1600     2402.34     0.6401     0.6216     0.7533     0.0223   
   1650     2477.07     0.6307     0.6143     0.9914     0.0206   
   1700     2552.45     0.6356     0.6211     0.7701     0.0173   
   1750     2626.54     0.6201     0.6064     0.856      0.0171   
   1800     2701.51     0.6293     0.609      0.7319     0.0185   
   1850     2775.87     0.6175     0.5954     0.8995     0.0145   
   1900     2850.95     0.6232     0.599      0.8498     0.0244   
   1950     2926.54     0.6264     0.6053     0.7744     0.0168   
   2000     3002.36     0.6161     0.5922     0.7697     0.0154   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       71.11      1.4129     1.4132     0.9181     0.9515   
   100       146.51     1.3939     1.3997     0.9446     0.7284   
   150       222.65     1.3568     1.3565     1.106      0.5393   
   200       298.95     1.3137     1.3158     0.7383     0.3848   
   250       374.15     1.1674     1.1608     0.7421     0.2674   
   300       449.29     0.8559     0.8519     0.6116     0.1882   
   350       525.29     0.7202     0.708      1.317      0.2065   
   400       600.71     0.7172     0.7035     1.3241     0.2108   
   450       675.88     0.7776     0.7651     1.0923     0.1329   
   500       751.18     0.7172     0.7035     1.129      0.0938   
   550       825.66     0.7458     0.7379     0.9009     0.0754   
   600       900.57     0.727      0.7167     0.9574     0.0695   
   650       975.13     0.7419     0.734      0.9355     0.0653   
   700      1049.38     0.7263     0.7104     0.7719     0.0486   
   750      1123.98     0.7401     0.7338     0.8098     0.043    
   800      1198.69     0.7249     0.7084     1.0055     0.0395   
   850      1274.02     0.7364     0.7232     0.627      0.0365   
   900      1348.85     0.7172     0.7035     0.6595     0.0323   
   950      1423.31     0.7143     0.7042     0.5896     0.0317   
   1000      1497.9     0.719      0.7007     0.6669     0.0267   
   1050     1573.45     0.7139     0.7025     0.7167     0.0253   
   1100     1648.66     0.7242     0.708      0.7475     0.0304   
   1150     1723.91     0.695      0.678      0.7008     0.0219   
   1200     1799.13     0.7002     0.6766     0.7574     0.0252   
   1250     1874.26     0.6955     0.676      0.7639     0.0203   
   1300     1949.14     0.6909     0.6715     0.6891     0.0198   
   1350     2024.75     0.6868     0.6678     0.6178     0.0188   
   1400     2100.37     0.6717     0.6557     0.6729     0.0234   
   1450     2175.63     0.6848     0.6584     0.8406     0.0167   
   1500     2250.11     0.6655     0.6486     0.5564     0.0164   
   1550     2325.97     0.6687     0.6443     0.4805     0.018    
   1600     2401.07     0.6602     0.6455     0.7606     0.0179   
   1650     2476.52     0.6551     0.6296     0.7761     0.018    
   1700     2552.06     0.6618     0.6394     0.6645     0.0175   
   1750     2628.09     0.6492     0.6273     0.6955     0.0213   
   1800     2704.43     0.6449     0.6248     0.7212     0.0239   
   1850     2779.29     0.6529     0.631      0.6494     0.0164   
   1900     2854.84     0.6455     0.6294     0.901      0.0136   
   1950     2929.82     0.6415     0.6173     0.8147     0.0208   
   2000     3003.41     0.6333     0.6151     0.842      0.0147   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       66.97      1.4105     1.4117     0.8769     0.8999   
   100       139.03     1.3763     1.3702     0.9971     0.7005   
   150       213.3      1.1434     1.136      1.0512     0.5314   
   200       287.93     0.7221     0.7129     0.9879     0.3896   
   250       361.9      0.7217     0.7081     0.9719     0.3133   
   300       436.52     0.7296     0.7163     0.7417     0.2395   
   350       510.35     0.7217     0.7081     0.6833     0.1538   
   400       585.58     0.7346     0.7256     0.6302     0.1322   
   450       659.91     0.7421     0.7292     0.6294     0.117    
   500       734.36     0.7398     0.7291     0.6743     0.0988   
   550       809.61     0.7217     0.7081     0.5919     0.0883   
   600       884.14     0.7351     0.7234     0.4279     0.0708   
   650       958.39     0.7217     0.7081     0.3866     0.0677   
   700      1033.18     0.7312     0.7224      0.47      0.0571   
   750      1108.04     0.7393     0.7266     0.606      0.0553   
   800      1182.92     0.734      0.7199     0.5607     0.0513   
   850      1257.24     0.7172     0.7044     0.3453     0.0397   
   900      1332.17     0.726      0.7113     0.3516     0.0358   
   950      1407.46     0.7253     0.7131     0.3692     0.0358   
   1000     1482.41     0.726      0.7113     0.4664     0.0409   
   1050     1556.88     0.7209     0.7095     0.437      0.0345   
   1100     1631.98     0.7291     0.7148     0.4213     0.0261   
   1150     1706.39     0.7172     0.7044     0.3332     0.0307   
   1200     1780.64     0.7149     0.7026     0.4016     0.0263   
   1250     1855.82     0.7296      0.72      0.3636     0.0226   
   1300     1930.59     0.726      0.7189     0.3209     0.025    
   1350     2004.73     0.7217     0.7081     0.2841     0.0243   
   1400      2079.5     0.7253     0.7131     0.3713     0.0196   
   1450     2154.57     0.7172     0.7044     0.3088     0.0188   
   1500      2229.2     0.732      0.721      0.3613     0.0179   
   1550     2304.13     0.726      0.7189     0.4405     0.0171   
   1600     2378.69     0.7231     0.7136     0.401      0.0174   
   1650     2452.64     0.7155     0.7088     0.3981     0.0154   
   1700     2526.68     0.7253     0.7131     0.3879     0.0152   
   1750      2601.2     0.7082     0.6967     0.4012     0.0135   
   1800     2675.96     0.7178     0.705      0.5127     0.0187   
   1850     2750.83     0.7126     0.6991     0.3946     0.0156   
   1900     2825.58     0.6966     0.6846     0.4449     0.0138   
   1950     2900.56     0.7028     0.6948     0.5458     0.0172   
   2000     2974.91     0.7074     0.6958     0.7531     0.0233   
