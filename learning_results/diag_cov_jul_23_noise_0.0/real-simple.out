Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  6.094944000244141
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       68.53     179.0289   178.1836    3.162      0.6931      0.0    
   200       137.98    126.3101   125.5963    2.249      0.232       0.0    
   300       207.36    115.3823   114.9118    2.3066     0.1234      0.0    
   400       276.63    107.8654   107.3593    1.5181     0.0809      0.0    
   500       346.19    106.5713   106.2783    1.0859     0.0552      0.0    
   600       415.8     105.7727   105.6051    0.9513     0.0409      0.0    
   700       485.38    105.3573   105.3305    1.1008     0.032       0.0    
   800       554.96    106.1377   105.4279    0.8039     0.026       0.0    
   900       623.93    104.667    104.4318    0.7619     0.0219      0.0    
   1000      692.83    105.251    104.9877    0.9434     0.0182      0.0    
   1100      761.72    105.7247   105.2706    1.0404     0.0157      0.0    
   1200      830.66    105.6109   104.8214    0.911      0.0135      0.0    
   1300      899.47    105.3763   104.5819    1.2484     0.012       0.0    
   1400      968.26    104.8296   104.5498    0.7661     0.0108      0.0    
   1500     1036.65    104.874    104.3803    0.6389     0.0095      0.0    
   1600     1104.76    104.6804   104.3648    0.6838     0.0088      0.0    
   1700     1172.85    104.9054   104.4605    0.7142     0.0078      0.0    
   1800     1241.48    104.9753   104.6595    0.6805     0.0073      0.0    
   1900     1310.33    105.0877   104.4872    0.7497     0.0066      0.0    
   2000     1379.17    104.9391   104.6014    0.8748     0.0061      0.0    
   2100     1448.02    104.9093   104.3811    0.7575     0.0055      0.0    
   2200     1516.84    104.4391   103.9964    0.706      0.0051      0.0    
   2300     1585.64    105.1548   104.7247    0.8713     0.0048      0.0    
   2400     1654.04    104.8135   104.3623    0.7179     0.0045      0.0    
   2500     1722.26    104.763    104.1269    0.5514     0.0042      0.0    
   2600     1790.54    104.9955   104.5514    0.6729     0.004       0.0    
   2700     1858.15    104.9523   104.2217    0.5824     0.0038      0.0    
   2800     1926.65    105.398    104.921     0.7569     0.0035      0.0    
   2900     1995.58    104.9558   104.437     0.8593     0.0033      0.0    
   3000     2064.41    105.1783   104.5495    0.7029     0.0031      0.0    
   3100     2133.19    105.3193   104.7645    0.7804     0.0031      0.0    
   3200     2201.97    104.4669   103.982     0.6655     0.0028      0.0    
   3300     2270.67    104.6029   104.3541    0.566      0.0027      0.0    
   3400     2339.35    104.4778   104.0654    0.6211     0.0026      0.0    
   3500     2407.99    104.934    104.7701    0.7494     0.0025      0.0    
   3600     2476.58    104.436    104.0394    0.7756     0.0024      0.0    
   3700     2545.14    104.4108   103.9959    0.5566     0.0023      0.0    
   3800     2613.74    104.8121   104.2772    0.5753     0.0022      0.0    
   3900     2682.41    104.1422   103.7497    0.695      0.0021      0.0    
   4000     2751.19    104.4472   103.6765    0.6679     0.002       0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       67.81     165.855    166.449     3.162      0.6399      0.0    
   200       137.21    129.8343   129.9799    2.8037     0.3221      0.0    
   300       206.59    113.6602   113.6232    1.9186     0.2014      0.0    
   400       276.01    112.6797   112.2881    1.8257     0.1418      0.0    
   500       345.42    107.5156   106.926     1.3544     0.1046      0.0    
   600       414.31    107.5085   106.8566    1.4564     0.0801      0.0    
   700       483.01    106.4993   106.0358    1.0098     0.063       0.0    
   800       552.13    105.5024   105.2962    0.9084     0.0515      0.0    
   900       621.59    105.3587   104.8058    0.9061     0.044       0.0    
   1000      690.99    105.2107   104.8417    0.8529     0.0363      0.0    
   1100      760.48    106.5489   105.9903    0.9479     0.0306      0.0    
   1200      829.96    106.0212    105.34      0.73      0.027       0.0    
   1300      899.43    106.4583   105.7414    0.8299     0.0235      0.0    
   1400      969.0     105.8118   104.8855    0.8161     0.0209      0.0    
   1500     1038.97    105.911    105.2363    0.7796     0.0187      0.0    
   1600     1108.65    106.0901   105.7411    1.2238     0.0164      0.0    
   1700     1178.23    104.8325   104.5057    0.8488     0.0153      0.0    
   1800      1247.8    104.9562   104.4106    0.6682     0.014       0.0    
   1900     1317.35    104.9838   104.2514    0.8032     0.0122      0.0    
   2000      1386.8    105.0761   104.3211    0.6535     0.0116      0.0    
   2100     1456.22    104.8994   104.4492    0.6891     0.0105      0.0    
   2200     1525.74    104.9677   104.6868    0.7476     0.0096      0.0    
   2300     1595.39    105.589    104.8188    0.7895     0.009       0.0    
   2400      1665.0    105.1763   104.6386    0.9622     0.0084      0.0    
   2500     1734.53    105.2322   104.8037    0.9571     0.0079      0.0    
   2600     1803.93    104.7643   104.427     0.869      0.0073      0.0    
   2700     1873.33    105.3684   104.5079    0.5847     0.0068      0.0    
   2800     1942.75    104.492    103.8876    0.4862     0.0064      0.0    
   2900     2012.14    104.3455   104.1434    0.5521     0.006       0.0    
   3000     2081.63    105.0078   104.2562    0.6838     0.0057      0.0    
   3100     2151.25    104.9239   104.1902    0.645      0.0056      0.0    
   3200     2220.86    105.2215   104.6426    0.9309     0.0051      0.0    
   3300     2290.53    105.1874   104.4971    0.807      0.0048      0.0    
   3400     2360.03    104.5753   103.9285    0.523      0.0045      0.0    
   3500     2429.55    105.0045   104.2133    0.4769     0.0044      0.0    
   3600     2499.06    104.8761   104.3978    0.593      0.0041      0.0    
   3700     2568.58    104.2958   103.9138    0.7601     0.0039      0.0    
   3800      2638.1    104.4935   103.8994    0.8345     0.0037      0.0    
   3900     2707.66    104.5433   104.1103    0.5733     0.0037      0.0    
   4000     2777.14    104.5034   103.9393    0.6388     0.0034      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       68.01     167.2999   166.4408    3.1606     0.5148     0.0001  
   200       138.27    128.7048   127.928     2.6702     0.3029      0.0    
   300       208.46    115.619    115.3451    2.1094     0.2217      0.0    
   400       278.68    110.3703   110.4453    1.7826     0.1605      0.0    
   500       348.94    108.538    108.1457    1.4551     0.1153      0.0    
   600       419.19    108.1538   107.5292    1.1881     0.0926      0.0    
   700       489.4     106.7991   106.2095    1.1045     0.0724      0.0    
   800       559.6     107.0037   106.346     1.1055     0.0622      0.0    
   900       629.89    105.6859   105.3459    0.9578     0.0503      0.0    
   1000      700.2     105.7596   105.1172    1.1408     0.0451      0.0    
   1100      770.47    106.1712   105.8264    0.9235     0.0392      0.0    
   1200      840.72    106.0937   105.6549    0.8812     0.0337      0.0    
   1300      910.97    105.7294   105.3812    0.8488     0.0297      0.0    
   1400      981.12    106.3493   105.8421    0.9149     0.0266      0.0    
   1500     1051.44    106.1218   105.6401    1.0602     0.0241      0.0    
   1600     1121.79    105.5003   105.1559    0.8015     0.021       0.0    
   1700     1192.16    105.7961   105.4595    0.8862     0.0194      0.0    
   1800     1261.53    105.1788   104.7917    0.8205     0.0174      0.0    
   1900     1331.82    105.7775   105.3317    0.6765     0.0158      0.0    
   2000     1402.15    105.2495   104.9382    0.8697     0.0147      0.0    
   2100     1472.58    105.2133   104.9397    0.8117     0.0136      0.0    
   2200     1542.98    105.2266   104.5176    0.9701     0.0126      0.0    
   2300     1613.37    105.2914   104.9838    0.7298     0.0117      0.0    
   2400     1683.63    105.1802   104.8494    0.7464     0.0108      0.0    
   2500     1753.29    105.2599   105.0373    0.6142      0.01       0.0    
   2600     1821.94    104.9516   104.7412    0.6292     0.0095      0.0    
   2700     1891.69    105.4894   104.965     0.6365     0.0089      0.0    
   2800     1962.09    105.0396   104.6741    0.8517     0.0083      0.0    
   2900     2032.44    105.1604   104.5688    0.6905     0.0078      0.0    
   3000     2102.85    105.1666   104.9119    0.6209     0.0075      0.0    
   3100     2173.13    104.8309   104.4156    0.7495     0.0069      0.0    
   3200     2243.44    104.8274   104.4294    0.4841     0.0067      0.0    
   3300     2313.87    105.1643   105.0218    0.8109     0.0062      0.0    
   3400     2384.26    104.8069   104.3685    0.6414     0.0062      0.0    
   3500     2454.75    104.4475   104.0414    0.5657     0.0057      0.0    
   3600     2525.15    104.5725   104.1301    0.6397     0.0056      0.0    
   3700     2595.68    104.5448   104.0971    0.502      0.0052      0.0    
   3800     2666.15    104.7036   104.3103    0.5148     0.0049      0.0    
   3900      2736.6    104.9623   104.5844    0.6802     0.0047      0.0    
   4000     2807.05    104.8226   104.385     0.7647     0.0044      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       68.27     161.9516   162.5414    3.161      0.445      0.0001  
   200       139.43    126.0789   125.8094    3.051      0.285       0.0    
   300       210.57    116.965    116.3296    2.0014     0.2008      0.0    
   400       281.8     112.9936   112.7617    1.9729     0.1545      0.0    
   500       352.99    112.1102   111.9384     1.91      0.1199      0.0    
   600       424.13    107.0721   107.0482    1.3602     0.0955      0.0    
   700       495.32    107.579    107.9845    1.0545     0.0789      0.0    
   800       566.46    106.2623   106.0567    0.8082     0.0669      0.0    
   900       637.51    105.6781   105.7973    0.9276     0.0591      0.0    
   1000      708.52    106.2363   106.3593    1.024      0.0499      0.0    
   1100      779.66    106.0286   106.0212    0.7169     0.043       0.0    
   1200      850.76    105.9954   105.9455    0.7681     0.0383      0.0    
   1300      921.73    105.2763   105.2477    0.8901     0.0339      0.0    
   1400      992.72    105.7201   105.5898    0.7857     0.0312      0.0    
   1500     1063.69    105.9356   105.8363    0.768      0.0276      0.0    
   1600     1134.63    104.7944   104.9923    0.8943     0.0249      0.0    
   1700     1205.59    105.7828   105.6469    0.8466     0.0221      0.0    
   1800     1276.57    105.4678   105.6094    0.8454     0.0205      0.0    
   1900     1347.54    105.1488   105.3327    1.0466     0.0191      0.0    
   2000     1418.52    106.6171   106.4793    1.0646     0.017       0.0    
   2100     1489.43    104.9941   104.8988    0.8615     0.016       0.0    
   2200     1560.41    105.1651   105.365     0.7392     0.0144      0.0    
   2300     1631.33    104.762    104.7584    0.6837     0.0135      0.0    
   2400     1702.03    105.5296   105.4935    0.8232     0.0125      0.0    
   2500     1772.93    105.3329   105.196     0.7094     0.0117      0.0    
   2600     1843.83    104.6775   104.8877    0.6494     0.011       0.0    
   2700     1914.69    105.0209   105.1319    0.6425     0.0105      0.0    
   2800     1985.54    105.0515   105.2594    0.6438     0.0097      0.0    
   2900      2056.5    105.0165    104.98     0.7868     0.0092      0.0    
   3000     2127.37    104.6401   104.7387    0.6624     0.0088      0.0    
   3100     2198.18    104.8618   104.9439    0.6916     0.0081      0.0    
   3200     2269.07    104.1592   104.289     0.5717     0.0078      0.0    
   3300     2339.93    104.8576   104.7171    0.5011     0.0074      0.0    
   3400     2410.78    105.0704   105.0422    0.8905     0.007       0.0    
   3500     2481.63    104.8336   104.9313    0.6322     0.0067      0.0    
   3600     2552.52    104.241    104.6033    0.6416     0.0064      0.0    
   3700     2623.39    104.7159   104.7873    0.8354     0.0061      0.0    
   3800     2694.25    104.9944   104.7688    0.8756     0.0058      0.0    
   3900     2765.15    104.1217   104.4376    0.5412     0.0055      0.0    
   4000     2836.02    105.1997   105.0653    0.7552     0.0052      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       67.88     166.1847   166.2762    3.1616     0.3906     0.0001  
   200       139.34    126.1792   126.2459    2.5658     0.2716      0.0    
   300       210.82    116.3974   117.0799    2.0903     0.1999      0.0    
   400       282.24    112.9702   113.5294    1.9566     0.1614      0.0    
   500       353.72    109.7733   109.7688    1.8214     0.1211      0.0    
   600       425.21    108.1493   108.274     1.2492     0.1014      0.0    
   700       496.65    111.0138   110.8763    1.3905     0.0819      0.0    
   800       568.09    106.1619   106.4125    1.0479     0.0688      0.0    
   900       639.57    106.0764   106.4085    1.1411     0.0619      0.0    
   1000      711.11    107.6712   107.483     1.1075     0.0525      0.0    
   1100      782.63    106.4182   106.5282    1.0276     0.0462      0.0    
   1200      854.21    105.6724   105.9415    0.8648     0.0405      0.0    
   1300      925.7     106.0283   106.2206    0.8405     0.0359      0.0    
   1400      997.22    105.5066   105.5407    0.8259     0.0326      0.0    
   1500     1068.78    105.5337   105.6783    0.7204     0.0293      0.0    
   1600      1140.3    105.9375   105.9882    0.6936     0.0263      0.0    
   1700     1211.77    105.8965   106.0448    0.924      0.0247      0.0    
   1800     1283.23    105.6237   106.0024    1.0023     0.0228      0.0    
   1900     1354.65    104.9656   105.2427    0.8104     0.0203      0.0    
   2000      1426.1    104.961    104.9755    0.8529     0.0185      0.0    
   2100     1497.59    105.3716   105.5749    0.6405     0.0167      0.0    
   2200     1569.08    105.4159   105.5679    0.8632     0.0158      0.0    
   2300     1640.51    104.6773   105.0555    0.9475     0.0148      0.0    
   2400     1712.03    105.0839   105.424     0.6715     0.0141      0.0    
   2500     1783.53    105.0007   105.0445    0.9655     0.0129      0.0    
   2600     1855.06    105.6224   105.6296    0.6721     0.0127      0.0    
   2700     1926.64    105.1217   105.0926    0.7652     0.0117      0.0    
   2800     1998.17    104.9896   104.9727    0.9398     0.0108      0.0    
   2900     2069.82    105.1272   105.2177    0.9187     0.0102      0.0    
   3000     2141.32    105.5356   105.5789    0.6523     0.0096      0.0    
   3100     2212.83    104.9094   104.9617    0.7119     0.0091      0.0    
   3200     2284.34    104.8115   104.7661    0.7935     0.0085      0.0    
   3300     2355.89    104.6865   104.8012    0.7517     0.0082      0.0    
   3400      2427.5    104.9143   104.9101    0.6228     0.008       0.0    
   3500     2499.07    104.2064   104.2707    0.6687     0.0075      0.0    
   3600     2570.64    104.9985   105.1646    0.8057     0.0071      0.0    
   3700     2642.22    104.8944   104.9994    0.9871     0.0067      0.0    
   3800     2713.83    104.7635   104.8561    0.7128     0.0064      0.0    
   3900     2785.42    104.7229   104.6509    0.8411     0.006       0.0    
   4000     2857.04    104.5704   104.6169    0.8301     0.006       0.0    
