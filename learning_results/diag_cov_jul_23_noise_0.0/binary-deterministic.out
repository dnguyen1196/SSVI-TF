Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.19127082824707
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.01      0.8013     0.7574     3.1233     0.7071   
   200       183.35     0.7245     0.6979     2.8764     0.208    
   300       275.26     0.7039     0.6831     2.9641     0.1403   
   400       367.5      0.7171     0.6958     3.0056     0.1183   
   500       459.72     0.7155     0.6953     2.8508     0.0985   
   600       551.29     0.7165     0.6987     2.9655     0.081    
   700       642.67     0.7109     0.6899     2.9219     0.0663   
   800       733.77     0.7165     0.7006     2.8853     0.0607   
   900       824.71     0.717      0.7064     2.7534     0.0464   
   1000      915.48     0.7173     0.706      2.7977     0.0394   
   1100     1004.04     0.7103     0.696      2.7585     0.0361   
   1200     1088.55     0.7139     0.7007     2.6017     0.0418   
   1300     1173.71     0.7206     0.7044     2.5824     0.0383   
   1400     1259.82     0.7109     0.6971     2.4386     0.0261   
   1500     1348.65     0.7217     0.7078     2.2646     0.0309   
   1600     1436.97     0.7211     0.7075     2.2126     0.0289   
   1700     1525.67     0.7106     0.6966     2.6208     0.0233   
   1800     1613.58     0.7113     0.6956     2.4849     0.0233   
   1900     1699.61     0.7094     0.6976     1.8547     0.0229   
   2000     1784.22     0.7139     0.7024     2.1549     0.0161   
   2100     1870.14     0.7171     0.707      2.4045     0.0129   
   2200     1957.72     0.7162     0.7064     2.237      0.0145   
   2300      2048.5     0.7108     0.6999     1.7712     0.012    
   2400     2139.21     0.7126     0.6989     1.7685     0.0131   
   2500     2229.61     0.7149     0.7047     2.1255     0.012    
   2600     2320.21     0.7158     0.7034     1.5237      0.01    
   2700     2410.78     0.717      0.707      2.0422     0.0087   
   2800     2499.61     0.7122     0.6993     2.5863     0.0091   
   2900     2588.77     0.7114     0.695      1.7601     0.0092   
   3000     2679.72     0.7136     0.7003     1.811      0.0092   
   3100     2770.73     0.7132     0.6994     1.9448     0.008    
   3200     2861.68     0.7148     0.7017     1.5924     0.0094   
   3300     2952.58     0.7132     0.7001     1.7929     0.0077   
   3400     3043.46     0.7106     0.6994     1.5528     0.0074   
   3500     3134.28     0.7172     0.707      1.3945     0.008    
   3600     3225.08     0.7131     0.701      1.7173     0.0061   
   3700     3315.94     0.7115     0.699      1.8742     0.0071   
   3800     3406.73     0.7139     0.7004     1.5727     0.0057   
   3900     3497.58     0.7152     0.7023     1.488      0.0079   
   4000     3588.51     0.7129     0.6996     1.721      0.0064   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100        91.6      0.7832     0.7533     3.0953     0.1339   
   200       185.91     0.7228     0.707      3.0468     0.1314   
   300       282.02     0.7151     0.7072     2.9947     0.1083   
   400       376.75     0.7094     0.6991     2.9143     0.1025   
   500       470.75     0.7171     0.7082     2.7741     0.1006   
   600       564.71     0.7172     0.7084     3.0506     0.1147   
   700       658.63     0.7172     0.7084     2.7588     0.1115   
   800       752.32     0.7172     0.7084     2.7907     0.111    
   900       846.04     0.7172     0.7084     2.3978     0.1257   
   1000      939.57     0.7172     0.7084     2.3891     0.1153   
   1100     1033.18     0.7172     0.7084     2.4738     0.1258   
   1200     1126.74     0.7172     0.7084     2.5277     0.1103   
   1300     1220.44     0.7172     0.7084     2.033      0.1254   
   1400      1314.2     0.7172     0.7084     1.915      0.1659   
   1500     1407.88     0.7172     0.7084     2.1699     0.1681   
   1600     1501.63     0.7172     0.7084     2.168      0.1488   
   1700     1595.39     0.7172     0.7084     2.1474     0.1573   
   1800     1689.17     0.7172     0.7084     1.8263     0.1882   
   1900     1782.89     0.7172     0.7084     2.2967     0.164    
   2000     1876.68     0.7172     0.7084     1.7278     0.1495   
   2100     1970.48     0.7172     0.7084     1.8584     0.1576   
   2200     2064.29     0.7172     0.7084     2.1937     0.1711   
   2300     2158.13     0.7172     0.7084     1.4665     0.1514   
   2400     2252.04     0.7172     0.7084     2.2581     0.1229   
   2500     2345.87     0.7172     0.7084     2.1235     0.2288   
   2600     2439.61     0.7172     0.7084     1.6404     0.1878   
   2700     2533.42     0.7172     0.7084     1.628      0.1402   
   2800     2627.16     0.7172     0.7084     1.4216     0.1432   
   2900     2720.99     0.7172     0.7084     1.9937     0.1177   
   3000      2814.7     0.7172     0.7084     1.3944     0.1376   
   3100     2908.38     0.7172     0.7084     1.4123     0.1367   
   3200     3002.09     0.7172     0.7084     1.9324     0.1474   
   3300     3095.72     0.7172     0.7084     1.3274     0.128    
   3400     3189.34     0.7172     0.7084     1.4002     0.1263   
   3500     3282.84     0.7172     0.7084     1.6545     0.1345   
   3600     3376.37     0.7172     0.7084     1.1283     0.1434   
   3700     3469.89     0.7172     0.7084     1.4745     0.1243   
   3800     3563.36     0.7172     0.7084     1.9962     0.1304   
   3900     3656.96     0.7172     0.7084     1.5722     0.2149   
   4000     3750.41     0.7172     0.7084     1.2711     0.0954   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.55      0.7544     0.7378     3.1379     0.1423   
   200       186.22     0.7132     0.7026     2.8466     0.1515   
   300       282.84     0.7192     0.7051     2.8976     0.1322   
   400       378.64     0.7158     0.7046     2.937      0.1164   
   500       473.58     0.7172     0.706      2.9499     0.1123   
   600       568.18     0.7172     0.706      2.6862     0.1139   
   700       662.71     0.7151     0.7041     2.534      0.1114   
   800       757.16     0.7172     0.706      2.7273     0.1162   
   900       851.58     0.7172     0.706      2.9503     0.1234   
   1000      945.77     0.7172     0.706       2.63      0.1112   
   1100     1040.07     0.7172     0.706      2.5079     0.1303   
   1200     1134.39     0.7172     0.706      2.5313     0.1347   
   1300     1228.64     0.7172     0.706      2.1191     0.1637   
   1400     1322.86     0.7172     0.706      2.1606     0.1501   
   1500     1417.12     0.7172     0.706      1.8981     0.138    
   1600     1511.26     0.7172     0.706      1.8647     0.1656   
   1700      1605.5     0.7172     0.706      2.0181     0.1622   
   1800     1699.72     0.7172     0.706      2.3064     0.1167   
   1900     1793.93     0.7172     0.706      1.8458     0.1514   
   2000     1888.07     0.7172     0.706      2.1498     0.1412   
   2100     1982.25     0.7172     0.706      1.9144     0.1803   
   2200     2076.53     0.7172     0.706      1.5531     0.1733   
   2300     2170.62     0.7172     0.706      1.5845     0.1493   
   2400     2263.68     0.7172     0.706      2.1462     0.1377   
   2500     2357.49     0.7172     0.706      1.8777     0.1591   
   2600     2451.51     0.7172     0.706      2.0254     0.178    
   2700     2545.53     0.7172     0.706      1.6726     0.1389   
   2800     2639.53     0.7172     0.706      1.6144     0.1372   
   2900     2733.66     0.7172     0.706      1.5209     0.1565   
   3000     2827.88     0.7172     0.706      1.3755     0.1547   
   3100     2921.97     0.7172     0.706      1.2702     0.147    
   3200     3016.06     0.7172     0.706      1.3017     0.1398   
   3300     3110.18     0.7172     0.706      1.2565     0.1488   
   3400     3204.22     0.7172     0.706      1.1873     0.1255   
   3500     3298.21     0.7172     0.706      1.4116     0.1391   
   3600     3392.21     0.7172     0.706      1.6287     0.1174   
   3700     3486.22     0.7172     0.706      1.3524     0.1032   
   3800     3580.14     0.7172     0.706      1.3605     0.0889   
   3900     3674.05     0.7172     0.706      1.219      0.1007   
   4000     3767.99     0.7172     0.706      1.4678     0.1276   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.52      0.7685     0.7522     3.113      0.1239   
   200       187.06     0.7196     0.7045     3.0051     0.1335   
   300       284.38     0.7172     0.7035     2.9831     0.1174   
   400       380.41     0.7172     0.7035     2.8782     0.1126   
   500       475.69     0.7172     0.7035     2.8968     0.1182   
   600       570.77     0.7172     0.7035     3.0335     0.109    
   700       665.7      0.7172     0.7035     2.6695     0.1248   
   800       760.69     0.7172     0.7035     2.5571     0.1251   
   900       855.64     0.7172     0.7035     2.6009     0.1371   
   1000      950.62     0.7172     0.7035     3.0125     0.1462   
   1100     1045.65     0.7172     0.7035     2.5052     0.1351   
   1200     1140.68     0.7161     0.701      2.2869     0.127    
   1300      1235.6     0.7172     0.7035     2.4792     0.1483   
   1400     1330.48     0.7172     0.7035     2.7361     0.1338   
   1500     1425.35     0.7172     0.7035     2.4657     0.1393   
   1600     1520.19     0.7172     0.7035     2.5851     0.1301   
   1700     1615.14     0.7172     0.7035     1.772      0.1215   
   1800     1710.07     0.7172     0.7035     1.7789     0.1894   
   1900     1804.95     0.7172     0.7035     1.6445     0.1573   
   2000     1899.83     0.7172     0.7035     1.5815     0.1413   
   2100     1994.75     0.7172     0.7035     1.9787     0.1791   
   2200     2089.59     0.7172     0.7035     2.1015     0.1655   
   2300     2184.64     0.7172     0.7035     1.5616     0.1555   
   2400     2279.49     0.7172     0.7035     1.5697     0.2101   
   2500     2374.33     0.7172     0.7035     1.7013     0.1427   
   2600     2469.38     0.7172     0.7035     1.3736     0.1158   
   2700     2564.27     0.7172     0.7035     2.0423     0.1507   
   2800     2659.07     0.7172     0.7035     1.3372     0.1795   
   2900     2753.85     0.7172     0.7035     1.6382     0.0938   
   3000     2848.63     0.7172     0.7035     1.8734     0.1525   
   3100     2943.41     0.7172     0.7035     1.3667     0.1192   
   3200     3038.13     0.7172     0.7035     1.7511     0.1228   
   3300     3132.96     0.7172     0.7035     1.494      0.1298   
   3400     3227.74     0.7172     0.7035     1.9505     0.1061   
   3500     3322.46     0.7172     0.7035     1.712      0.1203   
   3600     3417.47     0.7172     0.7035     1.7136     0.1025   
   3700     3512.25     0.7172     0.7035     1.6682     0.1454   
   3800     3606.92     0.7172     0.7035     1.4109     0.1113   
   3900     3701.56     0.7172     0.7035     1.1838     0.1117   
   4000     3796.25     0.7172     0.7035     1.3975     0.1149   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.59      1.0221     1.0034     3.0639     0.1162   
   200       187.67     0.7171     0.7045     2.8981     0.1331   
   300       285.32     0.7172     0.7044     2.8838     0.1188   
   400       381.26     0.7172     0.7044     2.9499     0.1125   
   500       475.39     0.7172     0.7044     2.9746     0.1063   
   600       568.76     0.7172     0.7044     2.8539     0.1054   
   700       662.14     0.7172     0.7044     2.6371     0.115    
   800       755.39     0.7172     0.7044     2.7909     0.1025   
   900       848.66     0.7172     0.7044     2.636      0.1004   
   1000      941.76     0.7172     0.7044     2.4842     0.1012   
   1100     1035.18     0.7172     0.7044     2.5668     0.1068   
   1200     1128.52     0.7172     0.7044     2.3273     0.1211   
   1300     1220.89     0.7172     0.7044     2.3435     0.1187   
   1400     1313.52     0.7172     0.7044     2.4162     0.1412   
   1500      1406.4     0.7172     0.7044     2.6414     0.1495   
   1600     1499.79     0.7172     0.7044     2.6472     0.1482   
   1700     1592.93     0.7172     0.7044     1.7236     0.1592   
   1800     1691.03     0.7172     0.7044     2.0524     0.145    
   1900      1789.5     0.7172     0.7044     2.042      0.1374   
   2000     1886.93     0.7172     0.7044     1.6884     0.1661   
   2100     1986.08     0.7172     0.7044     2.2053     0.1144   
   2200     2083.44     0.7172     0.7044     1.8682     0.1226   
   2300     2180.81     0.7172     0.7044     1.6136     0.127    
   2400     2278.18     0.7172     0.7044     1.7067     0.1737   
   2500     2376.56     0.7172     0.7044     1.9387     0.145    
   2600     2475.01     0.7172     0.7044     2.2126     0.1692   
   2700     2573.41     0.7172     0.7044     1.464      0.1924   
   2800     2671.66     0.7172     0.7044     1.9168     0.1367   
   2900     2769.94     0.7172     0.7044     1.3399     0.1416   
   3000     2867.62     0.7172     0.7044     1.4526     0.1224   
   3100     2965.75     0.7172     0.7044     1.3845     0.1388   
   3200     3064.65     0.7172     0.7044     2.1784     0.1567   
   3300     3162.19     0.7172     0.7044     1.2923     0.1135   
   3400     3259.49     0.7172     0.7044     1.9313     0.1219   
   3500      3356.8     0.7172     0.7044     1.7027     0.1173   
   3600     3454.47     0.7172     0.7044     2.155      0.1658   
   3700     3552.55     0.7172     0.7044     1.3212     0.123    
   3800     3650.62     0.7172     0.7044     1.7449     0.1223   
   3900     3748.72     0.7172     0.7044     1.3066     0.0916   
   4000     3846.83     0.7172     0.7044     1.2628     0.1548   
