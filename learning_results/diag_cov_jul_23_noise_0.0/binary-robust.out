Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.265664339065552
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       170.12     1.4063     1.4175     3.1612   48256.6068   0.0008  
   200       409.8      1.4096     1.4113     2.4438   36759.5511   0.0231  
   300       648.57     1.4168     1.4188     2.0068   35684.5933   0.1419  
   400       886.77     1.4153     1.4161     3.1296   6249.3789    0.1933  
   500      1122.54     1.4174     1.4248     2.4854   6174.3547    0.1263  
   600      1356.07     1.4164     1.4132     3.1544   6042.7531    0.0699  
   700      1588.37     1.417      1.4068     3.1064   2621.7133    0.0487  
   800      1820.56     1.4144     1.4145     2.8834   7906.8397    0.0335  
   900      2056.34     1.411      1.4117     1.1757   3013.3564    0.0276  
   1000     2288.01     1.4199     1.4148     1.1449   9052.6881    0.0275  
   1100     2517.36     1.4135     1.4103     3.1157   1202.7316    0.0116  
   1200     2752.05     1.4064     1.4036     2.7093    966.5481    0.0114  
   1300     2987.92     1.4109     1.4085     3.1619   211530.3754   0.0098  
   1400     3223.44     1.413      1.419      2.552    1522.5796    0.011   
   1500     3459.22     1.4118     1.4133     1.1234   58871.571    0.0104  
   1600     3695.53     1.4201     1.4107     3.0963   6326.4374    0.0086  
   1700     3930.76     1.4135     1.4068     0.8222   1152.5241    0.0048  
   1800      4165.9     1.4188     1.4124     3.1422   2942.4351    0.0017  
   1900     4401.21     1.4122     1.4104     1.5476   1083.2311    0.0064  
   2000     4636.61     1.4108     1.4084     0.721    3085.6511    0.0065  
   2100     4872.17     1.4151     1.3997     0.7046   2449.2301    0.0032  
   2200     5107.85     1.4058     1.4139     0.7959    1398.408    0.0045  
   2300     5343.04     1.3943     1.3961     0.8423   11155.3623   0.0032  
   2400     5576.79     1.4015     1.3871     2.5014    799.0946    0.0033  
   2500     5810.05     1.3862     1.3843     0.4583    998.017     0.002   
   2600     6042.67     1.3554     1.3497      0.6      340.0977    0.0022  
   2700      6275.5     1.265      1.2598     0.9967   4549.0813    0.0018  
   2800     6508.47     1.1341     1.1164     1.492      797.35     0.0026  
   2900     6741.36     1.1209     1.1138     3.0571    414.2606    0.0033  
   3000     6974.48     1.1418     1.1249     3.1471   2322.7693    0.0035  
   3100     7208.32     1.0594     1.052      2.4353   9779.3749    0.0041  
   3200      7442.5     0.9533     0.9444     2.3281   1466.4667    0.0043  
   3300     7674.68     0.9383     0.9328     3.1182    415.5375    0.0028  
   3400     7905.74     0.8892     0.8859     3.0191    831.9408    0.0031  
   3500     8136.94     0.8762     0.8688     2.9087    251.4573    0.0017  
   3600     8368.09     0.8429     0.834      1.5597   1455.2527    0.0024  
   3700     8599.26     0.8129     0.802      3.0302    701.1396    0.0029  
   3800     8830.39     0.8101     0.7939     1.2285   5023.9804    0.0014  
   3900     9061.54     0.8067     0.7854     3.161    5204.7299    0.0006  
   4000     9292.72     0.801      0.7836     3.1228   8077.2573    0.0022  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       167.19     1.1959     1.1842     3.1556    647.3699    0.0008  
   200       423.4      1.197      1.1991     2.4793   17566.5635   0.0004  
   300       681.68     0.984      0.9694     1.9999   6054.1092    0.0002  
   400       940.54     0.7993     0.7974     1.6444   4850.5671    0.0001  
   500      1199.52     0.7797     0.7722     1.4216   76501.6131   0.0001  
   600       1457.7     0.7439     0.7363     1.2912   6044.9374     0.0    
   700      1716.05     0.7385     0.7319     1.0372   1759.3516     0.0    
   800      1974.38     0.7359     0.7228     1.0052   5045.4077     0.0    
   900      2232.58     0.7386     0.7257     0.9419   411416.7318    0.0    
   1000     2490.77     0.737      0.7255     0.8292   1681.1367     0.0    
   1100     2748.85     0.7351     0.7248     0.8775   80968.781     0.0    
   1200      3007.0     0.7399     0.7265     1.7182   29071.3318    0.0    
   1300     3265.06     0.7333     0.7192     0.8291    814.9113     0.0    
   1400      3523.1     0.7435     0.7296     2.9412   1232869.0687    0.0    
   1500     3781.28     0.7514     0.7272     1.2513   5939.8559     0.0    
   1600     4039.41     0.7389     0.7308     0.7339   13671.6178    0.0    
   1700     4297.23     0.7319     0.7246     2.3862   4600.0676     0.0    
   1800     4555.12     0.7359     0.7253     2.5114   11160.8005    0.0    
   1900     4812.98     0.7364     0.7289     0.8846   13214.1707    0.0    
   2000     5070.94     0.728      0.7155     0.7452   6118.1304     0.0    
   2100      5329.0     0.731      0.7149     0.8477   10667.6881    0.0    
   2200     5586.69     0.7281     0.7173     0.7926   3175.7233     0.0    
   2300     5842.71     0.7274     0.7205     0.9621   8339.8685     0.0    
   2400     6093.34     0.7273     0.7189     0.7404   1296.8577     0.0    
   2500     6343.18     0.7376     0.724      0.636     724.732      0.0    
   2600     6592.16     0.7312     0.7215     2.7305   2005.4983     0.0    
   2700     6840.82     0.7303     0.7194     1.2302   2758.5524     0.0    
   2800      7090.2     0.7333     0.7231     1.8096   6807.3182     0.0    
   2900     7332.08     0.743      0.7308     0.6946   2380.3711     0.0    
   3000     7569.45     0.7383     0.7257     0.5608   2577.1415     0.0    
   3100     7800.53     0.7317     0.7201     0.6614   1937.0833     0.0    
   3200     8038.75     0.7288     0.7199     0.6977   3215.6716     0.0    
   3300     8272.36     0.7356     0.7237     0.7724   2570.1251     0.0    
   3400     8506.62     0.7661     0.7507     0.8762   285345.5431    0.0    
   3500     8743.06     0.7405     0.727      0.9896   12174.0432    0.0    
   3600     8980.92     0.7411     0.7237     2.2258   3913.2571     0.0    
   3700     9219.05     0.7394     0.726      2.0576   12033.9631    0.0    
   3800     9455.28     0.737      0.7217     0.5557   2011.9044     0.0    
   3900     9686.52     0.7382     0.7251     0.716    2596.6921     0.0    
   4000     9917.62     0.7314     0.7237     0.6104   4284.2139     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       149.73     1.4165     1.4087     3.1556     0.7164     0.0001  
   200       405.32     1.4151     1.414      1.487      2.5397     0.0001  
   300       660.63     1.4084     1.4173     2.5587   1655.1345    0.0001  
   400       917.28     1.4101     1.4132     2.055    1218.6148    0.0001  
   500      1177.22     1.418      1.4122     1.9289   2492.6329     0.0    
   600      1436.82     1.4144     1.4103     2.5906   22373.4303    0.0    
   700      1693.03     1.4088     1.4091     2.2954   3857.1394     0.0    
   800      1950.66     1.4147     1.4095     1.6713   5094.6363     0.0    
   900      2208.61     1.4152     1.4189     1.5777   6658.3822     0.0    
   1000     2465.91     1.4145     1.4141     1.3818   4050.1762     0.0    
   1100     2721.67     1.4214     1.414      1.3415   15608.9045    0.0    
   1200     2977.27     1.4189     1.4188     1.7053   18079.6868    0.0    
   1300     3232.99     1.4214     1.4181     1.2612   6422.3353     0.0    
   1400     3488.69     1.4113     1.4083     2.7343   36673.0441    0.0    
   1500      3744.3     1.4041     1.4163     1.1168   29142.898     0.0    
   1600     4002.87     1.4105     1.412      0.9593   68840.0361    0.0    
   1700     4261.73     1.4157     1.4122     1.0299   29453.8824    0.0    
   1800     4521.71     1.4131     1.4141     0.7222   12927.2078    0.0    
   1900     4780.49     1.4063     1.4101     0.7285   22189.6197    0.0    
   2000      5039.0     1.4199     1.4121     0.816    25566.4486    0.0    
   2100     5297.36     1.414      1.4194     1.0108   161390.6095    0.0    
   2200     5556.37     1.4096     1.4128     0.826    19587.803     0.0    
   2300     5815.51     1.4166     1.4129     0.6529   3997.2653     0.0    
   2400     6077.35     1.4095     1.417      0.8603   21808.6679    0.0    
   2500     6338.76     1.4112     1.4103     2.8956   3725.8871     0.0    
   2600     6599.34     1.4117     1.4178     0.922     793.286      0.0    
   2700     6859.87     1.4022     1.4122     0.7168   8758.0045     0.0    
   2800     7121.05     1.4198     1.4137     0.8884   12274.157     0.0    
   2900     7382.63     1.4179     1.4124     0.995    58671.349     0.0    
   3000      7644.4     1.4167     1.4123     0.9491   56286.6647    0.0    
   3100     7905.53     1.4194     1.4133     0.9557   65094.2045    0.0    
   3200     8166.46     1.4196     1.4127     0.7836   21257.2341    0.0    
   3300     8425.17     1.416      1.4139     0.8858   19788.1834    0.0    
   3400     8681.69     1.4168     1.4146     0.7399   6448.2948     0.0    
   3500     8941.03     1.4181     1.4108     0.717    74180.6869    0.0    
   3600     9199.99     1.4145     1.4179     0.8939   23052.7086    0.0    
   3700     9459.54     1.4098     1.4184     0.8188   22494.9516    0.0    
   3800     9718.53     1.4135     1.4183     0.8118   2820.2849     0.0    
   3900     9977.75     1.4091     1.4144     0.6905   47455.1093    0.0    
   4000     10237.35    1.4073     1.4156     0.5904   16750.0151    0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       152.29     1.4087     1.4171     3.155      0.2112     0.0001  
   200       434.44     1.4164     1.4132     1.1633     0.2535      0.0    
   300       716.52     1.4116     1.4222     1.2713     0.4573      0.0    
   400      1000.13     1.4154     1.4151     1.224      1.3482      0.0    
   500      1283.86     1.4121     1.4104     1.3591     50.309      0.0    
   600      1569.05     1.4164     1.4167     1.4969    723.7879     0.0    
   700      1856.12     1.4032     1.4148     1.5355   36304.2321    0.0    
   800      2145.04     1.4101     1.4165     2.2744    2107.214     0.0    
   900      2434.55     1.4105     1.419      1.8592   2701.6961     0.0    
   1000     2723.96     1.4129     1.4176     1.4195   9983.0982     0.0    
   1100     3012.04     1.4148     1.4128     3.1614   1631.1682     0.0    
   1200     3300.28     1.4049     1.4181     1.4634    3438.663     0.0    
   1300     3588.36     1.4074     1.4158     1.4533   111557.4461    0.0    
   1400     3876.44     1.4038     1.4141     1.3828   6669.5648     0.0    
   1500     4164.72     1.4168     1.4188     1.3859   9571.5854     0.0    
   1600     4454.44     1.4105     1.415      1.3752   44511.3798    0.0    
   1700     4743.34     1.4087     1.4153     1.4645   5468.0193     0.0    
   1800     5030.98     1.4121     1.4138     1.3106   5937.9577     0.0    
   1900     5317.65     1.4099     1.4085     1.7559   6451.5958     0.0    
   2000     5604.37     1.4074     1.4126     1.4805   23865.8153    0.0    
   2100     5891.11     1.4155     1.4097     1.2839   7137.7379     0.0    
   2200      6178.3     1.4143     1.4149     1.202    6796.9362     0.0    
   2300     6463.78     1.4125     1.4138     1.2531   511246.9478    0.0    
   2400     6748.94     1.4157     1.4142     1.3892   60766.0319    0.0    
   2500     7030.66     1.4178     1.416      1.2798   61986.3564    0.0    
   2600     7312.46     1.4134     1.4137     1.2796   23394.1714    0.0    
   2700     7594.25     1.416      1.4121     1.304    22300.4608    0.0    
   2800     7877.12     1.4162     1.4083     1.1555   58769.5055    0.0    
   2900     8165.43     1.4094     1.4127     1.2017   90324.3265    0.0    
   3000     8453.57      1.41      1.4177     1.2211   137686.0616    0.0    
   3100     8742.33     1.4143     1.4102     1.1766   111290.8213    0.0    
   3200     9029.69     1.4183     1.4161     0.8981   39859.2171    0.0    
   3300     9317.52     1.419      1.4141     1.0548    96197.44     0.0    
   3400      9599.4     1.4074     1.4088     0.8131   42466.3857    0.0    
   3500     9881.22     1.4158     1.4118     0.9404   31969.1391    0.0    
   3600     10162.73    1.4127     1.416      0.731    35471.2073    0.0    
   3700     10445.17    1.4107     1.4158     0.8079   46915.3859    0.0    
   3800     10731.96    1.4161     1.413      0.8868   154509.9093    0.0    
   3900     11021.77    1.4131     1.4115     0.943    1174554.662    0.0    
   4000     11306.76    1.4177     1.4164     0.9646   182655.1816    0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       151.74     0.9812     0.9829     3.1571     0.1342     0.0001  
   200       458.92     0.7697     0.7609     1.3203     0.8084      0.0    
   300       765.16     0.7424     0.728      1.1294     7.1864      0.0    
   400      1072.33     0.7447     0.7327     1.0301    151.0394     0.0    
   500      1379.99     0.7428     0.7283     1.0329    338.1293     0.0    
   600      1687.47     0.8427     0.8385     1.0343   1587.1562     0.0    
   700      1994.57     0.997      0.9927     1.0978   1653.3723     0.0    
   800      2302.99     1.3354     1.3316     1.2323   30763.1017    0.0    
   900      2611.82     1.4162     1.4122     1.0208    8888.485     0.0    
   1000     2920.81     1.4209     1.4135     1.3045    2733.516     0.0    
   1100     3232.88     1.4089     1.4197     1.3958   268834.3448    0.0    
   1200     3546.27     1.4195     1.4158     1.0717   5760.8803     0.0    
   1300     3860.07     1.4141     1.4143     1.1055   13949.9297    0.0    
   1400     4173.99     1.4112     1.4159     1.079    65414.0038    0.0    
   1500     4488.67     1.4141     1.4126     1.2188   81068.1714    0.0    
   1600     4804.57     1.4058     1.4162     1.1345    6383.911     0.0    
   1700     5118.72     1.4141     1.4123     1.4736   16076.3022    0.0    
   1800     5431.92     1.411      1.4122     1.2209    2971.63      0.0    
   1900     5743.51     1.4178     1.4148     2.223    35561.1465    0.0    
   2000     6052.93     1.4125     1.413      1.636    91438.6229    0.0    
   2100     6365.72     1.4251     1.4154     1.113    45462.529     0.0    
   2200     6679.87     1.4209     1.4143     3.099    8438.1079     0.0    
   2300     6994.12     1.4118     1.4156     1.4525   28797.664     0.0    
   2400     7309.03     1.4142     1.4137     1.355    83927.2262    0.0    
   2500     7622.02     1.4079     1.4135     2.1302   7181.5055     0.0    
   2600      7936.3     1.4196     1.4118     1.1924   1471.5205     0.0    
   2700     8250.52     1.4143     1.4144     1.1172   12589.0032    0.0    
   2800     8564.77     1.4196     1.4154     1.1703   18391.7308    0.0    
   2900     8877.92     1.4049     1.4148     2.7266   14607.0979    0.0    
   3000     9192.52     1.4224     1.416      1.169    9736.1714     0.0    
   3100     9506.97     1.4178     1.4179     1.1232   169486.4904    0.0    
   3200      9820.2     1.4109     1.4128     1.3158   46270.9989    0.0    
   3300     10132.8     1.4079     1.4147     1.2159   61769.3166    0.0    
   3400     10446.56    1.4041     1.4153     1.223    33692.8448    0.0    
   3500     10760.96    1.417      1.4134     1.1867   33006.0462    0.0    
   3600     11073.85    1.4201     1.4164     1.1721   11163.8772    0.0    
   3700     11387.9     1.4118     1.416      1.2397   31154.5043    0.0    
   3800     11702.45    1.4195     1.4131     1.3841   87871.9813    0.0    
   3900     12017.18    1.4114     1.4172     1.3259   35641.7623    0.0    
   4000     12331.13    1.4089     1.4151     1.3906   505642.6958    0.0    
