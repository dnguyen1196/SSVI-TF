Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  2.7754135131835938
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       138.11    169.7341   170.0819    3.1623     0.7643      0.0    
   200       276.52    125.6068   125.8063    2.5955     0.1741      0.0    
   300       415.82    114.9913   115.0736    2.1374     0.0907      0.0    
   400       557.65    109.9991   110.085     1.5356     0.0687      0.0    
   500       698.33    106.9109   106.6655    1.4581     0.0535      0.0    
   600       838.64    106.0649   105.5408    1.0806     0.0367      0.0    
   700       976.69    105.3842   105.0246    1.0524     0.0322      0.0    
   800      1114.82    105.8538   105.3813    1.0974     0.0349      0.0    
   900      1252.53    104.6269   104.4093    0.9942     0.0334      0.0    
   1000     1390.65    105.1449   104.613     0.7187     0.0265      0.0    
   1100     1530.32    104.7473   104.1867    0.8078     0.0242      0.0    
   1200     1668.67    105.2353   104.6062    0.6708     0.0223      0.0    
   1300     1806.88    104.8513    104.26     0.7771     0.0229      0.0    
   1400     1945.36    105.3795   104.6753    0.7514     0.0243      0.0    
   1500     2084.21    104.8228   104.4124    0.707      0.0211      0.0    
   1600     2224.02    105.238    104.5401    0.7144     0.0193      0.0    
   1700     2365.47    104.6894   104.159     0.7142     0.017       0.0    
   1800     2506.02    104.7054   104.4826    0.7768     0.0174      0.0    
   1900     2645.66    104.7965   104.4777    0.6865     0.0157      0.0    
   2000     2785.06    104.6135   103.8857    0.5811     0.0141      0.0    
   2100     2925.83    104.2864   103.8432    0.7912     0.0132      0.0    
   2200     3064.81    104.5287   104.2773    0.521      0.0122      0.0    
   2300     3204.86    105.0858   104.8228    0.8426     0.0123      0.0    
   2400     3345.73    104.6034   104.3207    0.6236     0.0137      0.0    
   2500     3486.41    104.5949   104.2921    0.6373     0.0154      0.0    
   2600      3627.7    104.354    103.8637    0.5882     0.0118      0.0    
   2700     3768.94    104.9239   104.0776    0.6727     0.0111      0.0    
   2800      3909.9    104.5651   103.9343    0.7153     0.0111      0.0    
   2900     4051.08    104.5562   103.9176    0.8218     0.0101      0.0    
   3000     4192.21    104.8175   104.3325    0.7241     0.0096      0.0    
   3100     4332.44    104.0334   103.7038    0.5146     0.0097      0.0    
   3200     4471.24    104.8021   104.2542    0.4628     0.0088      0.0    
   3300     4610.09    104.5465   103.8857    0.4043     0.0094      0.0    
   3400     4748.75    104.4196   103.9802    0.5009     0.0084      0.0    
   3500     4887.84    104.1697   103.7549    0.5483     0.0089      0.0    
   3600     5027.63    104.5007   104.0454    0.6408     0.0086      0.0    
   3700     5167.22    104.3018   103.7263    0.6032     0.009       0.0    
   3800     5306.86    104.8757    104.06     0.7391     0.0091      0.0    
   3900     5446.78    104.1825   103.7974    0.5276     0.0081      0.0    
   4000      5586.8    104.2884   103.7686    0.6545     0.0075      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       138.26    167.8624   168.5976     3.16      1.2874      0.0    
   200       276.9     121.5467   121.3645    2.5954     0.1506      0.0    
   300       415.82    111.6943   111.0872    2.0001     0.0796      0.0    
   400       555.01    109.0503   108.5618    1.6667     0.0824      0.0    
   500       693.97    107.8344   107.4898    2.0436     0.0853      0.0    
   600       833.75    106.8467   106.4509    1.204      0.0495      0.0    
   700       972.9     106.2001   105.809     1.2908     0.0358      0.0    
   800       1112.1    105.6775   105.1787    0.9964     0.042       0.0    
   900       1250.3    105.6911   105.2413    0.7595     0.0337      0.0    
   1000     1388.88    105.9235   105.1776    0.9533     0.0304      0.0    
   1100     1527.17    105.8823   105.2093    1.0461     0.0262      0.0    
   1200     1664.43    106.0485   105.1892    0.6916     0.0216      0.0    
   1300     1801.38    105.5092   104.7801    0.7556     0.0209      0.0    
   1400     1939.12    105.2162    104.5      0.8034     0.0198      0.0    
   1500     2079.12    105.2288   104.4494    0.6958     0.0208      0.0    
   1600      2217.5    104.5332   104.112     0.6762     0.0192      0.0    
   1700     2355.49    105.6055   105.0626    0.9038      0.02       0.0    
   1800     2493.54    105.075    104.4801    0.9272     0.0202      0.0    
   1900     2631.03    104.621    104.2266    0.7099     0.0161      0.0    
   2000     2768.39    104.7022   104.2296    0.6321     0.0154      0.0    
   2100     2905.84     104.67    104.1578    0.9418     0.0146      0.0    
   2200     3043.23    104.936    104.2095    0.8389     0.0154      0.0    
   2300     3180.26    104.9324   104.3467    0.6538     0.0151      0.0    
   2400      3317.1    105.2707   104.583     0.955      0.0144      0.0    
   2500      3454.2    104.802    104.1671    0.5761     0.0151      0.0    
   2600     3591.47    105.1093   104.546     0.594      0.0114      0.0    
   2700     3728.51    105.0521   104.5545    1.036      0.012       0.0    
   2800     3865.23    104.6803   104.0333    0.8347     0.0134      0.0    
   2900     4001.85    104.3322   103.9119    0.4652     0.0111      0.0    
   3000     4138.44    104.8451   104.1824    0.6611     0.0099      0.0    
   3100     4275.07    104.4852   104.0583    0.6111     0.0101      0.0    
   3200     4411.79    104.3452   103.7747    0.6367     0.0098      0.0    
   3300      4548.4    104.968    104.2875    0.5043     0.0102      0.0    
   3400     4685.17    104.5155   104.3052    0.6233     0.0101      0.0    
   3500     4821.74    104.5454   104.1357    0.7395     0.011       0.0    
   3600     4958.41    104.7198   104.1116    0.6054      0.01       0.0    
   3700     5095.18    104.1935   103.7941    0.5553     0.0087      0.0    
   3800     5231.88    104.548    103.9536    0.7279     0.0098      0.0    
   3900     5368.51    104.5765   104.018     0.7924     0.0085      0.0    
   4000     5505.04    104.5564   103.6423    0.6128     0.0081      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       135.75    177.6989   177.9305    3.1623     1.2464      0.0    
   200       272.52    129.0218   128.6091    2.6584     0.1531      0.0    
   300       409.29    114.5483   114.3509    2.145      0.1247      0.0    
   400       546.44    107.8165   107.4871    1.7933     0.0744      0.0    
   500       683.61    107.2445   106.7817    1.335      0.0625      0.0    
   600       820.75    106.7876   106.242     1.2324     0.0527      0.0    
   700       958.07    105.5565   105.089     0.9743     0.0383      0.0    
   800      1095.61    105.1346   104.7017    0.7419     0.0427      0.0    
   900      1232.84    105.4256   104.8701    0.9216     0.0346      0.0    
   1000     1370.02    105.8499   105.254     1.0402     0.0299      0.0    
   1100     1507.14    105.7234   105.1543    1.0177     0.0223      0.0    
   1200     1644.31    105.4003   105.2428    0.9804     0.0256      0.0    
   1300     1781.64    105.6843   105.2482    0.8894     0.0228      0.0    
   1400     1918.86    105.0295   104.855     0.8606     0.0285      0.0    
   1500     2056.52    104.8322   104.538     0.6945     0.021       0.0    
   1600     2193.89    105.4884   105.0677    0.7198     0.0215      0.0    
   1700     2331.42    105.4351   104.837     0.6326     0.0175      0.0    
   1800     2468.53    104.7805   104.4793    0.7883     0.0239      0.0    
   1900     2605.11    104.5029   104.2333    0.6431     0.0194      0.0    
   2000     2741.92    105.3038   104.6293    0.7659     0.016       0.0    
   2100     2878.56    104.509    104.384     0.7154     0.0158      0.0    
   2200     3014.98    105.1402   104.9931    0.7149     0.0163      0.0    
   2300     3151.12    105.0598   104.7672    0.6918     0.0185      0.0    
   2400     3286.68    104.9159   104.4944    0.8104     0.0164      0.0    
   2500     3422.35    105.1263   104.3865    0.8348     0.0152      0.0    
   2600     3557.95    104.8159   104.5878    0.6611     0.0129      0.0    
   2700     3693.75    104.5307   104.0369    0.6516     0.0128      0.0    
   2800     3829.44    104.8108   104.3104    0.6486     0.0132      0.0    
   2900     3965.06    104.6466   104.283     0.6957     0.0122      0.0    
   3000     4100.68    104.5694   104.1878    0.7282     0.0105      0.0    
   3100     4236.24    104.4476   103.9873    0.7161     0.013       0.0    
   3200     4372.47    104.3486   103.8928    0.5703     0.0112      0.0    
   3300     4509.31    104.3073   103.9022    0.4551     0.0104      0.0    
   3400      4646.1    104.4182   104.2189    0.6557     0.0099      0.0    
   3500     4776.92    104.4487   104.3282    0.644      0.0099      0.0    
   3600      4903.1    104.1561   104.0584    0.5321      0.01       0.0    
   3700      5030.3    104.4582   104.1287    0.5635     0.0109      0.0    
   3800     5160.48    104.1994   103.9272    0.5556     0.0098      0.0    
   3900      5290.6    104.4508   103.9855    0.4814     0.0091      0.0    
   4000     5420.44    104.8042   104.3376    0.6275     0.0087      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       126.97    172.6522   172.5872    3.1622     1.3628      0.0    
   200       255.69    122.9755   123.2635    2.8092     0.2696      0.0    
   300       383.97    112.3571   113.0797    2.4248     0.0843      0.0    
   400       512.26    108.6281   108.9685    1.7529     0.0534      0.0    
   500       641.18    107.1504    107.22     1.1585     0.0426      0.0    
   600       769.56    106.4396   106.5997    1.1197     0.0328      0.0    
   700       898.05    105.7014   105.9121    1.2696     0.0376      0.0    
   800      1026.45    105.928    106.0135    0.787      0.0277      0.0    
   900      1154.67    106.1198   106.2104    1.2945     0.0309      0.0    
   1000     1282.99    106.3683   106.4565    1.0437     0.0345      0.0    
   1100     1411.12    105.7745   105.9951    1.2008     0.0257      0.0    
   1200     1539.52    105.9258   105.8777    1.0665     0.0263      0.0    
   1300     1667.77    105.3423   105.4436    1.1579     0.0248      0.0    
   1400     1795.35    104.943    105.1575    0.6141      0.02       0.0    
   1500     1921.65    105.6831   105.3386    0.8149     0.0177      0.0    
   1600     2047.91    105.1474   105.2047    0.7409     0.0188      0.0    
   1700     2174.21    105.2339   105.1823    0.7246     0.0222      0.0    
   1800     2300.47    105.1833   105.2781    0.6297      0.02       0.0    
   1900     2426.91    105.1105   105.381     0.8828     0.0157      0.0    
   2000     2553.23    104.8608   105.0812    0.8873     0.0165      0.0    
   2100     2679.54    104.4332   104.4016    0.6977     0.0156      0.0    
   2200     2805.85    104.3952    104.43     0.5582     0.0148      0.0    
   2300     2932.18    105.2798   105.0612    0.7692     0.0137      0.0    
   2400     3058.49    104.4569   104.5321    0.6711     0.0168      0.0    
   2500     3185.15    104.815    104.7193    0.6722     0.0142      0.0    
   2600     3311.62    104.4603   104.6441    0.6002     0.0155      0.0    
   2700     3440.23    104.807    104.7679    0.7098     0.0146      0.0    
   2800      3571.2    104.1618   104.4754    0.4859     0.0119      0.0    
   2900     3702.12    104.7422   104.8391    0.9057     0.0123      0.0    
   3000     3831.57    105.1064   104.8118    0.6833     0.0118      0.0    
   3100      3961.0    104.6195   104.449     0.5689     0.0114      0.0    
   3200     4089.39    104.7293   104.6603    0.7109     0.011       0.0    
   3300     4218.48    104.6773   104.7878    0.5641     0.0103      0.0    
   3400     4347.55    105.0757   105.0454    0.6487     0.0114      0.0    
   3500     4476.68    104.3132   104.3369    0.5556     0.0099      0.0    
   3600     4605.66    104.6046   104.7799    0.6659     0.0108      0.0    
   3700     4734.24    104.4884    104.58     0.7032     0.0116      0.0    
   3800      4862.7    104.496    104.5428    0.5242     0.0095      0.0    
   3900     4991.54    104.5108   104.5528    0.6212     0.0088      0.0    
   4000     5120.65    104.561    104.5774    0.4695     0.0111      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       126.99    166.9585   167.1193    3.1592     1.3501      0.0    
   200       255.13    122.0235    121.56     2.5793     0.148       0.0    
   300       384.11    113.1338   113.4761    1.9363     0.0994      0.0    
   400       512.85    109.262    109.6739    1.5768     0.0555      0.0    
   500       641.52    106.1123   106.0619    1.2408     0.0482      0.0    
   600       770.15    105.7359   105.8261    1.1355     0.0447      0.0    
   700       899.04    105.1108   105.1646    1.0362     0.0359      0.0    
   800      1027.55    105.4564   105.3939    0.9815     0.0395      0.0    
   900      1155.89    105.9166   105.7773    1.0183     0.0344      0.0    
   1000     1284.28    104.8756   105.3185    0.9937     0.0305      0.0    
   1100      1413.1    105.5814   105.6747    0.8075     0.0272      0.0    
   1200     1542.01    105.7687   105.8737    0.8976     0.0274      0.0    
   1300     1670.22    105.1736   105.1935    0.6623     0.0258      0.0    
   1400     1796.97    105.0115   104.9096    0.9774     0.0261      0.0    
   1500     1923.69    105.406    105.4612    0.7362     0.0232      0.0    
   1600     2050.43    105.2765   105.2785    0.8018     0.0259      0.0    
   1700     2177.19    104.6436   104.8067    0.6953     0.0198      0.0    
   1800     2303.93    104.3243   104.6795    0.6467     0.0201      0.0    
   1900      2430.7    105.2821   105.3381    0.7781     0.0209      0.0    
   2000     2557.46    104.7415   104.8381    0.6132     0.0148      0.0    
   2100     2684.33    104.6236   104.8594    0.7073     0.0143      0.0    
   2200     2811.42    104.8076   104.9708    0.8229     0.0164      0.0    
   2300     2938.23    104.5588   104.5856    0.6719     0.0144      0.0    
   2400     3065.06    104.5486   104.5863    0.6445     0.0142      0.0    
   2500     3191.87    105.0048   105.1237    0.6542     0.0162      0.0    
   2600     3318.71    104.782    104.697     0.5651     0.0132      0.0    
   2700     3445.57    104.4054   104.4613    0.6285     0.0134      0.0    
   2800     3572.41    104.4576   104.5844    0.634      0.0147      0.0    
   2900     3699.22    104.667    104.7407    0.6267     0.0142      0.0    
   3000     3826.02    104.3841   104.4376    0.6423     0.0126      0.0    
   3100     3952.87    104.6126    104.53     0.4917     0.0115      0.0    
   3200     4079.71    104.5137   104.6252    0.6277     0.0117      0.0    
   3300     4206.59    104.507    104.5066    0.4943     0.0109      0.0    
   3400     4333.58    104.8681   104.8982    0.5694     0.0111      0.0    
   3500     4460.65    104.694     104.69     0.5523     0.0101      0.0    
   3600     4587.66    104.2698   104.5039    0.549      0.0108      0.0    
   3700     4714.66    104.4517   104.5515    0.6604     0.0106      0.0    
   3800     4841.67    104.6545   104.753     0.6982     0.0098      0.0    
   3900     4968.63    104.6059   104.5545    0.7341     0.0098      0.0    
   4000     5095.62    104.552    104.6315    0.5416     0.0091      0.0    
