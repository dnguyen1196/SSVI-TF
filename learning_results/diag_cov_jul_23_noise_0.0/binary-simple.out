Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.581617116928101
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100        92.6      0.7985     0.7605     3.1145     0.7031      0.0    
   200       249.33     0.7265     0.7081     2.9526     0.2166      0.0    
   300       406.34     0.7154     0.6928     3.016      0.1534      0.0    
   400       564.53     0.7105     0.6912     2.9924     0.1248      0.0    
   500       721.58     0.7104     0.696      2.9299     0.0992      0.0    
   600       877.74     0.7143     0.6956     2.892      0.0832      0.0    
   700      1033.56     0.7172     0.707      2.6825     0.0622      0.0    
   800      1189.36     0.7116     0.6946     2.665      0.0528      0.0    
   900      1344.92     0.7125     0.6966     2.936      0.0428      0.0    
   1000     1500.39     0.7155     0.7048     2.5641     0.0334      0.0    
   1100     1655.61     0.7122     0.6997     2.6961     0.0319      0.0    
   1200     1810.87     0.7107     0.6961     2.3043     0.0234      0.0    
   1300     1966.09     0.7158     0.7054     2.5937     0.0242      0.0    
   1400     2121.22     0.7171     0.707      2.3338     0.0199      0.0    
   1500     2276.24     0.7172     0.707      2.4063     0.0187      0.0    
   1600     2429.43     0.7172     0.707      2.3765     0.0166      0.0    
   1700     2582.25     0.7172     0.707      2.7457     0.0151      0.0    
   1800     2735.52     0.7172     0.707      2.3783     0.0141      0.0    
   1900     2888.93     0.7183     0.7021     1.8838     0.0132      0.0    
   2000     3042.68     0.7183     0.7043     2.3525     0.0149      0.0    
   2100     3196.17     0.7172     0.707      1.9659     0.0125      0.0    
   2200     3349.66     0.7133     0.7014     1.9677     0.0108      0.0    
   2300     3503.19     0.7134     0.699      1.8959     0.0107      0.0    
   2400     3656.82     0.7134     0.6967     2.5701     0.0103      0.0    
   2500     3810.44     0.7186     0.7065     1.8698     0.0085      0.0    
   2600     3964.45     0.7125     0.6983     1.5631     0.0098      0.0    
   2700     4118.05     0.7161     0.7051     2.2038     0.0101      0.0    
   2800     4271.58     0.717      0.7071     1.6228     0.0084      0.0    
   2900     4425.04     0.7172     0.707      1.7532     0.0092      0.0    
   3000     4578.39     0.7171     0.707      1.4312     0.0093      0.0    
   3100     4731.75     0.7172     0.707      1.8157     0.0073      0.0    
   3200     4885.14     0.7172     0.707      2.0399     0.0098      0.0    
   3300     5038.93     0.717      0.7071     1.388      0.0065      0.0    
   3400     5192.89     0.7217     0.7078     1.5723     0.0067      0.0    
   3500      5346.8     0.7134     0.7019     1.4304     0.0067      0.0    
   3600     5500.79     0.7145     0.7037     1.6037     0.0064      0.0    
   3700     5654.66     0.7217     0.7078     1.3344     0.0062      0.0    
   3800     5808.67     0.7217     0.7078     1.2697     0.0059      0.0    
   3900      5962.9     0.721      0.7058     1.7976     0.0076      0.0    
   4000     6117.42     0.7146     0.6997     1.4006     0.0067      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       93.17      0.9095     0.8839     3.107      0.1147      0.0    
   200       279.03     0.7272     0.713      3.0237     0.1231      0.0    
   300       467.32     0.7108      0.7       2.9284     0.1155      0.0    
   400       653.55     0.716      0.707      2.9469     0.1073      0.0    
   500       838.52     0.7171     0.7083     2.7654     0.1079      0.0    
   600      1023.11     0.7172     0.7084     2.7637     0.1067      0.0    
   700      1207.17     0.7172     0.7084     2.8553     0.1085      0.0    
   800      1390.97     0.7172     0.7084     2.6546     0.1248      0.0    
   900      1574.75     0.7172     0.7084     2.3329     0.1154      0.0    
   1000     1758.61     0.7172     0.7084     2.3918     0.1239      0.0    
   1100     1942.57     0.7172     0.7084     2.3439      0.12       0.0    
   1200     2126.57     0.7172     0.7084     2.2409     0.1156      0.0    
   1300     2310.32     0.7172     0.7084     2.4593     0.1515      0.0    
   1400     2493.78     0.7172     0.7084     2.2849     0.1475      0.0    
   1500     2677.45     0.7172     0.7084     2.1205     0.1596      0.0    
   1600      2861.8     0.7172     0.7084     2.1021     0.1233      0.0    
   1700     3045.64     0.7172     0.7084     2.0302     0.121       0.0    
   1800     3228.84     0.7172     0.7084     2.3641     0.1167      0.0    
   1900      3411.8     0.7172     0.7084     1.761      0.1546      0.0    
   2000     3594.92     0.7172     0.7084     1.8182     0.1115      0.0    
   2100     3777.98     0.7172     0.7084     1.7171     0.1402      0.0    
   2200     3961.02     0.7172     0.7084     2.1952     0.1446      0.0    
   2300     4144.03     0.7172     0.7084     2.0608     0.1172      0.0    
   2400     4327.04     0.7172     0.7084     1.7736     0.1559      0.0    
   2500     4510.19     0.7172     0.7084     1.6753     0.1684      0.0    
   2600     4693.23     0.7172     0.7084     2.3911     0.1255      0.0    
   2700     4876.35     0.7172     0.7084     1.7714     0.1307      0.0    
   2800     5059.37     0.7172     0.7084     1.7436     0.1402      0.0    
   2900     5242.76     0.7172     0.7084     1.6042     0.1564      0.0    
   3000     5426.33     0.7172     0.7084     1.9353     0.1657      0.0    
   3100     5610.13     0.7172     0.7084     1.4056     0.1382      0.0    
   3200     5793.72     0.7172     0.7084     2.0366     0.1467      0.0    
   3300     5977.31     0.7172     0.7084     1.3587     0.1263      0.0    
   3400     6160.64     0.7172     0.7084     1.4335     0.1624      0.0    
   3500     6344.08     0.7172     0.7084     1.3797     0.1766      0.0    
   3600     6527.32     0.7172     0.7084     1.4959     0.147       0.0    
   3700     6710.26     0.7172     0.7084     1.5392     0.1621      0.0    
   3800     6893.07     0.7172     0.7084     1.2776     0.1105      0.0    
   3900     7075.85     0.7172     0.7084     1.2472     0.1285      0.0    
   4000     7258.68     0.7172     0.7084     1.1777     0.1116      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       92.62      0.8342     0.8164     3.1066     0.1309      0.0    
   200       304.8      0.7165     0.7055     2.9006     0.144       0.0    
   300       518.72     0.717      0.705      2.9032     0.1315      0.0    
   400       731.79     0.7172     0.706      2.8339     0.1184      0.0    
   500       943.5      0.7172     0.706      2.7975     0.1139      0.0    
   600      1154.65     0.7172     0.706      2.6915     0.109       0.0    
   700      1365.58     0.7172     0.706      2.5804     0.1152      0.0    
   800      1561.85     0.7172     0.706      2.6776     0.128       0.0    
   900      1751.54     0.7172     0.706      2.6332     0.1228      0.0    
   1000     1939.42     0.7172     0.706      2.7096     0.1142      0.0    
   1100     2130.61     0.7172     0.706      2.5271     0.1287      0.0    
   1200     2319.92     0.7172     0.706      2.8199     0.1474      0.0    
   1300     2506.25     0.7172     0.706      1.9893     0.1457      0.0    
   1400     2691.09     0.7172     0.706      2.4484     0.1504      0.0    
   1500     2874.43     0.7172     0.706      2.7314     0.1278      0.0    
   1600     3062.09     0.7172     0.706      2.047      0.1325      0.0    
   1700     3249.25     0.7172     0.706      1.9452     0.1565      0.0    
   1800     3435.91     0.7172     0.706      2.0667     0.1384      0.0    
   1900     3624.23     0.7172     0.706      2.2863     0.1195      0.0    
   2000     3813.23     0.7172     0.706      2.0808     0.1362      0.0    
   2100     4001.58     0.7172     0.706      2.7934     0.136       0.0    
   2200     4185.75     0.7172     0.706      2.3498     0.1492      0.0    
   2300     4368.62     0.7172     0.706      1.476      0.1445      0.0    
   2400     4551.39     0.7172     0.706      1.9837     0.1846      0.0    
   2500     4734.16     0.7172     0.706      1.7925     0.1815      0.0    
   2600     4916.97     0.7172     0.706      1.434       0.14       0.0    
   2700     5099.68     0.7172     0.706      1.6538     0.1351      0.0    
   2800     5282.41     0.7172     0.706      1.3519     0.1629      0.0    
   2900     5465.21     0.7172     0.706      1.8889     0.1732      0.0    
   3000     5648.21     0.7172     0.706      1.5274     0.191       0.0    
   3100     5830.89     0.7172     0.706      1.856      0.116       0.0    
   3200     6013.47     0.7172     0.706      1.4613     0.1113      0.0    
   3300     6196.17     0.7172     0.706      2.0622     0.0977      0.0    
   3400     6379.53     0.7172     0.706      1.5953     0.1257      0.0    
   3500     6570.28     0.7172     0.706      1.2746     0.1082      0.0    
   3600     6759.99     0.7172     0.706      1.5506      0.09       0.0    
   3700     6949.17     0.7172     0.706      1.2922     0.1052      0.0    
   3800     7135.82     0.7172     0.706      2.305      0.095       0.0    
   3900     7321.91     0.7172     0.706      1.3678     0.094       0.0    
   4000     7507.07     0.7172     0.706      1.3059     0.0838      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       81.13      0.8017     0.7826     3.1007     0.1235     0.0001  
   200       292.51      0.72      0.7081     2.9654     0.1364      0.0    
   300       505.07     0.7121     0.6999     2.8317     0.1271      0.0    
   400       716.37     0.7171     0.7029     2.8178     0.1193      0.0    
   500       925.63     0.7172     0.7035     2.7978     0.1161      0.0    
   600      1134.13     0.7172     0.7035     2.6237     0.1193      0.0    
   700      1342.51     0.7172     0.7035     2.7627     0.1133      0.0    
   800      1551.33     0.7172     0.7035     2.6889     0.1222      0.0    
   900      1762.07     0.7172     0.7035     2.6191     0.1337      0.0    
   1000     1972.71     0.7172     0.7035     2.5628     0.1566      0.0    
   1100     2180.65     0.7172     0.7035     2.4526     0.126       0.0    
   1200     2388.04     0.7172     0.7035     2.757      0.1166      0.0    
   1300      2598.9     0.7172     0.7035     2.2239     0.1401      0.0    
   1400     2809.68     0.7172     0.7035     2.3738     0.1515      0.0    
   1500     3020.82     0.7172     0.7035     2.4968     0.1269      0.0    
   1600     3231.86     0.7172     0.7035     2.2283     0.1364      0.0    
   1700     3442.27     0.7172     0.7035     2.2516     0.1455      0.0    
   1800      3649.6     0.7172     0.7035     1.8861     0.1566      0.0    
   1900     3860.68     0.7172     0.7035     1.7837     0.1494      0.0    
   2000     4073.81     0.7172     0.7035     2.0984     0.1175      0.0    
   2100     4287.51     0.7172     0.7035     1.7984     0.1239      0.0    
   2200     4500.15     0.7172     0.7035     2.4568     0.1552      0.0    
   2300     4711.58     0.7172     0.7035     2.2078     0.1544      0.0    
   2400     4922.29     0.7172     0.7035     1.5232     0.1353      0.0    
   2500      5133.6     0.7172     0.7035     1.7965     0.1349      0.0    
   2600     5345.02     0.7172     0.7035     1.7931     0.1638      0.0    
   2700      5555.3     0.7172     0.7035     2.1356     0.1538      0.0    
   2800     5763.02     0.7172     0.7035     1.5813     0.1716      0.0    
   2900     5970.51     0.7172     0.7035     1.5982     0.1491      0.0    
   3000     6177.76     0.7172     0.7035     2.0627     0.148       0.0    
   3100      6388.8     0.7172     0.7035     2.1322     0.1731      0.0    
   3200     6600.54     0.7172     0.7035     1.7184     0.1432      0.0    
   3300     6811.34     0.7172     0.7035     1.3408     0.1303      0.0    
   3400      7020.3     0.7172     0.7035     1.4117     0.1148      0.0    
   3500     7229.19     0.7172     0.7035     1.5222     0.1091      0.0    
   3600      7442.4     0.7172     0.7035     1.6218     0.1162      0.0    
   3700     7654.83     0.7172     0.7035     1.8261     0.1417      0.0    
   3800     7868.01     0.7172     0.7035     1.2099     0.1045      0.0    
   3900     8079.63     0.7172     0.7035     1.7496     0.1513      0.0    
   4000     8288.26     0.7172     0.7035     1.2785     0.1286      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       84.61      1.0767     1.0634     3.0812     0.1128      0.0    
   200       343.59     0.7179     0.7087     2.9136     0.1325      0.0    
   300       604.93     0.717      0.7042     2.8562     0.118       0.0    
   400       866.5      0.7172     0.7044     2.9001     0.1082      0.0    
   500      1098.44     0.7172     0.7044     2.9202     0.1092      0.0    
   600      1329.74     0.7206     0.7072     3.0448     0.1033      0.0    
   700      1563.66     0.7172     0.7044     2.8264     0.1016      0.0    
   800      1798.04     0.7172     0.7044     2.6106     0.0969      0.0    
   900      2034.07     0.7172     0.7044     2.4957     0.114       0.0    
   1000     2270.64     0.7172     0.7044     2.5575     0.1292      0.0    
   1100     2509.25     0.7172     0.7044     2.2176     0.1152      0.0    
   1200     2745.73     0.7172     0.7044     2.2472     0.1052      0.0    
   1300     2981.27     0.7172     0.7044     2.2077     0.1154      0.0    
   1400      3216.1     0.7172     0.7044     2.1635     0.1443      0.0    
   1500     3450.68     0.7172     0.7044     2.6302     0.1458      0.0    
   1600     3686.25     0.7172     0.7044     2.0726      0.16       0.0    
   1700     3922.48     0.7172     0.7044     1.9409     0.1437      0.0    
   1800     4156.73     0.7172     0.7044     1.7944     0.1892      0.0    
   1900     4392.12     0.7172     0.7044     2.3524     0.151       0.0    
   2000     4628.38     0.7172     0.7044     1.894      0.1331      0.0    
   2100     4863.18     0.7172     0.7044     2.2511     0.1432      0.0    
   2200     5098.88     0.7172     0.7044     2.0648     0.1374      0.0    
   2300      5335.4     0.7172     0.7044     2.2293     0.1622      0.0    
   2400     5571.72     0.7172     0.7044     2.1415     0.1476      0.0    
   2500     5809.22     0.7172     0.7044     1.6918     0.1258      0.0    
   2600     6045.94     0.7172     0.7044     2.175      0.1445      0.0    
   2700     6280.83     0.7172     0.7044     1.8307     0.1261      0.0    
   2800     6515.84     0.7172     0.7044     1.8261     0.1279      0.0    
   2900     6748.84     0.7172     0.7044     1.597      0.1041      0.0    
   3000     6980.13     0.7172     0.7044     1.5733     0.1781      0.0    
   3100     7216.32     0.7172     0.7044     1.6883     0.152       0.0    
   3200      7452.6     0.7172     0.7044     1.5079     0.1212      0.0    
   3300     7688.46     0.7172     0.7044     1.533      0.1104      0.0    
   3400     7923.61     0.7172     0.7044     1.4208     0.1092      0.0    
   3500     8160.46     0.7172     0.7044     1.499      0.1375      0.0    
   3600     8395.32     0.7172     0.7044     1.5184     0.0972      0.0    
   3700     8630.94     0.7172     0.7044     1.8613     0.1306      0.0    
   3800     8865.49     0.7172     0.7044     1.6588     0.1495      0.0    
   3900     9100.36     0.7172     0.7044     1.364      0.1069      0.0    
   4000     9335.28     0.7172     0.7044     1.591      0.0862      0.0    
