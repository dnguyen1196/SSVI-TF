Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  8.69998574256897
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       104.4      3.8725     3.9064     0.2825     0.2259    57822.36   46697.45  
   200       340.72     3.9142     3.9532     0.2217     0.1631    56273.27   45436.18  
   300       576.2      3.9529     3.9808     0.2092     0.1222    55231.76   44586.36  
   400       811.22     3.9814     4.0113     0.1861     0.0974    54467.95   43962.32  
   500      1046.43     4.0065     4.0355     0.1664     0.082     53877.87   43480.63  
   600      1277.81     4.0212     4.054      0.1398     0.069     53402.29   43093.16  
   700      1508.06     4.0381     4.0695     0.1378     0.0601    53010.63   42774.39  
   800       1737.4     4.0495     4.0839     0.1266     0.0523    52686.58   42511.12  
   900      1971.19     4.0634     4.0965     0.1173     0.0463    52413.58   42289.66  
   1000      2203.7     4.0749     4.1041     0.1165     0.0414    52185.08   42104.73  
   1100     2432.91     4.0814     4.1122     0.1111     0.0374    51989.02   41946.22  
   1200     2665.44     4.0895     4.1203     0.1086     0.0352    51821.23   41810.84  
   1300     2901.08     4.0962     4.1279     0.1069     0.032     51682.53   41699.09  
   1400      3136.8     4.1033     4.134      0.0966     0.0273    51564.61   41604.17  
   1500     3372.42     4.104      4.139      0.1089     0.0269    51460.4    41520.6   
   1600     3608.74     4.1023     4.1346     0.1112     0.0251    51356.82   41437.49  
   1700     3845.17     4.0834     4.115      0.1261     0.0227    51212.37   41322.01  
   1800     4080.92     4.0381     4.0708     0.1368     0.0225    50964.4    41123.89  
   1900     4316.41     3.9547     3.9857     0.138      0.0209    50588.1    40823.79  
   2000     4551.82     3.818      3.8511     0.1608     0.0214    50128.96   40458.09  
   2100     4787.43     3.6152     3.642      0.1777     0.0201    49698.9    40117.95  
   2200     5023.13     3.3145     3.3463     0.1834     0.0194    49453.41   39923.82  
   2300     5258.61     2.9364     2.9579     0.1915     0.019     49414.15   39890.35  
   2400     5493.77     2.5153     2.5348     0.177      0.0166    49451.18   39922.38  
   2500     5728.89     2.182      2.2042     0.1876     0.0163    49656.92   40106.26  
   2600      5963.6     2.0979     2.1219     0.1827     0.0162    50275.86   40620.63  
   2700     6198.12     2.3793     2.3886     0.1807     0.015     51205.89   41370.0   
   2800      6432.5     2.9342     2.9479     0.144      0.014     52370.05   42310.65  
   2900     6666.92     3.6387     3.6511     0.1423     0.0138    53719.42   43406.92  
   3000     6901.36     4.4261     4.463      0.1786     0.0135    55511.59   44857.88  
   3100     7134.75     5.1931     5.2159     0.2078     0.0123    56966.08   46046.76  
   3200     7367.22     5.977      6.007      0.1239     0.012     58299.25   47174.73  
   3300     7600.03     6.7428     6.7834     0.2189     0.012     59026.77   47805.19  
   3400     7833.12     7.4828     7.5369     0.2092     0.0114    58963.4    47722.53  
   3500     8066.51     8.1923     8.2569     0.1536     0.0104    58329.66   47211.31  
   3600     8300.05     8.8941     8.962      0.1603     0.0105    57623.45   46612.34  
   3700      8533.4     9.582      9.6463     0.1043     0.0103    57106.42   46187.09  
   3800     8766.51    10.1954    10.2662     0.1575      0.01     56479.83   45637.93  
   3900     8999.98    10.7633    10.8372     0.1638     0.0094    55933.97   45209.99  
   4000     9232.88    11.2363    11.3243     0.1573     0.0093    55926.9    45186.87  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       106.05     3.8258     3.8432     0.2795     0.0845    59813.1    96784.98  
   200       395.69     3.8394     3.8579     0.2292     0.0768    59120.16   95653.27  
   300       685.35     3.8593     3.8719     0.1967     0.0692    58484.96   94615.31  
   400       974.87     3.877      3.8898     0.1762     0.071     57899.1    93656.51  
   500      1264.14     3.8915     3.9063     0.1578     0.063     57365.89   92782.91  
   600      1553.72     3.9071     3.9233     0.1538     0.0568    56881.31   91988.11  
   700      1843.01     3.9193     3.9342     0.1476     0.0535    56440.36   91264.7   
   800      2132.45     3.9285     3.9473     0.1337     0.0504    56033.98   90597.25  
   900      2421.62     3.9461     3.9608     0.1214     0.0477    55662.97   89988.17  
   1000     2710.74     3.9553     3.9736     0.117      0.0462    55319.25   89423.93  
   1100     3000.15     3.9627     3.9865     0.1263     0.0446    55001.9    88902.75  
   1200     3289.53     3.9758     3.9986     0.1077     0.043     54706.73   88417.96  
   1300     3578.81     3.9855     4.0073     0.1011     0.041     54435.16   87971.87  
   1400     3868.17     3.9976     4.0156     0.0972     0.0387    54179.35   87552.14  
   1500      4157.0     4.0077     4.0258     0.096      0.0345    53943.38   87165.02  
   1600     4445.81     4.0119     4.0338     0.0878     0.0336    53721.94   86802.12  
   1700     4734.71     4.0211      4.04      0.0962     0.0311    53513.31   86460.13  
   1800     5023.75     4.0299     4.0487     0.0966     0.0313    53320.03   86143.37  
   1900     5312.95     4.0388     4.057      0.0879     0.0291    53138.76   85846.64  
   2000     5601.86     4.0463     4.0659     0.0823     0.0284    52971.17   85572.2   
   2100     5887.69     4.0505     4.0712     0.086      0.0266    52812.61   85312.78  
   2200     6169.24     4.0569     4.0766     0.0841     0.0248    52665.64   85072.53  
   2300     6449.29     4.0648     4.0832     0.076      0.0257    52527.98   84847.75  
   2400     6729.64     4.068      4.0881     0.0788     0.0229    52400.04   84638.56  
   2500     7008.91     4.0725     4.0945     0.0748     0.023     52280.73   84443.69  
   2600     7283.43     4.0777     4.0989     0.0683     0.0221    52170.44   84263.65  
   2700     7550.44     4.0809     4.1038     0.0698     0.0198    52067.33   84095.39  
   2800     7809.96     4.0858     4.1085     0.0729     0.0192    51972.01   83939.75  
   2900      8076.3     4.0934     4.1136     0.0657     0.0187    51883.01   83794.38  
   3000     8336.95     4.094      4.1162     0.0635     0.0191    51800.39   83659.54  
   3100     8600.16     4.0993     4.1209     0.0666     0.0173    51725.2    83536.74  
   3200     8867.16     4.1026     4.1247     0.0656     0.0168    51655.29   83422.64  
   3300     9134.36     4.1059     4.1291     0.0635     0.0161    51590.06   83315.96  
   3400     9400.94     4.1102     4.1322     0.0633     0.0159    51531.02   83219.27  
   3500     9660.27     4.1131     4.1361     0.0611     0.0151    51477.38   83131.47  
   3600     9919.58     4.1181     4.1376     0.0621     0.0151    51427.29   83049.48  
   3700     10179.4     4.1198     4.1409     0.0596     0.0141    51381.9    82975.17  
   3800     10442.75    4.1203     4.1421     0.0644     0.0136    51337.18   82902.02  
   3900     10702.13    4.1184     4.1412     0.0747     0.0136    51283.25   82814.26  
   4000     10961.54    4.1137     4.1337     0.0772     0.0126    51202.02   82683.26  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       96.82      3.8181     3.8325     0.2754     0.0602    60021.35  145098.49  
   200       411.75     3.8314     3.8424     0.2355     0.0564    59491.88  143809.25  
   300       723.62     3.845      3.8575     0.2075     0.0555    58989.73  142585.45  
   400      1037.58     3.8555     3.8692     0.1777     0.0506    58517.36  141433.76  
   500      1348.72     3.8624     3.8816     0.1652     0.0511    58071.33  140346.93  
   600      1659.18     3.8831     3.8884     0.1775     0.0466    57646.19  139309.46  
   700      1970.01     3.8965     3.9021     0.1347     0.0462    57248.95  138340.01  
   800      2280.89     3.902      3.9145     0.1326     0.0444    56872.94  137421.89  
   900      2591.27     3.9126     3.9283     0.1279     0.0458    56520.84  136561.43  
   1000     2904.78     3.9249     3.9373     0.1188     0.0428    56187.35   135746.1  
   1100     3218.66     3.9357     3.9457     0.1206     0.0396    55876.98  134986.71  
   1200     3533.95     3.9487     3.956      0.1179     0.0416    55583.04  134267.07  
   1300     3846.99     3.9577     3.9675     0.1036     0.0367    55309.24  133596.86  
   1400     4160.38     3.9627     3.977      0.1096     0.0356    55049.76  132961.77  
   1500     4474.42     3.9744     3.9845     0.1034     0.0347    54804.96  132362.44  
   1600     4790.41     3.9786     3.9935     0.092      0.0344    54573.96  131796.95  
   1700     5107.65     3.9864     3.9997     0.0902     0.0324    54354.45   131259.9  
   1800     5424.22     3.9967     4.0083     0.0902     0.0298    54146.24  130750.55  
   1900     5740.54     4.0019     4.015      0.0863     0.0299    53951.08  130273.21  
   2000      6057.4     4.0091     4.0222     0.1007     0.0277    53763.8   129815.46  
   2100     6374.49     4.0174     4.0286     0.0856     0.0281    53587.08  129383.28  
   2200     6691.66     4.024      4.0365     0.0799     0.0257    53419.87  128975.22  
   2300     7008.44     4.0289     4.0437     0.0793     0.0251    53261.08  128587.55  
   2400     7322.27     4.0371     4.0492     0.0807     0.0242    53109.2   128217.02  
   2500     7635.84     4.0417     4.0543     0.0745     0.0242    52965.18   127865.7  
   2600     7950.38     4.0485     4.0597     0.0781     0.0221    52829.58  127535.21  
   2700     8264.94     4.0529     4.0669     0.0781     0.0224    52700.77  127221.49  
   2800     8579.91     4.0558     4.0714     0.0715     0.0215    52578.51  126923.89  
   2900     8894.78     4.0615     4.0765     0.0766     0.0212    52462.31  126641.15  
   3000     9209.84     4.0676     4.0814     0.0694      0.02     52352.43   126374.0  
   3100      9525.3     4.0729     4.0846     0.0757     0.0205    52248.9   126122.16  
   3200     9840.02     4.0765     4.0892     0.0665     0.0196    52151.05  125884.14  
   3300     10155.62    4.0802     4.0939     0.0611     0.0181    52059.65  125661.94  
   3400     10472.52    4.0839     4.0975     0.0652     0.0174    51973.15  125451.77  
   3500     10789.78    4.0874     4.102      0.0611     0.0167    51891.53  125253.37  
   3600     11106.55    4.0924     4.1075     0.063      0.0164    51814.73  125066.77  
   3700     11425.16    4.0952     4.1083     0.0593     0.0162    51742.77   124892.0  
   3800     11743.67    4.1003     4.1121     0.0585     0.0148    51674.99  124727.34  
   3900     12061.55    4.1022     4.1162     0.0581     0.0146    51612.39  124575.21  
   4000     12378.94    4.106      4.1198     0.0573     0.0148    51554.39  124434.14  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       97.15      3.818      3.8278     0.2783      0.05     60114.14  193906.43  
   200       466.43     3.8273     3.8348     0.2371     0.051     59670.65  192465.83  
   300       836.47     3.8437     3.8458     0.2187     0.044     59246.79  191087.95  
   400      1208.02     3.8483     3.8563     0.1784     0.0481    58834.75  189749.58  
   500      1576.99     3.856      3.8671     0.1886     0.0424    58439.81  188466.26  
   600      1944.36     3.8686     3.8771     0.1571     0.043     58064.2   187244.36  
   700      2311.99     3.877      3.8885     0.1496     0.0423    57707.84  186084.56  
   800       2679.6     3.8926     3.8986     0.1371     0.0403    57367.77  184978.54  
   900      3045.01     3.8983     3.9113     0.1305     0.0382    57044.56   183926.1  
   1000     3409.96     3.9086     3.9154     0.1328     0.0386    56736.53  182922.08  
   1100     3770.81     3.9166     3.9266     0.1199     0.0355    56444.23  181969.08  
   1200     4131.67     3.927      3.9346     0.1036     0.0349    56164.91  181058.07  
   1300     4493.98     3.9352     3.9453     0.1305     0.0343    55898.84  180190.11  
   1400     4863.56     3.9399     3.9528     0.1151     0.0329    55646.1   179365.42  
   1500     5232.57     3.949      3.9592     0.0995     0.031     55406.35   178583.1  
   1600     5601.13     3.9604     3.9667     0.1013     0.0314    55177.56  177836.44  
   1700     5968.76     3.966      3.974       0.1       0.0295    54959.09  177123.78  
   1800     6330.84     3.9722     3.9839     0.0997     0.029     54749.68  176440.46  
   1900     6692.48     3.9817     3.9901     0.0944     0.0284    54550.85  175791.73  
   2000     7054.19     3.9852     3.9983     0.0879     0.0273    54360.48  175170.77  
   2100     7422.73     3.9918     4.0035     0.0865     0.025     54179.72  174581.22  
   2200     7792.83     3.9991     4.0103     0.0865     0.025     54006.03  174014.78  
   2300     8157.86     4.0084     4.016      0.0767     0.0247    53840.11  173474.03  
   2400     8524.14     4.0132     4.022      0.0765     0.0243    53682.24  172959.53  
   2500     8889.24     4.0162      4.03      0.0741     0.0247    53530.75  172466.05  
   2600     9256.18     4.0236     4.0353     0.0777     0.023     53386.11  171995.29  
   2700     9622.84     4.0292     4.0399     0.0686     0.0213    53248.01  171545.57  
   2800     9988.98     4.0314     4.0461     0.0678     0.0225    53114.37  171110.81  
   2900     10355.44    4.0366     4.0511     0.0702     0.0199    52987.59  170698.62  
   3000     10717.88    4.0461     4.0567     0.0661     0.0197    52865.98  170303.32  
   3100     11083.57    4.0479     4.0605     0.0674     0.0199    52749.32   169924.3  
   3200     11452.61    4.0544     4.0642     0.0678     0.019     52637.66  169561.57  
   3300     11822.96    4.0571     4.0694     0.0669     0.0189    52531.77  169217.52  
   3400     12194.03    4.0613     4.0744     0.0661     0.0187    52430.42   168888.3  
   3500     12565.74    4.0693     4.0775     0.0658     0.0178    52333.45  168573.41  
   3600     12937.17    4.071      4.0821     0.0634     0.0164    52241.41  168274.53  
   3700     13307.33    4.0744     4.0861     0.0648     0.0174    52153.6   167989.42  
   3800     13674.74    4.0788     4.0899     0.0623     0.0161    52070.98  167720.87  
   3900     14040.64    4.0807     4.0933     0.0577     0.0163    51992.41   167465.9  
   4000     14408.48    4.0848     4.0966     0.0595     0.0151    51917.03   167221.3  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       97.35      3.8099     3.8312     0.2746     0.0439    60168.06  242670.99  
   200       521.16     3.8242     3.8422     0.2357     0.0404    59777.12  241084.16  
   300       943.37     3.839      3.8507     0.2127     0.0398    59397.78  239543.96  
   400      1366.25     3.839      3.8591     0.1766     0.041     59027.64  238041.99  
   500      1789.86     3.8554     3.867      0.1748     0.0399    58673.98  236605.96  
   600      2212.15     3.8641     3.8775     0.1676     0.0373    58332.64  235219.64  
   700       2635.5     3.871      3.8848     0.1401     0.0357    58007.44  233898.53  
   800      3058.86     3.8812     3.8943     0.1358     0.0369    57691.57  232615.34  
   900      3480.57     3.8892     3.9023     0.1389     0.035     57390.44  231391.34  
   1000     3903.26     3.8987     3.9118     0.1202     0.0328    57101.76  230216.94  
   1100     4325.36     3.9012     3.9184     0.1152     0.0335    56826.25  229095.62  
   1200     4748.06     3.9128      3.93      0.1093     0.0324    56560.6   228014.15  
   1300     5171.82     3.9202     3.9355     0.1153     0.0307    56307.49  226983.45  
   1400     5594.89     3.9297     3.9436     0.1042     0.0315    56063.75   225990.3  
   1500     6009.24     3.9352     3.9509     0.1075     0.0302    55831.32  225043.46  
   1600     6426.08     3.9426     3.9573     0.0943     0.0286    55608.69  224136.24  
   1700     6845.56     3.9498     3.9664     0.0934     0.0275    55394.91  223265.13  
   1800     7263.61     3.9576     3.9753     0.0994     0.0285    55187.94  222421.95  
   1900     7682.16     3.9667     3.979      0.0943     0.0277    54992.1   221624.22  
   2000     8100.69     3.9711     3.9863     0.0843     0.0262    54802.03  220849.86  
   2100     8514.45     3.9743     3.9941     0.0832     0.0266    54620.89  220111.58  
   2200     8929.03     3.9849     3.9984     0.0844     0.0242    54446.9   219402.48  
   2300     9345.75     3.9892     4.0057     0.0825     0.0237    54279.89  218722.25  
   2400     9761.09     3.9938     4.0138     0.0761     0.0224    54119.91  218070.48  
   2500     10165.44     4.0       4.0162     0.0815     0.0237    53967.13  217448.28  
   2600     10564.27    4.0049     4.0236     0.0778     0.0224    53818.79  216844.37  
   2700     10962.42    4.0102     4.0289     0.0692     0.0211    53675.99  216263.24  
   2800     11360.65    4.016      4.0335     0.0717     0.0213    53539.09  215705.49  
   2900     11758.99    4.0218     4.0399     0.074      0.0209    53407.38  215169.67  
   3000     12156.97    4.0275     4.0448     0.0733     0.0207    53280.52  214653.94  
   3100     12557.91    4.0327     4.0499     0.0671     0.0191    53159.28  214160.85  
   3200     12973.65    4.0353     4.0543     0.0664     0.0193    53041.7   213683.08  
   3300     13392.87    4.0387     4.0584     0.0695     0.0188    52928.94  213224.92  
   3400     13810.98    4.0453     4.0624     0.0647     0.0177    52820.53  212784.46  
   3500     14227.24    4.0489     4.0676     0.0628     0.0182    52716.16  212360.45  
   3600     14643.05    4.0549     4.071      0.0621     0.0169    52616.69  211956.59  
   3700     15078.07    4.0569     4.0752     0.0611     0.0163    52521.01  211567.78  
   3800     15510.31    4.0626     4.0801     0.0631     0.0164    52429.25  211195.09  
   3900     15943.77    4.068      4.082      0.0587     0.0159    52341.2   210837.39  
   4000     16377.08     4.07      4.0874     0.0574     0.0174    52257.04  210495.82  
