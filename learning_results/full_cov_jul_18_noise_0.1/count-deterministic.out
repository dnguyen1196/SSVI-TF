Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  9.240352153778076
max_count =  19  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       180.7      4.2621     4.3008     0.3127     0.2657   128255.98  104137.55  
   200       544.83     4.2106     4.2491     0.2959     0.0233   119567.72   97058.47  
   300       908.46     4.0211     4.0579     0.2807     0.0215    97974.87   79505.49  
   400      1272.86     3.6226     3.6564     0.2606     0.0164    71764.01   58207.03  
   500      1635.97     3.1124     3.1424     0.2277     0.0106    52261.39   42383.47  
   600      1999.04     2.6433     2.667      0.1871     0.0111    41155.43   33354.86  
   700      2362.53     2.2697     2.2879     0.1875     0.012     35311.16   28599.24  
   800      2726.86     1.9809     1.9947     0.1687     0.0113    32049.64   25939.01  
   900      3091.44     1.7627     1.7733     0.1994     0.0112    30169.73   24402.93  
   1000     3453.97     1.5937     1.6031     0.0866     0.0108    28971.83   23437.18  
   1100     3814.79     1.4692     1.4758     0.1624     0.0096    28252.53   22845.44  
   1200     4174.94     1.3772     1.3803     0.1402     0.0096    27749.52   22440.2   
   1300      4535.6     1.3068     1.3077     0.1854     0.0089    27426.98   22179.08  
   1400     4897.39     1.2537     1.2535     0.1321     0.0082    27199.9    21987.25  
   1500     5258.95     1.2138      1.21      0.0935     0.0074    27037.03   21850.15  
   1600     5620.26     1.183      1.1767     0.1692     0.0069    26926.2    21757.87  
   1700     5980.75     1.1568     1.1513     0.0921     0.0065    26835.75   21684.07  
   1800     6341.12     1.1389     1.132      0.1207     0.006     26769.91   21635.18  
   1900     6702.78     1.1241     1.1152     0.0897     0.0058    26706.8    21590.86  
   2000     7063.11     1.1092     1.1037     0.1072     0.0053    26667.25   21557.44  
   2100      7424.4     1.1002     1.0922     0.0973     0.0055    26645.2    21530.66  
   2200     7784.47     1.0914     1.0833     0.1161     0.0048    26610.97   21504.8   
   2300     8145.32     1.0852     1.0775     0.086      0.0043    26596.89   21496.62  
   2400     8505.92     1.0796     1.0704     0.1528     0.0045    26578.42   21475.76  
   2500     8867.37     1.0752     1.0636     0.0926     0.0043    26559.72   21458.43  
   2600     9228.05     1.0711     1.0607     0.1134     0.004     26558.97   21454.83  
   2700     9588.68     1.0679     1.0603     0.1187     0.0038    26534.58   21441.05  
   2800     9949.05     1.0634     1.0557     0.1147     0.0039    26528.43   21433.49  
   2900     10311.06    1.062      1.0536     0.0794     0.0033    26528.4    21428.79  
   3000     10671.06    1.0592     1.0496     0.1239     0.0034    26510.63   21421.14  
   3100     11031.55    1.0591     1.0462     0.0945     0.0034    26507.81   21415.44  
   3200     11392.4     1.0586     1.047      0.0963     0.0038    26499.44   21422.02  
   3300     11754.0     1.0549     1.0463     0.1414     0.0029    26514.79   21422.59  
   3400     12115.35    1.0539     1.0436     0.109      0.0029    26503.94   21412.6   
   3500     12476.11    1.0538     1.0432     0.1207     0.0029    26504.92   21419.68  
   3600     12836.72    1.0518     1.0422     0.0867     0.0029    26500.97   21412.85  
   3700     13197.8     1.0511     1.0386     0.0947     0.0024    26497.9    21410.92  
   3800     13558.85    1.0509     1.0396     0.0749     0.0027    26488.83   21401.34  
   3900     13919.76    1.0502     1.0372     0.0552     0.0023    26490.1    21403.86  
   4000     14281.04    1.0499     1.0383     0.0807     0.0024    26489.5    21406.68  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       179.61     4.2352     4.2595     0.0437     0.9497    93906.34  151911.39  
   200       622.0      4.2571     4.2816     0.0264     0.2935   112543.17  182058.42  
   300       1065.1     4.2614     4.2856     0.0259     0.1659   118733.31  192044.58  
   400      1507.02     4.2629     4.2875     0.0234     0.1107    121625.6  196707.45  
   500      1948.67     4.2636     4.288      0.0237     0.0784   123270.03  199357.15  
   600      2391.23     4.264      4.2884     0.0249     0.0604   124313.39  201037.92  
   700      2832.04     4.2642     4.2889     0.0255     0.0493   125032.07  202195.07  
   800      3273.34     4.2645     4.2888     0.0247      0.04    125553.42  203034.27  
   900      3714.72     4.2643     4.289      0.0239     0.0338   125941.88  203659.58  
   1000      4156.1     4.2644     4.289      0.025      0.0296   126235.29  204131.72  
   1100     4598.18     4.2645     4.2891     0.0263     0.0258   126455.04  204485.05  
   1200     5039.94     4.2644     4.289      0.027      0.0227   126609.44  204732.98  
   1300     5482.15     4.2644     4.2887     0.0351     0.0204   126702.51  204882.21  
   1400     5924.53     4.2637     4.2882     0.0344     0.0184   126726.85  204920.29  
   1500     6366.52     4.2628     4.2875     0.0411     0.0167   126657.37  204807.24  
   1600     6808.19     4.2614     4.286      0.0457     0.0153   126450.71  204472.33  
   1700     7250.35     4.2591     4.2836     0.0554     0.0143   126010.94  203760.58  
   1800     7691.58     4.2543     4.2787     0.0717     0.0133   125137.81  202348.75  
   1900     8130.98     4.2445     4.2688     0.0954     0.0131   123365.76  199477.69  
   2000     8569.46     4.2232     4.2471     0.1267     0.0131   119739.84  193608.45  
   2100      9008.2     4.1732     4.1975     0.1533     0.0147   112335.62  181626.92  
   2200      9440.4     4.063      4.086      0.1872     0.0191    99312.09  160550.82  
   2300     9868.66     3.8543     3.8768     0.1984     0.0232    82165.51  132822.49  
   2400     10297.44    3.5498     3.5714     0.1963     0.0233    65626.58  106108.11  
   2500     10723.91    3.2023     3.2212     0.1589     0.019     52977.21   85706.09  
   2600     11148.14    2.8627     2.8796     0.1269     0.0155    44406.62   71858.59  
   2700     11569.83    2.5655     2.5795     0.1323     0.0141    38918.41   62972.51  
   2800     11990.89    2.3109     2.3229     0.0877     0.0124    35297.93   57123.96  
   2900     12410.13    2.098      2.1086     0.0999     0.0123    32893.13   53231.39  
   3000     12828.75    1.9229     1.9312     0.1003     0.0116    31220.39   50536.2   
   3100     13246.57    1.7777     1.7832     0.0855     0.0108    30056.21   48648.31  
   3200     13666.55    1.6588     1.6633     0.1082     0.0103    29216.87   47298.43  
   3300     14083.49    1.5596     1.5626     0.0978     0.0091    28590.71   46279.63  
   3400     14500.59    1.4786     1.4796     0.0937     0.0084    28129.27   45530.44  
   3500     14917.76    1.411      1.4121     0.0947     0.0085    27792.32   44983.45  
   3600     15335.56    1.3552     1.3555     0.1993     0.0085    27542.93   44574.61  
   3700     15754.02    1.3076     1.3088     0.0793     0.0069    27291.78   44199.29  
   3800     16167.54    1.269      1.2691     0.0807     0.0064    27115.49   43916.67  
   3900     16589.86    1.2369     1.236      0.125      0.0061    26995.8    43705.39  
   4000     17015.43     1.21      1.2088     0.0946     0.0061    26886.76   43530.84  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       177.73     4.2182     4.2348     0.034      0.9608    85638.12  207122.89  
   200       697.8      4.253      4.2706     0.0236     0.3092   106956.46  258659.26  
   300      1215.44     4.2591     4.2767     0.0154     0.1791   115150.84  278410.56  
   400       1732.5     4.2619     4.2792     0.015      0.1165   119124.02  287975.95  
   500       2249.2     4.2625     4.2802     0.0147     0.0865   121399.86  293452.51  
   600      2766.86     4.2634     4.2811     0.0143     0.066    122847.64  296935.98  
   700      3283.03     4.2638     4.2814     0.0147     0.0529   123853.75  299355.23  
   800      3799.37     4.2641     4.2818     0.0147     0.044     124586.8  301118.14  
   900      4315.21     4.2644     4.282      0.0135     0.0367   125144.19  302458.73  
   1000     4831.58     4.2646     4.2822     0.0139     0.0314   125580.37  303507.64  
   1100     5340.25     4.2647     4.2823     0.0143     0.0277   125930.68  304349.97  
   1200     5844.33     4.2647     4.2824     0.0138     0.0248   126217.55  305039.52  
   1300     6347.09     4.2649     4.2826     0.0157     0.0216   126454.55  305609.36  
   1400      6850.8     4.2649     4.2826     0.0142     0.0198   126654.12   306089.1  
   1500     7355.15     4.2649     4.2826     0.014      0.0181    126822.0  306492.65  
   1600     7858.46     4.2648     4.2826     0.0133     0.0164    126964.2  306834.33  
   1700      8362.7     4.2649     4.2826     0.0165     0.0151   127083.35  307120.52  
   1800     8869.09     4.2647     4.2825     0.0175     0.0139   127181.52  307356.44  
   1900     9379.28     4.2649     4.2825     0.0185     0.013    127257.56  307539.19  
   2000     9887.86     4.2646     4.2823     0.0203     0.0121    127310.2   307665.4  
   2100     10394.23    4.2643     4.2821     0.0228     0.0114   127333.08  307720.09  
   2200     10899.36    4.264      4.2816     0.026      0.0106   127313.39  307672.15  
   2300     11407.23    4.2633     4.281      0.0312     0.0101   127228.82  307467.71  
   2400     11911.72    4.2619     4.2796     0.0356     0.0096   127013.92  306950.03  
   2500     12419.87    4.2591     4.2768     0.0473     0.0092   126514.14  305747.04  
   2600     12928.19    4.2527     4.2704     0.067      0.0092   125370.07  302988.88  
   2700     13434.79    4.2383     4.2559     0.0806     0.0096   122813.16  296828.08  
   2800     13936.74    4.2091     4.2266     0.0919     0.0111   117960.42  285129.17  
   2900     14435.29    4.157      4.1745     0.1126     0.0138   110336.04  266745.71  
   3000     14932.57    4.0745     4.0922     0.1032     0.0163   100435.27  242858.75  
   3100     15435.1     3.9569     3.9744     0.0991     0.0171    89412.58  216255.57  
   3200     15936.73    3.8043     3.8218     0.0988     0.016     78488.66   189891.3  
   3300     16439.48    3.6232     3.6408     0.0931     0.0137    68489.16  165753.42  
   3400     16944.16    3.4255     3.4428     0.0859     0.0109    60030.25  145344.85  
   3500     17450.27    3.2279     3.2437     0.0791     0.009     53253.32   128956.7  
   3600     17957.85    3.0332     3.0483     0.0683     0.0079    47867.21  115931.26  
   3700     18465.14    2.8505     2.8661     0.0785     0.0069    43743.74  105940.34  
   3800     18970.02    2.6813     2.6958     0.0766     0.0067    40503.89   98090.7   
   3900     19475.6     2.5245     2.5382     0.1062     0.006     37940.54   91900.51  
   4000     19982.15    2.3846     2.3971     0.0685     0.0056    35953.07   87080.36  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       179.8      4.2081     4.2224     0.034      0.9806    82343.23  265395.97  
   200       762.12     4.2501     4.2649     0.0203     0.3231   104367.75  336324.87  
   300      1346.24     4.2585     4.2734     0.0178     0.1784    113390.9  365333.23  
   400      1929.15     4.2608     4.2761     0.0161     0.1219   117852.67  379655.29  
   500      2508.43     4.2622     4.2776     0.0134     0.0881   120435.46  387942.34  
   600      3090.47     4.263      4.2786     0.0129     0.0676   122090.91  393252.97  
   700      3671.67     4.2636     4.2789     0.0125     0.0542   123236.62  396928.55  
   800      4252.14     4.2642     4.2792     0.0123     0.0452   124074.35  399614.99  
   900      4833.26     4.264      4.2794     0.0113     0.0385   124712.07  401660.16  
   1000      5415.1     4.2643     4.2797     0.0116     0.0327   125212.76  403265.82  
   1100     5995.98     4.2646     4.2798     0.0112     0.0284   125615.38  404556.77  
   1200     6576.47     4.2648     4.2799     0.0113     0.0252   125945.33  405614.52  
   1300     7156.68     4.2648     4.2801     0.0115     0.0225   126220.74  406497.38  
   1400     7737.12     4.2649     4.2801     0.0121     0.0203   126453.27  407242.89  
   1500     8317.66     4.2649     4.2801     0.0113     0.0187   126652.08  407880.09  
   1600     8897.36     4.265      4.2801     0.0111     0.0169   126824.02  408431.24  
   1700      9483.4     4.2651     4.2803     0.0128     0.0155   126973.09  408908.97  
   1800     10067.63    4.265      4.2803     0.0122     0.0142   127103.72  409327.61  
   1900     10654.08    4.265      4.2803     0.012      0.0133   127218.06  409693.94  
   2000     11239.08    4.265      4.2804     0.014      0.0124    127318.1  410014.52  
   2100     11822.67    4.2651     4.2804     0.0137     0.0116   127405.86  410295.66  
   2200     12406.43    4.2652     4.2803     0.0141     0.0107   127481.29  410537.17  
   2300     12989.49    4.265      4.2804     0.0145     0.0102   127546.45  410745.79  
   2400     13570.87    4.2651     4.2803     0.0147     0.0096   127600.15  410917.48  
   2500     14153.2     4.2651     4.2802     0.0171     0.0091   127642.14  411051.74  
   2600     14738.41    4.2648     4.2801     0.0185     0.0086   127670.86  411143.19  
   2700     15321.19    4.2647      4.28      0.0178     0.0082   127683.71  411183.85  
   2800     15904.22    4.2645     4.2798     0.0216     0.0078   127676.04   411158.4  
   2900     16487.69    4.264      4.2794     0.0234     0.0075   127639.71  411040.51  
   3000     17074.56    4.2636     4.2787     0.0264     0.0072   127561.19   410787.0  
   3100     17671.66    4.2626     4.2777     0.0307     0.007    127408.92  410296.19  
   3200     18270.85    4.2609     4.2762     0.0406     0.0068   127119.79  409364.61  
   3300     18868.99    4.2578     4.273      0.047      0.0066   126567.11  407585.72  
   3400     19464.13    4.2514     4.2668     0.0633     0.0067   125439.13  403955.43  
   3500     20055.59    4.2384     4.2536     0.0812     0.0072   123146.83  396581.82  
   3600     20645.8     4.2132     4.2283     0.0876     0.0083   118949.83  383082.47  
   3700     21237.52    4.167      4.1819     0.0998     0.0108   112082.32  360987.27  
   3800     21814.38    4.0897     4.1044     0.1159     0.0133   102423.45  329922.83  
   3900     22391.16    3.9705     3.9856     0.1164     0.0153    90836.12   292667.5  
   4000     22968.08    3.8073     3.822      0.1181     0.0153    78780.85  253905.78  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       176.02     4.2039     4.2257     0.0291     1.0105    80531.23  324979.72  
   200       840.15     4.2493     4.2715     0.0224     0.3236   102759.87  414808.32  
   300       1501.5     4.2574     4.2806     0.0152     0.1809   112338.22  453478.93  
   400      2163.71     4.2608     4.2839     0.0147     0.1227   117109.14  472725.81  
   500       2823.2     4.2621     4.2853     0.0122     0.0895   119878.46   483886.7  
   600      3481.33     4.2629     4.2861     0.0117     0.0711   121661.64  491073.69  
   700      4142.38     4.2635     4.2866     0.0118     0.0564   122891.29  496029.02  
   800      4818.66     4.2638     4.2871     0.0109     0.0454   123790.62  499653.35  
   900      5503.85     4.2641     4.2874     0.0107     0.039    124473.12  502403.83  
   1000     6188.01     4.2643     4.2876     0.0112     0.0333   125009.75  504566.36  
   1100     6875.79     4.2646     4.2877     0.0107     0.0291   125440.81  506303.73  
   1200     7561.66     4.2649     4.288      0.0121     0.0256   125794.31  507727.91  
   1300     8246.59     4.2647     4.2879     0.0103     0.023     126088.6  508913.69  
   1400     8929.61     4.2648     4.2881     0.0103     0.0205   126338.89  509921.98  
   1500     9614.51     4.265      4.288      0.0106     0.0186   126553.53  510786.52  
   1600     10294.67    4.2651     4.2883     0.0096     0.017     126739.2  511534.38  
   1700     10977.84    4.2652     4.2883     0.0102     0.0156   126901.47  512188.04  
   1800     11661.11    4.2652     4.2883     0.0108     0.0144   127043.99  512762.15  
   1900     12343.72    4.265      4.2884     0.0109     0.0134   127169.93  513269.35  
   2000     13027.66    4.2653     4.2883     0.0108     0.0125   127282.11   513721.2  
   2100     13710.63    4.2651     4.2884     0.011      0.0117   127381.97  514123.43  
   2200     14392.44    4.2654     4.2884     0.0114     0.011    127471.43   514483.8  
   2300     15076.83    4.2653     4.2885     0.0111     0.0103   127551.21  514805.08  
   2400     15762.11    4.2651     4.2885     0.0128     0.0097   127622.49  515092.18  
   2500     16445.53    4.2652     4.2884     0.0117     0.0092    127686.0  515347.86  
   2600     17129.25    4.2651     4.2884     0.0114     0.0087   127741.49   515571.2  
   2700     17811.04    4.2652     4.2884     0.0134     0.0083   127789.83  515765.92  
   2800     18494.56    4.2651     4.2884     0.0128     0.0079   127830.42  515929.24  
   2900     19171.57    4.2651     4.2883     0.0163     0.0076   127862.47  516058.03  
   3000     19827.98    4.2649     4.2882     0.0159     0.0072   127884.85  516147.78  
   3100     20491.72    4.2649     4.2881     0.0158     0.0069   127896.65  516195.12  
   3200     21147.08    4.2647     4.2879     0.0176     0.0067   127894.38   516185.7  
   3300     21802.94    4.2645     4.2876     0.019      0.0064   127873.46  516101.18  
   3400     22461.1     4.2641     4.2873     0.0211     0.0061   127825.47  515907.33  
   3500     23120.24    4.2634     4.2867     0.0259     0.006    127739.23  515559.69  
   3600     23782.68    4.2626     4.2857     0.0273     0.0058   127590.08  514958.18  
   3700     24449.1     4.2613     4.2843     0.0365     0.0056   127328.04  513900.85  
   3800     25112.81    4.2586     4.2817     0.0437     0.0056   126867.83  512045.45  
   3900     25775.44    4.2538     4.2769     0.0576     0.0056   126007.41  508578.32  
   4000     26436.9     4.2441     4.2672     0.0732     0.0059   124292.33  501661.83  
