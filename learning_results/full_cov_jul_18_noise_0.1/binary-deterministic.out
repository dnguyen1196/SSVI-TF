Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.999045133590698
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       132.72     0.7812     0.7361     3.1419     0.2362   
   200       271.45     0.714      0.6787     2.4587     0.119    
   300       410.13     0.6792     0.6156     2.4564     0.0922   
   400       550.79     0.6524     0.5758     2.0728     0.0661   
   500       689.55     0.627      0.5244     1.9448     0.0611   
   600       828.88     0.629      0.5225     1.9274     0.0489   
   700       968.86     0.6193     0.5018     2.0277     0.045    
   800      1107.76     0.6132     0.4804     1.6398     0.0384   
   900      1247.48     0.6195     0.4825     1.7238     0.036    
   1000     1387.92     0.6138     0.4615     1.6207     0.0318   
   1100     1526.57     0.6057     0.4645     1.5477     0.0352   
   1200     1665.01     0.6004     0.451      1.6359     0.0323   
   1300     1803.01     0.6025     0.4545     1.5073     0.0324   
   1400     1941.61     0.5925     0.4441     1.5835     0.0306   
   1500     2079.69     0.6091     0.4615     1.821      0.0332   
   1600     2217.62     0.5938     0.4366     1.7152     0.0297   
   1700     2355.56     0.5705     0.4079     1.4348     0.0292   
   1800      2493.1     0.5983     0.4391     1.6413     0.0306   
   1900     2630.38     0.634      0.5048     2.1371     0.0305   
   2000      2768.7     0.6278     0.5146     2.9023     0.0418   
   2100     2906.29     0.6531     0.5745     2.7266     0.0441   
   2200      3043.5     0.6364     0.5428     2.7229     0.0453   
   2300     3179.66     0.6346     0.5464     2.8922     0.0464   
   2400     3315.56     0.6225     0.5348     2.736      0.0442   
   2500      3450.2     0.6238     0.528      2.7695     0.0415   
   2600     3584.89     0.6031     0.5079     2.6433     0.0574   
   2700     3719.29     0.5922     0.4825     2.3769     0.0622   
   2800     3852.59     0.5849     0.4864     2.2854     0.0566   
   2900     3985.77     0.5847     0.4839     2.2679     0.0614   
   3000     4118.86     0.5748     0.4486     2.2508     0.0588   
   3100     4252.03     0.5798     0.4718     2.333      0.0493   
   3200     4384.57     0.5802     0.4821     2.2615     0.0588   
   3300     4517.43     0.5731     0.4615     2.4584     0.0545   
   3400     4650.88     0.5695     0.458      2.0038     0.0676   
   3500     4784.31     0.5647     0.4559     2.1735     0.0671   
   3600     4917.51     0.5691     0.4714     2.5945     0.0532   
   3700     5050.51     0.5601     0.4548     1.8876     0.0709   
   3800     5183.19     0.5509     0.4254     1.834      0.065    
   3900     5316.07     0.5499     0.4205     1.6241     0.0502   
   4000     5448.97     0.5623     0.4338     1.8868     0.0848   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       131.71     1.4123     1.4026     0.016      0.6145   
   200       270.62     1.4364     1.4283     0.0206     0.3816   
   300       409.87     1.4882     1.4801     0.0146     0.2196   
   400       547.45     1.4956     1.4933     0.0119     0.1305   
   500       684.82     1.4956     1.4933     0.0127     0.0843   
   600       822.34     1.4956     1.4933     0.0125     0.0592   
   700       959.22     1.4956     1.4933     0.0133     0.0436   
   800      1096.41     1.4956     1.4933     0.0134     0.0343   
   900      1235.37     1.4956     1.4933     0.0137     0.0275   
   1000     1374.75     1.4956     1.4933     0.014      0.0226   
   1100     1513.77     1.4956     1.4933     0.0128     0.0192   
   1200     1651.23     1.4956     1.4933     0.0155     0.0164   
   1300     1788.58     1.4956     1.4933     0.0153     0.0144   
   1400      1926.2     1.4956     1.4933     0.0163     0.0126   
   1500     2063.93     1.4956     1.4933     0.0142     0.0112   
   1600      2201.5     1.4956     1.4933     0.0152      0.01    
   1700     2339.08     1.4956     1.4933     0.0178     0.0091   
   1800     2476.06     1.4956     1.4933     0.0184     0.0083   
   1900     2613.63     1.4956     1.4933     0.0196     0.0076   
   2000     2752.83     1.4956     1.4933     0.0202     0.0071   
   2100     2891.86     1.4956     1.4933     0.0245     0.0066   
   2200     3030.22     1.4956     1.4933     0.0274     0.0061   
   2300     3167.27     1.4956     1.4933     0.0268     0.0059   
   2400     3305.27     1.4956     1.4933     0.0309     0.0054   
   2500     3444.02     1.4956     1.4933     0.0409     0.0051   
   2600     3583.07     1.4953     1.4932     0.0445     0.0048   
   2700     3722.73     1.416      1.4138     0.0494     0.0046   
   2800     3862.41     1.1388     1.1336     0.0681     0.0044   
   2900      4001.7     1.0662     1.0606     0.0855     0.0044   
   3000     4138.16     0.9017     0.8957     0.1253     0.0052   
   3100     4273.91     0.8192     0.8154     0.2065     0.0076   
   3200     4409.44     0.7406     0.7328     0.246      0.0121   
   3300     4545.11      0.73      0.7187     0.238      0.0135   
   3400     4681.31     0.7238     0.7129     0.1639     0.0138   
   3500      4818.9     0.7198     0.7106     0.1025      0.01    
   3600     4954.88     0.7209     0.7114     0.1065     0.0063   
   3700     5090.57     0.7209     0.7114     0.1192     0.0043   
   3800     5226.34     0.7209     0.7114     0.0953     0.0032   
   3900     5361.84     0.7209     0.7114     0.114      0.0028   
   4000     5497.36     0.7172     0.7084     0.1084     0.0024   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       130.11     1.4091     1.409      0.0328     0.5076   
   200       268.52     1.4151     1.4157     0.023      0.3595   
   300       407.36     1.4726     1.4668     0.0203     0.2397   
   400       547.8      1.4947     1.4917     0.0245     0.1561   
   500       687.45     1.4937     1.4966     0.0138     0.1049   
   600       825.93     1.4954     1.4991     0.0132     0.0734   
   700       964.19     1.4956     1.499      0.0146     0.0543   
   800      1102.82     1.4956     1.499      0.0123     0.0422   
   900      1240.77     1.4956     1.499      0.0128     0.0334   
   1000     1378.44     1.4956     1.499      0.0141     0.0278   
   1100     1516.72     1.4956     1.499      0.013      0.0233   
   1200     1654.85     1.4956     1.499      0.0131     0.0198   
   1300     1792.92     1.4956     1.499      0.013      0.0173   
   1400     1931.13     1.4956     1.499      0.0154     0.015    
   1500     2068.99     1.4956     1.499      0.0194     0.0135   
   1600     2207.58     1.4956     1.499      0.0145     0.0121   
   1700     2346.06     1.4956     1.499      0.0142     0.0109   
   1800     2484.49     1.4956     1.499      0.0182      0.01    
   1900     2623.16     1.4956     1.499      0.0185     0.0093   
   2000     2761.86     1.4956     1.499      0.0214     0.0084   
   2100     2900.76     1.4956     1.499      0.0214     0.0078   
   2200     3039.72     1.4956     1.499      0.0201     0.0073   
   2300     3177.89     1.4956     1.499      0.0244     0.0068   
   2400     3316.85     1.4956     1.499      0.0302     0.0064   
   2500     3455.48     1.4956     1.499      0.0374     0.0061   
   2600     3593.85     1.4258     1.4284     0.0451     0.0058   
   2700     3732.37     1.119      1.1159     0.057      0.0054   
   2800     3870.01     1.029      1.0302     0.076      0.0052   
   2900     4007.66     0.8442     0.8433     0.124      0.0058   
   3000     4145.39     0.7533     0.7509     0.2189     0.0084   
   3100     4282.42     0.7281     0.7197     0.3069     0.0152   
   3200     4419.56     0.7253     0.715      0.2783     0.0202   
   3300     4558.33     0.7172     0.706      0.1488     0.0163   
   3400     4696.56     0.7172     0.706      0.1252     0.0095   
   3500     4834.07     0.7172     0.706      0.1583     0.0057   
   3600     4971.82     0.7172     0.706      0.1256     0.0045   
   3700     5109.34     0.7253     0.715      0.1413     0.0032   
   3800     5246.45     0.7172     0.706      0.1363     0.0029   
   3900     5383.66     0.7296     0.7162     0.2403     0.0027   
   4000     5522.01     0.7172     0.706      0.1638     0.0027   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       129.76     1.4054     1.4193     0.0307     0.4624   
   200       269.02     1.4136     1.4103     0.0237     0.3423   
   300       407.61     1.4224     1.4152     0.0213     0.2418   
   400       546.04     1.4416     1.4339     0.0185     0.1646   
   500       684.21     1.4652     1.4578     0.0173     0.114    
   600       822.84     1.4832     1.4831     0.016      0.0804   
   700       961.3      1.491      1.4938     0.0145     0.0593   
   800      1098.86     1.4952     1.4968     0.0142     0.0458   
   900      1237.05     1.4955     1.4973     0.0132     0.0368   
   1000     1375.28     1.4956     1.4975     0.0146     0.0301   
   1100     1513.33     1.4956     1.4975     0.0145     0.0255   
   1200     1651.99     1.4956     1.4975     0.0129     0.0213   
   1300     1790.07     1.4956     1.4975     0.0145     0.0187   
   1400     1928.34     1.4956     1.4975     0.014      0.0162   
   1500     2066.87     1.4956     1.4975     0.0164     0.0146   
   1600     2205.54     1.4956     1.4975     0.0134     0.0129   
   1700      2344.0     1.4956     1.4975     0.0152     0.0117   
   1800     2482.56     1.4956     1.4975     0.0148     0.0107   
   1900     2620.87     1.4956     1.4975     0.0155     0.0099   
   2000     2760.09     1.4956     1.4975     0.0169     0.009    
   2100      2899.1     1.4956     1.4975     0.0233     0.0083   
   2200     3037.73     1.4956     1.4975     0.0202     0.0077   
   2300     3176.04     1.4956     1.4975     0.024      0.0072   
   2400     3314.58     1.4956     1.4975     0.0242     0.0068   
   2500     3454.51     1.4956     1.4975     0.0299     0.0065   
   2600     3593.63     1.4805     1.4815     0.0376     0.0061   
   2700     3732.24     1.1686     1.1671     0.0466     0.0057   
   2800     3872.06     1.0387     1.0393     0.0644     0.0055   
   2900     4011.39     0.8536     0.8521     0.111      0.0056   
   3000     4149.25     0.7562     0.7492     0.1981     0.0074   
   3100     4288.27     0.7253     0.7118     0.3237     0.016    
   3200      4426.7     0.7172     0.7035     0.2993     0.0244   
   3300     4563.91     0.7172     0.7035     0.1613     0.0184   
   3400     4700.87     0.7172     0.7035     0.1795      0.01    
   3500     4839.03     0.7172     0.7035     0.1644     0.0058   
   3600     4975.73     0.7172     0.7035     0.2699     0.0049   
   3700     5113.82     0.7172     0.7035     0.1699     0.0039   
   3800     5253.01     0.7172     0.7035     0.1554     0.003    
   3900     5391.26     0.7209     0.7087     0.2815     0.003    
   4000     5528.04     0.7172     0.7035     0.2376     0.0026   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       130.23     1.4123     1.4092     0.0417     0.4398   
   200       269.03     1.4157     1.4043     0.0293     0.3323   
   300       407.7      1.4234     1.4163     0.0261     0.2424   
   400       547.06     1.4453     1.437      0.022      0.1696   
   500       686.78     1.4533     1.4524     0.0202     0.1181   
   600       825.57     1.4691     1.4696     0.0192     0.0842   
   700       965.15     1.4932     1.4926     0.0186     0.0624   
   800      1104.38     1.495      1.4961     0.0171     0.048    
   900      1243.32     1.4955     1.4973     0.0154     0.0384   
   1000     1382.34     1.4956     1.4977     0.0153     0.0317   
   1100     1521.76     1.4956     1.4976     0.0139     0.0265   
   1200     1660.89     1.4956     1.4976     0.0144     0.0224   
   1300     1800.29     1.4956     1.4976     0.0145     0.0193   
   1400     1939.14     1.4956     1.4976     0.0139     0.0169   
   1500     2077.97     1.4956     1.4976     0.0163     0.0152   
   1600     2216.74     1.4956     1.4976     0.0142     0.0136   
   1700     2356.01     1.4956     1.4976     0.0147     0.0121   
   1800     2495.95     1.4956     1.4976     0.0182     0.011    
   1900     2634.84     1.4956     1.4976     0.0158     0.0101   
   2000     2774.03     1.4956     1.4976     0.0144     0.0093   
   2100      2913.5     1.4956     1.4976     0.0198     0.0086   
   2200     3053.74     1.4956     1.4976     0.0193     0.008    
   2300     3193.06     1.4956     1.4976     0.0224     0.0075   
   2400     3332.16     1.4956     1.4976     0.0242     0.0071   
   2500     3471.77     1.4956     1.4976     0.0255     0.0066   
   2600      3611.1     1.4902     1.4927     0.033      0.0062   
   2700     3751.43     1.214      1.2175     0.0411     0.0059   
   2800     3890.93     1.0704      1.07      0.0543     0.0057   
   2900     4030.12     0.8462     0.8437     0.0859     0.0055   
   3000     4168.51     0.769      0.7665     0.1539     0.0066   
   3100     4307.03     0.7253     0.7131     0.2932     0.0123   
   3200     4445.41     0.7209     0.7095     0.3426     0.0258   
   3300      4583.8     0.7172     0.7044     0.1963     0.023    
   3400     4722.97     0.726      0.7113     0.2108     0.0118   
   3500     4861.42     0.726      0.7113     0.2175     0.0078   
   3600     4999.48     0.726      0.7189     0.2643     0.0056   
   3700     5137.68     0.726      0.7113     0.2368     0.0045   
   3800     5276.69     0.7209     0.7095     0.163      0.0034   
   3900     5414.57     0.7172     0.7044     0.1702     0.0031   
   4000     5552.91     0.7172     0.7044     0.2998     0.0031   
