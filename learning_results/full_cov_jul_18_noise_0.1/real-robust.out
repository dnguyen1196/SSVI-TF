Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  7.175354957580566
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       275.63    260.2888   261.3468    3.162      0.1872      0.0    
   200       558.82    237.8799   237.2999    2.7573      0.0        0.0    
   300       842.08    232.4574   232.7215    2.4197      0.0        0.0    
   400      1121.75    230.4249   230.2983    1.5039      0.0        0.0    
   500       1401.0    230.1634   230.1541    1.4385      0.0        0.0    
   600      1681.52     230.9     230.6178    1.7896      0.0        0.0    
   700      1962.91    229.6336   229.5491    1.5019      0.0        0.0    
   800      2242.36    230.5204   230.235     1.517       0.0        0.0    
   900      2521.18    230.015    229.6862    1.5326      0.0        0.0    
   1000     2800.27    229.9303   229.251     1.2347      0.0        0.0    
   1100     3079.13    230.3104   230.1461    1.2401      0.0        0.0    
   1200     3358.04    229.8912   229.8741    1.3134      0.0        0.0    
   1300      3636.8    229.3429   229.5733    1.0747      0.0        0.0    
   1400     3915.51    228.9478   229.2692    1.0315      0.0        0.0    
   1500     4196.43    229.3206   229.4993    1.292       0.0        0.0    
   1600     4476.95    229.2714   229.4246    1.1405      0.0        0.0    
   1700     4755.66    229.1977   229.2326    0.9119      0.0        0.0    
   1800     5034.09    228.4755   228.6827    1.0754      0.0        0.0    
   1900     5312.55    229.2443   229.1301    1.0963      0.0        0.0    
   2000     5590.94    228.6866   228.7771    0.8903      0.0        0.0    
   2100     5869.56    228.8476   229.0038    1.1954      0.0        0.0    
   2200     6148.23    228.2573   228.9789    1.1097      0.0        0.0    
   2300     6431.26    228.1906   228.8132    0.995       0.0        0.0    
   2400     6714.98    228.683    228.5963    0.9288      0.0        0.0    
   2500     6995.84    229.2707   229.4044    1.249       0.0        0.0    
   2600     7277.34    228.9227   228.6801    0.8172      0.0        0.0    
   2700     7556.39    228.2332   228.5513    1.0528      0.0        0.0    
   2800     7835.12    228.8065   228.4752    0.7066      0.0        0.0    
   2900     8114.17    229.1852   229.2321    0.7801      0.0        0.0    
   3000      8392.9     227.98    228.6103    0.8152      0.0        0.0    
   3100     8671.93    228.392    228.3338    0.9507      0.0        0.0    
   3200     8950.47    228.1362   228.4752    0.8491      0.0        0.0    
   3300     9228.87    228.1212   228.2359    0.898       0.0        0.0    
   3400     9508.07    228.2789   228.3017    0.7794      0.0        0.0    
   3500     9786.23    228.1987   228.4411    0.7414      0.0        0.0    
   3600     10066.99   228.4157   228.7625    0.9263      0.0        0.0    
   3700     10343.36   228.0778   228.3793    0.8478      0.0        0.0    
   3800     10618.54    228.29    227.9549    1.069       0.0        0.0    
   3900     10893.31   228.3559   228.4917    0.7271      0.0        0.0    
   4000     11168.39   227.714    228.3001    0.9565      0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       268.11    246.5841   247.0258    3.0656      0.0        0.0    
   200       546.73    234.4666   233.8504    1.9139      0.0        0.0    
   300       824.62    230.5985   230.9438    1.7429      0.0        0.0    
   400      1109.03    229.1739   229.9624    1.1955      0.0        0.0    
   500      1393.35    229.3956   230.1593    1.7174      0.0        0.0    
   600      1676.91    229.6197   230.2016    1.4473      0.0        0.0    
   700      1957.18    229.126    229.2811    1.0392      0.0        0.0    
   800      2237.73    230.1598   230.2149    1.5487      0.0        0.0    
   900      2518.16    228.8847   229.2086    1.2961      0.0        0.0    
   1000     2798.26    228.6483   229.1552    1.0729      0.0        0.0    
   1100     3078.75    228.7371   229.1601    1.2607      0.0        0.0    
   1200     3358.69    228.6514   228.6921    1.2328      0.0        0.0    
   1300     3638.54    229.6248   229.5394    0.974       0.0        0.0    
   1400     3915.85    228.6505   229.5435    0.9737      0.0        0.0    
   1500     4192.95    228.8146   229.0003    1.1421      0.0        0.0    
   1600     4471.71    228.1584   229.0079    1.0348      0.0        0.0    
   1700     4749.33    228.5507   228.5078    1.0513      0.0        0.0    
   1800     5026.64    228.9393   228.6398    0.8326      0.0        0.0    
   1900     5304.72    228.8879   229.2655    1.3891      0.0        0.0    
   2000     5581.85    228.3931   229.1123    0.8843      0.0        0.0    
   2100     5859.24     228.5     228.8449    1.0243      0.0        0.0    
   2200     6137.26    228.2333   228.7941    1.0835      0.0        0.0    
   2300     6416.87    228.9587   229.0015    0.8338      0.0        0.0    
   2400     6694.14    228.1316   228.2258    0.9897      0.0        0.0    
   2500     6971.52    228.206    229.1349    1.0301      0.0        0.0    
   2600     7247.59    228.1554   228.7401    0.8409      0.0        0.0    
   2700     7524.37    228.1472   228.3805    0.7156      0.0        0.0    
   2800     7801.55    228.4428   228.6068    0.9781      0.0        0.0    
   2900     8080.82    228.6159   228.5843    0.7193      0.0        0.0    
   3000     8358.36    228.0614   228.7204    1.0568      0.0        0.0    
   3100     8635.99    228.3365   228.4754    0.916       0.0        0.0    
   3200     8912.91    228.1946   229.0105    0.7499      0.0        0.0    
   3300     9190.37    228.1258   228.1954    0.8312      0.0        0.0    
   3400     9468.05    228.5858   228.4942    0.8045      0.0        0.0    
   3500     9747.59    227.6285   228.4752    0.7613      0.0        0.0    
   3600     10025.26   228.7872   228.873     0.8025      0.0        0.0    
   3700     10302.87   228.2433   228.6335    1.2233      0.0        0.0    
   3800     10583.68   227.7767   228.1481    0.7377      0.0        0.0    
   3900     10864.13   228.2183   228.426     0.6382      0.0        0.0    
   4000     11144.7    227.4667   228.046     1.1495      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       272.07    246.4148   246.125     2.665       0.0        0.0    
   200       553.65    231.2651   231.3358    1.2337      0.0        0.0    
   300       835.52    230.7769   230.4239    1.5314      0.0        0.0    
   400      1116.75    228.9754   229.5819    1.0854      0.0        0.0    
   500      1397.51    228.4064   229.132     1.377       0.0        0.0    
   600      1678.67    228.4649   228.7127    0.8492      0.0        0.0    
   700      1959.74    228.2101   228.5287    1.0269      0.0        0.0    
   800      2244.54    228.3305   228.5895    1.1992      0.0        0.0    
   900      2525.95    228.2842   228.4887    1.1509      0.0        0.0    
   1000     2807.35    228.0454   228.5795    0.939       0.0        0.0    
   1100     3087.92    228.6005   229.1175    1.3321      0.0        0.0    
   1200     3368.68    227.7208   228.4399    0.9475      0.0        0.0    
   1300     3649.43    228.545    228.9509    1.0993      0.0        0.0    
   1400     3927.83    228.2894   228.9849    0.9765      0.0        0.0    
   1500     4205.23    228.2977   228.4447    1.0327      0.0        0.0    
   1600     4483.15    227.8117   228.3391    0.9921      0.0        0.0    
   1700     4760.91    228.0822   228.2699    1.0533      0.0        0.0    
   1800     5038.52     228.29    228.5566    0.8944      0.0        0.0    
   1900     5321.53    227.7341   228.3901    0.915       0.0        0.0    
   2000     5612.51    228.0853   228.6286    0.9532      0.0        0.0    
   2100     5902.61    227.8329   228.2541    0.7464      0.0        0.0    
   2200     6190.16    227.7283   228.3366    1.0281      0.0        0.0    
   2300     6491.48    227.4819   227.632     0.7766      0.0        0.0    
   2400     6779.53    227.4799   227.976     1.1679      0.0        0.0    
   2500     7067.46    227.624    228.2772    0.818       0.0        0.0    
   2600     7364.71    227.6505   227.926     0.7383      0.0        0.0    
   2700      7655.9    227.1616   228.0637    0.7778      0.0        0.0    
   2800     7949.81    227.8962   228.2318    0.9278      0.0        0.0    
   2900      8240.5    227.4316   227.9817    0.5677      0.0        0.0    
   3000     8532.32    227.4678   228.445     0.7633      0.0        0.0    
   3100     8828.95    228.5789   228.7883    0.776       0.0        0.0    
   3200     9127.45    227.8738   228.4939    0.799       0.0        0.0    
   3300     9422.75    227.7673   228.1695    0.8751      0.0        0.0    
   3400     9714.23    227.678    228.1714    0.8813      0.0        0.0    
   3500     10011.43   228.1138   228.6777    0.8902      0.0        0.0    
   3600     10304.55   227.8373   228.0179    0.6839      0.0        0.0    
   3700     10598.16   227.7231   227.9313    0.7661      0.0        0.0    
   3800     10889.68   227.678    228.0953    0.907       0.0        0.0    
   3900     11184.09   227.8479   228.0641    0.9586      0.0        0.0    
   4000     11479.47   227.7177   227.7392    0.6762      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       284.24    250.9084   250.8198    2.505      0.0001      0.0    
   200       587.9     231.9551   232.5112    1.0685      0.0        0.0    
   300       888.44    228.8934   229.1671    0.7028      0.0        0.0    
   400      1182.91    228.6997   229.459     0.9777      0.0        0.0    
   500      1481.36    228.3783   229.2872    0.7993      0.0        0.0    
   600      1770.44    228.0199   228.6192    0.8497      0.0        0.0    
   700      2065.04    227.3689   228.3802    1.1796      0.0        0.0    
   800      2361.96    227.7501   228.5506    0.8296      0.0        0.0    
   900      2657.29    227.1619   228.2694    0.8256      0.0        0.0    
   1000     2948.24    228.032    228.9826    0.9093      0.0        0.0    
   1100      3241.8    227.6603   228.5232    1.0625      0.0        0.0    
   1200     3533.23    227.967    228.9352    1.0557      0.0        0.0    
   1300     3817.02    227.2533   228.1782    0.8226      0.0        0.0    
   1400      4107.7    227.7106   228.3157    1.1427      0.0        0.0    
   1500      4400.8    227.4341   228.0698    0.8198      0.0        0.0    
   1600     4692.57    227.3717   227.9322    0.6764      0.0        0.0    
   1700     4992.19    227.8594   228.7588    0.8655      0.0        0.0    
   1800      5279.1    227.5411   228.2319    0.7553      0.0        0.0    
   1900     5567.53     227.8     228.2781    0.997       0.0        0.0    
   2000     5850.35    227.874    228.5751    1.1097      0.0        0.0    
   2100     6135.84    227.5738   228.5447    0.8258      0.0        0.0    
   2200     6432.91    227.4701   228.5607    1.0154      0.0        0.0    
   2300     6726.67    227.676    228.6029    0.9718      0.0        0.0    
   2400     7022.67    227.2173   228.1248    0.5914      0.0        0.0    
   2500     7314.19    227.4909   228.1883    0.7477      0.0        0.0    
   2600     7606.18    227.278    228.0209    0.6322      0.0        0.0    
   2700     7902.32    227.8041   228.5165    0.9536      0.0        0.0    
   2800     8192.62    227.6979   228.7399    0.7327      0.0        0.0    
   2900     8479.97    227.5886   228.4889    0.7333      0.0        0.0    
   3000     8766.31    227.5721   228.1762    0.5993      0.0        0.0    
   3100     9055.78    226.8897   227.8249    0.5319      0.0        0.0    
   3200     9351.06    226.7716   228.0905    0.7684      0.0        0.0    
   3300     9647.71    227.2547   228.2748     0.69       0.0        0.0    
   3400     9942.24    227.2649    228.11     0.6682      0.0        0.0    
   3500     10232.6    227.6965   228.5792    0.8471      0.0        0.0    
   3600     10527.87   227.2047   228.1725    0.7366      0.0        0.0    
   3700     10820.76   227.7358   228.3021    0.6701      0.0        0.0    
   3800     11108.44   227.0676   227.9659    0.6298      0.0        0.0    
   3900     11407.03   227.4501   228.1938    0.9336      0.0        0.0    
   4000     11699.06   227.3568   228.4093    0.6084      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       288.14    256.9006   256.3153    2.1433      0.0        0.0    
   200       580.76    232.5285   232.8292    0.7613      0.0        0.0    
   300       876.82    228.6416   229.2469    0.7134      0.0        0.0    
   400      1167.04    227.777    228.7678    1.3035      0.0        0.0    
   500      1457.43    228.2142   228.537     0.9145      0.0        0.0    
   600      1757.02    227.8724   228.3089    0.9505      0.0        0.0    
   700      2050.56    227.7526   228.0432    0.8066      0.0        0.0    
   800      2343.16    227.4017   227.8412    0.7827      0.0        0.0    
   900       2640.8    228.0926   228.5805    0.9135      0.0        0.0    
   1000     2932.96    227.389    228.2383    0.9336      0.0        0.0    
   1100     3233.13    227.1492   228.1592    0.8064      0.0        0.0    
   1200     3525.37    227.7014   228.2003    0.8267      0.0        0.0    
   1300     3819.98    227.423    228.065     0.5723      0.0        0.0    
   1400     4121.39    227.2827   227.8824    0.6069      0.0        0.0    
   1500     4420.21    227.9828   228.6033    0.6876      0.0        0.0    
   1600     4723.84    226.9853   227.8331    0.8393      0.0        0.0    
   1700      5011.7    227.4913   228.1859    0.7762      0.0        0.0    
   1800     5307.87    227.7514   228.0394    0.563       0.0        0.0    
   1900     5604.05    227.4516   228.0188    0.8103      0.0        0.0    
   2000     5899.68    227.532    227.9636    0.6085      0.0        0.0    
   2100     6197.06    227.4364   228.0762    0.8386      0.0        0.0    
   2200     6489.49    227.4166   228.0432    0.6987      0.0        0.0    
   2300     6779.46    227.6635   228.0733    0.7887      0.0        0.0    
   2400      7077.7    227.0699   228.1072    0.7656      0.0        0.0    
   2500      7366.7    227.1305   227.7748    0.5605      0.0        0.0    
   2600     7662.06    227.2614   228.1531    0.813       0.0        0.0    
   2700     7949.31    227.4054   228.0864    0.7233      0.0        0.0    
   2800     8236.01    227.3435   227.9526    0.6251      0.0        0.0    
   2900     8524.11    227.0469   227.6577    0.533       0.0        0.0    
   3000     8816.66    227.2513   228.019     0.6701      0.0        0.0    
   3100     9111.61    227.3161   228.0238    0.6482      0.0        0.0    
   3200      9410.1    227.9071   228.1038    0.8199      0.0        0.0    
   3300     9701.36    227.1537   228.0607    0.6462      0.0        0.0    
   3400     9993.81    227.4102   227.9096    0.6608      0.0        0.0    
   3500     10283.41   227.1945   228.0155    0.7066      0.0        0.0    
   3600     10573.2    227.2324   227.7675    0.5513      0.0        0.0    
   3700     10866.95   227.144    228.0012    0.711       0.0        0.0    
   3800     11163.89   227.4333   228.1373    0.7608      0.0        0.0    
   3900     11454.37   227.436    227.9508    0.7539      0.0        0.0    
   4000     11736.95   227.1966   227.7751    0.5344      0.0        0.0    
