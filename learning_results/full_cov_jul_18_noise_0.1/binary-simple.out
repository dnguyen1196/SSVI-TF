Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.724775791168213
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       130.67     0.7594     0.7358     3.1331     0.2584     0.0001  
   200       392.11     0.6979     0.6447     2.6235     0.1027      0.0    
   300       650.9      0.6733     0.5946     2.0454     0.0945      0.0    
   400       910.62     0.6575     0.5627     2.1885     0.066       0.0    
   500      1171.02     0.6415     0.5431     2.1451     0.0624      0.0    
   600      1431.24     0.6172     0.495      2.085      0.0507      0.0    
   700      1697.62     0.6137     0.4944     1.8084     0.0446      0.0    
   800      1963.94     0.6043     0.4695     1.8742     0.0411      0.0    
   900      2229.92     0.5979     0.455      1.9921     0.0394      0.0    
   1000      2495.5     0.5909     0.4585     1.9696     0.0373      0.0    
   1100     2758.65     0.5946     0.4632     2.1572     0.0379      0.0    
   1200      3018.7     0.5898     0.4559     2.334      0.0377      0.0    
   1300     3283.11     0.6061     0.4815     2.4128     0.0379      0.0    
   1400     3546.62     0.5991     0.4825     2.5789     0.037       0.0    
   1500     3808.18     0.6035     0.4907     2.4554     0.0361      0.0    
   1600     4068.64     0.5976     0.4792     2.9242     0.0407      0.0    
   1700     4327.95     0.6087     0.5167     2.751      0.0463      0.0    
   1800     4582.27     0.5834     0.4823     2.529      0.0393      0.0    
   1900     4832.61     0.5816     0.4675     2.4427     0.0409      0.0    
   2000     5080.87     0.5751     0.4497     2.636      0.0444      0.0    
   2100     5328.17     0.5841     0.4718     2.5461     0.0531      0.0    
   2200     5575.18     0.5694     0.4497     2.6346     0.0532      0.0    
   2300      5821.1     0.5719     0.4441     2.3178     0.0648      0.0    
   2400     6070.94     0.5637     0.4508     2.3666     0.0504      0.0    
   2500     6321.16     0.572      0.4637     2.7215     0.0502      0.0    
   2600     6570.01     0.5535     0.4186     1.9607     0.0648      0.0    
   2700     6814.53     0.5672     0.4402     2.2658     0.0615      0.0    
   2800     7059.37     0.5613     0.435      1.792      0.0481      0.0    
   2900     7306.83     0.5647     0.4352     2.4703     0.0644      0.0    
   3000     7553.32     0.5596     0.4327     2.2407     0.0609      0.0    
   3100     7799.45     0.5547     0.4159     2.2871     0.0531      0.0    
   3200     8044.94     0.5528     0.4231     1.6705     0.0581      0.0    
   3300     8285.29     0.5476     0.4109     2.0895     0.0494      0.0    
   3400     8528.43     0.5551     0.4217     2.1409     0.0561      0.0    
   3500     8772.19     0.5617     0.4233     2.1956     0.0575      0.0    
   3600     9011.51     0.5531     0.4109     1.6063     0.0621      0.0    
   3700     9255.67     0.5535     0.4231     2.0746     0.0871      0.0    
   3800     9502.64     0.5531     0.4195     2.0331     0.0747      0.0    
   3900     9748.19     0.5527     0.4266     1.663      0.0556      0.0    
   4000     9994.02     0.5505     0.4152     2.118      0.0524      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       132.86     1.4225     1.4219     0.0133     0.5653      0.0    
   200       451.12     1.4158     1.4121     0.0084     0.3452      0.0    
   300       770.52     1.4096     1.4202     0.0086     0.2094      0.0    
   400      1090.53     1.414      1.4142     0.0083     0.1337      0.0    
   500      1406.08     1.4177     1.4122     0.0088     0.0921      0.0    
   600      1723.73     1.4116     1.417      0.0089     0.0667      0.0    
   700      2043.25     1.412      1.4147     0.0088     0.0509      0.0    
   800      2362.71     1.4118     1.4134     0.0093     0.0398      0.0    
   900      2681.27     1.4183     1.4145     0.0097     0.0328      0.0    
   1000     3000.02     1.4143     1.4167     0.0088     0.0273      0.0    
   1100     3319.02     1.4131     1.4127     0.0098     0.0232      0.0    
   1200     3637.88     1.4153     1.4181     0.0096     0.0197      0.0    
   1300     3956.71     1.4211     1.4157     0.0102     0.0173      0.0    
   1400     4275.83     1.4155     1.4042     0.0107     0.0151      0.0    
   1500     4596.01     1.4128     1.4142      0.01      0.0134      0.0    
   1600      4915.8     1.4188     1.4166     0.0105     0.0122      0.0    
   1700     5235.18     1.4138     1.4202     0.0104     0.0109      0.0    
   1800     5547.44     1.4175     1.4101     0.0104      0.01       0.0    
   1900     5863.31     1.4059     1.4166     0.0107     0.0091      0.0    
   2000     6180.65     1.4109     1.4115     0.0104     0.0083      0.0    
   2100     6495.28     1.4144     1.4122     0.0104     0.0077      0.0    
   2200     6811.25     1.4142     1.4137     0.0108     0.007       0.0    
   2300     7128.81     1.4187     1.4146     0.0111     0.0066      0.0    
   2400     7448.47     1.4125     1.4126     0.0105     0.0062      0.0    
   2500     7768.42     1.4148     1.4113     0.0116     0.0058      0.0    
   2600     8088.65     1.4213     1.4093     0.0107     0.0055      0.0    
   2700     8408.99     1.4113     1.4094     0.0105     0.0052      0.0    
   2800     8730.55     1.4075     1.409      0.011      0.0049      0.0    
   2900      9051.1     1.4148     1.4166     0.0117     0.0047      0.0    
   3000     9372.49     1.4122     1.4143     0.0109     0.0044      0.0    
   3100     9693.62     1.4067     1.4053     0.0117     0.0042      0.0    
   3200     10012.63    1.4061     1.4036     0.0121     0.0041      0.0    
   3300     10331.41    1.4103     1.3995     0.0125     0.0038      0.0    
   3400     10650.19    1.3986     1.4072     0.0126     0.0037      0.0    
   3500     10969.67    1.3934     1.396      0.0133     0.0036      0.0    
   3600     11288.73    1.3916     1.3972     0.0131     0.0034      0.0    
   3700     11605.84    1.3811     1.3916     0.0139     0.0033      0.0    
   3800     11924.85    1.3832     1.383      0.0146     0.0032      0.0    
   3900     12245.55    1.387      1.3726     0.0148     0.0031      0.0    
   4000     12565.87    1.3709     1.3586     0.0158     0.003       0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       133.82     1.4105     1.4174     0.0185     0.4743      0.0    
   200       507.56     1.4147     1.4132     0.0142     0.3301      0.0    
   300       877.28     1.4146     1.4165     0.0143     0.2243      0.0    
   400      1239.19     1.4093     1.4177     0.0129     0.1521      0.0    
   500      1602.04     1.422      1.4101     0.0095     0.1067      0.0    
   600       1966.4     1.4194     1.4176     0.0101     0.0779      0.0    
   700      2330.29     1.405      1.4144     0.0117     0.0598      0.0    
   800      2695.01     1.4218     1.4158     0.0101     0.0474      0.0    
   900      3059.34     1.4098     1.4092     0.0102     0.0381      0.0    
   1000     3422.98     1.4148     1.4165     0.0113     0.0316      0.0    
   1100     3785.61     1.4043     1.4123     0.0095     0.0268      0.0    
   1200     4148.41     1.4183     1.4107      0.01      0.023       0.0    
   1300     4511.15     1.4136     1.4142     0.0101     0.0198      0.0    
   1400     4875.58     1.4096     1.4176     0.0105     0.0174      0.0    
   1500     5244.18     1.4137     1.4098     0.0096     0.0155      0.0    
   1600     5609.81     1.417      1.4138     0.0101     0.0138      0.0    
   1700     5974.91     1.4075     1.4145     0.0114     0.0126      0.0    
   1800     6340.31     1.4226     1.4093     0.0108     0.0115      0.0    
   1900      6707.3     1.4188     1.4111     0.0113     0.0106      0.0    
   2000     7074.32     1.4125     1.4109     0.0103     0.0095      0.0    
   2100     7440.35     1.4143     1.4208     0.0105     0.009       0.0    
   2200     7805.54     1.414      1.4132     0.0105     0.0083      0.0    
   2300     8178.71     1.414      1.415      0.0112     0.0077      0.0    
   2400     8553.45     1.4206     1.412      0.0108     0.0072      0.0    
   2500     8919.69     1.4163     1.4102     0.0108     0.0067      0.0    
   2600     9286.68     1.4096     1.4111     0.0113     0.0063      0.0    
   2700     9657.74     1.411      1.404      0.0107     0.006       0.0    
   2800     10025.86    1.4153     1.413      0.0113     0.0057      0.0    
   2900     10394.48    1.4151     1.4126     0.011      0.0053      0.0    
   3000     10764.46    1.405      1.4047     0.0116     0.0051      0.0    
   3100     11136.45    1.406      1.4116     0.0123     0.0049      0.0    
   3200     11504.37    1.4055     1.4073     0.0119     0.0047      0.0    
   3300     11871.53    1.3959     1.4003     0.0126     0.0044      0.0    
   3400     12236.64    1.4071     1.4006     0.0132     0.0042      0.0    
   3500     12600.9     1.3965     1.3931     0.0142     0.0041      0.0    
   3600     12963.98    1.381      1.3802     0.015      0.0039      0.0    
   3700     13329.57    1.3794     1.3816     0.0171     0.0038      0.0    
   3800     13693.4     1.3693     1.3713     0.0159     0.0037      0.0    
   3900     14058.4     1.3561     1.3559     0.0207     0.0036      0.0    
   4000     14421.19    1.3406     1.3418     0.0207     0.0035      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       128.28     1.4175     1.4137     0.0281     0.4363      0.0    
   200       541.45     1.4118     1.415      0.0226     0.3184      0.0    
   300       954.1      1.4184     1.4162     0.0194     0.2262      0.0    
   400      1367.34     1.4224     1.4111     0.0142     0.1593      0.0    
   500       1781.0     1.413      1.4195     0.0157     0.1151      0.0    
   600      2194.66     1.4183     1.4162     0.012      0.0837      0.0    
   700      2607.83     1.4137     1.4161     0.0133     0.0637      0.0    
   800      3022.27     1.4085     1.4154     0.0117     0.0504      0.0    
   900       3436.6     1.4106     1.4128     0.0129     0.0412      0.0    
   1000     3850.35     1.4102     1.4174     0.0103     0.0335      0.0    
   1100     4272.83     1.4151     1.4182     0.0127     0.0283      0.0    
   1200     4690.25     1.4108     1.4131     0.011      0.0243      0.0    
   1300     5109.35     1.4067     1.4179     0.0118     0.0211      0.0    
   1400     5529.32     1.4214     1.4151     0.0114     0.0184      0.0    
   1500     5947.16     1.4122     1.4157     0.0118     0.0165      0.0    
   1600     6366.58     1.4186     1.4109     0.011      0.0148      0.0    
   1700     6784.74     1.4184     1.4122     0.0118     0.0133      0.0    
   1800     7201.51     1.4188     1.4122     0.011      0.012       0.0    
   1900     7614.36     1.4203     1.4148     0.0113     0.0109      0.0    
   2000     8028.14     1.4125     1.4161     0.0107     0.0103      0.0    
   2100     8443.62     1.4179     1.4115     0.0124     0.0093      0.0    
   2200     8859.14     1.4168     1.416      0.0111     0.0086      0.0    
   2300     9273.52     1.4144     1.4173     0.0108     0.008       0.0    
   2400     9687.51     1.4144     1.415      0.0114     0.0076      0.0    
   2500     10100.04    1.4052     1.4167     0.011      0.007       0.0    
   2600     10513.7     1.4125     1.4116     0.0117     0.0066      0.0    
   2700     10928.35    1.4043     1.4177     0.0126     0.0062      0.0    
   2800     11342.46    1.4104     1.413      0.0113     0.0059      0.0    
   2900     11758.51    1.413      1.4111     0.0132     0.0056      0.0    
   3000     12176.9     1.4056     1.4055     0.0123     0.0054      0.0    
   3100     12593.29    1.4015     1.4051     0.0134     0.0051      0.0    
   3200     13009.17    1.4019     1.4009     0.0147     0.0049      0.0    
   3300     13425.69    1.3911     1.4031     0.0148     0.0047      0.0    
   3400     13843.55    1.3941     1.3966     0.0153     0.0044      0.0    
   3500     14261.31    1.388       1.39      0.0154     0.0043      0.0    
   3600     14676.94    1.378      1.374      0.0178     0.0042      0.0    
   3700     15094.66    1.366      1.3716     0.0209     0.004       0.0    
   3800     15512.2     1.3491     1.3534     0.0217     0.0039      0.0    
   3900     15928.38    1.328      1.3294     0.0244     0.0038      0.0    
   4000     16343.0     1.3008     1.2982     0.0324     0.0037      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       128.86     1.4133     1.4148     0.0333     0.4185      0.0    
   200       594.54     1.4173     1.414      0.0279     0.3115      0.0    
   300      1060.08     1.4191     1.4135     0.0224     0.2267      0.0    
   400      1525.03     1.4072     1.4142     0.0187     0.1621      0.0    
   500      1990.06     1.4241     1.4134     0.0173     0.117       0.0    
   600      2459.83     1.408      1.4138     0.0138     0.0871      0.0    
   700      2926.12     1.4142     1.4164     0.0149     0.0666      0.0    
   800      3393.83     1.4078     1.4116     0.0133     0.0523      0.0    
   900      3858.94     1.4165     1.4181     0.013      0.0425      0.0    
   1000     4323.15     1.4053     1.4164     0.0147     0.0349      0.0    
   1100     4790.25     1.4116     1.4189     0.0127     0.0295      0.0    
   1200     5261.95     1.4088     1.4157     0.0126     0.0252      0.0    
   1300      5733.6     1.4118     1.4135     0.0134     0.0219      0.0    
   1400     6203.31     1.4113     1.4168     0.0118     0.019       0.0    
   1500     6676.43     1.4154     1.4156     0.0124     0.017       0.0    
   1600     7151.97     1.413      1.416      0.012      0.0152      0.0    
   1700     7628.85     1.4271     1.4151     0.0132     0.0138      0.0    
   1800     8104.16     1.4128     1.4151     0.0114     0.0124      0.0    
   1900     8583.01     1.4156     1.4106     0.0117     0.0113      0.0    
   2000     9055.87     1.4174     1.4144     0.0121     0.0103      0.0    
   2100     9530.81     1.4095     1.4177     0.0113     0.0095      0.0    
   2200     9997.76     1.4142     1.4119     0.0117     0.0088      0.0    
   2300     10466.07    1.4133     1.4138     0.0116     0.0083      0.0    
   2400     10934.87    1.4142     1.4145     0.0137     0.0077      0.0    
   2500     11401.92    1.4122     1.4146     0.0121     0.0072      0.0    
   2600     11869.99    1.4102     1.4115     0.0137     0.0068      0.0    
   2700     12340.69    1.4133     1.4108     0.013      0.0064      0.0    
   2800     12811.53    1.4132     1.4068     0.0136     0.006       0.0    
   2900     13282.32    1.4114     1.4072     0.013      0.0058      0.0    
   3000     13758.75    1.4042     1.4041     0.0136     0.0055      0.0    
   3100     14236.73    1.408      1.4019     0.0141     0.0053      0.0    
   3200     14711.74    1.4046     1.3987     0.0152     0.005       0.0    
   3300     15201.4     1.3911     1.3953     0.0166     0.0048      0.0    
   3400     15689.98    1.3869     1.3906     0.017      0.0045      0.0    
   3500     16189.23    1.3804     1.376      0.0205     0.0044      0.0    
   3600     16674.1     1.3686     1.3652     0.0238     0.0043      0.0    
   3700     17165.38    1.3508     1.3455     0.0283     0.0041      0.0    
   3800     17644.76    1.3153     1.3154     0.0337     0.0041      0.0    
   3900     18131.18    1.2629     1.259      0.0426     0.004       0.0    
   4000     18619.37    1.1664     1.1596     0.0638     0.004       0.0    
