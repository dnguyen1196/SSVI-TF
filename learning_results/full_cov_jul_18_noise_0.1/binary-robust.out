Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.772160291671753
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       265.6      0.9413     0.9292     3.0625     0.4162     0.003   
   200       659.43     0.7753     0.7658     1.968      3.5913     0.0014  
   300      1049.95     0.7549     0.7313     1.7515    70.7141     0.0046  
   400      1440.73     0.7557     0.7409     1.8291    359.0335    0.0232  
   500      1830.59     0.7458     0.7214     1.6539    874.1454    0.0013  
   600      2216.33     0.7443     0.7192     1.632     39.9872     0.005   
   700      2600.48     0.7402     0.7283     1.5396     9.3451     0.0034  
   800       2984.0     0.7347     0.7206     1.8545     17.214     0.0046  
   900      3366.48     0.7417     0.7284     1.6227    50.3341     0.0002  
   1000     3749.32     0.7444     0.7279     1.8475    604.7859    0.0161  
   1100     4132.72     0.7452     0.7215     1.9279   3173.4633    0.0236  
   1200     4517.93     0.739      0.724      2.2062   10056.3739   0.0643  
   1300     4900.46     0.7238     0.7055     1.8668   1201.9845    0.0125  
   1400     5281.38     0.7136     0.687      2.5074   3460.6571    0.0062  
   1500     5661.58     0.699      0.6819     1.8169    940.0254    0.0005  
   1600     6042.17     0.6964     0.6657     1.7381   1128.2451    0.0035  
   1700     6424.71     0.6932     0.6598     2.1105   1829.5465    0.022   
   1800     6809.02     0.688      0.6615     1.7183    158.2629    0.0069  
   1900     7192.79     0.6884     0.6527     2.4621   3831.9377    0.0064  
   2000     7572.59     0.6828     0.6493     2.4765   2720.6136    0.0188  
   2100     7952.01     0.6766     0.6434     2.4334    725.5199    0.0169  
   2200     8332.06     0.6766     0.6394     2.5359   4361.3481    0.0398  
   2300     8712.72     0.6896     0.6621     2.4748   14048.7788   0.1906  
   2400     9093.21     0.672      0.6323     2.1128    912.9017    0.0297  
   2500     9474.16     0.662      0.6166     2.0871    662.7039    0.0244  
   2600     9856.46     0.6626     0.6081     2.1681    388.0207    0.0054  
   2700     10239.87    0.6505     0.597      2.451     4753.01     0.0351  
   2800     10620.0     0.6512     0.5958     2.5196   14691.9552   0.0562  
   2900     11001.57    0.6758     0.6312     2.5569   233548.6279   0.4172  
   3000     11383.57    0.6609     0.601      2.1469   10897.1417   0.0576  
   3100     11766.42    0.6729     0.6076     1.8116   6333.8516    0.0009  
   3200     12151.02    0.6746     0.6098     2.0405   2855.9503    0.012   
   3300     12536.01    0.6667     0.6027     1.3008    248.4539    0.0078  
   3400     12918.41    0.6892     0.6336     2.4127   2899.9327    0.0071  
   3500     13298.89    0.6813     0.619      1.7912   1277.4601    0.0063  
   3600     13679.03    0.6671     0.6155     1.2468    227.5605     0.0    
   3700     14058.46    0.6649     0.6155     1.9325   1372.6229    0.0038  
   3800     14438.16    0.6669     0.6056     0.953     169.3011    0.0062  
   3900     14819.04    0.6651     0.6134     2.2351   14269.9912   0.013   
   4000     15199.07    0.6654     0.6158     2.7853   12318.8557   0.0102  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       264.14     1.4118     1.4184     0.2332     0.0016     0.0037  
   200       702.21     1.4087     1.4198     0.3223     0.0028     0.0012  
   300      1143.62     1.4197     1.4091     0.351      0.0031     0.0003  
   400      1583.15     1.4161     1.412      0.3096     0.0032     0.0002  
   500      2025.59     1.4151     1.4116     0.2826     0.0032     0.0001  
   600      2468.59     1.4122     1.4143     0.2942     0.0031     0.0001  
   700      2909.24     1.4121     1.409      0.3372     0.0032     0.0002  
   800      3351.84     1.4097     1.4158     0.3691     0.0031     0.0001  
   900      3796.18     1.4101     1.4183     0.336      0.003       0.0    
   1000     4237.69     1.4088     1.4104     0.3376     0.003      0.0001  
   1100     4678.55     1.4056     1.4073     0.3714     0.003      0.0003  
   1200     5120.49     1.3981     1.4085     0.3202     0.0029     0.0003  
   1300     5564.24     1.3973     1.3931     0.4899     0.0028      0.0    
   1400     6006.45     1.3219     1.3266     0.4353     0.0028     0.0003  
   1500     6449.34     0.7742     0.7695     0.7186     0.0058     0.0003  
   1600     6890.75     0.7593     0.7433     0.7307     0.0256     0.0008  
   1700     7333.51     0.7362     0.7265     0.6823     0.029      0.0003  
   1800     7775.14     0.7382     0.7264     0.6121     0.0348     0.0004  
   1900      8215.9     0.734      0.7252     0.5512     0.0396     0.0017  
   2000     8656.08     0.7323     0.7235     0.6031     0.0419     0.0006  
   2100     9096.52     0.7362     0.723      0.4979     0.0442     0.0007  
   2200     9538.02     0.7412     0.7206     0.4805     0.0594     0.0011  
   2300      9978.1     0.7363     0.7202     0.5027     0.0518     0.0002  
   2400     10417.48    0.7336     0.7248     0.4434     0.0519     0.0009  
   2500     10858.24    0.7295     0.722      0.564      0.0616     0.0002  
   2600     11298.17    0.7326     0.7154     0.4604     0.0597     0.0005  
   2700     11737.15    0.7397     0.725      0.5402     0.0647     0.0018  
   2800     12175.55    0.7284     0.7235     0.5997     0.0648     0.0008  
   2900     12619.24    0.7367     0.7292     0.6902     0.0793     0.0001  
   3000     13060.16    0.7326     0.7267     0.6314     0.0713     0.0018  
   3100     13504.5     0.7281     0.7201     0.6816     0.0709     0.0002  
   3200     13946.42    0.7326     0.7215     0.6624     0.0785     0.0011  
   3300     14387.08    0.7327     0.7177     0.5505     0.0755     0.0009  
   3400     14829.85    0.7291     0.7244     0.6288     0.0764     0.0013  
   3500     15271.03    0.7329      0.72      0.4935     0.063      0.0013  
   3600     15714.93    0.7335     0.7171     0.5543     0.086      0.0019  
   3700     16159.05    0.7322     0.7225     0.5244     0.0808      0.0    
   3800     16602.3     0.7359     0.7284     0.5187     0.088      0.0015  
   3900     17047.66    0.7316     0.7296     0.5832     0.0846     0.0001  
   4000     17494.23    0.731      0.7194     0.5554     0.1042     0.0013  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       267.67     1.4157     1.4149     0.3853     0.0019     0.0003  
   200       770.02     1.4177     1.4115     0.3167     0.002      0.0002  
   300      1271.55     1.4166     1.4159     0.5661     0.002      0.0002  
   400      1771.94     1.4133     1.4139     0.3679     0.002      0.0002  
   500      2277.57     1.4183     1.4137     0.3394     0.002      0.0002  
   600      2781.54     1.4214     1.4173     0.4088     0.002      0.0002  
   700      3288.01     1.4189     1.4108     0.4051     0.002      0.0001  
   800      3791.63     1.4089     1.4148     0.3792     0.0019     0.0003  
   900      4295.09     1.408      1.415      0.4432     0.0019      0.0    
   1000     4797.05     1.4208     1.4147     0.4111     0.002      0.0006  
   1100     5293.65      1.41      1.4061     0.3073     0.0019      0.0    
   1200     5795.03     1.4077     1.4112     0.3463     0.0019     0.0003  
   1300      6295.8     1.407      1.403      0.3777     0.0019     0.0004  
   1400      6796.6     1.4162     1.4068     0.4049     0.0019     0.0002  
   1500     7295.56     1.4027     1.4047     0.4841     0.0019     0.0001  
   1600     7793.14     1.3697     1.3578     0.4052     0.0019     0.0002  
   1700     8288.06     0.8167     0.8138      0.75      0.0039     0.0001  
   1800     8783.07     0.7322     0.7313     0.7104     0.0175     0.0007  
   1900     9284.65     0.7299     0.7208     0.7157     0.0226     0.0021  
   2000     9781.98     0.7272     0.7226     0.571      0.0248     0.0013  
   2100     10279.03    0.7376     0.7257     0.7381     0.0307     0.0003  
   2200     10772.52    0.7231     0.7153     0.7403     0.0244     0.0003  
   2300     11266.81    0.7326     0.7232     0.8326     0.0286     0.0005  
   2400     11762.29    0.7273     0.7205     0.7652     0.025      0.0007  
   2500     12259.25    0.725      0.7184     0.911      0.0282     0.0013  
   2600     12754.19    0.7292     0.717      0.8267     0.0275     0.0002  
   2700     13248.66    0.7376     0.7202     0.8125     0.0261     0.0002  
   2800     13741.08    0.7275     0.7179     0.6219     0.0267     0.0008  
   2900     14236.3     0.726      0.7152     0.9173     0.0282     0.0002  
   3000     14731.82    0.7272     0.7209     0.6825     0.0266     0.0024  
   3100     15225.96    0.7345     0.7219     0.8564     0.0259     0.0016  
   3200     15717.13    0.7328     0.7242     0.6542     0.027      0.002   
   3300     16207.15    0.7345     0.7276     0.8091     0.0286     0.0008  
   3400     16699.48    0.7276     0.719      0.7011     0.027      0.0003  
   3500     17191.84    0.7293     0.7202     0.6918     0.0304     0.0011  
   3600     17681.65    0.724       0.72      0.5713     0.0345     0.0019  
   3700     18168.27    0.7253     0.7178     0.5956     0.0293     0.0006  
   3800     18654.96    0.7278     0.7136     0.6339     0.0391     0.0015  
   3900     19146.41    0.7293     0.7204     0.6086     0.0363     0.0016  
   4000     19633.61    0.7321     0.7245     0.685      0.0341     0.0004  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       264.07     1.413      1.4176     0.4018     0.0014     0.0004  
   200       813.48     1.4116     1.413      0.4273     0.0015     0.0001  
   300      1364.65     1.4155     1.4146     0.3711     0.0015     0.0002  
   400      1918.91     1.4104     1.4133     0.4594     0.0015     0.0002  
   500       2468.0     1.4139     1.4119     0.5745     0.0015     0.0002  
   600       3019.7     1.4097     1.4136     0.3865     0.0016     0.0003  
   700      3571.28     1.4204     1.4156     0.3368     0.0016     0.0004  
   800      4125.68     1.4123     1.4186     0.4519     0.0015     0.0001  
   900      4679.72     1.4088     1.4142     0.352      0.0015     0.0004  
   1000      5226.9     1.4165     1.4125     0.4106     0.0015     0.0004  
   1100      5777.9     1.4111     1.4147     0.3701     0.0015     0.0005  
   1200     6323.77     1.4199     1.4121     0.3727     0.0015      0.0    
   1300     6873.05     1.4033     1.408      0.3502     0.0015      0.0    
   1400     7420.45     1.4177     1.4113     0.3972     0.0015      0.0    
   1500     7968.97     1.4101     1.4114     0.3901     0.0014     0.0005  
   1600     8521.99     1.4174     1.4077     0.3585     0.0014     0.0005  
   1700     9075.32     1.4048     1.4116     0.3963     0.0015     0.0003  
   1800     9625.63     1.4084     1.4115     0.3363     0.0014     0.0001  
   1900     10172.93    1.3986     1.4058     0.3502     0.0016     0.0003  
   2000     10721.74    1.4004     1.397      0.454      0.0015     0.0002  
   2100     11270.57    1.3349     1.3397     0.5577     0.0016     0.0001  
   2200     11821.9     0.7722     0.762      0.7215     0.0031     0.0006  
   2300     12370.94    0.7446     0.7269     0.6871     0.0148     0.0007  
   2400     12918.81    0.7408     0.7284     0.5875     0.0191     0.0015  
   2500     13469.28    0.7377     0.7264     0.5988     0.0202     0.0021  
   2600     14016.97    0.7296     0.7202     0.5486     0.0198     0.0007  
   2700     14562.3     0.7281     0.7132     0.571      0.0214     0.0013  
   2800     15109.04    0.7311     0.7162     0.396      0.0263     0.0047  
   2900     15655.25    0.7239     0.711      0.4761     0.0249     0.0019  
   3000     16199.02    0.7294     0.7188     0.5167     0.0228     0.0027  
   3100     16749.54    0.7274     0.7154     0.355      0.0276     0.0017  
   3200     17300.12    0.7343     0.7154     0.5139     0.0318     0.004   
   3300     17853.59    0.7315     0.7192     0.5334     0.0277     0.0021  
   3400     18393.51    0.7244     0.714      0.4291     0.0304     0.0008  
   3500     18932.72    0.7271     0.7162     0.342      0.0343     0.0017  
   3600     19474.78    0.7251     0.7116     0.4524     0.0347     0.0027  
   3700     20020.02    0.726      0.7176     0.3599     0.0318     0.0005  
   3800     20561.23    0.7283     0.716      0.4592     0.0404     0.0003  
   3900     21107.5     0.7291     0.7168     0.4078     0.0407     0.0026  
   4000     21647.36    0.7285     0.7168     0.3809     0.0529     0.0037  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       263.63     1.4032     1.4114     0.3003     0.0012     0.0013  
   200       869.27     1.4221     1.4127     0.365      0.0013     0.0001  
   300       1467.6     1.4147     1.4136     0.4232     0.0013     0.0007  
   400      2065.87     1.4166     1.4137     0.4043     0.0012     0.0002  
   500      2659.85     1.4215     1.4128     0.4433     0.0013     0.0002  
   600      3256.57     1.4179     1.4129     0.3391     0.0012     0.0007  
   700      3852.18     1.4178     1.4161     0.376      0.0012     0.0004  
   800      4446.64     1.4122     1.4142     0.3408     0.0013     0.0004  
   900      5045.36     1.4134     1.417      0.4159     0.0013     0.0005  
   1000     5652.66     1.4172     1.4129     0.4207     0.0012     0.0001  
   1100     6267.83     1.413      1.4084     0.3837     0.0012      0.0    
   1200      6885.2     1.4177     1.417      0.3456     0.0012      0.0    
   1300     7501.41     1.4095     1.4115     0.3094     0.0012     0.0005  
   1400     8118.47     1.4157     1.4127     0.3231     0.0012     0.0006  
   1500     8736.33     1.4177     1.4163     0.3958     0.0012     0.0002  
   1600     9354.34     1.4125     1.4152     0.3668     0.0012      0.0    
   1700     9972.06     1.4126     1.4111     0.3257     0.0013     0.0001  
   1800     10590.42    1.4141     1.413      0.3301     0.0012      0.0    
   1900     11207.84    1.4067     1.4129     0.4488     0.0012     0.0002  
   2000     11825.39    1.407      1.4087     0.3611     0.0013     0.0001  
   2100     12445.04    1.4104     1.4095     0.3567     0.0012     0.0003  
   2200     13062.5     1.4047     1.408      0.3509     0.0012     0.0001  
   2300     13680.51    1.3946     1.3994     0.3326     0.0012     0.0006  
   2400     14299.08    1.3796     1.3824     0.408      0.0012     0.0007  
   2500     14917.27    1.2535     1.2476     0.6633     0.0012     0.0006  
   2600     15534.99    0.7411     0.7268     0.9544     0.0034     0.001   
   2700     16147.67    0.7367     0.7246     0.9337     0.0111     0.0005  
   2800     16763.64    0.7208     0.7094     0.8485     0.014      0.0023  
   2900     17379.43    0.727      0.7181     0.7036     0.0151     0.0005  
   3000     17993.06    0.7295     0.7201     0.7736     0.0157     0.0015  
   3100     18605.56    0.7333     0.7222     0.6318     0.0152     0.003   
   3200     19211.57    0.7341     0.7203     0.6996     0.0146     0.0022  
   3300     19811.85    0.7328     0.721      0.7486     0.0178     0.0061  
   3400     20403.35    0.7224     0.7154     0.6364     0.0164     0.0018  
   3500     20991.68    0.7326     0.7187     0.8016     0.0175     0.0075  
   3600     21580.55    0.7271     0.7178     0.6013     0.0165     0.0008  
   3700     22167.33    0.7266     0.716      0.5111     0.0168     0.0024  
   3800     22753.31    0.731      0.7179     0.6778     0.0184     0.0053  
   3900     23342.02    0.7295     0.7169     0.6615     0.0172     0.0027  
   4000     23932.9     0.7271     0.7165     0.5647     0.0168     0.003   
