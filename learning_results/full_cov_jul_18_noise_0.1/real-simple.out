Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  6.891992568969727
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       107.99    272.583    273.5334    3.1613     0.3063      0.0    
   200       223.92    241.6968   240.4866    2.7049     0.1482      0.0    
   300       342.21    236.7764   236.3048    2.0219     0.0457      0.0    
   400       458.84    233.805    233.5442    2.3576     0.0183      0.0    
   500       574.51    233.0745   233.3628    1.5404     0.0189      0.0    
   600       692.22    231.9164   231.4363    1.483      0.0095      0.0    
   700       809.01    233.6369   232.8248    1.6523     0.0087      0.0    
   800       924.91    231.2401   230.5777    1.5425     0.0069      0.0    
   900      1040.45    232.0918   231.8231    1.3051     0.0052      0.0    
   1000     1159.16    230.1121   230.0178    1.2227     0.004       0.0    
   1100     1274.39    230.2842   229.9673    1.2744     0.0037      0.0    
   1200     1389.57    229.6022   229.7285    1.3414     0.0033      0.0    
   1300     1507.14    230.0205   229.7589    1.1225     0.0027      0.0    
   1400     1624.84    229.4604   229.6863    1.1386     0.0022      0.0    
   1500     1742.41    229.5653   229.9545    1.2412     0.0021      0.0    
   1600     1860.16    229.2346   229.6816    0.981      0.0017      0.0    
   1700     1977.84    229.051    229.2016    1.1868     0.0017      0.0    
   1800     2095.51    228.7772   228.7748    1.2871     0.0016      0.0    
   1900     2213.17    230.7091   230.3567    1.0995     0.0014      0.0    
   2000     2330.89    228.7001   228.6774    0.8502     0.0012      0.0    
   2100     2448.73    229.7944   230.5291    1.255      0.0011      0.0    
   2200     2566.42    228.8016   228.7283    0.8494     0.0008      0.0    
   2300     2683.04    229.3594   229.383     0.9767     0.0009      0.0    
   2400     2798.18    228.8996   228.985     0.9238     0.0008      0.0    
   2500     2913.18    228.6487   228.8108    1.2142     0.0008      0.0    
   2600      3029.7    228.2824   229.6889    0.8249     0.0007      0.0    
   2700     3145.89    228.8426   229.6909    1.3599     0.0007      0.0    
   2800     3261.95    228.895    229.7636    1.1012     0.0006      0.0    
   2900     3378.14    229.6782   229.4567    1.2144     0.0006      0.0    
   3000     3494.21    228.761    229.1954    0.9133     0.0005      0.0    
   3100     3610.37    228.9024   228.9181    0.9698     0.0006      0.0    
   3200     3726.54    228.8559   228.9972    0.9587     0.0005      0.0    
   3300     3842.87    228.2789   229.0911    0.8701     0.0004      0.0    
   3400     3959.02    229.0549   228.9694    0.8909     0.0005      0.0    
   3500     4075.22    227.6218   228.2149    1.017      0.0004      0.0    
   3600     4191.25    228.865    228.673     0.8804     0.0004      0.0    
   3700     4307.55    228.9403   228.9568    0.977      0.0003      0.0    
   3800     4423.67    228.7447   228.5087    0.7741     0.0003      0.0    
   3900     4538.36    229.0617   228.6642    0.9309     0.0003      0.0    
   4000     4651.92    229.0478   229.086     0.8995     0.0003      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       107.85    245.3685   245.5397    2.6653     0.3982      0.0    
   200       224.06    233.4456   234.1001    2.0626     0.076       0.0    
   300       339.73    231.0633   231.954     1.7545     0.0343      0.0    
   400       455.65    230.4925   231.1512    1.6176     0.0197      0.0    
   500       571.73    230.2165   230.926     1.4498     0.0125      0.0    
   600       687.82    229.853    230.4573    1.2799     0.0086      0.0    
   700       803.53    229.3474   230.4367    1.5796     0.006       0.0    
   800       919.92    228.3561   229.3371    1.3626     0.0046      0.0    
   900      1035.56    228.6037   228.8638    1.3138     0.0037      0.0    
   1000     1151.51    229.6013   229.6135    1.1549     0.0032      0.0    
   1100     1268.29    230.6291   230.9137    1.3428     0.0026      0.0    
   1200     1386.49    229.5604   230.0801    1.1968     0.0021      0.0    
   1300     1504.77    229.9683   230.6886    0.9329     0.002       0.0    
   1400     1623.11    229.4005   230.0313    1.1858     0.0018      0.0    
   1500     1741.35    228.5102   229.0271    1.0619     0.0016      0.0    
   1600     1859.76    229.2329   229.5819    1.1907     0.0012      0.0    
   1700     1977.32    230.0848   230.5245    0.8212     0.0012      0.0    
   1800     2093.94    229.0363   229.2638    1.3567     0.001       0.0    
   1900     2209.29    229.4118   229.5276    1.061      0.001       0.0    
   2000     2325.57    229.0414   229.6273    1.1572     0.0008      0.0    
   2100     2443.79    228.8597   229.2304    0.8536     0.0008      0.0    
   2200     2561.89    228.711    229.2328    0.9349     0.0008      0.0    
   2300     2679.78    228.8687   229.1512    1.1142     0.0007      0.0    
   2400     2797.91    228.5464   229.1589    0.9412     0.0006      0.0    
   2500     2915.98    228.6219   228.7966    0.8373     0.0006      0.0    
   2600     3034.23    228.4007   228.686     0.8632     0.0005      0.0    
   2700     3152.48    229.306    229.3308    0.8952     0.0005      0.0    
   2800     3270.67    228.8038   228.9273    0.9101     0.0005      0.0    
   2900     3388.86    229.0244   229.5486    0.9718     0.0004      0.0    
   3000     3505.32    228.651    228.9421    0.7221     0.0004      0.0    
   3100     3621.11    228.2238   228.8924    0.9911     0.0004      0.0    
   3200     3737.86    228.1497   228.6533    0.8142     0.0004      0.0    
   3300     3856.26    229.2406   229.3603    0.8798     0.0004      0.0    
   3400     3974.71    228.2195   228.7657    0.9389     0.0003      0.0    
   3500     4092.33    228.1144   228.6591    0.8237     0.0003      0.0    
   3600     4208.23    228.0677   228.3884    0.8101     0.0003      0.0    
   3700     4323.92    228.7369   228.9992    0.6705     0.0003      0.0    
   3800     4439.79    228.5472   228.6043    0.7737     0.0003      0.0    
   3900     4558.64    228.6943   228.9117    0.6769     0.0002      0.0    
   4000     4678.05    228.8549   229.0967    0.8458     0.0002      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       112.74    248.5628   247.8642    2.476      0.3067      0.0    
   200       232.45    231.7057   231.7317    1.3588     0.0675      0.0    
   300       352.12    229.9976   230.0282    1.4363     0.0321      0.0    
   400       471.85    229.0909   229.789     1.1636     0.0178      0.0    
   500       591.51    229.0001   229.6849    1.1049     0.0113      0.0    
   600       711.48    229.3627   229.4455    1.3328     0.0087      0.0    
   700       832.12    228.934    229.3232    0.9927     0.0062      0.0    
   800       950.29    228.138    228.7531    0.9982     0.005       0.0    
   900      1069.79    228.3612   229.3546    0.9659     0.0044      0.0    
   1000     1189.54    228.8254   228.933     1.1993     0.0036      0.0    
   1100     1309.42    228.8719   229.1922    0.8446     0.0028      0.0    
   1200     1429.12    229.5875   229.8753    0.9929     0.0023      0.0    
   1300     1549.16    228.5409   228.5972    1.0551     0.0021      0.0    
   1400     1668.75    228.5107   228.8654    1.0145     0.0018      0.0    
   1500     1788.64    228.1295   228.605     1.0796     0.0016      0.0    
   1600     1908.75    228.2765   229.0425    1.0447     0.0014      0.0    
   1700     2027.38    228.9818   228.959     1.0769     0.0012      0.0    
   1800     2144.69    228.7919   228.786     1.0337     0.0011      0.0    
   1900     2262.48    228.4368   228.4957    1.2213     0.001       0.0    
   2000     2381.66    229.1295   229.3533    0.8574     0.001       0.0    
   2100     2501.04    228.459    229.4684    1.0179     0.0008      0.0    
   2200      2620.9    228.5502   228.8308    0.9662     0.0008      0.0    
   2300     2740.61    228.1618   228.6466    0.8631     0.0007      0.0    
   2400     2860.13    228.346    228.7144    1.0245     0.0007      0.0    
   2500     2979.36    228.2632   228.6153    0.9529     0.0007      0.0    
   2600     3098.84    229.0697   229.0592    1.0052     0.0006      0.0    
   2700      3218.1    228.5629   228.9147    0.7734     0.0005      0.0    
   2800     3337.41    227.9103   227.9586    0.9101     0.0005      0.0    
   2900     3456.49    228.2614   228.4092    0.7258     0.0005      0.0    
   3000     3575.76    227.7415   228.5292    1.357      0.0005      0.0    
   3100     3695.37    227.9776   228.5403    0.5786     0.0004      0.0    
   3200     3814.68    228.8566   228.9198    0.8615     0.0004      0.0    
   3300     3934.07    227.6876   228.5412    0.9157     0.0004      0.0    
   3400     4053.47    228.5038   229.354     0.7799     0.0004      0.0    
   3500     4173.07    227.8726   228.4873    0.7846     0.0003      0.0    
   3600     4292.88    227.5776   228.3868    0.6494     0.0003      0.0    
   3700     4412.54    227.5211   228.3245    0.7858     0.0003      0.0    
   3800     4532.47    227.9163   228.2009    0.8839     0.0003      0.0    
   3900     4651.78    228.0165   228.3495    0.9115     0.0003      0.0    
   4000     4771.29    228.2388   228.981     0.8001     0.0002      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       108.26    252.9453   252.6088    2.4229     0.4272      0.0    
   200       226.76    231.7294   232.6066    1.0213     0.0789      0.0    
   300       345.27    229.083    229.7515    0.9417     0.0324      0.0    
   400       463.93    227.8431   228.7387    0.8577     0.0198      0.0    
   500       582.78    227.9716   228.9399    0.8765     0.0118      0.0    
   600       701.31    227.7748   228.7785    0.6428     0.0091      0.0    
   700       819.81    228.092    229.2251    1.3926     0.0073      0.0    
   800       938.38    227.5585   228.9455    0.8012     0.0054      0.0    
   900       1057.2    228.0485   228.7858     0.96      0.0038      0.0    
   1000     1175.49    228.5363   229.1027    0.766      0.0034      0.0    
   1100      1293.4    228.4251   229.3258    0.9061     0.0031      0.0    
   1200      1409.7    227.6006   228.8445    0.9896     0.0023      0.0    
   1300     1527.06    227.5009   228.3929    1.0507     0.002       0.0    
   1400     1645.74    228.3669   229.143     0.9248     0.0019      0.0    
   1500     1763.71    228.5513   229.0611    1.1103     0.0017      0.0    
   1600     1882.23    228.0308   229.1169    0.9656     0.0015      0.0    
   1700      2000.4    228.1259   228.893     0.7636     0.0013      0.0    
   1800     2118.65    227.5674   228.5361    0.7697     0.0012      0.0    
   1900     2236.53    227.8695   228.8519    0.6697     0.0011      0.0    
   2000     2352.46    227.4545   228.3842    0.748      0.001       0.0    
   2100     2470.66    228.0548   229.3292    0.9337     0.0009      0.0    
   2200     2588.76    228.2076   229.0043    1.0066     0.0008      0.0    
   2300     2706.39    227.4222   228.5483    1.001      0.0008      0.0    
   2400     2824.79    228.3555   229.2843    0.985      0.0007      0.0    
   2500     2943.24    227.8014   228.4678    0.6213     0.0007      0.0    
   2600      3061.9    228.3589   229.6224    0.8225     0.0006      0.0    
   2700     3180.15    227.5572   228.7346    0.9177     0.0006      0.0    
   2800     3298.96    227.8053    228.96     0.8855     0.0005      0.0    
   2900     3417.86    229.4083   229.6686    0.8159     0.0005      0.0    
   3000     3536.46    227.5613   228.435     0.587      0.0005      0.0    
   3100     3654.81    227.7061   228.668     0.6165     0.0004      0.0    
   3200     3773.28    227.9177   228.7039    0.6639     0.0004      0.0    
   3300     3891.48    227.6042   228.5829    0.7394     0.0004      0.0    
   3400     4009.92    227.6114   229.0721    0.6194     0.0004      0.0    
   3500      4128.1    227.1796   228.3702    0.809      0.0004      0.0    
   3600     4246.62    227.445    228.6342    0.6114     0.0003      0.0    
   3700      4364.9    226.9468   228.3909    0.6265     0.0003      0.0    
   3800     4483.56    227.487    228.3877    0.9397     0.0003      0.0    
   3900     4602.03    227.4354    228.48     0.6235     0.0003      0.0    
   4000     4720.77    227.693    228.6046    0.689      0.0003      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       108.63    258.5206   257.7697    2.2145     0.4897      0.0    
   200       228.0     232.3591   232.6929     0.74      0.0837      0.0    
   300       347.82    229.2412   229.5787    0.6041     0.0365      0.0    
   400       466.9     228.7416   228.9581    0.882      0.0209      0.0    
   500       586.38    227.6564   228.3553    0.7196     0.0128      0.0    
   600       706.35    227.8286   228.8029    0.8347     0.0092      0.0    
   700       825.74    227.5202   228.4244    0.7659     0.007       0.0    
   800       945.17    227.0101   228.0342    0.758      0.0058      0.0    
   900      1063.56    227.3072   228.367     0.8132     0.0044      0.0    
   1000     1181.18    227.4197   228.5992    1.4731     0.0035      0.0    
   1100     1300.17    227.3453   228.4033    0.9348     0.0029      0.0    
   1200     1418.75    227.0628   228.256     0.6466     0.0026      0.0    
   1300     1536.22    227.6388   228.4117    0.6897     0.0022      0.0    
   1400     1654.27    227.9411   228.1755    0.8179     0.0019      0.0    
   1500     1773.61    227.429    228.6436    0.8563     0.0018      0.0    
   1600     1891.13    227.6378   228.0802    0.6848     0.0016      0.0    
   1700     2007.77    227.6438   228.4273    0.9155     0.0015      0.0    
   1800     2127.45    227.4892   228.162     0.8133     0.0013      0.0    
   1900     2246.53    226.9315   228.0967    0.8027     0.001       0.0    
   2000      2365.2    227.6974   228.4727    0.992      0.0009      0.0    
   2100      2482.4    227.6781   228.2709    0.7751     0.0009      0.0    
   2200     2601.21    228.654    229.0463    0.6349     0.0008      0.0    
   2300      2719.7    227.8108   228.2744    0.8385     0.0008      0.0    
   2400      2836.5    227.794    228.4882    0.7326     0.0008      0.0    
   2500      2953.2    227.1485   227.9716    0.5884     0.0007      0.0    
   2600     3071.71    227.6651   228.4866    0.6236     0.0006      0.0    
   2700     3190.81    227.8425   228.3929    0.6732     0.0006      0.0    
   2800     3309.31    227.5014   228.2457    0.7418     0.0006      0.0    
   2900     3428.31    227.5576   228.485     0.8525     0.0005      0.0    
   3000     3546.98    227.5788   228.2462    0.6306     0.0005      0.0    
   3100     3666.76    227.768    228.3433    0.8193     0.0004      0.0    
   3200     3785.79    227.4876   228.014     0.7131     0.0004      0.0    
   3300     3904.39    227.7094   228.0215    0.7525     0.0004      0.0    
   3400     4023.34    227.3719   228.0744    0.7452     0.0004      0.0    
   3500     4142.33    227.3134   228.2715    0.8096     0.0003      0.0    
   3600     4260.89    227.6186   228.2038    0.6379     0.0004      0.0    
   3700     4379.54    227.0459   228.0953    0.6696     0.0003      0.0    
   3800     4498.05    227.4818   228.4089    0.7203     0.0003      0.0    
   3900     4616.91    227.4268   228.0014    0.5664     0.0003      0.0    
   4000     4735.57    227.3189   228.0475    0.6244     0.0003      0.0    
