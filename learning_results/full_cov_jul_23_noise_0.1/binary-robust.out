Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.297865629196167
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       370.63     0.8034     0.8021     3.0859     0.0297     0.0043  
   200       875.46     0.7651     0.7519     2.119      0.0393     0.005   
   300      1378.73     0.7593     0.7385     1.9237     0.0313     0.0034  
   400      1880.82     0.7477     0.7239     1.6477     0.0216     0.0015  
   500      2383.12     0.742      0.7179     1.9702     0.0185     0.0021  
   600      2881.01     0.7249     0.7123     1.5766     0.0171     0.0012  
   700       3381.5     0.7322     0.7141     1.4055     0.0139      0.0    
   800      3882.84     0.7292     0.7141     1.4903     0.0116     0.0004  
   900      4384.73     0.7307     0.7144     1.3056     0.0108     0.0005  
   1000      4885.0     0.7447     0.7182     1.2311     0.0103     0.0016  
   1100     5386.49     0.7356     0.7154     1.4716     0.0092     0.0003  
   1200     5888.82     0.7308     0.7171     1.1438     0.0091     0.0006  
   1300     6391.31     0.7354     0.7197     1.0269     0.0076     0.001   
   1400     6893.88     0.7248     0.7129     1.2393     0.0075     0.0016  
   1500     7393.57     0.7278     0.7102     1.1577     0.0068     0.0027  
   1600      7894.1     0.7322     0.7136     1.1724     0.0068     0.0032  
   1700     8395.51     0.7314     0.7146     0.9331     0.006      0.0001  
   1800     8895.34     0.7304     0.719      1.1462     0.0054     0.0013  
   1900      9393.0     0.7388     0.715      0.9024     0.0059     0.001   
   2000     9886.07     0.7254     0.7197     1.0627     0.0048     0.0037  
   2100     10373.53    0.7295     0.7161     0.8897     0.0054     0.0007  
   2200     10856.02    0.7305     0.7141     1.066      0.0047     0.0011  
   2300     11338.68     0.73      0.7181     1.0618     0.0044     0.0005  
   2400     11823.27    0.7311     0.7137     0.9765     0.0045     0.0008  
   2500     12316.16    0.7329     0.713      0.7862     0.0046     0.0014  
   2600     12816.13    0.7347     0.7264     0.9553     0.0045     0.0005  
   2700     13316.45    0.7366     0.7219     0.8382     0.0043     0.0003  
   2800     13816.59    0.7311     0.7139     0.787      0.0043     0.0006  
   2900     14315.5     0.7276     0.7082     0.7747     0.0036     0.0008  
   3000     14810.0     0.7342     0.7211     0.9201     0.0036     0.0021  
   3100     15292.67    0.7343     0.7185     0.8528     0.0037     0.0006  
   3200     15775.81    0.7267     0.7106     0.6466     0.0035     0.0008  
   3300     16257.54    0.7347     0.7167     0.8502     0.0033     0.0018  
   3400     16741.78    0.7317     0.7129     0.8231     0.0033     0.0009  
   3500     17229.02    0.7293     0.7122     0.8289     0.0035     0.0005  
   3600     17714.42    0.7294     0.7167     0.7731     0.0034     0.0012  
   3700     18203.29    0.7324     0.714      0.7655     0.0033     0.0005  
   3800     18687.88    0.7277     0.7126     0.6801     0.0029     0.0015  
   3900     19174.41    0.7311     0.7146     0.777      0.003      0.0011  
   4000     19657.03    0.7356     0.7148     0.7152     0.0031     0.0001  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       358.58     0.8928     0.8916     3.1547     0.2188     0.0006  
   200       892.66     0.7503     0.7416     1.4904     0.3354     0.0002  
   300      1427.45     0.7364     0.7304     1.2459     0.3963     0.0002  
   400      1966.81     0.7452     0.7299     1.1465     0.4415     0.0002  
   500      2498.03     0.7366     0.7234     1.0106     0.4805     0.0002  
   600      3031.79     0.7408     0.732      0.9217     0.6103     0.0002  
   700       3563.5     0.7396     0.7284     0.8809     0.6835     0.0002  
   800       4099.8     0.7357     0.7286     0.7503     1.061      0.0002  
   900      4638.46     0.7368     0.7295     0.8698     1.2435     0.0002  
   1000     5177.09     0.7406     0.729      0.7144     1.5389     0.0002  
   1100     5713.44     0.7366     0.7266     0.6666     1.7112     0.0002  
   1200     6246.93     0.7394     0.7361     0.8363     2.9214     0.0002  
   1300     6777.01     0.7432     0.731      0.7578     3.4276     0.0002  
   1400     7306.16     0.7446     0.7338     0.6312     4.8038     0.0002  
   1500      7833.6     0.7504     0.7393     0.708      6.8069     0.0002  
   1600     8352.05     0.7495     0.7389     0.7265    10.2129     0.0002  
   1700      8857.7     0.7511     0.7424     0.586     13.6268     0.0002  
   1800     9344.49     0.7513     0.7424     0.6218     18.912     0.0002  
   1900     9830.35     0.765      0.7482     0.6398    28.6592     0.0001  
   2000     10316.41    0.7724     0.7626     0.6601    78.6383     0.0001  
   2100     10801.94    0.7971     0.7814     0.6158    98.7962     0.0001  
   2200     11286.97    0.8429     0.8261     0.6933    204.5063    0.0002  
   2300     11771.65    0.9428     0.9401     0.6474    399.7795    0.0002  
   2400     12257.94    1.1016     1.1007     0.8244   1356.0819    0.0001  
   2500     12742.98    1.2793     1.2756     0.8083   3946.6715    0.0001  
   2600     13228.64    1.3933     1.3974     1.018    40598.7169   0.0001  
   2700     13714.49    1.4014     1.4124     1.2606   579858.4453   0.0002  
   2800     14199.46    1.4097     1.4084     0.9286   12099.0117   0.0001  
   2900     14684.28    1.4048     1.4169     0.5077   2319.3622    0.0001  
   3000     15170.28    1.4123     1.4123     0.4322    802.9168    0.0001  
   3100     15655.51    1.4063     1.4075     0.4805    465.3655    0.0001  
   3200     16139.64    1.4086     1.414      2.8406     96.313      0.0    
   3300     16625.98    1.4117     1.4081     0.5247    60.1167     0.0001  
   3400     17111.24    1.411      1.4162     0.372     33.1201     0.0001  
   3500     17596.63    1.4081     1.4078     0.2889    101.3188    0.0001  
   3600     18083.68    1.4042     1.4122     0.3027    66.9762      0.0    
   3700     18568.15    1.4135     1.4079     0.2829    53.8399      0.0    
   3800     19053.52    1.4077     1.4127     0.4339    53.8902      0.0    
   3900     19538.79    1.4047     1.4002     0.2763    32.0115      0.0    
   4000     20024.01    1.3603     1.3538     0.371     18.5098      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       331.29     1.4092     1.414      3.1579     0.2657     0.0001  
   200       876.52     1.4138     1.4151     1.2835     0.1165     0.0001  
   300      1418.63     1.4071     1.4107     1.2323     0.0876     0.0001  
   400      1962.63     1.4149     1.4161     1.092      0.0699     0.0001  
   500      2506.94     1.4121     1.4133     1.0447     0.0628      0.0    
   600      3048.81     1.4174     1.4148     1.0613     0.0556      0.0    
   700      3591.87     1.416      1.4122     1.0012     0.0475      0.0    
   800      4136.12     1.4026     1.4131     1.0741     0.0453      0.0    
   900      4680.29     1.4184     1.4146     0.9752     0.0405      0.0    
   1000     5224.68     1.4097     1.4151     0.9432     0.0377      0.0    
   1100      5768.9     1.4184     1.4129     0.8244     0.0362      0.0    
   1200     6313.84     1.4152     1.4089     0.9162     0.0343      0.0    
   1300     6857.59     1.4097     1.4118     0.9563     0.0327      0.0    
   1400     7401.07     1.4155     1.4145     0.9435     0.0318      0.0    
   1500     7944.36     1.4129     1.4158     0.8368     0.0298      0.0    
   1600      8487.5     1.4126     1.4074     0.8263     0.0283      0.0    
   1700     9031.69     1.4143     1.4153     0.7555     0.0286      0.0    
   1800     9575.34     1.4128     1.4136     0.7798     0.0275      0.0    
   1900     10119.48    1.4206     1.4171     0.7841     0.0279      0.0    
   2000     10665.36    1.4176     1.413      0.7713     0.0265      0.0    
   2100     11208.69    1.4064     1.4197     0.7766     0.0247      0.0    
   2200     11753.09    1.4094     1.4165     0.7357     0.0245      0.0    
   2300     12296.07    1.4184     1.4121     0.8013     0.0234      0.0    
   2400     12839.7     1.4164     1.4126     0.829      0.0231      0.0    
   2500     13382.93    1.4192     1.4116     0.6731     0.0225      0.0    
   2600     13929.86    1.4131     1.4122     0.7207     0.0216      0.0    
   2700     14488.42    1.4157     1.4164     0.761      0.0214      0.0    
   2800     15055.71    1.4187     1.4119     0.737      0.0213      0.0    
   2900     15615.33    1.4119     1.4154     0.7167     0.0211      0.0    
   3000     16177.85    1.4176     1.4181     0.6571     0.0205      0.0    
   3100     16742.75    1.4196     1.4159     0.6586     0.0202      0.0    
   3200     17300.21    1.424      1.4181     0.6577     0.0196      0.0    
   3300     17844.36    1.4186     1.4184     0.6548     0.019       0.0    
   3400     18388.7     1.4169     1.4131     0.716      0.019       0.0    
   3500     18933.61    1.4217     1.4129     0.6918     0.0188      0.0    
   3600     19477.43    1.4157     1.4211     0.6472     0.0195      0.0    
   3700     20020.84    1.4136     1.4108     0.684      0.0182      0.0    
   3800     20565.98    1.4106     1.4172     0.6279     0.0185      0.0    
   3900     21134.88    1.4177     1.4252     0.6018     0.0194      0.0    
   4000     21703.64    1.4169     1.4196     0.6223     0.0177      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       340.34     1.1936     1.1859     3.1576     0.4156     0.0001  
   200       951.15     0.7942     0.7891     1.355      2.2946      0.0    
   300      1581.34     0.8908     0.8731     1.1423    91.9385      0.0    
   400      2212.99     1.3809     1.3732     2.7519   1284.0104     0.0    
   500       2849.4     1.4244     1.4116     1.4585    1042.97      0.0    
   600      3481.93     1.4091     1.4181     1.1986   5383.6722     0.0    
   700      4113.02     1.4091     1.4177     3.1046   1870.8709     0.0    
   800      4746.25     1.4104     1.4131     1.0952    715.7189     0.0    
   900      5381.14     1.412      1.4188     1.0013   8237.9306     0.0    
   1000     6015.01     1.4098     1.4159     1.0489    1514.595     0.0    
   1100     6633.21     1.4168     1.4157     1.0548    374.2201     0.0    
   1200     7245.43     1.4185     1.4195     1.0347    51.8858      0.0    
   1300     7865.77     1.4138     1.4101     0.9322     35.042      0.0    
   1400     8477.49     1.4108     1.4144     0.8736     20.959      0.0    
   1500     9097.73     1.4147     1.411      0.9223    11.1739      0.0    
   1600     9721.76     1.4102     1.4113     0.874      4.852       0.0    
   1700     10335.44    1.413      1.4097     0.8782     5.6626      0.0    
   1800     10950.29    1.414      1.4146     0.848      4.5671      0.0    
   1900     11575.42    1.4042     1.4192     0.8755     2.7433      0.0    
   2000     12190.71    1.4108     1.4121     0.8502     1.8217      0.0    
   2100     12804.17    1.415      1.4132     0.8359     2.0678      0.0    
   2200     13396.61    1.4161     1.4142     0.897      0.9331      0.0    
   2300     13989.61    1.4195     1.4135     0.8234     0.974       0.0    
   2400     14584.38    1.4183     1.4183     0.8094     0.7405      0.0    
   2500     15199.04    1.411      1.4174     0.8279     0.6803      0.0    
   2600     15831.02    1.4125     1.4135     0.7582     0.5943      0.0    
   2700     16461.81    1.4042     1.4117     0.8103     0.5007      0.0    
   2800     17095.57    1.4124     1.4173     0.7561      0.46       0.0    
   2900     17729.96    1.4119     1.4152     0.7886     0.5289      0.0    
   3000     18363.23    1.4079     1.4156     0.7815     0.3703      0.0    
   3100     18962.68     1.42      1.4158     0.7656     0.3326      0.0    
   3200     19574.3     1.4088     1.411      0.8935     0.3096      0.0    
   3300     20188.59    1.4218     1.4184     0.7525     0.3486      0.0    
   3400     20803.52    1.4148     1.4113     0.7645     0.261       0.0    
   3500     21420.31    1.4097     1.4139     0.7081     0.2646      0.0    
   3600     22042.06    1.408      1.4144     0.745      0.2194      0.0    
   3700     22660.28    1.4138     1.4147     0.6824     0.2188      0.0    
   3800     23279.53    1.4175     1.4175     0.6887     0.1973      0.0    
   3900     23896.77    1.4136     1.4119     0.7008     0.2062      0.0    
   4000     24513.79    1.4149     1.408      0.7099     0.1951      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       340.02     1.421      1.4142     3.157      0.4949     0.0001  
   200      1010.93     1.4147     1.4127     1.4178     0.3373      0.0    
   300      1679.79     1.4166     1.4135     1.3452     0.2795      0.0    
   400      2341.68     1.4086     1.4184     1.3006     0.2453      0.0    
   500      3004.34     1.4066     1.4117     1.2915     0.2304      0.0    
   600      3661.57     1.4166     1.4145     1.2539     0.2323      0.0    
   700      4329.82     1.4153     1.4139     1.233      0.2306      0.0    
   800      4995.98     1.4147     1.414      1.1733     0.2254      0.0    
   900      5659.83     1.414      1.4181     1.2401     0.2355      0.0    
   1000     6328.94     1.4107     1.415      1.1997     0.2349      0.0    
   1100     6994.16     1.4119     1.4181     1.1341     0.2402      0.0    
   1200     7653.09     1.4149     1.4159     1.2574     0.2386      0.0    
   1300     8317.28     1.4032     1.4132     1.1258     0.2506      0.0    
   1400     8986.79     1.4146     1.4173     1.0828     0.2657      0.0    
   1500     9645.77     1.4041     1.415      1.0806     0.2697      0.0    
   1600     10307.25    1.4126     1.415      1.0813     0.301       0.0    
   1700     10971.01    1.421      1.4169     1.0581     0.3146      0.0    
   1800     11630.89    1.4097     1.4129     1.0419     0.3258      0.0    
   1900     12292.17     1.42      1.4138     1.1475     0.3568      0.0    
   2000     12958.74    1.4093     1.4151     1.1216     0.3725      0.0    
   2100     13628.04    1.414      1.4159     1.1306     0.4107      0.0    
   2200     14293.08    1.4097     1.4156     1.1111     0.463       0.0    
   2300     14961.67    1.4203     1.4181     1.0886     0.4831      0.0    
   2400     15616.83    1.416      1.416      1.0697     0.5327      0.0    
   2500     16275.06    1.423      1.4147     1.0702     0.7634      0.0    
   2600     16934.89    1.4144     1.4134     1.1185     1.0768      0.0    
   2700     17596.76    1.4104     1.4168     1.1174     2.0171      0.0    
   2800     18256.64    1.413      1.4128     1.1055     3.4733      0.0    
   2900     18918.41    1.4211     1.4141     1.1023    34.5734      0.0    
   3000     19577.74    1.4202     1.4165     1.1515   7186.7995     0.0    
   3100     20232.76    1.4088     1.4151     1.1113   1910.4562     0.0    
   3200     20880.22    1.4205     1.4128     1.0274    948.3228     0.0    
   3300     21514.63    1.4159     1.4148     1.0485    66.1821      0.0    
   3400     22147.89    1.4154     1.4137      1.01     106.7975     0.0    
   3500     22783.1     1.4199     1.411      1.066     193.9605     0.0    
   3600     23416.27    1.4085     1.412      1.169    58061.1412    0.0    
   3700     24052.07    1.4208     1.4174     1.0998   17241.5289    0.0    
   3800     24687.94    1.4109     1.4144     1.1151    638.1392     0.0    
   3900     25321.31    1.4224     1.4169     1.1585   8553.0951     0.0    
   4000     25953.78    1.4194     1.417      1.1006   3190.0677     0.0    
