Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  3.592768430709839
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       106.23     1.4094     1.4156     3.1431   39168.1636   0.1377  
   200       262.67      1.41      1.4146     3.0108   173159.9202   0.4007  
   300       418.39     1.4158     1.4135     1.894    20055.1915   0.8983  
   400       573.93     1.4112     1.4061     3.0938   7542.3477    0.0186  
   500       729.25     1.4227     1.422      1.6298    5800.312    0.0867  
   600       884.52     1.4093     1.419      3.1598   5176.3467    0.0366  
   700      1039.55     1.4157     1.4147     3.1588   70985.0837   0.0453  
   800       1194.7     1.4196     1.4072     2.7006   16037.6764   0.0277  
   900      1349.26     1.4102     1.4184     1.0795   3479.3847    0.0187  
   1000     1503.54     1.417      1.4052     1.8349   6015.1542    0.0292  
   1100     1656.94     1.4144     1.4115     0.931     3093.99     0.0141  
   1200     1809.95     1.4259     1.4151     2.8791   3387.8217    0.0113  
   1300      1963.0     1.4172     1.4071     3.0616   3196.5255    0.0135  
   1400     2115.95     1.4104     1.4079     0.967     1199.197    0.013   
   1500     2268.04     1.406      1.4071     2.8301    373.711     0.0078  
   1600     2419.88     1.3998     1.4085     2.3127   2202.1756    0.0063  
   1700     2572.25     1.4221     1.4159     3.0898   5365.8018    0.0124  
   1800     2724.82     1.4096     1.4119     1.1957    834.0867    0.0091  
   1900     2877.23     1.4117     1.4117     0.9407   4934.3352     0.01   
   2000     3029.59     1.4222      1.41      3.1276   13712.2719   0.0067  
   2100     3181.95     1.4139     1.416      3.1405   59634.3008   0.0017  
   2200     3334.33     1.4167     1.4224     2.7227   8388.1986    0.0055  
   2300      3486.0      1.42      1.4099     2.8703    868.4393    0.0063  
   2400     3637.47     1.4152     1.4155     0.6966   1457.9224    0.0051  
   2500     3788.48     1.4152     1.407      2.2902    930.756     0.0031  
   2600     3939.32     1.4146     1.4125     2.852     383.4329    0.0032  
   2700     4089.93     1.4135     1.4149     2.1749   1259.9193    0.0028  
   2800     4241.05     1.416       1.41      3.1621   3459.4029    0.0024  
   2900     4392.23     1.4162      1.41      2.9019   1274.3519    0.0056  
   3000     4543.64     1.4178     1.4075     1.3828   55587.0291   0.0003  
   3100     4695.76     1.4139     1.4062     0.8999   143723.6246   0.0066  
   3200      4848.2     1.4139     1.4155     0.6474   14352.6732   0.0049  
   3300     4999.79     1.4056     1.4121     3.0938   2276.5495    0.0032  
   3400     5151.07     1.4131     1.4184     1.3102   1789.0042    0.0036  
   3500      5302.0     1.4227     1.4032     3.0872    803.9398    0.0029  
   3600     5452.48     1.4134     1.4134     1.3077   1182.2274    0.0011  
   3700     5602.89     1.4117     1.4127     0.5643   6338.8337    0.002   
   3800      5753.4     1.4181     1.4105     0.7976   2855.2704    0.0013  
   3900     5903.75     1.4135     1.4076     0.615    2520.1372    0.002   
   4000     6054.15     1.4181     1.4014     0.9793    692.9439    0.0018  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       103.52     1.4151     1.4136     0.109      0.2617     0.0003  
   200       273.81     1.417      1.4104     0.1438     0.3722     0.0003  
   300       444.31     1.4194     1.4152     0.1101     0.744       0.0    
   400       614.9      1.4116     1.4048     0.1141     1.5776      0.0    
   500       785.83     1.4193     1.4135     0.692     6757.342    0.0063  
   600       959.88     1.4157     1.413      0.8524   1529.5907    0.0381  
   700      1136.32     1.423      1.4062     0.9878   4241.5854    0.0458  
   800      1313.13     1.4166     1.4161     0.8632   4622.6847    0.2209  
   900       1489.8     1.4108     1.4123     0.7274    415.9513    0.1373  
   1000     1666.46     1.4149     1.4171     0.7558   1426.6618    0.006   
   1100     1843.37     1.4188     1.4079     0.841    1867.1111    0.0077  
   1200     2020.17     1.4155     1.4157     0.8251   1148.2566    0.0099  
   1300     2196.38     1.4101     1.4117     0.9005   6880.1034    0.0146  
   1400     2371.79      1.41      1.4145     0.7447   2329.9617    0.0019  
   1500     2547.51     1.4138     1.4195     0.8304   3944.3885    0.0299  
   1600     2723.16     1.4109     1.4164     0.8104    628.7317    0.0137  
   1700     2898.43     1.4139     1.4151     0.862    2361.9698    0.0167  
   1800      3074.0     1.4161     1.4195     1.4951   1871.6149    0.0298  
   1900     3249.36     1.4185     1.4172     2.0959   1904.8071    0.0154  
   2000     3425.19     1.4195     1.4137     0.8389   6290.3044    0.0281  
   2100     3600.92     1.4124     1.4116     2.7232   15036.8458   0.0031  
   2200      3776.7     1.4176     1.4146     0.9494   11765.3986   0.0077  
   2300     3952.64     1.416      1.4141     1.9417   7701.5986    0.0093  
   2400      4128.8     1.4168     1.4116     1.2974   1223.3194    0.0022  
   2500     4305.01     1.4185     1.413      0.8108   8871.6245    0.038   
   2600     4480.89     1.4248     1.4183     0.6781   2219.1276    0.0203  
   2700     4657.06     1.4212     1.4149     0.883    2257.8225    0.0351  
   2800     4833.27     1.4122     1.4111     0.9738   13645.3836   0.0522  
   2900     5009.47     1.4088     1.4124     2.5352   10304.6561   0.0843  
   3000     5185.12     1.4043     1.4197     2.0346   11271.085    0.0226  
   3100     5360.85     1.4107     1.4162     1.4256   4975.7128    0.3201  
   3200     5536.65     1.4182     1.4123     2.0872   13584061.1446   0.0863  
   3300     5713.28     1.4129     1.4143     3.1457   74044.3581   0.013   
   3400     5889.17     1.4165     1.4135     2.8728   5064.9304    0.012   
   3500     6064.52     1.4131     1.4163     1.0128   2297.1046    0.036   
   3600     6239.69     1.414      1.4207     0.6425    843.516     0.0118  
   3700     6414.92     1.4112     1.4168     0.6655   1370.6917     0.0    
   3800      6590.4     1.4196     1.4148     1.4567    3927.43     0.0193  
   3900     6765.91     1.4049     1.4101     0.6711   95820.3243   0.022   
   4000     6941.67     1.4114     1.4175     0.8613   20340.0676   0.0142  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       106.82     1.4114     1.4182     0.0401     0.0225     0.006   
   200       297.39     1.4127     1.4177     0.0487     0.0397     0.0025  
   300       486.84     1.4115     1.4155     0.0493     0.066      0.0009  
   400       675.82     1.4101     1.4158     0.0716     0.1064     0.0004  
   500       864.5      1.4232     1.4152     0.0802     0.1673     0.0001  
   600      1052.95     1.4121     1.4208     0.0758     0.2079     0.0002  
   700      1241.34     1.4152     1.4147     0.0917     0.3005     0.0001  
   800      1429.92     1.4056     1.4095     0.0886     0.6069     0.0001  
   900       1618.7     1.4143     1.4124     0.0914     2.4238     0.0002  
   1000     1807.54     1.4234     1.4099     0.2152    227.2475    0.0018  
   1100     1997.02     1.423      1.4171     0.5911    734.811     0.0031  
   1200     2188.62     1.4186     1.4165     0.8175    722.6174    0.0055  
   1300     2382.26     1.421      1.4192     0.6414    885.1715    0.0041  
   1400     2576.77     1.4203     1.4146     0.7783   8476.2699    0.1265  
   1500     2772.29     1.4153     1.4156     0.7203   1240.7679    0.0083  
   1600     2967.57     1.4088     1.4117     0.8187   7555.3334    0.0129  
   1700     3162.96     1.4139     1.4141     0.9818   3694.9858    0.0106  
   1800     3358.52     1.4203     1.4131     0.7148    4352.389    0.0197  
   1900     3554.57     1.4138     1.4171     1.151    1526.3767    0.0146  
   2000     3749.69     1.4124     1.4136     0.6447    402.6249    0.0094  
   2100      3944.5     1.4166     1.4139     0.5484    511.9518    0.0063  
   2200     4139.48     1.4111     1.4128     0.6708   1154.8754    0.0084  
   2300     4334.52     1.4194     1.4213     0.5729   1878.7035    0.0073  
   2400     4529.53     1.4166     1.4173     0.6863    459.1822    0.0074  
   2500     4724.53     1.4182     1.4132     0.7541   4449.6531    0.0103  
   2600      4919.8     1.4104     1.4127     0.7309   19490.6417   0.0103  
   2700     5115.46     1.4183     1.419      3.1261   3602.0904    0.0016  
   2800     5311.83     1.4111     1.4124     0.7032   1487.4339    0.0137  
   2900     5508.25     1.4153     1.4172     0.6048   2229.5279    0.0137  
   3000     5704.05     1.4162     1.4081     0.9999    481.0169    0.0093  
   3100     5899.64     1.4152     1.4164     0.5838   1951.5066    0.0093  
   3200     6095.34     1.4108     1.4149     1.0382   9199.7289    0.0075  
   3300     6291.29     1.3994     1.4133     0.7052   1026.4165    0.0062  
   3400     6487.14     1.4133     1.4121     0.6914   4417.3317    0.0159  
   3500     6683.07     1.4056     1.4108     0.8051   76611.7914   0.0195  
   3600     6879.39     1.4127     1.4148     1.0284   39519.2075   0.0282  
   3700     7075.65     1.4188     1.4165     0.8109   10051.6308   0.0214  
   3800      7272.6     1.4126     1.4183     0.6853   2338.5913    0.0123  
   3900     7468.84     1.4153     1.4132     0.6562   2533.3482    0.0112  
   4000     7665.25     1.4163     1.4132     2.161    14616.2558   0.0182  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       107.23     1.4156     1.4193     0.0341     0.0132     0.0058  
   200       319.67     1.4153     1.4165     0.0402     0.0271     0.0027  
   300       530.72     1.4088     1.4154     0.043      0.0381     0.0014  
   400       741.18     1.4113     1.4107     0.0571     0.0534     0.001   
   500       950.69     1.4106     1.4138     0.0547     0.0696     0.0005  
   600      1159.94     1.4137     1.4189     0.0628     0.0844     0.0004  
   700      1369.46     1.4067     1.4144     0.0804     0.1189     0.0002  
   800      1578.49     1.4149     1.4183     0.0949     0.1528     0.0003  
   900      1788.06     1.414      1.4153     0.0866     0.2335     0.0001  
   1000     1997.39     1.417      1.4153      0.08      0.3412     0.0001  
   1100      2206.8     1.4129     1.4177     0.0865     0.6903      0.0    
   1200     2416.31     1.4177     1.4173     0.081      1.7415     0.0001  
   1300     2626.05     1.4085     1.4126     0.116      93.306     0.0003  
   1400     2836.26     1.4116     1.4173     0.6552   1214.9442    0.004   
   1500     3048.77     1.4186     1.4134     0.6023    620.2831    0.0039  
   1600     3263.83     1.4132     1.413      0.8835   38688.8592   0.0282  
   1700     3480.44     1.4121     1.4096     3.0949    691.3446    0.0194  
   1800     3696.19     1.4069     1.4119     0.6793    2835.33     0.0131  
   1900     3912.05     1.4152     1.4157     0.8704    4409.511    0.0017  
   2000     4128.43     1.4119     1.4124     0.7025    807.2351    0.0169  
   2100     4344.44     1.4092     1.4163     0.8941   4760.0985    0.0091  
   2200      4560.5     1.4155     1.4171     0.6359    564.6153    0.0131  
   2300     4776.49     1.4113     1.4123     0.6996   3559.5077    0.0087  
   2400     4992.98     1.4101     1.413      0.8043   5815.3231    0.0102  
   2500     5209.19     1.4193     1.4129     0.6933   4841.3185    0.0149  
   2600     5425.24     1.4295     1.413      0.727    1301.1822    0.0072  
   2700     5641.87     1.4121     1.4172     0.7123   1838.4526    0.0146  
   2800     5858.21     1.4254     1.415      0.6184    898.7238    0.0078  
   2900     6073.94     1.4095     1.4152     2.9409    981.352     0.0073  
   3000      6290.0     1.4118     1.4157     0.9875   2680.3665    0.0109  
   3100     6506.44     1.4173     1.4107     0.6501   2324.9051    0.0009  
   3200     6722.99     1.4087     1.4152     0.6988   37483.6856   0.0118  
   3300     6939.23     1.4158     1.4178     0.6721   2407.4743    0.0136  
   3400     7155.41     1.4118     1.411      0.6032   1910.8504    0.0086  
   3500     7370.94     1.4179     1.415      0.6946   1752.5281    0.0061  
   3600     7586.71     1.4138     1.4143     0.3613   2620.3723    0.001   
   3700     7802.39     1.4178     1.4161     2.4088    502.8236    0.0055  
   3800     8018.61     1.4115     1.4131     0.6706   3499.2966    0.0064  
   3900     8234.49     1.4164     1.4144     0.5865   1735.6943    0.008   
   4000     8450.35     1.4119     1.4167     0.5633   1750.8238    0.0052  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       106.81     1.4114     1.414      0.0252     0.0109     0.0038  
   200       338.67     1.4095     1.4126     0.0342     0.0189     0.0022  
   300       569.73     1.4143     1.415      0.0369     0.0262     0.0013  
   400       799.9      1.4097     1.4154     0.0362     0.034      0.0011  
   500      1029.88     1.4222     1.414      0.0498     0.0459     0.0005  
   600      1259.54     1.4172     1.4133     0.0436     0.0467     0.0004  
   700       1489.0     1.4112     1.4185     0.0479     0.0612     0.0002  
   800      1718.49     1.4123     1.4111     0.075      0.0773     0.0001  
   900       1947.9     1.4138     1.4166     0.0554     0.1011     0.0001  
   1000     2177.16     1.4077     1.4153     0.0548     0.1146     0.0001  
   1100     2406.45     1.406      1.4152     0.0642     0.1513      0.0    
   1200     2635.75     1.4196     1.4108     0.0779     0.1934      0.0    
   1300     2865.12     1.4209     1.4131     0.0713     0.2528     0.0002  
   1400     3094.46     1.4158     1.4121     0.0743     0.4263     0.0001  
   1500     3323.68     1.4221     1.4181     0.0745     0.8322     0.0003  
   1600     3552.99     1.4144     1.4157     0.0756     1.6986     0.0005  
   1700     3782.55     1.4257     1.4137     0.0888     7.2403     0.0003  
   1800     4012.33     1.4196     1.4139     0.1742    474.7871    0.001   
   1900     4243.39     1.4161     1.4161     0.6878    384.1616    0.0015  
   2000     4477.34     1.4127     1.4143     0.6541    341.4512    0.0053  
   2100     4712.65     1.4134     1.4133     0.7168   130655.8717   0.0284  
   2200     4948.88     1.4125     1.4142     0.619     796.3432    0.0111  
   2300      5184.6     1.4106     1.4153     0.6648    936.1894    0.0069  
   2400     5420.59     1.4112     1.4144     0.6227    816.813     0.0065  
   2500      5656.9     1.4157     1.4172     0.688    9713.6106    0.0023  
   2600     5893.86     1.4094     1.4126     0.6929   3976.1661    0.0086  
   2700     6130.41     1.4161     1.4136     0.5136    241.5113    0.0067  
   2800     6366.96     1.4129     1.4134     0.6354    604.8194    0.0074  
   2900     6603.39     1.4121     1.4141     0.5925    604.6177    0.0062  
   3000     6840.05     1.4136     1.4149     3.1469    890.0165    0.0017  
   3100     7076.82     1.416      1.4149     0.679     6409.429    0.0084  
   3200      7313.4     1.4199     1.4167     0.6846    876.243     0.0062  
   3300     7549.92     1.4174     1.4199     0.5456   9790.1584    0.0069  
   3400     7786.78     1.4131     1.4165     0.6772    591.7337    0.0053  
   3500     8023.63     1.4203     1.4086     0.4894   1562.6277    0.0051  
   3600     8260.45     1.4137     1.4184     0.678    2845.3284    0.005   
   3700     8495.53     1.4092     1.4152     0.6933   6128.3869    0.0037  
   3800     8730.15     1.416      1.4195     0.7002   2450.1156    0.0097  
   3900     8964.66     1.4107     1.4135     0.6609    811.7465    0.0101  
   4000      9199.3     1.4133     1.415      2.1386    656.0355    0.0106  
