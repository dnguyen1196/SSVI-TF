Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  8.38706111907959
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       214.16    258.7474   259.7354    3.1622     0.7633      0.0    
   200       428.46    237.4756   236.8397    2.6595     0.2923      0.0    
   300       642.91    232.5376   232.7835    2.2713     0.1346      0.0    
   400       857.33    230.3471   230.2099     1.63      0.1158      0.0    
   500      1071.81    230.3773   230.2756    1.5102     0.1114      0.0    
   600      1286.24    231.1414   230.8103    1.8323     0.0828      0.0    
   700      1500.71    229.5725   229.5025    1.4541     0.0623      0.0    
   800      1715.16    230.6896   230.4004    1.4787     0.0532      0.0    
   900      1929.61    229.9279   229.6316    1.5477     0.0526      0.0    
   1000     2143.99    229.927    229.2756    1.1588     0.0442      0.0    
   1100     2358.43    230.3308   230.1722    1.1795     0.0452      0.0    
   1200      2572.9    229.8842   229.8587    1.2999     0.0422      0.0    
   1300     2787.31    229.2773   229.5057    1.0907     0.0404      0.0    
   1400     3001.74    228.9036   229.2273    1.0185     0.0373      0.0    
   1500     3216.25    229.2925   229.4552    1.3037     0.0341      0.0    
   1600     3430.75    229.3959   229.5486    1.1709     0.0321      0.0    
   1700     3645.34    229.1823   229.2134    0.9059     0.0285      0.0    
   1800      3860.1    228.4815   228.6751    1.0748     0.0227      0.0    
   1900     4074.87    229.2395   229.0963    1.0873     0.0292      0.0    
   2000     4289.63    228.6944   228.7862    0.8539     0.0279      0.0    
   2100     4504.35    228.882    229.0392    1.2117     0.0247      0.0    
   2200     4719.04    228.215    228.9116    1.1399     0.0261      0.0    
   2300     4933.77    228.2488   228.8648    0.9667     0.0206      0.0    
   2400     5148.48    228.7181   228.5999    0.8665     0.0201      0.0    
   2500     5363.24    229.2494   229.3906    1.2299     0.016       0.0    
   2600     5578.07    228.9451   228.687     0.837      0.0229      0.0    
   2700     5792.81    228.2596   228.5824    1.0197     0.0162      0.0    
   2800     6007.55    228.8111   228.4891    0.7191     0.0166      0.0    
   2900     6222.48    229.1829   229.2252    0.7856     0.0224      0.0    
   3000     6437.32    227.9989   228.6002    0.8098     0.0148      0.0    
   3100     6652.25    228.3897   228.3266    0.9469     0.0186      0.0    
   3200     6866.39    228.1654   228.486     0.8581     0.0157      0.0    
   3300     7078.82    228.1315   228.2386    0.8891     0.0128      0.0    
   3400     7291.26    228.2898   228.3178    0.7768     0.016       0.0    
   3500     7503.68    228.2481   228.475     0.7832     0.0125      0.0    
   3600     7716.15    228.426    228.7618    0.934      0.0163      0.0    
   3700     7928.53    228.1121   228.4091    0.837      0.0132      0.0    
   3800     8140.97    228.3689   228.0339    1.0791     0.0107      0.0    
   3900     8353.46    228.3639   228.5026    0.7096     0.0126      0.0    
   4000     8565.79    227.7559   228.3043    0.948      0.0098      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       213.18    247.0714   247.4525    3.0651     0.8762      0.0    
   200       427.1     234.7009   234.122     1.9486     0.2791      0.0    
   300       641.0     230.4377   230.7631    1.7076     0.1799      0.0    
   400       855.24    229.1387   229.9112    1.2819     0.1429      0.0    
   500      1069.47    229.5023   230.2909    1.6967     0.1482      0.0    
   600       1283.7    229.6611   230.2244    1.3787     0.1206      0.0    
   700      1497.97    229.2561   229.3689    1.0674     0.0975      0.0    
   800      1712.22    230.1747   230.2237    1.513      0.0667      0.0    
   900      1926.52    228.9139   229.2412    1.2372     0.0551      0.0    
   1000      2140.8    228.634    229.1086    1.0515     0.0587      0.0    
   1100     2355.09    228.8016   229.2109    1.2417     0.0641      0.0    
   1200     2569.37     228.65    228.6867    1.2283     0.0601      0.0    
   1300     2783.71    229.5638   229.5129    0.9656     0.0432      0.0    
   1400     2997.95    228.6039   229.4942    0.9621     0.0472      0.0    
   1500     3212.22    228.7795   228.9663    1.1136      0.04       0.0    
   1600      3426.5    228.1614   229.0092    1.0151     0.0378      0.0    
   1700     3640.75    228.6057   228.5381    1.069      0.0328      0.0    
   1800      3855.0    228.857    228.5535    0.8675     0.0326      0.0    
   1900     4069.19    228.8454   229.2265    1.3721     0.0316      0.0    
   2000     4283.51    228.3914   229.0939    0.8672     0.0302      0.0    
   2100     4497.85    228.5537   228.8854    1.0417     0.0239      0.0    
   2200     4712.19    228.1662   228.741     1.0924     0.0304      0.0    
   2300     4926.32    228.9284   228.9638    0.7598     0.0216      0.0    
   2400     5140.46    228.1939   228.2736    0.9932     0.0217      0.0    
   2500      5354.5    228.2708   229.1878    1.016      0.0281      0.0    
   2600     5568.52    228.1342   228.7538    0.8313     0.0193      0.0    
   2700     5782.61    228.1647   228.4127    0.7285     0.0253      0.0    
   2800     5996.61    228.4339   228.6086    0.9572     0.0201      0.0    
   2900     6210.59    228.6111   228.5928    0.7178     0.0246      0.0    
   3000     6424.68    228.0078   228.6765    1.0029     0.0189      0.0    
   3100     6638.56    228.2969   228.4497    0.9147     0.0159      0.0    
   3200     6852.58    228.1792   228.9949    0.7387     0.0228      0.0    
   3300     7066.57    228.1334   228.1994    0.8439     0.0136      0.0    
   3400     7280.64    228.6086   228.506      0.81      0.0195      0.0    
   3500     7494.62    227.6173   228.4458    0.7573     0.0197      0.0    
   3600     7708.63    228.8231   228.905     0.8129     0.0156      0.0    
   3700     7922.57    228.265    228.6648    1.2119     0.0149      0.0    
   3800     8136.57    227.7926   228.1704    0.7264     0.0132      0.0    
   3900     8350.63    228.232    228.4475    0.6397     0.0124      0.0    
   4000     8564.61    227.5034   228.0724    1.1394     0.0115      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       213.52    246.7489   246.4681    2.6558     0.9364      0.0    
   200       428.19    231.2562   231.3367    1.2304     0.2735      0.0    
   300       642.92    230.7153   230.3972    1.5714     0.2364      0.0    
   400       857.64    228.964    229.5529    1.0986     0.1786      0.0    
   500      1072.43    228.4066   229.1047    1.3702     0.1514      0.0    
   600      1287.21    228.5325   228.7679    0.8264     0.1197      0.0    
   700       1502.2    228.2164   228.5585    1.0086     0.0998      0.0    
   800      1717.26    228.4654   228.7049    1.2113     0.1208      0.0    
   900      1932.14    228.3423   228.5279    1.0985     0.084       0.0    
   1000     2147.14    228.1546   228.6722    0.931      0.0592      0.0    
   1100     2362.13    228.6094   229.1577    1.3534     0.0521      0.0    
   1200     2576.99    227.676    228.408     0.9603     0.0462      0.0    
   1300     2791.96    228.6814   229.1097    1.1607     0.0572      0.0    
   1400     3006.75    228.2796   228.9733    0.9495     0.0449      0.0    
   1500     3221.88    228.4149   228.5438    1.0481     0.0382      0.0    
   1600     3436.91    227.8298   228.3106    0.9787     0.0471      0.0    
   1700     3651.98    228.0722   228.2428    1.0472     0.0342      0.0    
   1800     3867.01    228.2546   228.5297    0.8715     0.0341      0.0    
   1900     4082.15    227.7884   228.4551    0.8864     0.0327      0.0    
   2000     4297.26    228.1314   228.683     0.981      0.0317      0.0    
   2100      4512.4    227.7824   228.176     0.7405     0.0379      0.0    
   2200     4727.43    227.7066   228.3206    1.0229     0.0246      0.0    
   2300     4942.53    227.5174   227.6557    0.7897     0.0244      0.0    
   2400     5157.68    227.544    228.0331    1.1613     0.0283      0.0    
   2500     5372.81    227.6806   228.337     0.8282     0.0199      0.0    
   2600     5588.04    227.6177   227.9105    0.7263     0.0206      0.0    
   2700     5803.01    227.1579   228.0782    0.8099     0.0244      0.0    
   2800     6017.95    227.9431   228.2767    0.9222     0.0206      0.0    
   2900     6233.06    227.3739   227.9256    0.571      0.0179      0.0    
   3000     6448.24    227.5277   228.4937    0.7554     0.0248      0.0    
   3100     6663.36    228.5631   228.7899    0.7896     0.0209      0.0    
   3200     6878.41    227.8473   228.4928    0.7878     0.015       0.0    
   3300     7093.39    227.8357   228.2342    0.8471     0.0235      0.0    
   3400     7308.42    227.7165   228.1959    0.8712     0.0164      0.0    
   3500      7523.5    228.1713   228.7315    0.8847     0.0177      0.0    
   3600     7738.55    227.8707   228.0355    0.6969     0.0157      0.0    
   3700     7953.63    227.6939   227.911     0.7417     0.019       0.0    
   3800     8168.77    227.7239   228.1435    0.906      0.0136      0.0    
   3900     8383.91    227.8138   228.0228    0.9883     0.0143      0.0    
   4000     8599.06    227.707    227.7233    0.6632     0.0139      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       214.39    251.4058   251.3158    2.5025     0.9784      0.0    
   200       430.89    231.8429   232.4155    1.0427     0.2947      0.0    
   300       647.44    228.9986   229.2547    0.6433     0.2009      0.0    
   400       864.08    228.6145   229.3645    1.0098     0.2292      0.0    
   500      1080.63    228.3364   229.2323    0.8158     0.1534      0.0    
   600      1297.36    228.1015   228.6938    0.8364     0.1152      0.0    
   700      1514.06    227.3877   228.4068    1.092      0.0865      0.0    
   800      1730.78    227.7611   228.5666    0.8164     0.0827      0.0    
   900      1947.52    227.2247   228.3072    0.8335     0.0663      0.0    
   1000     2164.26    228.0267   228.9581    0.8731     0.0587      0.0    
   1100     2380.83    227.7275   228.576     1.0391     0.0558      0.0    
   1200     2597.36    227.8988   228.8723    1.0478     0.0489      0.0    
   1300     2813.89    227.2997   228.2285    0.8187     0.0499      0.0    
   1400     3030.45    227.7921   228.3866    1.1456     0.0583      0.0    
   1500     3246.81    227.5129   228.1339    0.8355     0.0446      0.0    
   1600      3463.3    227.3686   227.9311    0.6643     0.0396      0.0    
   1700     3679.77    227.9221   228.8158    0.8586     0.0404      0.0    
   1800     3896.09    227.565    228.2636    0.696      0.0433      0.0    
   1900      4112.5    227.7595   228.2411    0.9859     0.0331      0.0    
   2000     4328.89    227.8798   228.5704    1.1023     0.0257      0.0    
   2100     4545.19    227.5979   228.5674    0.788      0.0303      0.0    
   2200     4761.63    227.5122   228.5921    1.0054     0.0364      0.0    
   2300     4977.92    227.6656   228.6093    0.9903     0.0285      0.0    
   2400     5194.34    227.1996   228.0897    0.5911     0.0284      0.0    
   2500     5410.72    227.5552   228.2431    0.7473     0.028       0.0    
   2600     5627.11    227.3203   228.0625    0.6285     0.0312      0.0    
   2700      5843.5    227.8566   228.5869    0.9551     0.0185      0.0    
   2800     6059.95    227.7102   228.7335    0.7425     0.0182      0.0    
   2900     6276.38    227.5812   228.4762    0.7142     0.0225      0.0    
   3000     6492.72    227.6533   228.2566    0.5893     0.0245      0.0    
   3100     6709.07    226.9042   227.8599    0.5342     0.0211      0.0    
   3200     6925.46    226.7504   228.0854    0.7454     0.0277      0.0    
   3300     7141.82    227.2716   228.3014    0.691      0.0198      0.0    
   3400     7358.18    227.342    228.1799    0.6531     0.0152      0.0    
   3500     7574.39    227.6303   228.4913    0.8337     0.0159      0.0    
   3600     7790.67    227.1887   228.1552    0.7652     0.0162      0.0    
   3700     8006.88    227.7917   228.3576    0.6675     0.0164      0.0    
   3800     8223.03    227.0786   227.9792    0.6488     0.016       0.0    
   3900     8439.28    227.5015   228.2353    0.9334     0.0134      0.0    
   4000     8655.55    227.3996   228.4536    0.6073     0.0159      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       213.32    257.2895   256.7301    2.142      0.7901      0.0    
   200       429.24    232.5793   232.852     0.7541     0.3807      0.0    
   300       645.11    228.7256   229.3181    0.7182     0.1904      0.0    
   400       860.94    227.809    228.7925    1.3181     0.1585      0.0    
   500      1076.74    228.2589   228.5958    0.8981     0.116       0.0    
   600      1292.57    227.9139   228.3442    0.9457     0.1618      0.0    
   700      1508.43    227.8353   228.1347    0.7928     0.0929      0.0    
   800      1724.21    227.4877   227.9354    0.7765     0.0786      0.0    
   900      1940.03    228.025    228.5216    0.9076     0.0701      0.0    
   1000     2155.85    227.437    228.2901    0.9524     0.0598      0.0    
   1100     2371.72    227.2285   228.2211    0.8146     0.049       0.0    
   1200      2587.7    227.7263   228.2388    0.8413     0.0674      0.0    
   1300     2803.58    227.4299   228.113     0.5746     0.0523      0.0    
   1400     3019.44    227.3455   227.9424    0.652      0.0497      0.0    
   1500     3235.29    228.0093   228.6349    0.694      0.0459      0.0    
   1600     3448.87    227.0249   227.862     0.8044     0.0417      0.0    
   1700     3643.15    227.4731   228.1657    0.7879     0.0569      0.0    
   1800      3838.6    227.7895   228.0831    0.5616     0.0334      0.0    
   1900     4034.07    227.4641   228.0382    0.8206     0.0582      0.0    
   2000     4228.43    227.5629   227.9969    0.6311     0.0308      0.0    
   2100     4423.44    227.3628   228.0259    0.8193     0.0422      0.0    
   2200     4618.95    227.4245   228.0591    0.7287     0.0298      0.0    
   2300     4813.12    227.7437   228.1595    0.8069     0.0342      0.0    
   2400     5007.28    227.1352   228.1645    0.7803     0.034       0.0    
   2500     5201.64    227.1491   227.7972    0.5471     0.0232      0.0    
   2600     5395.17    227.2896   228.1683    0.8147     0.0249      0.0    
   2700     5589.65    227.4212   228.0977    0.7149     0.0248      0.0    
   2800     5784.73    227.3886   227.9901    0.6227     0.0226      0.0    
   2900     5979.73    227.0713   227.6764    0.5217     0.0242      0.0    
   3000      6174.0    227.2434   228.0033    0.6742     0.0199      0.0    
   3100     6368.71    227.3574   228.0672    0.6525     0.0258      0.0    
   3200      6562.9    227.9066   228.1122    0.816      0.0219      0.0    
   3300     6758.42    227.1176   228.0267    0.6314     0.0186      0.0    
   3400      6954.4    227.4649   227.9547    0.6547     0.0177      0.0    
   3500     7162.38    227.156    227.9755    0.7014     0.0244      0.0    
   3600     7378.43    227.2659   227.7969    0.5626     0.0271      0.0    
   3700     7594.72    227.0977   227.963     0.7081     0.0221      0.0    
   3800     7811.28    227.399    228.1118     0.75      0.0195      0.0    
   3900     8027.73    227.4732   227.9841    0.7276     0.0239      0.0    
   4000     8244.07    227.2172   227.7959    0.5307     0.0155      0.0    
