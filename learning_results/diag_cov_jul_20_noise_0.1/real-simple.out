Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  6.955545902252197
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       59.71     274.4975   275.285     3.1603     0.6948      0.0    
   200       120.11    242.2761   240.9655    2.7687     0.2337      0.0    
   300       180.53    235.4249   235.1853    2.0985     0.1273      0.0    
   400       240.95    232.854    232.7149    1.9559     0.0792      0.0    
   500       301.34    231.6646   232.2086    1.754      0.0565      0.0    
   600       361.75    231.5005   231.3307    1.3553     0.0416      0.0    
   700       422.16    233.4689   232.8653    2.4715     0.0325      0.0    
   800       482.57    231.2366   230.5363    1.6487     0.0258      0.0    
   900       542.99    232.2167   231.9889    1.443      0.0214      0.0    
   1000      603.39    230.258    230.1994    1.2013     0.0182      0.0    
   1100      663.8     230.1077   229.8646    1.1525     0.0157      0.0    
   1200      724.2     229.7854   229.8654    1.2406     0.0137      0.0    
   1300      784.59    229.914    229.7196    1.294      0.012       0.0    
   1400      845.01    229.4649   229.7049    1.3624     0.0108      0.0    
   1500      905.41    229.5956   229.9475    1.1667     0.0094      0.0    
   1600      965.81    229.383    229.6969    0.9549     0.0085      0.0    
   1700     1026.22    229.1749   229.2378    1.0861     0.0077      0.0    
   1800      1086.6    228.8608   228.8119    1.3285     0.0072      0.0    
   1900     1147.01    230.7215   230.3447    1.1116     0.0065      0.0    
   2000     1207.43    228.8022   228.7441    0.8584     0.0061      0.0    
   2100     1267.85    229.8472   230.5417    1.3058     0.0055      0.0    
   2200     1328.29    228.8443   228.7295    0.8392     0.0053      0.0    
   2300     1388.74    229.3425   229.3947    1.0054     0.0048      0.0    
   2400     1449.17    228.9381   229.0324    0.8953     0.0045      0.0    
   2500      1509.6    228.7082   228.8726    1.2209     0.0042      0.0    
   2600     1570.03    228.2678   229.6741    0.8283     0.004       0.0    
   2700     1630.46    228.8912   229.708     1.3198     0.0037      0.0    
   2800     1690.92    228.8227   229.7718    1.0765     0.0035      0.0    
   2900     1751.39    229.7279   229.4734    1.201      0.0034      0.0    
   3000     1811.87    228.7119   229.1618    0.877      0.0032      0.0    
   3100     1872.33    228.8695   228.8868    0.9585     0.003       0.0    
   3200     1932.78    228.8557   229.0127    0.9289     0.0028      0.0    
   3300     1993.13    228.3166   229.1221    0.8841     0.0027      0.0    
   3400     2053.14    229.1105   228.975     0.8575     0.0026      0.0    
   3500     2113.15    227.6096   228.1933    0.9755     0.0024      0.0    
   3600     2173.15    228.8613    228.65     0.8268     0.0024      0.0    
   3700     2233.12    228.9513   228.9822    1.0075     0.0023      0.0    
   3800      2293.1    228.7494   228.505     0.7409     0.0022      0.0    
   3900      2353.1    229.0623   228.688     0.9332     0.0021      0.0    
   4000      2413.1    229.1463    229.16     0.8635     0.002       0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       59.37     246.7199   246.5236    2.6375     0.6151      0.0    
   200       120.01    233.9099   234.0002    2.0552     0.3197      0.0    
   300       180.66    231.2011   231.9713    1.8322     0.2022      0.0    
   400       241.33    230.8359   231.3923    1.6286     0.1389      0.0    
   500       301.99    230.6788   231.3075    1.6829     0.1025      0.0    
   600       362.65    229.4352   230.1999    1.2193     0.0802      0.0    
   700       423.31    229.3914   230.5777    1.3793     0.0629      0.0    
   800       483.97    228.6104   229.5983    1.476      0.0519      0.0    
   900       544.62    228.5412   228.9044    1.3171     0.0433      0.0    
   1000      605.27    229.6198   229.7799    1.1161     0.0361      0.0    
   1100      665.94    231.0687   231.3973    1.5096     0.0311      0.0    
   1200      726.61    229.5591   230.1005    1.1229     0.0268      0.0    
   1300      787.27    229.9595   230.6732    0.9363     0.0237      0.0    
   1400      847.93    229.5562   230.1162    1.2703     0.0216      0.0    
   1500      908.59    228.6399   229.0852    1.0143     0.0185      0.0    
   1600      969.25    229.2224   229.5793    1.1484     0.0165      0.0    
   1700     1029.92    229.9446   230.4811    0.8261     0.0151      0.0    
   1800     1090.56    229.1455   229.3444    1.4401     0.014       0.0    
   1900     1151.21    229.4552   229.5415    1.0732     0.0123      0.0    
   2000     1211.87    229.0667   229.6368    1.151      0.0115      0.0    
   2100     1272.51    228.9057   229.291     0.8497     0.0105      0.0    
   2200     1333.15    228.6367   229.1924    0.8875     0.0098      0.0    
   2300     1393.77    228.9653   229.2061    1.1418     0.0091      0.0    
   2400     1454.41    228.5302   229.168     0.9503     0.0083      0.0    
   2500     1515.08    228.5725   228.786     0.839      0.0078      0.0    
   2600     1575.75    228.4656   228.7011    0.8662     0.0073      0.0    
   2700      1636.4    229.3307   229.358     0.8865     0.007       0.0    
   2800     1697.06    228.8606   228.9606    0.8973     0.0065      0.0    
   2900     1757.72    228.9828   229.5406    0.9787     0.0061      0.0    
   3000     1818.37    228.6072   228.8546    0.7377     0.0058      0.0    
   3100     1879.03    228.1538   228.853     0.9961     0.0057      0.0    
   3200     1939.69    228.095    228.6316    0.8576     0.0052      0.0    
   3300     2000.32    229.2783   229.3926    0.9465     0.0048      0.0    
   3400     2060.97    228.2711   228.8047    0.8653     0.0046      0.0    
   3500     2121.62    228.0704   228.6369    0.8083     0.0045      0.0    
   3600     2182.27    228.0847   228.4018    0.8148     0.0042      0.0    
   3700     2242.92    228.7747   229.0284    0.6721     0.0041      0.0    
   3800     2303.56    228.5659   228.6093    0.7728     0.0038      0.0    
   3900     2364.21    228.6159   228.8518    0.6543     0.0038      0.0    
   4000     2424.86    228.8769   229.0947    0.8344     0.0034      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       59.65     249.5831   248.7354    2.5392     0.4808      0.0    
   200       121.53    232.9452   233.1473    1.4655     0.3025      0.0    
   300       183.73    230.477    230.5104    1.6178     0.2183      0.0    
   400       246.07    229.3994   230.1845    1.417      0.1612      0.0    
   500       308.41    228.9068   229.5525    1.246      0.1137      0.0    
   600       370.74    229.9143   229.9996    1.3289     0.092       0.0    
   700       433.08    228.5652   229.2424    1.0634     0.0722      0.0    
   800       495.5     228.2166   228.7043    0.977      0.0616      0.0    
   900       557.82    228.415    229.2625    0.9241     0.0507      0.0    
   1000      620.12    228.8875   228.9543    1.0518     0.0442      0.0    
   1100      682.57    229.0763   229.3493    0.9104     0.0385      0.0    
   1200      744.86    229.6276    229.9      0.9852     0.033       0.0    
   1300      807.11    228.5074   228.5728    0.9623     0.0289      0.0    
   1400      869.33    228.3569   228.7151    0.9865     0.026       0.0    
   1500      931.84    227.9767   228.4833    1.0452     0.0235      0.0    
   1600      994.33    228.1496   228.9525    1.0173     0.0208      0.0    
   1700     1056.97    228.9495   228.8968    1.0397     0.019       0.0    
   1800     1119.56    228.7255   228.7459    1.0866     0.0176      0.0    
   1900     1181.24    228.3286   228.5411    1.2492     0.0157      0.0    
   2000     1242.78    229.3186   229.4818    0.8866     0.0149      0.0    
   2100     1304.31    228.2865   229.2771    1.0074     0.0133      0.0    
   2200     1365.84    228.6798   228.9486    0.8724     0.0125      0.0    
   2300     1427.37    228.2167   228.7026    0.869      0.0116      0.0    
   2400      1488.9    228.2839   228.6336    0.954      0.0108      0.0    
   2500     1550.41    228.235    228.5824    0.9322     0.0098      0.0    
   2600     1611.93    228.9317   228.9249    0.9721     0.0093      0.0    
   2700     1673.47    228.5591   228.9117    0.8149     0.0088      0.0    
   2800     1735.01    227.8251   227.9071    0.8708     0.0081      0.0    
   2900     1796.56    228.2213   228.3993    0.711      0.0078      0.0    
   3000     1858.11    227.739    228.5103    1.3631     0.0072      0.0    
   3100     1919.64    227.9298   228.474     0.5561     0.0068      0.0    
   3200      1981.2    228.8089   228.8581    0.8441     0.0065      0.0    
   3300     2042.74    227.6871   228.5315    0.8621     0.0061      0.0    
   3400     2104.27    228.4112   229.2143    0.7895     0.006       0.0    
   3500      2165.8    227.8711   228.4599    0.7834     0.0056      0.0    
   3600     2227.35    227.5967   228.383     0.6657     0.0054      0.0    
   3700     2288.85    227.457    228.2681    0.7518     0.005       0.0    
   3800     2350.36    227.8437   228.1718    0.8508     0.0048      0.0    
   3900     2411.88    227.9703    228.29     0.9327     0.0046      0.0    
   4000     2473.39    228.1381   228.8983    0.7726     0.0044      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       59.73     254.1535   253.9549    2.3933     0.4148      0.0    
   200       121.78    232.3706   233.1536    0.9875     0.2892      0.0    
   300       183.82    229.9198   230.5088    0.8615     0.2026      0.0    
   400       245.86    228.432    229.2224    0.924      0.1539      0.0    
   500       307.92    228.4176   229.3736    1.0017     0.1196      0.0    
   600       369.99    227.9945   228.8992    0.7732     0.0959      0.0    
   700       432.07    228.4899   229.536     1.3527     0.0788      0.0    
   800       494.1     227.9366   229.1889    0.8358     0.0669      0.0    
   900       556.12    228.129    228.9081    0.9983     0.0582      0.0    
   1000      618.18    228.5631   229.1963    0.7608     0.0494      0.0    
   1100      680.24    228.2518   229.1971    0.8402     0.043       0.0    
   1200      742.28    227.7403   229.001     1.0185     0.0376      0.0    
   1300      804.28    227.6747   228.5769    1.0256     0.0331      0.0    
   1400      866.3     228.3526   229.0497    0.8578     0.0302      0.0    
   1500      928.3      228.6     229.1308    1.0722     0.0267      0.0    
   1600      990.34    228.0411   229.1064    0.9003     0.0241      0.0    
   1700     1052.36    228.1352   228.9017    0.6829     0.0215      0.0    
   1800     1114.37    227.4534   228.4639    0.8305     0.0198      0.0    
   1900     1176.39    227.8231   228.8366    0.7376     0.0183      0.0    
   2000     1238.42    227.3726   228.3331    0.6661     0.0167      0.0    
   2100     1300.45    228.0654   229.2534    0.9909     0.0157      0.0    
   2200     1362.48    228.2242   228.9979    1.0027     0.014       0.0    
   2300     1424.52    227.3629   228.4523    0.9209     0.0133      0.0    
   2400     1486.56    228.3603   229.2813    1.0039     0.0122      0.0    
   2500      1548.6    227.8371   228.5091    0.6094     0.0115      0.0    
   2600     1610.65    228.218    229.479     0.8255     0.0107      0.0    
   2700     1672.69    227.4924   228.6202    0.9547     0.0102      0.0    
   2800     1734.74    227.7032   228.8546    0.889      0.0095      0.0    
   2900     1796.78    229.3262   229.6084    0.8274     0.0089      0.0    
   3000     1858.84    227.5034   228.3638    0.5633     0.0086      0.0    
   3100     1920.87    227.6825   228.6079    0.5754     0.0079      0.0    
   3200     1982.91    227.897    228.6834    0.6792     0.0078      0.0    
   3300     2044.94    227.5212   228.5387    0.7042     0.0072      0.0    
   3400     2106.97    227.513    228.943     0.6246     0.0068      0.0    
   3500     2168.99    227.1323   228.3772    0.778      0.0066      0.0    
   3600      2231.0    227.3799   228.5331    0.5887     0.0063      0.0    
   3700     2293.04    226.8845   228.3125    0.6022     0.0059      0.0    
   3800     2355.05    227.5093   228.4002    0.9279     0.0056      0.0    
   3900     2417.07    227.4449   228.4623    0.6294     0.0056      0.0    
   4000     2479.14    227.6475   228.5621    0.6447     0.0052      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       59.71     259.6946   259.3443    2.2253     0.3661      0.0    
   200       122.36    233.5085   233.6616    0.7525     0.2654      0.0    
   300       184.98    230.0617   230.6545    0.9217     0.1956      0.0    
   400       247.63    229.2356   229.3945    0.7978     0.1584      0.0    
   500       310.27    228.231    228.8404    0.8074     0.1206      0.0    
   600       372.91    228.2469   229.1882    0.9075     0.1014      0.0    
   700       435.53    227.7255   228.5674    0.7371     0.0823      0.0    
   800       498.16    227.2101   228.1656    0.6786     0.0682      0.0    
   900       560.8     227.4938   228.5744    0.8423     0.0618      0.0    
   1000      623.26    227.3322   228.4877    1.4737     0.0523      0.0    
   1100      685.4     227.4313   228.4888    0.9262     0.0459      0.0    
   1200      747.53    227.1284   228.337     0.6882     0.0407      0.0    
   1300      809.67    227.813    228.5495    0.6743     0.036       0.0    
   1400      871.84    227.9567   228.2274    0.8394     0.0324      0.0    
   1500      933.98    227.3718   228.6205    0.8212     0.0299      0.0    
   1600      996.12    227.323    227.8902    0.9175     0.0269      0.0    
   1700     1058.26    227.6794   228.4783    0.9921     0.025       0.0    
   1800     1120.39    227.5213   228.1752    0.7578     0.0229      0.0    
   1900     1182.53    227.004    228.1675    0.9263     0.0204      0.0    
   2000     1244.65    227.6831   228.4464    0.8519     0.0187      0.0    
   2100     1306.79    227.6544   228.2434    0.7833     0.0169      0.0    
   2200     1368.92    228.6882   229.0725    0.6245     0.016       0.0    
   2300     1431.02    227.8488   228.292     0.8031     0.015       0.0    
   2400     1493.14    227.7787   228.4862    0.7206     0.0141      0.0    
   2500     1555.27    227.206    228.0264    0.5498     0.0129      0.0    
   2600     1617.38    227.6138   228.484     0.5941     0.0127      0.0    
   2700     1679.51    227.8301    228.38     0.696      0.0116      0.0    
   2800     1741.64    227.4923   228.2143    0.7829     0.0106      0.0    
   2900     1803.75    227.4257   228.4022    0.8143     0.0101      0.0    
   3000     1865.74    227.6199   228.2447    0.6395     0.0095      0.0    
   3100     1927.73    227.786    228.4058    0.7604     0.0091      0.0    
   3200     1989.82    227.3908   227.9673    0.6877     0.0085      0.0    
   3300     2051.86    227.7576   228.0672    0.7192     0.0082      0.0    
   3400     2114.11    227.2978   228.0139    0.7573     0.0079      0.0    
   3500     2176.54    227.1933   228.1762    0.8068     0.0075      0.0    
   3600     2238.95    227.5495   228.1862    0.6357     0.0071      0.0    
   3700     2301.36    227.0765   228.1199    0.696      0.0067      0.0    
   3800     2363.78    227.4924   228.4038    0.7093     0.0064      0.0    
   3900      2426.2    227.3651   227.9379    0.5777     0.0061      0.0    
   4000     2488.59    227.3065   228.0568    0.6412     0.006       0.0    
