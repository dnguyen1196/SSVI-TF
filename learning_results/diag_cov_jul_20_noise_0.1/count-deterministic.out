Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  8.894232511520386
max_count =  19  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       94.47      3.9019     3.9311     0.2853     0.2295    57838.85   46685.87  
   200       295.07     3.9464     3.9761     0.2203     0.1547    56303.17   45448.6   
   300       494.87     3.9769     4.0102     0.1897     0.117     55266.09   44610.36  
   400       696.5      4.0029     4.0376     0.1748     0.0939    54493.01   43983.89  
   500       895.29     4.028      4.0614     0.1684     0.0788    53897.39   43500.18  
   600      1093.69     4.0458     4.0833     0.1477     0.0689    53421.44   43113.62  
   700      1292.21     4.061      4.0948     0.1257     0.0583    53029.68   42795.53  
   800       1490.7     4.0735     4.1111     0.1236     0.0525    52703.35   42530.67  
   900      1689.22     4.088      4.1255     0.1187     0.0465    52430.82   42309.77  
   1000     1887.88     4.0962     4.1308     0.1166     0.0411    52196.06   42119.79  
   1100     2087.56     4.1069     4.1419     0.1015     0.0372    52000.42   41961.73  
   1200     2287.09     4.1109     4.1481     0.0983     0.0349    51834.79   41828.13  
   1300      2486.8     4.1188     4.1572     0.1035     0.0319    51694.6    41715.09  
   1400     2686.55     4.1258     4.1639      0.1       0.0286    51575.69   41619.48  
   1500     2886.35     4.1305     4.1659     0.0954     0.0286    51473.72   41537.93  
   1600     3086.15     4.1297     4.168      0.0987     0.0242    51379.09   41462.13  
   1700     3285.98     4.1182     4.1582     0.1143     0.024     51262.19   41368.76  
   1800     3485.89     4.094      4.1304     0.1215     0.0225    51076.07   41220.07  
   1900     3685.76     4.0424     4.0756     0.1284     0.0212    50777.24   40981.15  
   2000     3885.63     3.9577     3.9949     0.1362     0.0199    50371.45   40656.34  
   2100     4085.38     3.8283     3.8659     0.1516     0.0198    49882.27   40265.06  
   2200     4285.11     3.6312     3.6673     0.1641     0.0191    49381.86   39863.88  
   2300     4482.15     3.353      3.3855     0.1754     0.0197    48978.73   39538.26  
   2400      4679.0     3.0038     3.0391     0.1769     0.0171    48690.35   39302.79  
   2500     4875.74     2.6368     2.655      0.1592     0.0162    48520.25   39173.19  
   2600     5072.39     2.309      2.3243     0.1585     0.0154    48605.62   39259.21  
   2700     5269.02     2.1433     2.1556     0.1478     0.0144    49059.79   39634.22  
   2800     5465.86     2.2271     2.2278     0.1392     0.0135    49826.16   40251.13  
   2900     5662.83     2.5436     2.5433     0.1386     0.0128    50704.64   40959.8   
   3000     5859.77     3.0501     3.0511     0.1307     0.0121    51734.12   41799.03  
   3100     6056.91      3.65      3.6653     0.1549     0.0116    52818.57   42686.2   
   3200     6254.03     4.3202     4.3418     0.148      0.0119    53978.09   43643.98  
   3300     6451.37     4.9997     5.0337     0.2258     0.0114    55098.41   44570.02  
   3400     6648.78     5.6695     5.6993     0.152      0.0104    55839.81   45168.09  
   3500     6846.33     6.336      6.3975     0.1847     0.011     56548.66   45718.48  
   3600     7043.98     7.0402     7.0793     0.1352     0.0099    56672.94   45813.16  
   3700     7241.71     7.6717     7.7169     0.1352     0.0096    56478.72   45644.68  
   3800     7439.45     8.2872     8.3466     0.1255     0.0093    56092.39   45331.53  
   3900     7635.81     8.8627     8.9251     0.1458     0.0091    55915.18   45179.65  
   4000     7832.22     9.4696     9.5138     0.1348     0.009     55687.4    44977.99  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100        93.0      3.8561     3.8699     0.1217     0.0824    59790.82   96678.93  
   200       336.08     3.8721     3.8888     0.1012     0.0857    59098.51   95556.44  
   300       579.04     3.8879     3.9031     0.1005     0.0739    58467.6    94533.32  
   400       821.95     3.8996     3.9206     0.0878     0.0676    57893.27   93601.17  
   500      1064.64     3.9135     3.9329     0.0865     0.0614    57371.43   92753.68  
   600      1307.32     3.9246     3.9524     0.0876     0.0591    56894.27   91977.25  
   700      1550.02     3.9403     3.9643     0.0786     0.057     56458.52   91267.67  
   800      1792.62     3.961      3.9773     0.0812     0.0509    56053.76   90607.83  
   900      2035.11     3.9671     3.9888     0.0768     0.0501    55680.54   89998.78  
   1000      2277.7     3.9759     3.9986     0.0677     0.0445    55335.37   89434.91  
   1100     2520.24     3.9924     4.0102     0.0657     0.0439    55017.95   88916.01  
   1200     2762.74     4.0011     4.0203     0.0708     0.0403    54724.17   88435.56  
   1300     3005.27     4.0086     4.0302     0.0635     0.0383    54452.69   87991.61  
   1400     3247.86     4.0162     4.0408     0.0601     0.0363    54200.44   87578.71  
   1500     3490.39     4.0295     4.0483     0.0585     0.0341    53963.87   87191.35  
   1600     3732.91     4.0382     4.0566     0.0621     0.0347    53743.61   86830.92  
   1700     3975.65     4.0458     4.0653     0.0605     0.0325    53537.28   86493.47  
   1800     4219.11     4.0502     4.0727     0.0593     0.0303    53344.8    86178.79  
   1900     4462.67     4.0599      4.08      0.0554     0.0288    53163.69   85882.52  
   2000     4706.15     4.0663     4.0903     0.0534     0.0275    52996.09   85608.23  
   2100      4949.8     4.0723     4.0953     0.0601     0.0264    52839.25   85351.61  
   2200      5193.4     4.0793     4.0991     0.0572     0.0252    52691.2    85109.6   
   2300     5436.98     4.0849     4.1055     0.0472     0.0243    52552.7    84883.18  
   2400     5680.55     4.0898     4.1118     0.0524     0.0242    52425.52   84675.65  
   2500     5924.18     4.0951     4.1175     0.0483     0.0223    52306.0    84480.58  
   2600     6167.78     4.1004     4.1219     0.043      0.0221    52194.06   84297.94  
   2700     6411.33     4.1053     4.1264     0.0456     0.0213    52090.26   84128.58  
   2800     6654.02     4.1087     4.1322     0.0452     0.0195    51993.98   83971.51  
   2900     6899.06     4.1137     4.1376     0.0426     0.018     51904.87   83826.19  
   3000     7143.66     4.1189     4.1393     0.0505     0.0181    51822.13   83691.26  
   3100     7387.46     4.1209     4.146      0.0424     0.0174    51745.56   83566.43  
   3200     7631.09     4.1242     4.1477     0.0432     0.0171    51675.21   83451.76  
   3300     7876.24     4.1278     4.1508     0.0429     0.0167    51610.03   83345.61  
   3400     8119.58     4.1327     4.1555     0.0425     0.0156    51551.07   83249.51  
   3500     8362.27     4.1359     4.1566     0.0394     0.0163    51496.38   83160.46  
   3600     8604.63     4.1388      4.16      0.0479     0.0148    51446.7    83079.41  
   3700     8846.95     4.1428     4.1654     0.045      0.0143    51402.14   83006.61  
   3800     9089.29     4.1449     4.1679     0.0369     0.0145    51361.8    82940.72  
   3900     9331.69     4.1459     4.1706     0.0407     0.0134    51325.86   82882.04  
   4000     9574.09     4.1498     4.1738     0.0404     0.0131    51293.95   82829.85  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       92.96      3.8506     3.8577     0.0831     0.0599    60000.78   144945.2  
   200       382.03     3.8582     3.8696     0.0931     0.057     59481.62  143687.64  
   300       672.55     3.8736     3.8877     0.081      0.0533    58986.64  142489.38  
   400       964.47     3.8811     3.8973     0.0769     0.0594    58513.11   141343.9  
   500      1257.73     3.8954     3.9109     0.0744     0.0515    58069.46  140270.65  
   600      1547.44     3.9077     3.9206     0.068      0.0476    57654.65  139266.95  
   700      1836.44     3.9179     3.9367     0.0647     0.0489    57258.46  138309.17  
   800      2125.24     3.927      3.9444     0.0655     0.0519    56887.48  137410.85  
   900      2414.53     3.9396     3.9562     0.0686     0.043     56538.75  136565.48  
   1000     2707.56     3.952      3.9644     0.0682     0.0433    56212.12  135773.59  
   1100     3000.57     3.9565     3.9744     0.0611     0.0389    55903.15  135023.82  
   1200     3293.29     3.9671     3.9837     0.0617     0.0386    55610.3   134312.95  
   1300     3586.51     3.9804     3.9937     0.0587     0.0376    55337.82   133651.4  
   1400     3879.11     3.9878     4.0014     0.0597     0.0369    55078.44  133020.62  
   1500     4170.45     3.9958     4.0094     0.0553     0.033     54831.29  132419.83  
   1600     4463.26     4.0059     4.0193     0.0591     0.0325    54599.5   131856.08  
   1700     4756.31     4.009      4.0268     0.0546     0.0341    54378.28   131317.9  
   1800     5049.26     4.0149     4.0342     0.067      0.0299    54169.24  130809.22  
   1900     5342.33     4.0225     4.0391     0.0538     0.029     53972.23  130329.76  
   2000      5635.0     4.0311     4.0481     0.053      0.0294    53784.1   129871.79  
   2100     5928.23     4.0381     4.0547     0.0525     0.0274    53605.59  129437.53  
   2200     6220.83     4.0467     4.0612     0.0507     0.027     53435.33  129023.34  
   2300     6514.35     4.0508     4.0658     0.046      0.0251    53276.97  128638.09  
   2400     6807.79     4.0581     4.075      0.0465     0.0242    53125.42  128269.45  
   2500     7100.96     4.0661     4.0802     0.044      0.0246    52982.22  127921.29  
   2600     7393.81     4.0675     4.0867     0.0528     0.0229    52846.83  127592.15  
   2700     7686.82     4.075      4.0903     0.0451     0.0231    52716.9   127276.18  
   2800      7979.7     4.0811     4.0951     0.0454     0.0226    52595.14  126980.21  
   2900     8272.73     4.0842     4.1001     0.0415     0.0222    52479.69  126699.62  
   3000     8565.06     4.0885     4.1044     0.0465     0.0201    52370.01  126433.28  
   3100     8858.18     4.0926     4.1094     0.0387     0.0188    52265.95  126180.65  
   3200     9151.46     4.0966     4.1147     0.0423     0.0188    52167.98  125942.87  
   3300     9444.59     4.1032     4.1189     0.0411     0.0175    52075.53  125718.44  
   3400     9736.22     4.1051     4.1229     0.0431     0.0173    51988.89  125508.17  
   3500     10029.64    4.1123     4.1245     0.0368     0.0175    51907.37  125310.54  
   3600     10323.16    4.113      4.1308     0.0456     0.0164    51830.4   125123.87  
   3700     10616.88    4.116      4.1343     0.0474     0.0151    51757.99  124948.55  
   3800     10910.29     4.12      4.1374     0.039      0.0161    51690.78   124785.7  
   3900     11204.27    4.1245     4.1402     0.0377     0.015     51627.77  124632.99  
   4000     11496.94    4.1253     4.1431     0.0437     0.015     51568.95  124490.41  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       94.14      3.8426     3.857      0.0745     0.0489    60094.2   193718.86  
   200       431.6      3.8589     3.8678     0.0709     0.0472    59656.67  192305.82  
   300       767.34     3.8658     3.8759     0.0683     0.046     59237.77  190953.71  
   400       1103.1     3.8723     3.8869     0.0681     0.0448    58833.99  189652.43  
   500      1438.68     3.8821     3.894      0.0667     0.0433    58446.62  188403.68  
   600      1774.23     3.8954     3.9069     0.0673     0.044     58077.66  187213.13  
   700      2109.76     3.9034     3.9167     0.061      0.0417    57725.89  186078.14  
   800      2445.29     3.9177     3.9242     0.0687     0.0408    57390.35  184995.21  
   900      2780.68     3.9244     3.9322     0.0576     0.0374    57068.09  183954.72  
   1000     3116.67     3.9357     3.9418     0.0645     0.0415    56762.19  182966.41  
   1100     3452.75     3.942      3.9541     0.0661     0.0353    56470.8   182023.96  
   1200      3788.8     3.9505     3.9626     0.0597     0.0356    56192.19  181123.16  
   1300     4124.82     3.9564     3.9716     0.0562     0.0341    55928.82  180270.82  
   1400      4460.8     3.9703     3.9782     0.0558     0.0312    55677.34  179456.34  
   1500     4796.79     3.9732     3.9853     0.0533     0.0309    55437.02   178677.6  
   1600     5132.75     3.9809     3.9945     0.0558     0.0311    55207.7    177934.1  
   1700     5468.71     3.9922     4.0018     0.0504     0.0299    54988.1   177221.16  
   1800     5804.75     3.9975     4.0078     0.0503     0.0309    54778.72  176541.92  
   1900     6140.81     4.0045     4.0165     0.0533     0.027     54579.76  175895.95  
   2000     6476.68     4.0123     4.0223     0.0518     0.0281    54388.78  175275.98  
   2100     6812.53     4.0173     4.0296     0.047      0.0285    54206.81  174685.09  
   2200     7148.46     4.0217     4.0354     0.0476     0.0259    54033.73  174122.83  
   2300     7484.31      4.03      4.043      0.0519     0.0254    53866.16  173578.64  
   2400     7820.28     4.0358     4.0473     0.0473     0.0242    53706.66  173060.66  
   2500     8156.34     4.0402     4.0541     0.046      0.0226    53553.97  172564.42  
   2600     8492.38     4.0462     4.0601     0.049      0.0235    53409.37  172094.74  
   2700      8828.5     4.0506     4.0638     0.0428     0.0229    53268.85  171638.15  
   2800     9164.52     4.0573     4.069      0.0437     0.0226    53135.38  171204.78  
   2900     9500.58     4.0615     4.0734     0.0388     0.0202    53007.58  170790.04  
   3000     9836.69     4.0657     4.0795     0.0434     0.0198    52885.55  170393.48  
   3100     10172.9     4.072      4.0844     0.0449     0.0204    52767.69  170010.84  
   3200     10508.98    4.078      4.0879     0.039      0.0198    52655.8   169647.59  
   3300     10845.09    4.0785     4.0929     0.0424     0.0188    52549.6   169302.99  
   3400     11181.21    4.0846     4.0965     0.0367     0.0179    52447.94  168973.01  
   3500     11517.33    4.0884     4.103      0.0387     0.0183    52351.01  168658.67  
   3600     11853.69    4.0925     4.1064     0.0397     0.0166    52259.78  168362.75  
   3700     12189.98    4.0972     4.1099     0.049      0.0169    52172.35  168078.94  
   3800     12526.11    4.1004     4.1121     0.0367     0.0161    52089.29  167809.31  
   3900     12862.28    4.1061     4.1178     0.0355     0.016     52010.13  167552.43  
   4000     13198.58    4.1081     4.1212     0.0407     0.0156    51934.6   167307.32  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       93.32      3.8453     3.8605     0.0651     0.0438    60153.85  242531.41  
   200       476.13     3.8533     3.8691     0.0681     0.0429    59768.44  240976.35  
   300       859.14     3.8647      3.88      0.0597     0.0401    59393.79  239463.39  
   400      1241.94     3.872      3.888      0.067      0.0413    59032.41  238003.96  
   500      1624.66     3.8797     3.8957     0.0545     0.0385    58686.29  236608.01  
   600       2007.3     3.8894     3.9044     0.0577     0.0386    58348.01  235242.97  
   700      2390.53     3.8948     3.9152     0.067      0.0377    58022.95  233931.56  
   800      2773.75     3.9053     3.9243     0.0529     0.0353    57711.89  232675.26  
   900      3156.85     3.9126     3.9325     0.0549     0.0353    57416.39  231481.76  
   1000     3539.62     3.9217     3.9401     0.0575     0.036     57130.18  230325.29  
   1100     3922.74     3.9313     3.949      0.0632     0.0343    56854.72  229211.28  
   1200     4305.64     3.9348     3.9542     0.0535     0.034     56591.15  228146.14  
   1300     4688.42     3.9463     3.9648     0.051      0.0303    56341.68  227136.09  
   1400     5071.23     3.9524     3.9703     0.0494     0.0296    56099.26  226153.83  
   1500     5454.16     3.9612     3.9781     0.0535     0.0312    55868.48  225218.07  
   1600     5836.95     3.9627     3.9876     0.0521     0.0297    55645.7   224315.17  
   1700      6219.8     3.9746     3.9943     0.0513     0.0282    55432.14  223449.15  
   1800     6602.64     3.9792     4.0016     0.047      0.0268    55226.97  222616.94  
   1900     6985.69     3.988      4.0065     0.0495     0.0268    55030.68  221820.45  
   2000     7368.41     3.9922     4.0118     0.044      0.0286    54841.13  221050.86  
   2100     7751.28     4.0002     4.019      0.0499     0.0277    54658.31  220308.49  
   2200     8134.29     4.0065     4.0265     0.0507     0.0251    54482.82   219595.2  
   2300     8517.34     4.0126     4.0317     0.0414     0.0247    54315.11  218914.07  
   2400     8900.33     4.017       4.04      0.0451     0.0255    54153.63  218257.72  
   2500     9283.09     4.0205     4.0452     0.0463     0.0229    53999.2   217630.28  
   2600     9665.89     4.029      4.0481     0.0451     0.022     53850.45  217025.41  
   2700     10048.62    4.0326     4.0542     0.0443     0.022     53706.81  216441.66  
   2800     10431.37    4.0401     4.0609     0.0427     0.021     53568.27  215878.91  
   2900     10814.24    4.0442     4.0655     0.0399     0.0204    53435.65  215340.07  
   3000     11197.11    4.0487     4.0723     0.0421     0.0208    53307.21  214817.95  
   3100     11579.88    4.0546     4.0757     0.0397     0.0196    53184.9   214321.05  
   3200     11962.66    4.0588     4.0797     0.0391     0.0196    53067.37  213843.54  
   3300     12345.62    4.0641     4.0839     0.0371     0.0194    52954.19  213383.56  
   3400     12728.07    4.0695     4.0897     0.0382     0.0185    52844.76   212939.1  
   3500     13110.25    4.0725     4.0934     0.0372     0.0175    52740.55   212516.0  
   3600     13492.47    4.0771     4.0969     0.0377     0.0176    52640.41  212109.59  
   3700     13874.51    4.0792     4.1019     0.0372     0.0168    52543.66   211717.0  
   3800     14256.46    4.0831     4.1049     0.038      0.0166    52451.84  211344.43  
   3900     14638.5     4.0883     4.1089     0.0357     0.0159    52363.61  210986.64  
   4000     15020.55    4.0914     4.1136     0.0363     0.0159    52279.07  210643.54  
