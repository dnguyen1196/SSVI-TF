Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.973515272140503
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       80.99      0.7779     0.738      3.1274     0.7064   
   200       163.04     0.7144     0.7014     2.9752     0.2353   
   300       245.69     0.7171     0.6964     3.0349     0.1498   
   400       329.01     0.7141     0.6948     3.0569     0.098    
   500       411.39     0.7227     0.7038     2.882      0.0788   
   600       493.18     0.7188     0.6984     2.9229     0.0652   
   700       574.67     0.7165     0.7054     2.6638     0.072    
   800       656.0      0.7194     0.7047     2.9592     0.0547   
   900       737.24     0.7161     0.697      2.766      0.068    
   1000      818.44     0.7209     0.7087     2.5469     0.0614   
   1100      899.63     0.7138     0.6935     2.6649     0.0471   
   1200      980.81     0.7153     0.6976     2.8128     0.0419   
   1300     1061.95     0.7243     0.7017     2.613      0.0561   
   1400     1143.08     0.7144     0.6999     2.5786     0.0569   
   1500     1224.19     0.7193     0.702      2.2508     0.0378   
   1600      1305.3     0.7136     0.6921     2.2665     0.0394   
   1700     1386.37     0.7161     0.7053     2.1835     0.0388   
   1800     1467.38     0.7172     0.707      1.9613     0.0414   
   1900     1548.12     0.717      0.7067     1.8592     0.0667   
   2000     1628.86     0.7161     0.7054     1.9315     0.0493   
   2100     1709.62     0.7172     0.707      2.3937     0.0327   
   2200     1790.37     0.7176     0.7067     2.3398     0.0388   
   2300     1871.16     0.7179     0.7055     1.6541     0.0286   
   2400     1952.18     0.7183     0.7055     1.8133     0.0274   
   2500     2033.43     0.7178     0.707      2.1022     0.0422   
   2600     2114.69     0.719      0.7047     2.0796     0.0258   
   2700     2195.84     0.7188     0.6977     2.1298     0.0237   
   2800     2277.16     0.7202     0.6956     1.641      0.0325   
   2900     2358.46     0.7261     0.7055     2.151      0.0345   
   3000     2439.75     0.7263     0.7055     1.8208     0.0279   
   3100     2521.06     0.7255     0.7026     1.7614     0.0249   
   3200     2602.38     0.7237     0.7011     1.5823     0.0216   
   3300      2683.7     0.7237     0.7046     1.5442     0.0205   
   3400     2765.03     0.7181     0.7019     2.3039      0.02    
   3500     2846.35     0.717      0.6967     2.1626     0.0195   
   3600     2927.69     0.7188     0.7014     2.1345     0.0218   
   3700     3009.04     0.7176     0.7033     1.5783     0.0163   
   3800     3090.36     0.7209     0.7046     1.4139     0.0153   
   3900     3171.69     0.7207     0.7046     1.7792     0.0183   
   4000     3253.02     0.7178     0.7046     1.4188     0.0128   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       81.23      1.4145     1.4189     0.0007     0.1604   
   200       163.28     1.4104     1.4135     0.0006     0.1139   
   300       245.17     1.3882     1.3896     0.001      0.1045   
   400       326.9      1.4015     1.3988     0.0007     0.0982   
   500       408.49     1.4375     1.4326     0.0007     0.0922   
   600       489.96     1.4163     1.4129     0.0006     0.0859   
   700       571.34     1.4512     1.4492     0.0006     0.0804   
   800       652.64     1.4076     1.4018     0.0007     0.0752   
   900       733.88     1.4292     1.4257     0.0005     0.0705   
   1000      815.08     1.4323     1.4295     0.0004     0.0654   
   1100      896.27     1.4267     1.4226     0.0004     0.0611   
   1200      977.41     1.4247     1.4196     0.0007     0.0565   
   1300     1058.55     1.4188     1.4144     0.0005     0.0525   
   1400     1139.71     1.4433     1.4394     0.0004     0.0486   
   1500     1220.85     1.4444     1.4409     0.0007     0.0453   
   1600     1302.03     1.4681     1.4661     0.0005     0.0418   
   1700     1383.19     1.4686     1.4668     0.0004     0.039    
   1800      1464.4     1.4956     1.4933     0.0005     0.0365   
   1900     1545.61     1.4956     1.4933     0.0005     0.034    
   2000     1626.82     1.4956     1.4933     0.0004     0.032    
   2100     1708.01     1.4956     1.4933     0.0005      0.03    
   2200     1789.23     1.4956     1.4933     0.0006     0.0284   
   2300     1870.45     1.4956     1.4933     0.0005     0.0267   
   2400     1951.65     1.4956     1.4933     0.0004     0.0251   
   2500     2032.87     1.4956     1.4933     0.0005     0.0239   
   2600      2114.1     1.4956     1.4933     0.0006     0.0225   
   2700     2195.32     1.4956     1.4933     0.0005     0.0212   
   2800     2276.54     1.4956     1.4933     0.0005     0.0203   
   2900     2356.91     1.4956     1.4933     0.0003     0.0192   
   3000     2437.13     1.4956     1.4933     0.0004     0.0181   
   3100     2517.35     1.4956     1.4933     0.0005     0.0173   
   3200      2597.7     1.4956     1.4933     0.0006     0.0165   
   3300     2678.07     1.4956     1.4933     0.0004     0.0157   
   3400     2758.43     1.4956     1.4933     0.0005     0.0151   
   3500     2838.81     1.4956     1.4933     0.0004     0.0145   
   3600     2919.19     1.4956     1.4933     0.0005     0.0138   
   3700     2999.55     1.4956     1.4933     0.0004     0.0131   
   3800     3079.94     1.4956     1.4933     0.0008     0.0125   
   3900     3160.32     1.4956     1.4933     0.0005     0.012    
   4000     3240.69     1.4956     1.4933     0.0004     0.0115   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       80.45      1.4155     1.413      0.0011     0.1749   
   200       162.15     1.4155     1.4185     0.0016     0.1536   
   300       243.6      1.4188     1.424      0.0011     0.1384   
   400       324.87     1.4085     1.4149     0.001      0.1253   
   500       406.01     1.4092     1.4181     0.0007     0.1144   
   600       487.04     1.4232     1.4303     0.001      0.1034   
   700       567.99     1.4189     1.4314     0.0006     0.0939   
   800       648.9      1.4221     1.4343     0.0009     0.0844   
   900       729.78     1.4161     1.4247     0.0006     0.0763   
   1000      810.66     1.4186     1.427      0.0007     0.0688   
   1100      891.57     1.4605     1.4685     0.0005     0.0615   
   1200      972.5      1.4561     1.4644     0.0006     0.0554   
   1300     1053.42     1.4771     1.4859     0.0008     0.0506   
   1400     1134.37     1.4842     1.4902     0.0006     0.0458   
   1500     1215.35     1.4878     1.4953     0.0005     0.0419   
   1600     1296.32     1.4911     1.496      0.0008     0.0384   
   1700     1377.29     1.4911     1.496      0.0006     0.0355   
   1800     1458.28     1.4911     1.4961     0.0005     0.0329   
   1900     1539.26     1.4914     1.4963     0.0005     0.0307   
   2000     1620.27     1.4924     1.4973     0.0004     0.028    
   2100     1701.27     1.4932     1.499      0.0005     0.0264   
   2200     1782.24     1.4956     1.499      0.0006     0.0241   
   2300      1863.2     1.4956     1.499      0.0005     0.0227   
   2400     1944.19     1.4956     1.499      0.0007     0.0213   
   2500     2025.16     1.4956     1.499      0.0006     0.0199   
   2600     2106.14     1.4956     1.499      0.0004     0.0185   
   2700     2187.13     1.4956     1.499      0.0004     0.0173   
   2800     2268.09     1.4956     1.499      0.0006     0.0163   
   2900     2349.09     1.4956     1.499      0.0006     0.0153   
   3000     2430.11     1.4956     1.499      0.0012     0.0146   
   3100     2511.11     1.4956     1.499      0.0005     0.0137   
   3200     2592.11     1.4956     1.499      0.0005     0.0131   
   3300     2673.12     1.4956     1.499      0.0005     0.0122   
   3400     2754.14     1.4956     1.499      0.0006     0.0117   
   3500     2835.14     1.4956     1.499      0.0007     0.0113   
   3600     2916.13     1.4956     1.499      0.0006     0.0106   
   3700     2997.15     1.4956     1.499      0.0006     0.0101   
   3800     3078.14     1.4956     1.499      0.0005      0.01    
   3900     3159.13     1.4956     1.499      0.0007     0.0094   
   4000     3240.12     1.4956     1.499      0.0006     0.0091   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       80.61      1.4127     1.4173     0.0018     0.1962   
   200       162.98     1.4062     1.4115     0.0011     0.1751   
   300       245.08     1.414      1.4121     0.001      0.1572   
   400       327.01     1.4037     1.4022     0.0019     0.1398   
   500       408.79     1.3936     1.3909     0.0012     0.1242   
   600       490.48     1.4097     1.4053     0.0008     0.1106   
   700       572.12     1.3874     1.3856     0.001      0.0976   
   800       653.78     1.4043     1.4021     0.0012     0.0857   
   900       735.43     1.3967     1.3962     0.0008     0.0752   
   1000      817.1      1.3589     1.3656     0.0009     0.0665   
   1100      898.75     1.3913     1.3931     0.0008     0.0591   
   1200      980.44     1.4187     1.4218     0.0006     0.0526   
   1300     1062.14     1.4296     1.4285     0.0008     0.0472   
   1400     1143.82     1.4423     1.4435     0.0007     0.0425   
   1500     1225.52     1.4409     1.4423     0.0007     0.0389   
   1600      1307.2     1.4504     1.4535     0.0009     0.035    
   1700      1388.9     1.4521     1.455      0.001      0.032    
   1800     1470.61     1.4724     1.4728     0.0005     0.029    
   1900     1552.32     1.4739     1.4733     0.0007     0.0266   
   2000     1634.02     1.4747     1.4747     0.0006     0.0248   
   2100     1715.73     1.4778     1.4788     0.0007     0.0227   
   2200     1797.42     1.4801     1.4818     0.0008     0.0211   
   2300     1879.14     1.4947     1.4968     0.0006     0.0195   
   2400     1960.86     1.4956     1.4975     0.0005     0.0183   
   2500     2042.54     1.4956     1.4975     0.001      0.017    
   2600     2124.23     1.4956     1.4975     0.0006     0.0157   
   2700     2205.94     1.4956     1.4975     0.0005     0.0151   
   2800      2287.6     1.4956     1.4975     0.0007     0.0141   
   2900     2369.31     1.4956     1.4975     0.0006     0.0132   
   3000      2451.0     1.4956     1.4975     0.0007     0.0126   
   3100      2532.7     1.4956     1.4975     0.0005     0.012    
   3200     2614.39     1.4956     1.4975     0.0005     0.0114   
   3300     2696.08     1.4956     1.4975     0.0007     0.0106   
   3400     2777.76     1.4956     1.4975     0.0006     0.0104   
   3500     2859.46     1.4956     1.4975     0.0006     0.0096   
   3600     2941.16     1.4956     1.4975     0.0008     0.0093   
   3700     3022.81     1.4956     1.4975     0.0009     0.0088   
   3800     3104.46     1.4956     1.4975     0.0006     0.0085   
   3900     3186.15     1.4956     1.4975     0.0006     0.0081   
   4000     3267.84     1.4956     1.4975     0.0006     0.0079   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       80.54      1.3972     1.4013     0.0021     0.2105   
   200       163.35     1.4075     1.4145     0.0018     0.1855   
   300       245.82     1.4065     1.4124     0.0016     0.1649   
   400       328.09     1.4179     1.4154     0.0017     0.1456   
   500       410.23     1.4181     1.4085     0.0011     0.1278   
   600       492.36     1.4156     1.4102     0.0011     0.1118   
   700       574.42     1.4422     1.4344     0.0009     0.097    
   800       656.5      1.4377     1.4307     0.0008     0.0846   
   900       738.64     1.4463     1.4415     0.0009     0.0734   
   1000      820.81     1.4312     1.4327     0.0008     0.0641   
   1100      902.97     1.4472     1.4445     0.0008     0.0566   
   1200      985.15     1.4624     1.4586     0.0009     0.0498   
   1300     1067.33     1.4589     1.4546     0.0007     0.0446   
   1400      1149.5     1.4561     1.455      0.0008      0.04    
   1500     1231.71     1.4812     1.4812     0.0008     0.0356   
   1600     1313.88     1.484      1.4825     0.0008     0.0327   
   1700     1396.09     1.4819     1.4815     0.0006     0.0292   
   1800     1478.25     1.4629     1.4615     0.0009     0.0266   
   1900     1560.33     1.4623     1.4617     0.0007     0.0242   
   2000      1642.6     1.4888     1.4884     0.0008     0.0223   
   2100     1724.23     1.4907     1.4912     0.0006     0.0207   
   2200     1805.86     1.4939     1.4946     0.0007     0.0189   
   2300     1887.49     1.4932     1.495      0.0006     0.0176   
   2400     1969.13     1.4956     1.4976     0.0008     0.0166   
   2500     2050.76     1.4956     1.4976     0.0006     0.0151   
   2600     2132.38     1.4956     1.4976     0.0007     0.0141   
   2700     2213.99     1.4956     1.4976     0.0008     0.0134   
   2800     2295.62     1.4956     1.4976     0.0008     0.0127   
   2900     2377.22     1.4956     1.4976     0.0007     0.0118   
   3000      2458.8     1.4956     1.4976     0.0007     0.0111   
   3100     2540.41     1.4956     1.4976     0.0008     0.0105   
   3200     2622.12     1.4956     1.4976     0.0006      0.01    
   3300     2703.44     1.4956     1.4976     0.0005     0.0096   
   3400     2784.75     1.4956     1.4976     0.0006     0.0092   
   3500     2866.06     1.4956     1.4976     0.0006     0.0087   
   3600     2947.37     1.4956     1.4976     0.0006     0.0083   
   3700     3028.69     1.4956     1.4976     0.0007     0.008    
   3800     3110.02     1.4956     1.4976     0.0008     0.0075   
   3900     3191.38     1.4956     1.4976     0.0007     0.0072   
   4000     3270.39     1.4956     1.4976     0.0012     0.007    
