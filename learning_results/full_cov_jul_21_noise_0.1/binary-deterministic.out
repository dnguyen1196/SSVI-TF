Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  7.366682767868042
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       139.35     0.7812     0.7361     3.1419     0.2362   
   200       286.78     0.714      0.6787     2.4587     0.119    
   300       435.24     0.6792     0.6156     2.4564     0.0922   
   400       583.44     0.6524     0.5758     2.0728     0.0661   
   500       731.86     0.627      0.5244     1.9448     0.0611   
   600       879.98     0.629      0.5225     1.9274     0.0489   
   700      1027.78     0.6193     0.5018     2.0277     0.045    
   800      1175.74     0.6132     0.4804     1.6398     0.0384   
   900      1324.04     0.6195     0.4825     1.7238     0.036    
   1000     1472.25     0.6138     0.4615     1.6207     0.0318   
   1100      1620.4     0.6057     0.4645     1.5477     0.0352   
   1200     1767.83     0.6004     0.451      1.6359     0.0323   
   1300     1914.75     0.6025     0.4545     1.5073     0.0324   
   1400     2061.59     0.5925     0.4441     1.5835     0.0306   
   1500     2208.14     0.6091     0.4615     1.821      0.0332   
   1600      2354.8     0.5938     0.4366     1.7152     0.0297   
   1700     2500.71     0.5705     0.4079     1.4348     0.0292   
   1800     2646.37     0.5983     0.4391     1.6413     0.0306   
   1900     2792.09     0.634      0.5048     2.1371     0.0305   
   2000     2938.52     0.6278     0.5146     2.9023     0.0418   
   2100     3084.88     0.6531     0.5745     2.7266     0.0441   
   2200     3231.17     0.6364     0.5428     2.7229     0.0453   
   2300     3377.87     0.6346     0.5464     2.8922     0.0464   
   2400     3524.37     0.6225     0.5348     2.736      0.0442   
   2500     3670.11     0.6238     0.528      2.7695     0.0415   
   2600     3815.85     0.6031     0.5079     2.6433     0.0574   
   2700     3960.37     0.5922     0.4825     2.3769     0.0622   
   2800     4105.32     0.5849     0.4864     2.2854     0.0566   
   2900      4249.7     0.5847     0.4839     2.2679     0.0614   
   3000     4393.69     0.5748     0.4486     2.2508     0.0588   
   3100     4537.33     0.5798     0.4718     2.333      0.0493   
   3200      4680.9     0.5802     0.4821     2.2615     0.0588   
   3300     4824.17     0.5731     0.4615     2.4584     0.0545   
   3400     4967.21     0.5695     0.458      2.0038     0.0676   
   3500     5110.03     0.5647     0.4559     2.1735     0.0671   
   3600      5253.1     0.5691     0.4714     2.5945     0.0532   
   3700      5395.9     0.5601     0.4548     1.8876     0.0709   
   3800     5538.06     0.5509     0.4254     1.834      0.065    
   3900     5678.11     0.5499     0.4205     1.6241     0.0502   
   4000     5818.41     0.5623     0.4338     1.8868     0.0848   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       138.18     0.8151     0.7906     3.119      0.1299   
   200       284.76     0.7242     0.6935     2.0916     0.0848   
   300       431.58     0.6612     0.6248     2.0707     0.0679   
   400       578.4      0.6484     0.6049     1.8073     0.0594   
   500       724.89     0.6404     0.5869     2.1032     0.0505   
   600       871.3      0.6158     0.5659     1.7415     0.0409   
   700      1017.73     0.598      0.5391     1.6541     0.0389   
   800      1163.83     0.6009     0.5389     1.7825     0.0377   
   900      1309.72     0.591      0.5264     1.9032     0.0346   
   1000     1455.44     0.5803     0.5007     1.5869     0.0363   
   1100     1601.07     0.5831     0.5155     1.6125     0.0291   
   1200     1746.52     0.5624     0.4785     1.7617     0.0257   
   1300     1892.62     0.5724     0.4844     1.513      0.0275   
   1400     2038.01     0.5499     0.4716     1.6058     0.0258   
   1500     2183.42     0.549      0.4663     1.107      0.0221   
   1600     2328.55     0.541      0.4562     1.5539     0.0214   
   1700     2473.55     0.5468     0.4519     1.1848     0.0191   
   1800     2618.52     0.5441     0.4524     1.2339     0.0184   
   1900     2763.79     0.5319     0.4415     1.2253     0.0203   
   2000     2908.42     0.542      0.4409     1.2412     0.0185   
   2100     3053.15     0.5408     0.4361     1.1705     0.0202   
   2200     3197.68     0.5325     0.4268     1.2733     0.0168   
   2300     3342.42     0.5367     0.4226     1.1639     0.0167   
   2400     3486.76     0.5359     0.4274     1.2908     0.016    
   2500     3632.28     0.5287     0.4293     1.1903     0.0147   
   2600     3777.63     0.5267     0.4236     1.2934     0.0166   
   2700     3923.21     0.5209     0.4089     1.0004     0.0172   
   2800     4068.61     0.5296     0.4148     1.0613     0.0166   
   2900     4213.76     0.5232     0.4073     1.1071     0.0169   
   3000     4358.81     0.5195     0.4095     1.1102     0.0143   
   3100     4504.06     0.5214     0.4094     0.9643     0.0153   
   3200     4648.23     0.5266      0.42      1.1645     0.0153   
   3300     4791.99     0.5163     0.4076     0.965      0.0154   
   3400     4935.92     0.5121     0.395      1.0715     0.0144   
   3500     5079.67     0.5113     0.3996     1.0352     0.0143   
   3600     5223.89     0.5079     0.3937     1.2138     0.0144   
   3700     5367.54     0.5203     0.399      0.9173     0.0133   
   3800     5511.37     0.5177     0.4005     0.9184     0.0152   
   3900     5654.98     0.5205     0.402      1.0823     0.0143   
   4000     5798.77     0.5093     0.3906     0.9202     0.0138   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       137.38     0.8862     0.8749     3.1243     0.0932   
   200       284.11     0.7068     0.6873     2.0518     0.0579   
   300       431.47     0.6785     0.6498     1.9792     0.0488   
   400       578.93     0.6458     0.6082     1.6398     0.0416   
   500       726.98     0.6266     0.5863     2.0716     0.0538   
   600       874.74     0.6219     0.5763     1.7779     0.0332   
   700      1022.29     0.5896     0.5475     1.6188     0.0314   
   800      1170.15     0.5692     0.5319     1.5207     0.0277   
   900      1317.99     0.5717     0.5305     1.6443     0.0279   
   1000     1470.66     0.5644     0.5222     1.5376     0.0284   
   1100      1618.3     0.5457     0.5039     1.5606     0.0243   
   1200     1767.19     0.5553     0.506      1.4989     0.0316   
   1300     1915.68     0.5455     0.4984     1.5964     0.0209   
   1400     2063.91     0.5427     0.4844     1.4185     0.0245   
   1500     2212.15     0.5401     0.4866     1.4749     0.0189   
   1600     2360.32     0.5381     0.4777     1.3061     0.019    
   1700     2508.38     0.5317     0.4656     1.1597     0.0307   
   1800     2655.98     0.527      0.4657     1.1903     0.0223   
   1900     2803.09     0.5319     0.4635     1.1372     0.0156   
   2000     2950.46     0.5189     0.4582     1.1491     0.0246   
   2100     3097.61     0.5281     0.462      1.4473     0.0274   
   2200     3244.69     0.5146     0.4525     1.2753     0.0175   
   2300     3391.62     0.5175     0.4517     1.2787     0.0265   
   2400     3538.43     0.518      0.4517     1.1935     0.017    
   2500     3683.84     0.5168     0.4455     1.2295     0.0138   
   2600     3828.94     0.5052     0.4438     1.5252     0.0158   
   2700     3974.07     0.5056     0.4353     0.9713     0.0133   
   2800     4119.25     0.5085     0.4395     1.0924     0.0148   
   2900     4265.52     0.5039     0.4293     1.0852     0.019    
   3000     4412.33     0.5061     0.4348     1.0467     0.0232   
   3100      4558.7     0.5042     0.4326     1.2054     0.0121   
   3200     4705.04     0.5017     0.4267     1.0434     0.0111   
   3300     4851.11     0.4991     0.4219     0.9456     0.0213   
   3400     4997.63     0.4922     0.4239     0.9042     0.0119   
   3500     5144.03     0.4959     0.4175     0.8772     0.0111   
   3600     5290.26     0.4907     0.4219     1.0953     0.0104   
   3700     5436.15     0.4954     0.425      1.0101     0.0132   
   3800     5581.96     0.4946     0.4158     0.9304     0.0118   
   3900     5727.84     0.4957     0.4168     0.894      0.0113   
   4000     5873.79     0.4941     0.4217     1.0127     0.0131   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       138.4      0.8384     0.8275     3.1264     0.0794   
   200       287.37     0.7321     0.7079     2.0989     0.0458   
   300       436.28     0.6722     0.6474     1.7648     0.0451   
   400       584.64     0.6462     0.6212     1.8262     0.0413   
   500       731.78     0.6255     0.5947     1.6876     0.0408   
   600       878.59     0.6164     0.5793     1.7761     0.0333   
   700      1025.16     0.593      0.5507     1.5377     0.0302   
   800      1171.13     0.5894     0.558      1.6556     0.0308   
   900      1316.22     0.5678     0.5339     1.4388     0.0257   
   1000     1460.56     0.5528     0.5222     1.6495     0.0241   
   1100     1605.04     0.5556     0.5122     1.5647     0.0264   
   1200     1749.14     0.5389     0.5008     1.3415     0.0262   
   1300     1893.25     0.5392     0.5046     1.665      0.0227   
   1400     2037.31     0.5489     0.5046     1.6908     0.0221   
   1500     2181.66     0.5319     0.4913     1.3396     0.032    
   1600     2325.52     0.5295     0.4901     1.2584     0.0196   
   1700     2469.37     0.5273     0.4831     1.4367     0.0185   
   1800     2613.21     0.5258     0.4715      1.18      0.0182   
   1900     2757.25     0.5364     0.4864     1.4021     0.0176   
   2000     2901.84     0.5177     0.4774     1.2696     0.018    
   2100     3046.34     0.5335     0.4732     1.2444     0.0164   
   2200     3189.99     0.5171     0.4713     1.6609     0.0306   
   2300     3333.89     0.5185     0.4734     1.0647     0.0232   
   2400     3477.85     0.5079     0.4569     1.278      0.0136   
   2500     3622.96     0.5099     0.4531     1.1045     0.0137   
   2600     3767.98     0.5023     0.4548     1.1604     0.013    
   2700     3912.15     0.5104     0.4555     1.3926     0.0162   
   2800     4057.48     0.4983     0.4549     1.0502     0.0209   
   2900     4202.56     0.4949     0.4546     1.019      0.0211   
   3000     4347.12     0.4975     0.447      1.0094     0.0184   
   3100     4491.24     0.4974     0.4482     1.0899     0.0133   
   3200     4635.21     0.4902     0.4356     0.8882     0.0143   
   3300     4778.98     0.4956     0.4434     0.9846     0.0137   
   3400     4922.43     0.4828     0.4354     1.0042     0.011    
   3500     5065.84     0.4871     0.4379     0.9054     0.0134   
   3600     5209.35     0.4946     0.4326     0.9897     0.0104   
   3700     5352.97     0.5009     0.4391     0.8736     0.0128   
   3800     5496.87     0.4881     0.4302     0.9986     0.0105   
   3900     5640.89     0.4874     0.4305     1.3271     0.0129   
   4000     5784.45     0.4922     0.4245     0.9292     0.0145   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       136.69     0.8361     0.8287     3.1382     0.0765   
   200       286.63     0.7462     0.7269     1.9903     0.0501   
   300       436.35     0.6911     0.6747     1.8621     0.0415   
   400       586.96     0.6661     0.6453     1.8496     0.0365   
   500       737.14     0.6355     0.6107     1.8593     0.0364   
   600       887.38     0.621      0.5913     1.6325     0.0335   
   700      1038.22     0.611      0.5792     1.6758     0.0306   
   800      1189.42     0.5851     0.5512     1.8601     0.0287   
   900      1340.19     0.5724     0.5423     1.7315     0.0252   
   1000     1490.67     0.5677     0.5319     1.4529     0.0219   
   1100     1639.74     0.5603     0.5296     1.4581     0.0221   
   1200      1788.4     0.5587     0.5208     1.7024     0.0221   
   1300     1937.28     0.5547     0.5156     1.4268     0.0265   
   1400     2087.16     0.5384     0.5005     1.5453     0.0191   
   1500     2236.93     0.532      0.4927     1.4091     0.0267   
   1600     2387.51     0.5244     0.4868     1.4416     0.0169   
   1700     2536.48     0.5296     0.492      1.3996     0.0155   
   1800     2688.65     0.5172     0.4772     1.2378     0.0213   
   1900     2840.28     0.5105     0.4701     1.2466     0.0193   
   2000     2991.65     0.518      0.4717     1.1488     0.0256   
   2100     3143.06     0.5129     0.4664     1.157      0.0217   
   2200     3294.13     0.5147     0.468      1.2125     0.0155   
   2300     3446.34     0.5058     0.4667     1.138      0.0188   
   2400     3597.63     0.5041     0.4643     1.2765     0.0178   
   2500     3749.16     0.4988     0.4536     0.9979     0.0127   
   2600     3899.97     0.498      0.4538     1.2494     0.0109   
   2700     4050.81     0.5066     0.462      1.3711     0.0156   
   2800     4202.11     0.4977     0.4538     0.9721     0.0148   
   2900     4353.01     0.4988     0.4501     0.9909     0.0308   
   3000     4503.93     0.4928     0.451      1.0163     0.0142   
   3100      4655.0     0.4889     0.4444     0.8889     0.0318   
   3200     4805.96     0.4889     0.4442     0.9645     0.0141   
   3300     4957.04     0.4782     0.4351     0.9217     0.0121   
   3400     5107.82     0.4838     0.4374     0.9982     0.0138   
   3500     5258.71     0.4807     0.4345     0.9951     0.0109   
   3600      5409.7     0.4856     0.4366     0.975      0.0082   
   3700     5560.74     0.4775     0.4327     0.8963     0.0141   
   3800     5712.35     0.4843     0.4343     0.9339     0.0162   
   3900     5864.81     0.4706     0.4307     0.8544     0.0099   
   4000     6017.52     0.4785     0.4302     1.0049     0.0104   
