Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  9.475237369537354
max_count =  19  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       154.3      4.2621     4.3008     0.3127     0.2657   128255.98  104137.55  
   200       502.32     4.2106     4.2491     0.2959     0.0233   119567.72   97058.47  
   300       849.13     4.0211     4.0579     0.2807     0.0215    97974.87   79505.49  
   400      1197.13     3.6226     3.6564     0.2606     0.0164    71764.01   58207.03  
   500      1544.83     3.1124     3.1424     0.2277     0.0106    52261.39   42383.47  
   600      1892.41     2.6433     2.667      0.1871     0.0111    41155.43   33354.86  
   700      2240.21     2.2697     2.2879     0.1875     0.012     35311.16   28599.24  
   800      2587.74     1.9809     1.9947     0.1687     0.0113    32049.64   25939.01  
   900       2935.3     1.7627     1.7733     0.1994     0.0112    30169.73   24402.93  
   1000     3282.64     1.5937     1.6031     0.0866     0.0108    28971.83   23437.18  
   1100      3630.5     1.4692     1.4758     0.1624     0.0096    28252.53   22845.44  
   1200     3976.06     1.3772     1.3803     0.1402     0.0096    27749.52   22440.2   
   1300     4320.42     1.3068     1.3077     0.1854     0.0089    27426.98   22179.08  
   1400     4665.33     1.2537     1.2535     0.1321     0.0082    27199.9    21987.25  
   1500     5010.24     1.2138      1.21      0.0935     0.0074    27037.03   21850.15  
   1600     5355.54     1.183      1.1767     0.1692     0.0069    26926.2    21757.87  
   1700     5700.02     1.1568     1.1513     0.0921     0.0065    26835.75   21684.07  
   1800     6044.67     1.1389     1.132      0.1207     0.006     26769.91   21635.18  
   1900     6389.67     1.1241     1.1152     0.0897     0.0058    26706.8    21590.86  
   2000     6734.25     1.1092     1.1037     0.1072     0.0053    26667.25   21557.44  
   2100     7079.72     1.1002     1.0922     0.0973     0.0055    26645.2    21530.66  
   2200     7423.69     1.0914     1.0833     0.1161     0.0048    26610.97   21504.8   
   2300     7768.43     1.0852     1.0775     0.086      0.0043    26596.89   21496.62  
   2400     8112.94     1.0796     1.0704     0.1528     0.0045    26578.42   21475.76  
   2500     8457.63     1.0752     1.0636     0.0926     0.0043    26559.72   21458.43  
   2600     8802.21     1.0711     1.0607     0.1134     0.004     26558.97   21454.83  
   2700     9146.83     1.0679     1.0603     0.1187     0.0038    26534.58   21441.05  
   2800     9491.86     1.0634     1.0557     0.1147     0.0039    26528.43   21433.49  
   2900     9836.82     1.062      1.0536     0.0794     0.0033    26528.4    21428.79  
   3000     10181.08    1.0592     1.0496     0.1239     0.0034    26510.63   21421.14  
   3100     10525.72    1.0591     1.0462     0.0945     0.0034    26507.81   21415.44  
   3200     10870.64    1.0586     1.047      0.0963     0.0038    26499.44   21422.02  
   3300     11215.62    1.0549     1.0463     0.1414     0.0029    26514.79   21422.59  
   3400     11560.68    1.0539     1.0436     0.109      0.0029    26503.94   21412.6   
   3500     11905.28    1.0538     1.0432     0.1207     0.0029    26504.92   21419.68  
   3600     12250.09    1.0518     1.0422     0.0867     0.0029    26500.97   21412.85  
   3700     12595.69    1.0511     1.0386     0.0947     0.0024    26497.9    21410.92  
   3800     12940.81    1.0509     1.0396     0.0749     0.0027    26488.83   21401.34  
   3900     13285.39    1.0502     1.0372     0.0552     0.0023    26490.1    21403.86  
   4000     13630.88    1.0499     1.0383     0.0807     0.0024    26489.5    21406.68  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       153.11     4.2633     4.2877     0.3149     0.2339   128918.23  208437.82  
   200       582.53     4.2155     4.2395     0.2999     0.0219   121283.24  196032.06  
   300      1010.86     4.0268     4.0497     0.2891     0.0227    99708.99  161107.82  
   400      1438.39     3.6299     3.6506     0.2776     0.0128    72491.39  117125.15  
   500      1863.98     3.1211     3.1379     0.2324     0.0084    52257.51   84477.96  
   600      2291.31     2.6576     2.6702     0.1878     0.0086    41099.74   66454.11  
   700      2717.28     2.2869     2.2981     0.169      0.0096    35258.59   57039.59  
   800      3143.29     1.9968     2.0056     0.2468     0.0106    31974.28   51748.69  
   900      3569.85     1.7751     1.7819     0.1378     0.0107    30033.11   48617.04  
   1000     3995.94     1.6082     1.6138     0.1611     0.0104    28851.14   46705.88  
   1100     4421.78     1.4834     1.4886     0.1688      0.01     28105.45   45505.89  
   1200     4847.05     1.3924     1.3949     0.2189     0.0096    27606.18   44702.5   
   1300     5271.89     1.3222     1.3228     0.118      0.009     27273.71   44161.22  
   1400     5696.82     1.2666     1.2677     0.0988     0.0086    27035.06   43775.18  
   1500     6121.33     1.2223     1.2218     0.1191     0.0081    26856.93   43483.88  
   1600     6545.88     1.1891     1.1889     0.1513     0.0076    26732.0    43303.61  
   1700     6970.57     1.1622     1.1609     0.0518     0.0073    26631.63   43117.93  
   1800      7395.7     1.1408     1.138      0.0787     0.0067    26569.03   43004.53  
   1900     7820.29     1.1259     1.1235     0.1429     0.0063    26529.79   42936.49  
   2000     8244.33     1.1121     1.1105     0.1576     0.006     26478.5    42863.36  
   2100     8668.62     1.1002     1.097      0.0945     0.0056    26438.75   42792.87  
   2200     9092.85     1.091      1.0883     0.1257     0.0053    26407.04   42738.62  
   2300     9517.88     1.0846     1.0814     0.0956     0.0051    26386.57   42705.84  
   2400      9941.8     1.0777     1.0747     0.1012     0.0048    26350.97   42654.84  
   2500     10365.86    1.0704     1.0686     0.0742     0.0046    26337.74   42630.24  
   2600     10790.06    1.0671     1.0639     0.1228     0.0043    26329.39   42607.95  
   2700     11214.22    1.0646     1.0603     0.1014     0.0041    26309.55   42582.92  
   2800     11638.44    1.0605     1.0563     0.0999     0.004     26304.71   42574.23  
   2900     12063.14    1.0575     1.0529     0.1151     0.0038    26298.86   42557.16  
   3000     12487.25    1.0529     1.0499     0.0969     0.0035    26275.31   42534.84  
   3100     12911.57    1.052      1.0477     0.1178     0.0035    26288.33   42536.35  
   3200     13335.88    1.0503     1.0472     0.089      0.0033    26282.01   42540.04  
   3300     13760.35    1.0483     1.0454     0.137      0.0031    26265.11   42511.41  
   3400     14184.75    1.048      1.0436     0.1292     0.003     26262.55   42502.77  
   3500     14608.95    1.0474     1.0429     0.0759     0.0029    26266.96   42509.83  
   3600     15033.39    1.0451     1.0421     0.162      0.0027    26291.18   42540.61  
   3700     15457.85    1.0431     1.0406     0.0744     0.0026    26249.7    42506.34  
   3800     15882.78    1.0424     1.0396     0.0879     0.0025    26236.67   42481.37  
   3900     16307.84    1.0414     1.0389     0.1066     0.0024    26246.07   42479.28  
   4000     16733.34    1.041      1.038      0.0783     0.0023    26241.62   42473.59  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       153.49     4.2637     4.2813     0.3122     0.2237   129105.29  311965.62  
   200       670.45     4.2182     4.2356     0.3003     0.0203   121993.96  294731.49  
   300      1183.84     4.0327     4.0494     0.2949     0.0186   100629.74  243090.47  
   400       1696.5     3.6325     3.6483     0.2669     0.0116    72657.25  175572.85  
   500       2208.8     3.1181     3.1327     0.2911     0.0078    52039.48  125870.92  
   600      2721.85     2.6548     2.667      0.1914     0.0083    40929.59   99035.06  
   700      3233.88     2.277      2.2876     0.1994     0.0095    35029.4    84788.26  
   800      3746.68     1.9854     1.9951     0.2276     0.0101    31788.97   76926.81  
   900      4259.28     1.7631     1.7712     0.215      0.0103    29872.0    72303.47  
   1000     4771.53     1.5983     1.6065      0.18      0.0101    28716.64   69519.24  
   1100     5284.29     1.4704     1.4768     0.1272     0.0097    27959.09   67673.08  
   1200     5795.57     1.3754     1.3797     0.1765     0.0093    27472.64   66478.77  
   1300     6307.23     1.3059     1.3097     0.141      0.0089    27141.93   65696.84  
   1400     6819.22     1.2526     1.2552     0.133      0.0085    26912.41   65126.32  
   1500     7330.28     1.212      1.2129     0.1096     0.0081    26754.54   64727.68  
   1600     7848.51     1.1796     1.1805     0.174      0.0077    26658.67   64493.28  
   1700     8364.17     1.1544     1.1557     0.1549     0.0073    26539.37   64228.73  
   1800     8882.67     1.1358     1.1353     0.1488     0.007     26484.09   64089.46  
   1900     9400.87     1.1211     1.1204     0.1456     0.0066    26428.25   63938.47  
   2000     9915.38     1.1073     1.1071     0.1382     0.0062    26381.56   63829.43  
   2100     10430.14    1.096      1.0949     0.0774     0.0061    26342.33   63734.98  
   2200     10943.91    1.0872     1.0844     0.1309     0.0058    26320.37   63664.09  
   2300     11460.85    1.0794     1.0777      0.16      0.0054    26296.99   63612.81  
   2400     11976.37    1.073      1.0708     0.1088     0.0052    26269.8    63547.74  
   2500     12494.52    1.0677     1.0659     0.1041     0.005     26257.37   63518.68  
   2600     13009.84    1.0635     1.0615     0.1205     0.0048    26243.88   63479.28  
   2700     13525.5     1.0597     1.0573     0.1294     0.0046    26220.4    63426.43  
   2800     14038.3     1.0554     1.0533     0.1551     0.0045    26207.04   63411.82  
   2900     14551.53    1.0538     1.0515      0.11      0.0042    26214.73   63426.66  
   3000     15065.04    1.0501     1.0484     0.0752     0.004     26197.44   63388.15  
   3100     15578.43    1.0499     1.0461     0.1176     0.0039    26206.85   63366.39  
   3200     16093.21    1.0467     1.0441     0.0836     0.0037    26194.29   63355.72  
   3300     16608.71    1.0447     1.0415     0.0803     0.0036    26174.32   63313.19  
   3400     17123.43    1.0448     1.0404     0.1109     0.0034    26175.11   63312.17  
   3500     17638.37    1.0429     1.0396     0.0842     0.0033    26165.61   63286.46  
   3600     18153.09    1.0412     1.0372     0.0954     0.0032    26163.84   63276.42  
   3700     18667.72    1.0397     1.0368     0.076      0.0031    26161.0    63271.01  
   3800     19181.9      1.04      1.0361     0.0782     0.003     26163.21   63270.94  
   3900     19698.59    1.0376     1.0349     0.1056     0.0029    26149.9    63250.41  
   4000     20214.51    1.039      1.0349     0.0848     0.0028    26149.75   63245.23  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       153.17     4.2639     4.2791     0.3129     0.246    129188.25  415989.05  
   200       750.38     4.219      4.234      0.2996     0.0178   122288.62  393710.04  
   300      1348.95     4.034      4.0484     0.291      0.0162   101009.79  325170.78  
   400      1948.54     3.6382     3.6517     0.2685     0.0113    72994.34  235115.27  
   500      2547.92     3.1254     3.1382     0.2143     0.0068    52198.0   168310.26  
   600      3146.92     2.6603     2.6722     0.2171     0.0076    41002.07  132320.49  
   700      3748.34     2.289      2.2994     0.1996     0.0092    35141.3   113484.71  
   800      4350.45     2.0019     2.0115     0.1357     0.0099    31892.92  103000.88  
   900      4950.88     1.7819     1.7901     0.2391     0.0102    29987.28   96822.47  
   1000      5552.3     1.6138     1.6197     0.1023      0.01     28783.72   92933.96  
   1100     6154.34     1.4877     1.4937     0.1278     0.0096    28032.78   90509.0   
   1200     6757.75     1.3947     1.3996     0.1777     0.0092    27542.73   88942.81  
   1300     7360.43     1.3224     1.3266     0.1514     0.0088    27184.87   87811.54  
   1400     7963.46     1.2673     1.271      0.131      0.0083    26949.82   87028.72  
   1500     8565.06     1.2244     1.2279     0.1492     0.008     26774.43   86461.72  
   1600     9167.09     1.1907     1.1943     0.1936     0.0077    26651.3    86063.8   
   1700     9771.25     1.1672     1.1694     0.1307     0.0073    26565.9    85765.02  
   1800     10379.03    1.1457     1.1479     0.123      0.007     26475.91   85531.17  
   1900     10983.91    1.1281     1.1308     0.0709     0.0066    26412.34   85301.09  
   2000     11589.52    1.1152     1.117      0.0846     0.0064    26368.95   85154.64  
   2100     12193.9     1.1062     1.1063     0.1194     0.0061    26357.86   85080.84  
   2200     12799.78    1.0946     1.0959     0.1729     0.0057    26301.65   84919.28  
   2300     13408.73    1.0863     1.0869     0.1332     0.0056    26270.9    84835.7   
   2400     14016.3     1.0793     1.0804     0.102      0.0053    26242.27   84751.21  
   2500     14622.68    1.0736     1.0752     0.0835     0.0051    26237.32   84735.29  
   2600     15230.73    1.0672     1.0688     0.1282     0.0049    26215.21   84653.41  
   2700     15838.43    1.0638     1.0647     0.1004     0.0047    26199.98   84607.02  
   2800     16443.39    1.0603     1.0605     0.0972     0.0046    26194.32   84570.75  
   2900     17047.56    1.0565     1.0565     0.1253     0.0044    26180.05   84526.61  
   3000     17652.25    1.0533     1.0537     0.0919     0.0042    26162.3    84482.93  
   3100     18254.82    1.0535     1.0516     0.1262     0.004     26197.99   84530.3   
   3200     18857.15    1.0479     1.0479     0.1539     0.0039    26143.23   84428.36  
   3300     19459.23    1.0461     1.0452     0.0882     0.0038    26138.67   84398.6   
   3400     20060.67    1.0439     1.0434     0.1485     0.0037    26151.87   84434.44  
   3500     20662.64    1.0424     1.0418     0.1055     0.0036    26126.13   84349.26  
   3600     21264.5     1.0413     1.0401     0.1495     0.0035    26122.72   84341.91  
   3700     21865.19    1.0393     1.0399     0.1248     0.0034    26115.69   84318.8   
   3800     22465.46    1.0386     1.0383     0.093      0.0032    26107.77   84288.43  
   3900     23065.1     1.0376     1.0372     0.1079     0.0031    26104.84   84290.85  
   4000     23665.28    1.0373     1.0372     0.0949     0.0031    26111.62   84303.39  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       153.08     4.2644     4.2876     0.3135     0.2076   129291.26  521788.63  
   200       835.99     4.2237     4.2465     0.3005     0.0174   123046.39  496457.33  
   300      1517.46     4.0464     4.0684     0.2956     0.017    102351.75  412904.95  
   400      2199.65     3.6608     3.6811     0.2692     0.0104    74268.66   299682.4  
   500      2887.24     3.1537     3.1725     0.2172     0.0068    53027.4   214205.48  
   600      3572.09     2.6733     2.6898     0.2671     0.0079    41199.19  166527.07  
   700      4258.07     2.2925     2.3077     0.1902     0.0089    35173.82  142204.64  
   800      4944.24     2.0027     2.016      0.1539     0.0095    31892.35  128942.39  
   900       5630.5     1.7825     1.7944     0.2009     0.0096    29968.36  121135.29  
   1000     6316.67     1.6131     1.6246     0.1999     0.0096    28767.63  116335.37  
   1100     7000.42     1.4858     1.4961     0.143      0.0093    27998.44   113186.6  
   1200     7686.89     1.3884     1.3988     0.114      0.0089    27483.76  111142.03  
   1300     8372.93     1.3175     1.3265     0.1948     0.0085    27145.33  109766.66  
   1400     9059.34     1.2631     1.2718     0.1908     0.0082    26911.84  108838.86  
   1500     9745.31     1.2192     1.2284     0.2495     0.0078    26743.69  108131.37  
   1600     10431.39    1.1872     1.1957     0.2365     0.0075    26628.82  107661.53  
   1700     11117.04    1.1605     1.168      0.1174     0.0071    26517.58  107179.73  
   1800     11803.43    1.1393     1.1464     0.1413     0.0068    26437.24  106895.05  
   1900     12489.99    1.1248     1.1302     0.119      0.0065    26392.78  106658.67  
   2000     13176.65    1.1088     1.1151     0.1143     0.0063    26325.56  106424.65  
   2100     13863.55    1.0976     1.1032     0.0744     0.0061    26288.46  106276.76  
   2200     14549.51    1.0876     1.0937      0.09      0.0058    26255.33  106140.34  
   2300     15236.63    1.0814     1.0851     0.1066     0.0055    26246.42  106061.37  
   2400     15924.09    1.0732     1.0777     0.1197     0.0053    26215.91  105959.11  
   2500     16611.64    1.0687     1.0728     0.188      0.0051    26209.75  105923.28  
   2600     17299.7     1.0632     1.0672     0.0858     0.0049    26179.06  105804.84  
   2700     17988.51    1.0591     1.0623     0.1101     0.0048    26171.35  105755.38  
   2800     18678.11    1.055      1.0578     0.1241     0.0046    26157.12   105685.1  
   2900     19368.0     1.0521     1.0549     0.0607     0.0044    26141.5   105647.81  
   3000     20058.16    1.0483     1.0518     0.1278     0.0043    26125.5   105587.51  
   3100     20748.89    1.0459     1.0493     0.1047     0.0041    26118.43  105538.41  
   3200     21440.14    1.0455     1.0482     0.1114     0.004     26130.01  105548.44  
   3300     22135.05    1.0425     1.0456     0.1607     0.0039    26096.47   105469.1  
   3400     22829.65    1.0406     1.0435     0.0705     0.0037    26093.62  105460.47  
   3500     23527.85    1.0392     1.0415     0.0709     0.0037    26088.57  105424.67  
   3600     24228.41    1.0384      1.04       0.12      0.0036    26081.5   105404.14  
   3700     24924.08    1.0365     1.039      0.0622     0.0035    26075.65  105380.56  
   3800     25617.07    1.0364     1.0385     0.0752     0.0033    26079.09  105381.15  
   3900     26308.31    1.0341     1.0377     0.0873     0.0033    26057.67  105366.59  
   4000     26999.48    1.034      1.037      0.0607     0.0032    26059.79  105326.62  
