Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  9.590564727783203
max_count =  19  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       522.41     4.1433     4.1796     0.3071     2.6068   128953.55  104739.46     0.0    
   200      1231.97     4.1015     4.138      0.2955     1.3846   122212.48   99240.34     0.0    
   300      1940.58     3.9485     3.9832     0.2842     0.7938    103091.4   83676.5      0.0    
   400      2647.67     3.6258     3.6569     0.2602     0.5852    76564.76   62096.75     0.0    
   500      3352.12     3.1617     3.1875     0.223      0.7393    54713.79   44341.06     0.0    
   600      4054.35     2.6659     2.6859     0.2299     0.7642    41761.04   33791.89     0.0    
   700      4754.92     2.225      2.2373     0.1989     0.8824    34839.86   28160.48     0.0    
   800      5453.99     1.8731     1.8787     0.2074     0.6787    31179.83   25199.69     0.0    
   900      6148.08     1.6215     1.6245     0.2278     1.5348    29242.47   23629.04     0.0    
   1000     6839.79     1.4491     1.4476     0.2552     0.5121    28191.11   22776.24     0.0    
   1100     7531.01     1.3296     1.3276     0.1604     0.428     27606.3    22306.6      0.0    
   1200     8221.69     1.2499     1.2436     0.1282     0.1303    27206.91   21972.35     0.0    
   1300     8913.04     1.1959     1.1907     0.1576     0.3727    26973.58   21803.47     0.0    
   1400     9604.08     1.1608     1.1543     0.1457     0.539     26814.37   21667.07     0.0    
   1500     10294.81    1.139      1.1322     0.1192     0.0309    26762.23   21625.76     0.0    
   1600     10986.04    1.1217     1.1133     0.1259     0.3716    26682.35   21557.17     0.0    
   1700     11676.89    1.1073     1.1018     0.0852     0.2356    26598.48   21489.99     0.0    
   1800     12367.54     1.1       1.0914     0.1504     0.0122    26586.35   21469.47     0.0    
   1900     13058.48    1.0901     1.082      0.0733     0.4077    26537.93   21432.79     0.0    
   2000     13749.86    1.0855     1.0778     0.1212     0.4433    26498.12   21400.38     0.0    
   2100     14440.56    1.0791     1.0707     0.0814     0.2672    26485.49   21389.07     0.0    
   2200     15130.89    1.0759     1.0678     0.1488     0.4079    26454.41   21369.41     0.0    
   2300     15821.11    1.0722     1.0643     0.0991     0.0219    26440.07   21362.61     0.0    
   2400     16511.34     1.07      1.0639     0.1086     0.0122    26429.93   21353.93     0.0    
   2500     17201.46    1.0685     1.0597     0.1802     0.1721    26413.48   21337.19     0.0    
   2600     17891.54    1.0677     1.0588     0.1042     0.0156    26412.91   21329.08     0.0    
   2700     18581.64    1.0654     1.058      0.1218     0.0116    26408.56   21335.46     0.0    
   2800     19271.8     1.065      1.0563     0.0589     0.014     26379.29   21316.36     0.0    
   2900     19962.04    1.0633     1.0554     0.1142     0.4353    26375.45   21316.46     0.0    
   3000     20652.3     1.0624     1.0536     0.0666     0.0139    26376.81   21307.11     0.0    
   3100     21342.77    1.0645     1.0545     0.1128     0.4963    26390.31   21321.35     0.0    
   3200     22033.19    1.0639     1.0534     0.0636     0.0113    26376.42   21301.76     0.0    
   3300     22723.12    1.0598     1.051      0.0768     0.1093    26372.27   21306.27     0.0    
   3400     23413.42    1.0577     1.0499     0.0922     0.4849    26348.57   21290.67     0.0    
   3500     24103.72    1.0602     1.052      0.1092     0.0135    26341.35   21275.62     0.0    
   3600     24794.02    1.0573     1.0487     0.079      0.2558    26342.88   21277.26     0.0    
   3700     25484.78    1.0576     1.0479     0.0736     0.0139    26330.96   21268.18     0.0    
   3800     26174.46    1.059      1.0517     0.088      0.5197    26312.4    21259.83     0.0    
   3900     26864.18    1.0588     1.0485     0.0722     0.0127    26324.89   21255.94     0.0    
   4000     27554.01    1.0581     1.0503     0.0964     0.0491    26324.08   21259.71     0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       518.93     4.1593     4.1822     0.3125     1.7205   129194.64  208913.73     0.0    
   200      1302.76     4.1203     4.143      0.3032     0.9347   123129.33   199085.1     0.0    
   300      2086.23     3.9682     3.9904     0.2864     0.3817    104243.3  168527.88     0.0    
   400      2870.44     3.6459     3.6662     0.2802     0.1858    77352.14  125068.44     0.0    
   500       3652.8     3.184      3.2001     0.2231     0.3974    55181.1    89207.24     0.0    
   600      4431.64     2.6874     2.7009     0.2361     1.8186    42001.35   67930.85     0.0    
   700      5208.57     2.2453     2.2542     0.1981     1.2509    34936.24   56495.74     0.0    
   800      5984.06     1.8984     1.9049     0.2823     0.0786    31250.01   50556.36     0.0    
   900      6759.91     1.6496     1.6535     0.2206     0.9416    29293.35   47381.81     0.0    
   1000     7533.75     1.4754     1.4772     0.2445     2.4774    28233.53   45678.36     0.0    
   1100     8307.92     1.3554     1.3584     0.1295     0.7359    27552.04   44598.1      0.0    
   1200     9080.82     1.273      1.274      0.1909     0.7507    27151.01   43954.97     0.0    
   1300     9853.88     1.2167     1.2163     0.1065     0.1608    26904.68   43564.07     0.0    
   1400     10626.11    1.1771     1.1772     0.1514     0.1226    26753.63   43299.64     0.0    
   1500     11401.37    1.1513     1.1518      0.15      0.1461    26640.34   43119.42     0.0    
   1600     12172.72    1.1259     1.1268     0.0994     0.2834    26539.82   42955.11     0.0    
   1700     12946.81    1.1098     1.1087     0.1181     0.6473    26489.08   42887.12     0.0    
   1800     13722.15     1.1       1.0993     0.1009     1.1028    26457.8    42819.29     0.0    
   1900     14496.19    1.0909     1.0906     0.1441     0.3301    26400.96   42738.55     0.0    
   2000     15270.43    1.0841     1.0822     0.0651     0.4101    26388.75   42716.58     0.0    
   2100     16043.86    1.0763     1.0744     0.1023     0.4999    26374.15   42687.66     0.0    
   2200     16817.53    1.0705     1.0683     0.1363     0.8805    26338.18   42622.79     0.0    
   2300     17591.67    1.0694     1.0681     0.0957     1.1252    26331.02   42620.03     0.0    
   2400     18365.14    1.0699     1.0677     0.1404     2.4861    26324.68   42606.13     0.0    
   2500     19138.72    1.0852     1.0821     0.1307     7.8157    26380.28   42682.93     0.0    
   2600     19912.7     2.4057     2.3795     0.1706    389.9091   26492.49   42853.3      0.0    
   2700     20685.76    1.1396     1.1373     0.1843    125.7563     nan        nan        0.0    
   2800     21459.44    1.076      1.0732     0.0838     9.6163      nan        nan        0.0    
   2900     22231.72    1.0746     1.0701     0.162      2.6487      nan        nan        0.0    
   3000     23004.84    1.1006     1.0964     0.1708     7.4661      nan        nan        0.0    
   3100     23776.1     4.1262     4.061      0.2435   1114.2185     nan        nan        0.0    
   3200     24548.73    1.8786     1.8283     0.2217    463.2425     nan        nan        0.0    
   3300     25321.46    1.1223     1.1184     0.1349    45.4335      nan        nan        0.0    
   3400     26089.81    1.0847     1.0803     0.0614     6.6419      nan        nan        0.0    
   3500     26860.04    1.0731     1.0694     0.0629     2.1242      nan        nan        0.0    
   3600     27632.4     1.0641     1.0623     0.0728     1.4964      nan        nan        0.0    
   3700     28405.21    1.0611     1.0576     0.0989     0.722       nan        nan        0.0    
   3800     29178.24    1.0594     1.0566     0.107      0.7234      nan        nan        0.0    
   3900     29948.98    1.0592     1.0552     0.0708     0.3388      nan        nan        0.0    
   4000     30716.27    1.056      1.0538     0.0644     0.2995      nan        nan        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       518.07     4.1647     4.1822     0.3114     2.7052   129320.34  312512.13     0.0    
   200       1381.5     4.1302     4.1471     0.3042     1.8161   123940.96  299519.73     0.0    
   300      2243.83     3.9835     3.9996     0.2912     1.0908   105319.73  254610.46     0.0    
   400       3104.4     3.6627     3.6784     0.2734     1.2458    77978.52   188623.6     0.0    
   500      3961.18     3.1994     3.2151     0.3013    12.5618    55385.27  134106.37     0.0    
   600      4815.92     2.7095     2.7236     0.2395    70.3375      nan        nan        0.0    
   700      5668.75     2.2838     2.2952     0.2615    13.1775      nan        nan        0.0    
   800      6519.98     1.9372     1.9479     0.2298    39.1604      nan        nan        0.0    
   900      7369.73     1.6714     1.6783     0.2309     1.4142      nan        nan        0.0    
   1000     8219.21     1.4886     1.4955     0.1657     0.9615      nan        nan        0.0    
   1100     9067.93     1.367      1.3709     0.2062     1.7151      nan        nan        0.0    
   1200     9916.99     1.3139     1.3192     0.1904    15.9232      nan        nan        0.0    
   1300     10765.72    1.2468     1.2481     0.2127    40.5156      nan        nan        0.0    
   1400     11614.66    1.2291     1.2287     0.2275    83.7698      nan        nan        0.0    
   1500     12463.34    1.1633     1.1627     0.1183     5.2642      nan        nan        0.0    
   1600     13311.54    1.1529     1.1509     0.1284      8.62       nan        nan        0.0    
   1700     14159.65    1.135      1.1342     0.2817    100.7523     nan        nan        0.0    
   1800     15007.49    1.1039     1.1023     0.0948     2.9438      nan        nan        0.0    
   1900     15855.46    1.0924     1.0914     0.1416     0.6846      nan        nan        0.0    
   2000     16703.7     1.0852     1.0824     0.0878     0.4374      nan        nan        0.0    
   2100     17551.45    1.0773     1.0755     0.1466     0.1671      nan        nan        0.0    
   2200     18397.58    1.0699     1.0676     0.0908     0.2116      nan        nan        0.0    
   2300     19245.23    1.0662     1.0652     0.0543     0.1133      nan        nan        0.0    
   2400     20095.34    1.0629     1.0602     0.0727     0.083       nan        nan        0.0    
   2500     20945.62    1.0591     1.0563     0.1633     0.1011      nan        nan        0.0    
   2600     21795.69    1.0551     1.0521     0.1173     0.1778      nan        nan        0.0    
   2700     22645.67    1.0571     1.0534     0.1017     0.0572      nan        nan        0.0    
   2800     23495.39    1.0528     1.0491     0.0689     0.1495      nan        nan        0.0    
   2900     24345.63    1.052      1.0482     0.1085     0.038       nan        nan        0.0    
   3000     25195.29    1.0487     1.0454     0.1249     0.0506      nan        nan        0.0    
   3100     26045.3     1.0489     1.0442     0.0886     0.027       nan        nan        0.0    
   3200     26895.06    1.047      1.0432     0.0854     0.3034      nan        nan        0.0    
   3300     27743.84    1.0446     1.0409     0.0486     0.0603      nan        nan        0.0    
   3400     28594.12    1.0456     1.0417     0.0843     0.0754      nan        nan        0.0    
   3500     29444.39    1.046      1.0405     0.058      0.0516      nan        nan        0.0    
   3600     30295.43    1.0426     1.0385     0.0795     0.0291      nan        nan        0.0    
   3700     31145.62    1.0416     1.0366     0.0527     0.0182      nan        nan        0.0    
   3800     31996.24    1.0424     1.0383     0.0623     0.029       nan        nan        0.0    
   3900     32846.14    1.041      1.0366     0.0466     0.0278      nan        nan        0.0    
   4000     33696.33    1.0416     1.036      0.1009     0.0121      nan        nan        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       520.58     4.169      4.1838     0.3126   1034.2026     nan        nan        0.0    
   200      1464.13     4.1514     4.1658     0.3126   2463.1358     nan        nan        0.0    
   300      2407.25     4.0453     4.0612     0.2908    479.799      nan        nan        0.0    
   400      3348.24     3.8024     3.8174     0.2877    399.4755     nan        nan        0.0    
   500      4287.14     3.6881     3.7144     0.2504    607.2397     nan        nan        0.0    
   600      5225.57     2.9871     3.0029     0.2291   5359.7181     nan        nan        0.0    
   700      6158.85     2.4967     2.509      0.2671    898.8119     nan        nan        0.0    
   800      7090.99      inf        inf       0.3009   6091.0496     nan        nan        0.0    
