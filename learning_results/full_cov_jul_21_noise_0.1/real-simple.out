Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  7.430491924285889
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       117.06    272.583    273.5334    3.1613     0.3063      0.0    
   200       243.09    241.6968   240.4866    2.7049     0.1482      0.0    
   300       369.58    236.7764   236.3048    2.0219     0.0457      0.0    
   400       495.17    233.805    233.5442    2.3576     0.0183      0.0    
   500       620.92    233.0745   233.3628    1.5404     0.0189      0.0    
   600       746.49    231.9164   231.4363    1.483      0.0095      0.0    
   700       872.31    233.6369   232.8248    1.6523     0.0087      0.0    
   800       997.83    231.2401   230.5777    1.5425     0.0069      0.0    
   900      1123.42    232.0918   231.8231    1.3051     0.0052      0.0    
   1000     1248.88    230.1121   230.0178    1.2227     0.004       0.0    
   1100     1374.35    230.2842   229.9673    1.2744     0.0037      0.0    
   1200      1499.8    229.6022   229.7285    1.3414     0.0033      0.0    
   1300     1625.52    230.0205   229.7589    1.1225     0.0027      0.0    
   1400     1751.76    229.4604   229.6863    1.1386     0.0022      0.0    
   1500     1877.09    229.5653   229.9545    1.2412     0.0021      0.0    
   1600     2002.47    229.2346   229.6816    0.981      0.0017      0.0    
   1700     2128.03    229.051    229.2016    1.1868     0.0017      0.0    
   1800     2253.28    228.7772   228.7748    1.2871     0.0016      0.0    
   1900     2378.56    230.7091   230.3567    1.0995     0.0014      0.0    
   2000     2503.99    228.7001   228.6774    0.8502     0.0012      0.0    
   2100     2629.52    229.7944   230.5291    1.255      0.0011      0.0    
   2200     2754.83    228.8016   228.7283    0.8494     0.0008      0.0    
   2300     2880.31    229.3594   229.383     0.9767     0.0009      0.0    
   2400      3005.6    228.8996   228.985     0.9238     0.0008      0.0    
   2500     3130.85    228.6487   228.8108    1.2142     0.0008      0.0    
   2600     3256.02    228.2824   229.6889    0.8249     0.0007      0.0    
   2700     3381.13    228.8426   229.6909    1.3599     0.0007      0.0    
   2800     3506.45    228.895    229.7636    1.1012     0.0006      0.0    
   2900      3631.7    229.6782   229.4567    1.2144     0.0006      0.0    
   3000     3757.13    228.761    229.1954    0.9133     0.0005      0.0    
   3100      3882.4    228.9024   228.9181    0.9698     0.0006      0.0    
   3200     4007.54    228.8559   228.9972    0.9587     0.0005      0.0    
   3300     4132.71    228.2789   229.0911    0.8701     0.0004      0.0    
   3400     4257.05    229.0549   228.9694    0.8909     0.0005      0.0    
   3500     4381.49    227.6218   228.2149    1.017      0.0004      0.0    
   3600     4505.74    228.865    228.673     0.8804     0.0004      0.0    
   3700      4630.4    228.9403   228.9568    0.977      0.0003      0.0    
   3800     4754.92    228.7447   228.5087    0.7741     0.0003      0.0    
   3900     4879.11    229.0617   228.6642    0.9309     0.0003      0.0    
   4000     5003.32    229.0478   229.086     0.8995     0.0003      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       116.85    261.1498   263.3654    3.1616     0.3083      0.0    
   200       243.91    236.5032   238.1607    2.8157     0.084       0.0    
   300       370.75    235.0272   236.3392    2.5022     0.0296      0.0    
   400       497.34    233.7105   234.3671    2.2212     0.0184      0.0    
   500       623.86    231.969    232.7747    1.5869     0.0105      0.0    
   600       750.37    230.7643   231.5489    1.5984     0.0053      0.0    
   700       876.4     230.4303   231.6065    1.5396     0.0073      0.0    
   800      1002.41    228.9127   229.9993    1.4956     0.0045      0.0    
   900      1128.54    229.1683   229.3158    1.2634     0.003       0.0    
   1000     1254.51    230.088    230.1054     1.14      0.0034      0.0    
   1100     1380.27    231.735    231.8838    1.6399     0.0029      0.0    
   1200     1505.98    229.8729   230.4595    1.2079     0.0019      0.0    
   1300     1631.75    230.5725   231.1829    1.0003     0.0013      0.0    
   1400     1757.69    229.9276   230.5621    1.356      0.0012      0.0    
   1500     1883.31    229.0118   229.5972    1.1291     0.0016      0.0    
   1600     2008.97    229.7201   230.0851    1.2305     0.0016      0.0    
   1700     2134.72    230.5113   230.9695    0.9197     0.001       0.0    
   1800     2260.18    229.405    229.6237    1.2137     0.0011      0.0    
   1900     2385.66    229.776    229.9178    1.2439     0.0008      0.0    
   2000     2511.19    229.4317   229.9981     1.25      0.0008      0.0    
   2100     2636.65    229.1893   229.5221    0.8539     0.0006      0.0    
   2200     2762.06    228.9083   229.4488    0.9787     0.0007      0.0    
   2300     2887.56    229.3365   229.5703    1.1378     0.0005      0.0    
   2400     3013.02    228.892    229.5283    1.0126     0.0005      0.0    
   2500     3138.32    228.861    228.9969    0.9358     0.0006      0.0    
   2600     3263.61    228.829    229.1066    1.0839     0.0004      0.0    
   2700      3389.1    229.7385   229.6656    1.0438     0.0004      0.0    
   2800     3514.69    229.1598   229.2861    0.9743     0.0004      0.0    
   2900     3639.98    229.3508   229.8894    1.0757     0.0004      0.0    
   3000     3765.89    229.0439   229.2804    0.774      0.0004      0.0    
   3100      3892.3    228.4732   229.1604    1.0799     0.0004      0.0    
   3200     4018.05    228.416    228.8768    0.8609     0.0004      0.0    
   3300     4143.43    229.5613   229.681     0.8842     0.0004      0.0    
   3400      4268.8    228.3938   228.9183    1.0413     0.0003      0.0    
   3500     4394.19    228.3604   228.8675    0.886      0.0003      0.0    
   3600     4519.84    228.271    228.5843    0.7568     0.0003      0.0    
   3700     4645.51    228.9697   229.1694    0.7022     0.0003      0.0    
   3800     4771.11    228.6992    228.73     0.8179     0.0003      0.0    
   3900      4896.8    228.8893   229.1036    0.6857     0.0003      0.0    
   4000     5022.56    229.0167   229.1953    0.7238     0.0002      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       116.65    270.9521   270.1646    3.1605     0.2896     0.0001  
   200       242.67    242.1395   241.9788    2.7437     0.0604      0.0    
   300       367.92    235.3803   235.1664    1.927      0.0301      0.0    
   400       492.7     233.011    233.7009    1.8813     0.0207      0.0    
   500       617.32    232.2718   233.0208    1.9464     0.0098      0.0    
   600       743.51    231.7676   231.8461    1.8114     0.0102      0.0    
   700       868.18    230.6167   230.8223    1.2479     0.0072      0.0    
   800       992.68    229.7228   230.3942    1.2627     0.005       0.0    
   900      1117.39    230.2023   231.3726    1.4766     0.0056      0.0    
   1000     1241.91    230.5686   230.6085    1.4101     0.0052      0.0    
   1100     1366.29    230.6565   230.9595    1.2213     0.0029      0.0    
   1200     1490.76    231.4534   231.7001    1.3058     0.0021      0.0    
   1300     1615.09    229.7609   229.8132    1.3638     0.0026      0.0    
   1400     1739.57    230.0177   230.0911    1.4237     0.0018      0.0    
   1500     1863.99    229.0826   229.5372    1.2729     0.0017      0.0    
   1600     1988.55    229.5799   230.434     1.4092     0.0015      0.0    
   1700     2112.76    229.8899   229.8599    1.136      0.0013      0.0    
   1800      2236.9    230.0278   229.8056    1.2163     0.0013      0.0    
   1900     2360.95    229.3535   229.3607    1.6724     0.0012      0.0    
   2000     2485.49    230.1544   230.2727    0.9666     0.0012      0.0    
   2100     2609.42    229.4066   230.5759    1.4397     0.0007      0.0    
   2200     2733.27    229.3532   229.5329    1.0962     0.0009      0.0    
   2300     2857.25    228.7695   229.2112    1.003      0.0008      0.0    
   2400     2981.35    229.1127   229.4505    0.985      0.0009      0.0    
   2500     3105.28    229.122    229.4567    1.0578     0.0008      0.0    
   2600     3229.06    229.9518   229.8212    1.1561     0.0006      0.0    
   2700     3352.92    229.2929   229.6687    0.8458     0.0006      0.0    
   2800     3476.55    228.346    228.3654    1.063      0.0006      0.0    
   2900      3600.3    228.696    228.9065    0.9566     0.0004      0.0    
   3000     3724.01    228.2536   229.0726    1.1973     0.0006      0.0    
   3100     3847.88    228.4302   228.9927    0.7213     0.0004      0.0    
   3200     3971.57    229.5446   229.523     0.929      0.0003      0.0    
   3300     4095.03    228.0795   228.8349    0.9803     0.0004      0.0    
   3400     4218.54    229.1132   230.0238    0.9148     0.0004      0.0    
   3500      4342.3    228.2344   228.8666    0.8321     0.0004      0.0    
   3600     4465.71    227.9407   228.825     0.7672     0.0004      0.0    
   3700     4589.15    227.6815   228.5391    0.8103     0.0004      0.0    
   3800     4712.57    228.3962    228.64     1.0236     0.0004      0.0    
   3900     4836.09    228.5229   228.8313    1.0242     0.0003      0.0    
   4000      4959.6    228.6705   229.4366    0.9623     0.0002      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       116.63    258.7655   259.7523    3.1614     0.5842     0.0001  
   200       243.95    238.9654   239.9102    2.7288     0.0701      0.0    
   300       370.99    233.8313   235.0741    2.4088     0.0249      0.0    
   400       497.92    232.037    232.9812    2.3638     0.0226      0.0    
   500       625.06    231.4951   232.5558    1.4632     0.0095      0.0    
   600       752.14    230.3377   231.3614    1.578      0.0099      0.0    
   700       879.25    231.8004   232.7868    1.5587     0.0092      0.0    
   800      1006.23    230.2783   231.5683    1.4197     0.0067      0.0    
   900      1133.67    229.8272   230.4082    1.3537     0.0038      0.0    
   1000     1260.51    231.2151   231.6049    1.5809     0.0032      0.0    
   1100     1387.51    231.387    232.3525    1.2371     0.0044      0.0    
   1200     1514.52    229.5719   230.9862    1.5693     0.0018      0.0    
   1300     1641.29    229.2793   230.0479    1.3841     0.0018      0.0    
   1400      1768.1    230.2031   231.0266    1.427      0.0021      0.0    
   1500     1895.12    230.8125   231.2011    1.3857     0.0023      0.0    
   1600     2022.06    229.7082   230.754     1.042      0.0018      0.0    
   1700     2148.79    229.6029   230.3968    1.1505     0.0014      0.0    
   1800     2275.65    228.721    229.8615    1.0739     0.0013      0.0    
   1900     2402.41    229.0879   230.1588    1.0666     0.0011      0.0    
   2000     2529.08     228.99    229.785     0.8401     0.0013      0.0    
   2100     2655.79    229.2877   230.7404    1.1361     0.0013      0.0    
   2200     2782.56    229.7959   230.4177    1.3155     0.0009      0.0    
   2300     2910.22    228.2735   229.4048    1.2532     0.0007      0.0    
   2400     3037.15    230.1066   231.0439    1.3329     0.0008      0.0    
   2500     3164.03    228.9987   229.5684    0.9335     0.0008      0.0    
   2600     3290.84    229.6197   231.0231    1.0495     0.0007      0.0    
   2700      3417.6    228.8475   230.094     1.0849     0.0005      0.0    
   2800     3544.41    228.8832   230.1213    1.3436     0.0005      0.0    
   2900     3671.14    231.2506   231.1541    1.028      0.0006      0.0    
   3000     3797.69    228.3138   229.3073    0.7375     0.0005      0.0    
   3100     3924.25    228.6692   229.6295    0.8728     0.0004      0.0    
   3200     4050.78    229.1518   229.7602    0.9454     0.0005      0.0    
   3300     4177.29    228.609    229.6782     0.81      0.0004      0.0    
   3400     4303.82    228.7042   230.279     0.8527     0.0003      0.0    
   3500     4430.42    227.7425   228.914     1.0139     0.0004      0.0    
   3600     4557.02    228.0873   229.4211    0.732      0.0004      0.0    
   3700     4684.19    227.7763   229.2531    0.8113     0.0003      0.0    
   3800     4810.73    228.2456   229.1104    1.1103     0.0003      0.0    
   3900     4937.38    228.1297   229.1626    0.8276     0.0003      0.0    
   4000     5063.86    228.7932   229.6408    0.8864     0.0004      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       117.22    266.0098   265.7441    3.1616     0.5498     0.0001  
   200       245.86    236.0074   236.172     2.4902     0.0973      0.0    
   300       374.13    235.5161   235.6358    2.6559     0.0325      0.0    
   400       502.11    234.4135   234.0717    1.9173     0.0251      0.0    
   500       629.98    230.6358   231.6594    1.7541     0.0115      0.0    
   600       757.87    232.0571   233.2252    1.5862     0.0078      0.0    
   700       885.55    231.5136   232.6113    1.5818     0.007       0.0    
   800      1013.46    229.9337   230.9979    1.6611     0.009       0.0    
   900      1141.35    230.7211   232.0007    1.5092     0.0043      0.0    
   1000     1269.04    230.3422   231.6176    1.9055     0.003       0.0    
   1100     1396.78     229.66    230.8686    1.571      0.0027      0.0    
   1200     1525.24    228.6983   229.9503    1.3488     0.0035      0.0    
   1300     1652.85    230.2151   231.0119    1.1433     0.0026      0.0    
   1400      1780.2    230.1062   230.1825    1.6421     0.0024      0.0    
   1500     1907.51    229.7183   231.0872    1.3598     0.0022      0.0    
   1600     2034.75    229.4399   229.6601    1.1513     0.0018      0.0    
   1700     2161.99    229.778    230.4845    1.2784     0.0019      0.0    
   1800     2289.25    229.1648   229.8785    1.1347     0.0014      0.0    
   1900     2416.41    228.5349   229.7249    1.5218     0.0006      0.0    
   2000     2543.52    229.8017   230.5338    1.292      0.0007      0.0    
   2100     2671.25    229.6707   230.1178    1.2055     0.001       0.0    
   2200     2798.31    231.0044   231.2275    1.2602     0.0008      0.0    
   2300     2925.41    229.4934   229.9221    1.1765     0.0008      0.0    
   2400     3052.35    229.5888   230.3069    1.0623     0.0009      0.0    
   2500     3179.32    228.5057   229.3492    0.9942     0.0008      0.0    
   2600      3307.0    229.002    229.8967    0.8748     0.0007      0.0    
   2700     3433.86    229.1604   229.6332    0.9919     0.0007      0.0    
   2800     3560.81    228.4604   229.3327    1.0189     0.0006      0.0    
   2900      3687.8    229.0349   229.945     1.1101     0.0006      0.0    
   3000     3814.58    229.0559   229.7281    0.9521     0.0005      0.0    
   3100     3941.36    228.8267   229.4219    1.1242     0.0004      0.0    
   3200     4068.37    228.5979   229.0734    0.7802     0.0003      0.0    
   3300     4195.14    228.5114   228.8677    0.8754     0.0004      0.0    
   3400      4322.0    228.1486   228.9901    1.1195     0.0004      0.0    
   3500     4448.82    228.6888   229.6697    1.2666     0.0003      0.0    
   3600     4575.68    228.7099   229.2709    0.9366     0.0004      0.0    
   3700     4702.54    227.8249   228.9378    0.9953     0.0003      0.0    
   3800     4829.53    228.6653   229.5694    0.922      0.0004      0.0    
   3900     4956.33    228.2072   228.7392    0.7561     0.0004      0.0    
   4000     5083.83    228.1261   228.8741    0.8367     0.0003      0.0    
