Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  7.211491823196411
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       291.06    260.2888   261.3468    3.162      0.1872      0.0    
   200       591.92    237.8799   237.2999    2.7573      0.0        0.0    
   300       892.89    232.4574   232.7215    2.4197      0.0        0.0    
   400       1194.0    230.4249   230.2983    1.5039      0.0        0.0    
   500       1495.2    230.1634   230.1541    1.4385      0.0        0.0    
   600      1796.76     230.9     230.6178    1.7896      0.0        0.0    
   700      2097.72    229.6336   229.5491    1.5019      0.0        0.0    
   800      2398.97    230.5204   230.235     1.517       0.0        0.0    
   900      2700.13    230.015    229.6862    1.5326      0.0        0.0    
   1000     3001.41    229.9303   229.251     1.2347      0.0        0.0    
   1100     3302.59    230.3104   230.1461    1.2401      0.0        0.0    
   1200     3604.52    229.8912   229.8741    1.3134      0.0        0.0    
   1300     3905.46    229.3429   229.5733    1.0747      0.0        0.0    
   1400     4206.28    228.9478   229.2692    1.0315      0.0        0.0    
   1500     4507.39    229.3206   229.4993    1.292       0.0        0.0    
   1600     4808.21    229.2714   229.4246    1.1405      0.0        0.0    
   1700     5109.09    229.1977   229.2326    0.9119      0.0        0.0    
   1800     5407.28    228.4755   228.6827    1.0754      0.0        0.0    
   1900      5704.9    229.2443   229.1301    1.0963      0.0        0.0    
   2000     6002.76    228.6866   228.7771    0.8903      0.0        0.0    
   2100     6300.56    228.8476   229.0038    1.1954      0.0        0.0    
   2200     6598.52    228.2573   228.9789    1.1097      0.0        0.0    
   2300     6896.37    228.1906   228.8132    0.995       0.0        0.0    
   2400     7194.38    228.683    228.5963    0.9288      0.0        0.0    
   2500     7492.05    229.2707   229.4044    1.249       0.0        0.0    
   2600     7789.79    228.9227   228.6801    0.8172      0.0        0.0    
   2700     8087.55    228.2332   228.5513    1.0528      0.0        0.0    
   2800      8385.2    228.8065   228.4752    0.7066      0.0        0.0    
   2900     8683.04    229.1852   229.2321    0.7801      0.0        0.0    
   3000     8980.61     227.98    228.6103    0.8152      0.0        0.0    
   3100     9278.22    228.392    228.3338    0.9507      0.0        0.0    
   3200     9575.87    228.1362   228.4752    0.8491      0.0        0.0    
   3300     9873.54    228.1212   228.2359    0.898       0.0        0.0    
   3400     10171.16   228.2789   228.3017    0.7794      0.0        0.0    
   3500     10468.87   228.1987   228.4411    0.7414      0.0        0.0    
   3600     10766.44   228.4157   228.7625    0.9263      0.0        0.0    
   3700     11063.87   228.0778   228.3793    0.8478      0.0        0.0    
   3800     11361.35    228.29    227.9549    1.069       0.0        0.0    
   3900     11658.67   228.3559   228.4917    0.7271      0.0        0.0    
   4000     11956.22   227.714    228.3001    0.9565      0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       289.41    268.4212   269.5076    3.1622     0.3061      0.0    
   200       589.78    238.7635   238.4346    3.1613      0.0        0.0    
   300       889.63    235.1929   235.4305    2.2827      0.0        0.0    
   400      1189.64    232.128    232.5599    2.0048      0.0        0.0    
   500       1489.8    230.4921   231.1763    1.2647      0.0        0.0    
   600      1790.08    230.4169   230.9232    1.6848      0.0        0.0    
   700      2090.34    229.9107   229.9172    1.5239      0.0        0.0    
   800      2391.06    230.6827   230.4882    1.6887      0.0        0.0    
   900      2691.52    229.4047   229.6712    1.3813      0.0        0.0    
   1000      2992.2    229.0774   229.5324    1.4498      0.0        0.0    
   1100     3293.21    229.2567   229.6981    1.1513      0.0        0.0    
   1200     3593.86    229.0917   229.1293    1.4632      0.0        0.0    
   1300     3894.68    230.1578   230.009     1.1793      0.0        0.0    
   1400     4196.08    228.8832   229.8182    0.9868      0.0        0.0    
   1500     4496.74    229.0234   229.2101    1.0838      0.0        0.0    
   1600     4797.55    228.6387   229.4933    1.2735      0.0        0.0    
   1700     5099.16    228.8748   228.8159    0.9264      0.0        0.0    
   1800     5399.73    229.2826   228.9377    0.9027      0.0        0.0    
   1900     5700.41    229.201    229.5858    1.339       0.0        0.0    
   2000     6001.23    228.6411   229.3763    0.9046      0.0        0.0    
   2100     6302.03    228.9605   229.2142    1.1796      0.0        0.0    
   2200     6602.74    228.4083   228.9965    1.1196      0.0        0.0    
   2300     6903.34    229.1456   229.214     0.838       0.0        0.0    
   2400     7203.86    228.4135   228.4791    1.0239      0.0        0.0    
   2500     7504.51    228.486    229.4041    1.0347      0.0        0.0    
   2600     7805.13    228.3581   228.9173    0.7886      0.0        0.0    
   2700     8105.57    228.3678   228.5887    0.835       0.0        0.0    
   2800      8406.0    228.6038   228.7386    0.743       0.0        0.0    
   2900     8706.61     228.88    228.8162    0.7371      0.0        0.0    
   3000      9007.1    228.2062   228.8762    1.0866      0.0        0.0    
   3100     9307.66     228.39    228.5513    0.875       0.0        0.0    
   3200     9608.76    228.3821   229.1903    0.7842      0.0        0.0    
   3300     9909.28    228.3596   228.4045    0.9199      0.0        0.0    
   3400     10209.9    228.8243   228.7114    0.8121      0.0        0.0    
   3500     10510.35   227.7651   228.6308    0.7691      0.0        0.0    
   3600     10810.97   229.0003   229.0836    0.8225      0.0        0.0    
   3700     11111.67   228.3927   228.734     1.3502      0.0        0.0    
   3800     11412.73   227.9336    228.31     0.7853      0.0        0.0    
   3900     11713.05   228.329    228.5448    0.6355      0.0        0.0    
   4000     12013.37   227.6126   228.1697    1.2358      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       288.69    267.177    266.7787    3.1622     0.4567      0.0    
   200       587.59    239.194    238.3697    2.9448      0.0        0.0    
   300       886.72    236.8656   236.1129    2.8405      0.0        0.0    
   400       1186.0    232.3675   232.7407    1.8424      0.0        0.0    
   500      1485.01    231.3523   231.6191    2.1914      0.0        0.0    
   600      1784.28    230.2581   230.2322    1.2656      0.0        0.0    
   700      2083.26    229.9596   230.1849    1.3705      0.0        0.0    
   800      2382.35    229.7301   230.0379    1.5524      0.0        0.0    
   900       2681.4    229.3048   229.5845    1.335       0.0        0.0    
   1000     2980.83    229.0242   229.4161    1.226       0.0        0.0    
   1100     3280.14    229.8664   230.3873    1.4022      0.0        0.0    
   1200      3579.2    228.4997   229.3187    1.0684      0.0        0.0    
   1300     3878.66    229.5889   230.0506    1.3852      0.0        0.0    
   1400      4177.8    228.9887   229.718     1.1804      0.0        0.0    
   1500     4477.14    229.1037   229.3715    1.1116      0.0        0.0    
   1600     4777.44    228.6796   229.2634    1.1883      0.0        0.0    
   1700     5077.02    228.9523   229.1813    1.1647      0.0        0.0    
   1800     5376.64    229.3787   229.5459    1.1959      0.0        0.0    
   1900      5676.3    228.7824   229.3853    1.2213      0.0        0.0    
   2000     5976.21    228.7604   229.2643    0.9946      0.0        0.0    
   2100     6275.81    228.5866   228.9188    1.0212      0.0        0.0    
   2200     6576.14    228.5489   229.2213    1.222       0.0        0.0    
   2300     6876.43    228.3274   228.4156    1.0962      0.0        0.0    
   2400     7176.27    228.1132   228.6035    1.1741      0.0        0.0    
   2500     7475.86    228.2161   228.8123    0.9691      0.0        0.0    
   2600     7775.58    228.3082   228.4924    0.8284      0.0        0.0    
   2700     8075.18    227.7809   228.6957    1.0395      0.0        0.0    
   2800     8375.11    228.4155   228.782     1.2504      0.0        0.0    
   2900     8674.76    228.0937   228.6759    0.7406      0.0        0.0    
   3000     8974.33    228.0052   229.0119    0.7124      0.0        0.0    
   3100     9274.06    229.1184   229.3173    0.9404      0.0        0.0    
   3200     9574.14    228.4347   229.0546    0.9088      0.0        0.0    
   3300     9873.96    228.1704   228.599     0.832       0.0        0.0    
   3400     10173.83   227.9328   228.5389    0.9725      0.0        0.0    
   3500     10473.76   228.5893   229.2215    0.9705      0.0        0.0    
   3600     10773.43   228.5487   228.7518    0.8832      0.0        0.0    
   3700     11074.0    228.344    228.4954    0.8506      0.0        0.0    
   3800     11374.66   228.3024   228.7227    0.951       0.0        0.0    
   3900     11675.58   228.3065   228.5227    0.9713      0.0        0.0    
   4000     11975.62   228.1246   228.1145    0.7894      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       290.25    263.034    264.3893    3.1622     0.6909      0.0    
   200       591.72    238.6243   239.1502    2.8702      0.0        0.0    
   300       892.38    233.8966   233.8533    2.2575      0.0        0.0    
   400      1193.21    233.046    233.7955    1.9682      0.0        0.0    
   500      1494.26    231.3912   232.3536    1.8009      0.0        0.0    
   600      1796.31    230.4023   230.731     1.5405      0.0        0.0    
   700      2097.84    229.5933   230.694     1.4755      0.0        0.0    
   800      2399.63    229.8053   230.7494    1.6318      0.0        0.0    
   900      2703.74    229.0433   230.0828    1.4216      0.0        0.0    
   1000      3013.6    230.2596   231.2886    1.5428      0.0        0.0    
   1100     3317.33    229.566    230.3264    1.5593      0.0        0.0    
   1200     3622.62    229.6765   230.5172    1.4342      0.0        0.0    
   1300     3927.63    228.8591   229.6946    1.3599      0.0        0.0    
   1400     4235.11    229.3606   229.7983    1.3384      0.0        0.0    
   1500     4543.33    228.6534   229.3975    1.1489      0.0        0.0    
   1600     4849.44    228.4428   228.8826    1.0717      0.0        0.0    
   1700     5151.58    229.3195   230.3023    1.1557      0.0        0.0    
   1800      5454.2    228.5574   229.1678    1.0299      0.0        0.0    
   1900     5756.24    229.1164   229.3121    1.1819      0.0        0.0    
   2000     6059.71    229.4175   230.0554    1.2039      0.0        0.0    
   2100      6364.9    228.6179   229.6518    0.9232      0.0        0.0    
   2200     6674.18    228.6023   229.766     1.1131      0.0        0.0    
   2300     6976.96    229.3719   230.2271    1.2395      0.0        0.0    
   2400     7278.37    228.1793   229.1706    0.9194      0.0        0.0    
   2500     7580.69    228.1083   228.9718    1.3641      0.0        0.0    
   2600     7881.91    228.0372   228.7681    0.8151      0.0        0.0    
   2700     8183.93    228.4254   229.1015    0.805       0.0        0.0    
   2800     8484.78    228.8403   230.0349    0.8371      0.0        0.0    
   2900     8785.18    228.6687   229.6193    1.0536      0.0        0.0    
   3000     9085.68    228.513    229.0611    0.7732      0.0        0.0    
   3100     9385.85    227.6284   228.5771    1.0595      0.0        0.0    
   3200     9686.19    227.4704   228.7282    0.6523      0.0        0.0    
   3300     9986.51    228.0801   229.0995    0.8874      0.0        0.0    
   3400     10286.75   228.0897   228.9218    0.7632      0.0        0.0    
   3500     10587.09   228.9003   229.7072    1.201       0.0        0.0    
   3600     10888.09   227.8759   228.9856    0.8905      0.0        0.0    
   3700     11188.48   228.6003   229.1436    0.9281      0.0        0.0    
   3800     11488.8    227.9724   228.7079    0.7508      0.0        0.0    
   3900     11789.31   228.0591   228.7661    0.8102      0.0        0.0    
   4000     12089.67   228.0279   229.0683    0.7113      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       290.19    268.0401   267.7434    3.1618     4.7489      0.0    
   200       592.31    241.9058   241.7039    2.9552      0.0        0.0    
   300       894.17    231.7476   232.3279    1.8381      0.0        0.0    
   400      1196.22    231.0348   232.701     2.1957      0.0        0.0    
   500      1498.43    231.7277   231.8611    1.7195      0.0        0.0    
   600      1800.68    231.4295   231.6533    1.6904      0.0        0.0    
   700      2102.91    230.2866   230.4415    1.4559      0.0        0.0    
   800       2405.4    230.0218   230.3657    1.3188      0.0        0.0    
   900      2707.39    231.0657   231.4972    1.7489      0.0        0.0    
   1000     3009.52    229.3334   230.2266    1.4351      0.0        0.0    
   1100     3311.56    228.9548   230.2733    1.5757      0.0        0.0    
   1200     3613.73    229.8134   230.2104    1.6955      0.0        0.0    
   1300     3915.72    228.8742   229.6226    0.9813      0.0        0.0    
   1400     4218.08    229.2206   229.8139    1.3841      0.0        0.0    
   1500     4520.11    229.5234   230.2887    1.2917      0.0        0.0    
   1600     4822.21    228.0768   228.9283    0.9688      0.0        0.0    
   1700      5124.2    229.1504   229.7646    1.2592      0.0        0.0    
   1800      5426.0    229.1007   229.3409    1.0112      0.0        0.0    
   1900     5727.82    229.0267   229.489     0.9909      0.0        0.0    
   2000     6030.19    229.1663   229.4854    1.0216      0.0        0.0    
   2100     6332.06    229.125    229.6537    1.3161      0.0        0.0    
   2200     6633.99    228.9379   229.5524    0.9079      0.0        0.0    
   2300     6937.36    228.7022   229.1248    1.0866      0.0        0.0    
   2400     7240.27    228.127     229.13     1.0238      0.0        0.0    
   2500     7543.13    228.2232   228.8749    0.8665      0.0        0.0    
   2600     7846.46    228.1042   229.2464    1.4023      0.0        0.0    
   2700     8150.57    228.6456   229.318     1.3369      0.0        0.0    
   2800     8454.15    228.132    228.8203    0.7834      0.0        0.0    
   2900     8757.71    227.7632   228.3536    0.777       0.0        0.0    
   3000     9061.66    228.6505   229.3103    1.0185      0.0        0.0    
   3100     9365.62    228.3587   228.9388    0.9377      0.0        0.0    
   3200     9670.13    228.9234   229.0618    0.913       0.0        0.0    
   3300      9974.2    228.094    229.029     0.8296      0.0        0.0    
   3400     10278.01   228.4684   228.9293    0.9819      0.0        0.0    
   3500     10580.66   227.9607   228.7915    1.1347      0.0        0.0    
   3600     10885.15   228.1665   228.7602    0.7891      0.0        0.0    
   3700     11189.07   227.9183   228.8442    0.6529      0.0        0.0    
   3800     11493.39   228.0939   228.9041    0.8777      0.0        0.0    
   3900     11797.85   228.577    229.0014    1.2429      0.0        0.0    
   4000     12103.65   228.1053   228.6858    0.711       0.0        0.0    
