Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  7.182990550994873
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       250.52     0.9413     0.9292     3.0625     0.4162     0.003   
   200       643.49     0.7753     0.7658     1.968      3.5913     0.0014  
   300      1033.59     0.7549     0.7313     1.7515    70.7141     0.0046  
   400      1422.68     0.7557     0.7409     1.8291    359.0335    0.0232  
   500      1812.36     0.7458     0.7214     1.6539    874.1454    0.0013  
   600      2200.41     0.7443     0.7192     1.632     39.9872     0.005   
   700      2587.33     0.7402     0.7283     1.5396     9.3451     0.0034  
   800      2973.54     0.7347     0.7206     1.8545     17.214     0.0046  
   900      3359.43     0.7417     0.7284     1.6227    50.3341     0.0002  
   1000     3745.48     0.7444     0.7279     1.8475    604.7859    0.0161  
   1100     4132.33     0.7452     0.7215     1.9279   3173.4633    0.0236  
   1200     4518.39     0.739      0.724      2.2062   10056.3739   0.0643  
   1300      4904.2     0.7238     0.7055     1.8668   1201.9845    0.0125  
   1400     5288.64     0.7136     0.687      2.5074   3460.6571    0.0062  
   1500     5669.76     0.699      0.6819     1.8169    940.0254    0.0005  
   1600     6049.64     0.6964     0.6657     1.7381   1128.2451    0.0035  
   1700     6429.58     0.6932     0.6598     2.1105   1829.5465    0.022   
   1800     6809.88     0.688      0.6615     1.7183    158.2629    0.0069  
   1900     7190.33     0.6884     0.6527     2.4621   3831.9377    0.0064  
   2000     7570.62     0.6828     0.6493     2.4765   2720.6136    0.0188  
   2100     7950.52     0.6766     0.6434     2.4334    725.5199    0.0169  
   2200     8330.85     0.6766     0.6394     2.5359   4361.3481    0.0398  
   2300     8711.71     0.6896     0.6621     2.4748   14048.7788   0.1906  
   2400     9092.82     0.672      0.6323     2.1128    912.9017    0.0297  
   2500     9473.87     0.662      0.6166     2.0871    662.7039    0.0244  
   2600     9854.46     0.6626     0.6081     2.1681    388.0207    0.0054  
   2700     10235.2     0.6505     0.597      2.451     4753.01     0.0351  
   2800     10616.27    0.6512     0.5958     2.5196   14691.9552   0.0562  
   2900     10998.38    0.6758     0.6312     2.5569   233548.6279   0.4172  
   3000     11381.37    0.6609     0.601      2.1469   10897.1417   0.0576  
   3100     11762.98    0.6729     0.6076     1.8116   6333.8516    0.0009  
   3200     12143.8     0.6746     0.6098     2.0405   2855.9503    0.012   
   3300     12525.34    0.6667     0.6027     1.3008    248.4539    0.0078  
   3400     12905.84    0.6892     0.6336     2.4127   2899.9327    0.0071  
   3500     13287.19    0.6813     0.619      1.7912   1277.4601    0.0063  
   3600     13667.8     0.6671     0.6155     1.2468    227.5605     0.0    
   3700     14048.39    0.6649     0.6155     1.9325   1372.6229    0.0038  
   3800     14428.73    0.6669     0.6056     0.953     169.3011    0.0062  
   3900     14808.68    0.6651     0.6134     2.2351   14269.9912   0.013   
   4000     15189.31    0.6654     0.6158     2.7853   12318.8557   0.0102  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       245.9      1.4139     1.4168     3.1619   13815.7696   0.0004  
   200       665.9      1.4105     1.4176     1.8441   37845.7444   0.0001  
   300      1087.32     1.4091     1.4112     1.0199   9704.5943    0.0001  
   400      1509.29     1.4162     1.4122     0.754    8429.8181    0.0001  
   500      1931.37     1.4174     1.4192     0.6095   1154.3226     0.0    
   600      2354.59     1.4143     1.4167     0.6886   1393.6534     0.0    
   700      2778.13     1.4142     1.4159     0.4895   4684.1292     0.0    
   800      3201.99     1.4165     1.4117     0.4617    544.3583     0.0    
   900      3626.16     1.4139     1.4165     0.3091    91.6069      0.0    
   1000     4050.15     1.4139     1.4171     0.2689    37.0661      0.0    
   1100     4474.94     1.4129     1.414      0.2814    214.7711     0.0    
   1200     4899.19     1.4146     1.4129     0.6421    5212.037     0.0    
   1300     5324.49     1.406      1.4061     0.8049   7224.4592     0.0    
   1400     5749.37     1.4175     1.4007     0.4986    978.7335     0.0    
   1500     6173.88     1.4046     1.4036     0.2366    158.6811     0.0    
   1600     6598.25     1.4046     1.396      0.2445    49.3385      0.0    
   1700     7022.82     1.3845     1.3726     0.2316     27.07       0.0    
   1800     7446.95     1.2546     1.2614     0.5061    13.1199      0.0    
   1900     7871.39     0.7753     0.7639     0.6373    15.2976      0.0    
   2000     8296.83     0.763      0.7558     0.7032    45.5083      0.0    
   2100     8722.55     0.7491     0.7391     3.0957    57.9671      0.0    
   2200     9147.75     0.7355     0.7244     2.937     386.5127     0.0    
   2300     9573.28     0.7298     0.7224     3.0256   2001.7375     0.0    
   2400     9998.15     0.7469     0.7334     0.749     85.5402      0.0    
   2500     10422.64    0.7389     0.7264     3.1231    31.0686      0.0    
   2600     10847.24    0.7366     0.716      0.5118    394.5282     0.0    
   2700     11271.84    0.7425     0.7293     2.6231   1536.7879     0.0    
   2800     11696.8     0.726      0.7123     1.2863   2429.0792     0.0    
   2900     12121.78    0.7468     0.7319     1.1511   76207.0115    0.0    
   3000     12547.48    0.7383     0.7206     0.8409   43625.5472    0.0    
   3100     12973.1     0.7283     0.7209     0.5767   3625.6748     0.0    
   3200     13398.55    0.7311     0.7211     0.5098   1183.7691     0.0    
   3300     13823.72    0.7286     0.7185     0.5434    566.3455     0.0    
   3400     14249.59    0.7368     0.7264     3.1621    296.0875     0.0    
   3500     14674.93    0.7364     0.7215     3.161     221.2396     0.0    
   3600     15100.63    0.7328     0.7194     1.5334    138.2777     0.0    
   3700     15526.5     0.7281     0.7194     3.1473    695.8712     0.0    
   3800     15952.52    0.7488     0.7341     3.1612    94.8289      0.0    
   3900     16378.5     0.7328     0.723      0.4316    182.4709     0.0    
   4000     16804.7     0.742      0.7186     0.6086   4603.1933     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       244.6      1.419      1.4154     3.1611   2903.2043    0.0001  
   200       721.04     1.4156     1.4121     1.6182    739.4776    0.0001  
   300      1200.66     1.4158     1.4132     0.9394   1095.2641    0.0001  
   400      1681.54     1.4117     1.412      0.7409    59.1662      0.0    
   500      2162.64     1.411      1.4164     0.4484    32.1636      0.0    
   600      2645.82     1.4175     1.4085     0.5631   1114.2381     0.0    
   700       3127.9     1.4148     1.4133     1.0004   30740.1687    0.0    
   800      3610.51     1.4149     1.4154     0.4986   1282.9875     0.0    
   900      4093.98     1.423      1.4133      0.59     100.1653     0.0    
   1000     4578.06     1.4188     1.4115     0.3849    161.2924     0.0    
   1100      5061.9     1.4082     1.4157     0.8896   22875.1877    0.0    
   1200     5546.37     1.3945     1.4052     0.7627   3450.4867     0.0    
   1300      6030.3     1.3679     1.366      0.396     261.0408     0.0    
   1400     6520.43     0.7592     0.7537     0.7895    69.0882      0.0    
   1500      7007.6     0.7478     0.742      0.8302    20.9027      0.0    
   1600     7498.34     0.7396     0.7352     0.9409    14.2938      0.0    
   1700     7990.57     0.7328     0.7212     3.1621    10.6469      0.0    
   1800     8483.02     0.7358     0.7279     0.7721    305.4789     0.0    
   1900      8970.8     0.727      0.7154     0.5516    55.5825      0.0    
   2000     9458.13     0.7356     0.725      3.1622    119.9528     0.0    
   2100     9946.56     0.7347     0.7197     0.7394     4.6724      0.0    
   2200     10437.43    0.7259     0.7132     2.1847    14.3714      0.0    
   2300     10924.03    0.734      0.722      2.5821     4.0111      0.0    
   2400     11411.27    0.7326     0.7164     1.3209     3.558       0.0    
   2500     11898.14    0.7917     0.7769     3.1599    826.4931     0.0    
   2600     12383.15    0.7333     0.724      3.1177    148.945      0.0    
   2700     12869.34    0.7405     0.7336     3.1513    176.2606     0.0    
   2800     13354.5     0.7509     0.7379     3.1356    50.2955      0.0    
   2900     13839.71    0.7328     0.7239     3.1264     9.3882      0.0    
   3000     14324.97    0.7615     0.7494     3.1595    39.8245      0.0    
   3100     14809.46    0.7428     0.7322     3.1467    38.8007      0.0    
   3200     15293.58    0.7299     0.7206     0.6768    413.0737     0.0    
   3300     15779.3     0.7327     0.721      1.2272    109.5284     0.0    
   3400     16265.21    0.7285     0.7183     3.161     680.8912     0.0    
   3500     16751.44    0.7302     0.7194     0.553     82.0237      0.0    
   3600     17237.62    0.7256     0.7132     0.5452    224.2501     0.0    
   3700     17723.87    0.7245     0.7157     3.0709    68.3435      0.0    
   3800     18210.09    0.7227      0.71      0.5125    430.8888     0.0    
   3900     18696.56    0.7285     0.7191     0.4085    84.8545      0.0    
   4000     19183.58    0.7297     0.716      0.4551    146.498      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       243.5      1.4089     1.4163      3.16    1160.2267    0.0001  
   200       774.73     1.4063     1.4141     1.793    3614.0928    0.0001  
   300      1309.86     1.4203     1.4194     1.2575   3086.3924     0.0    
   400      1847.82     1.418      1.416      0.7403   1348.7034     0.0    
   500      2385.96     1.4074     1.4107     0.9989   3277.8876     0.0    
   600      2925.96     1.4079     1.4131     0.9389   11728.269     0.0    
   700      3465.65     1.4116     1.4106     0.8049   1128.8592     0.0    
   800       4006.1     1.4059     1.4067     3.1614    310.0145     0.0    
   900      4545.09     1.4117     1.4148     1.1199   3434.2977     0.0    
   1000     5085.41     1.4135     1.4156     0.9628   4247.8293     0.0    
   1100     5626.38     1.411      1.4167     0.7224   3276.7845     0.0    
   1200     6167.68     1.4155     1.4125     0.6317    202.0044     0.0    
   1300     6709.64     1.3908     1.3877     0.4152    48.0169      0.0    
   1400     7251.51     0.7646     0.7515     0.7512    16.8918      0.0    
   1500     7792.13     0.7461     0.7346     0.6886     7.1413      0.0    
   1600     8333.18     0.7343     0.7212     0.6136     5.7801      0.0    
   1700     8875.74     0.7316     0.7215     0.5953     4.8159      0.0    
   1800     9420.41      0.72      0.7086     0.6294     5.7322      0.0    
   1900     9967.36     0.7284     0.7149     3.1531     11.875      0.0    
   2000     10509.84    0.7483     0.731      3.1622    550.6129     0.0    
   2100     11055.06    0.7335     0.7225     3.1619    467.0533     0.0    
   2200     11598.61    0.7217     0.7156     0.6794    696.6369     0.0    
   2300     12142.96    0.7295     0.7186     3.1405   18721.8892    0.0    
   2400     12686.08    0.7264     0.7127     3.1529   1424.5812     0.0    
   2500     13226.75    0.7305     0.7183     0.9472    263.1061     0.0    
   2600     13767.76    0.7321     0.7171     3.1541   1743.4948     0.0    
   2700     14308.41    0.7263     0.7084     2.722     301.0962     0.0    
   2800     14848.65    0.7289     0.7207     1.5873    120.8795     0.0    
   2900     15391.68    0.7249     0.7092     3.1002    50.2882      0.0    
   3000     15935.19    0.7323     0.712      2.9533    21.5467      0.0    
   3100     16478.5     0.7282     0.7098     1.0974    25.5932      0.0    
   3200     17019.12    0.7411     0.7218     1.7745    53.8255      0.0    
   3300     17559.68    0.7336     0.723      0.9174    521.8084     0.0    
   3400     18101.45    0.7296     0.7161     1.3035   9140.6111     0.0    
   3500     18641.79    0.7284     0.7184     3.0275    522.9447     0.0    
   3600     19181.16    0.7448     0.7288     1.8908   5233.1351     0.0    
   3700     19722.98    0.7269     0.7143     0.7428    521.7414     0.0    
   3800     20265.31    0.7352     0.7198     0.8326    272.011      0.0    
   3900     20807.5     0.7291     0.7114     0.4727    266.9221     0.0    
   4000     21350.54    0.7289     0.7149     0.5082    659.8219     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       244.12     1.4187     1.4116     3.1352   11589.4631    0.0    
   200       821.03     1.419      1.4135     1.3514    585.5302     0.0    
   300       1405.2     1.408      1.4156     2.0801   37210.5252    0.0    
   400       1991.6     1.4101     1.4105     3.1509    732.3445     0.0    
   500       2579.0     1.4176     1.4047     3.0575    421.4097     0.0    
   600      3167.68     1.4071     1.4051     1.1041   1207.9753     0.0    
   700      3756.69     1.4199     1.4141     2.3092   133925.4096    0.0    
   800      4345.33     1.4044     1.4085     3.1617    191.3443     0.0    
   900      4936.21     1.4021     1.3953     1.3731   11777.735     0.0    
   1000     5527.29     1.3641     1.3565     0.9499    724.6783     0.0    
   1100     6120.14     0.7822     0.7671     0.9386     181.6       0.0    
   1200     6711.97     0.7832     0.773      0.9789    103.2983     0.0    
   1300     7303.05     0.7531     0.7408     0.8789    64.0889      0.0    
   1400      7896.8     0.7271     0.7193     0.8314    30.9851      0.0    
   1500     8490.54     0.7414     0.7272     3.1587    182.6468     0.0    
   1600      9084.0     0.738      0.7236     3.1243    6701.821     0.0    
   1700     9675.51     0.7449     0.7373     1.0701    600.3629     0.0    
   1800     10267.27    0.7368     0.7251     3.1467    94.6507      0.0    
   1900     10859.17    0.7335     0.7208     2.7384    81.8238      0.0    
   2000     11451.11    0.744      0.7329     3.1094   1296.2104     0.0    
   2100     12042.96    0.7513     0.7408     3.158     309.303      0.0    
   2200     12633.68    0.7252     0.7122     0.7897    330.4676     0.0    
   2300     13225.54    0.7298     0.7118     2.6525   4709.8624     0.0    
   2400     13816.86    0.7549     0.7458     3.0508    124.9967     0.0    
   2500     14407.5     0.7521     0.7429     2.3262   2097.7064     0.0    
   2600     14999.03    0.7785     0.7616     3.1607    324.0575     0.0    
   2700     15592.32    0.7489     0.7382     0.5547   2309.6032     0.0    
   2800     16185.95    0.7419     0.728      2.2219   26539.3187    0.0    
   2900     16778.4     0.7326     0.7221     3.0162   1273.0824     0.0    
   3000     17370.99    0.7357     0.7266     2.3029    214.5739     0.0    
   3100     17963.6     0.7433     0.7324     2.9176   2041.2814     0.0    
   3200     18556.73    0.732      0.7207     3.069    100306.1836    0.0    
   3300     19148.56    0.7383     0.7286     0.7355   1849.8398     0.0    
   3400     19741.01    0.7289     0.7173     0.5004    306.0387     0.0    
   3500     20334.92    0.7405     0.7336     3.1242    423.5902     0.0    
   3600     20930.41    0.7453     0.7326     0.6837    518.8529     0.0    
   3700     21525.45    0.7297     0.7204     2.9867   3359.0455     0.0    
   3800     22120.29    0.731      0.7211     3.1622   2017.2792     0.0    
   3900     22714.63    0.7315     0.7216     0.4791    248.3884     0.0    
   4000     23306.48    0.724      0.7163     2.8594    105.6206     0.0    
