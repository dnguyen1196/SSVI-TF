Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  8.157579183578491
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       555.78     4.119      4.1518     0.3116     2.3907   128622.85   104296.2     0.0    
   200      1297.62     4.0715     4.1036     0.2954     1.4688   121129.05   98230.46     0.0    
   300      2039.29     3.9096     3.9396     0.2847     1.054    101379.56   82219.42     0.0    
   400      2779.94     3.576      3.6055     0.2559     0.3603    74868.91   60708.31     0.0    
   500      3517.42     3.1021     3.1272     0.2161     0.0835    53488.13   43345.86     0.0    
   600      4252.14     2.5945     2.6132     0.2279     0.7867    40776.97   33001.66     0.0    
   700      4984.83     2.1464     2.1592     0.2156     1.123     34113.53   27570.84     0.0    
   800      5712.08     1.7955     1.8016     0.1936     0.7935    30678.41   24772.68     0.0    
   900       6437.5     1.5396     1.5405     0.142      0.767     28779.19   23234.4      0.0    
   1000     7162.95     1.3561     1.3538     0.1187     0.7778    27766.09   22402.59     0.0    
   1100     7888.73     1.2369     1.2302     0.1583     0.6461    27170.75   21920.5      0.0    
   1200     8613.39     1.1562     1.1487     0.1549     0.5325    26805.34   21626.61     0.0    
   1300     9337.72     1.1022     1.0937     0.1556     0.1028    26596.65   21468.54     0.0    
   1400     10062.14    1.0666     1.0543     0.1841     0.0238    26445.39   21349.54     0.0    
   1500     10786.08    1.0417     1.0295     0.1584     0.4442    26351.2    21267.09     0.0    
   1600     11509.57    1.021      1.0081     0.0854      0.01     26280.84   21206.32     0.0    
   1700     12231.9     1.0071     0.9935     0.1013     0.2289    26231.45   21165.31     0.0    
   1800     12956.11    0.9977     0.9848     0.1226     0.0315    26191.17   21135.06     0.0    
   1900     13681.96    0.9872     0.9717     0.1415     0.0146    26141.67   21097.32     0.0    
   2000     14403.03    0.9774     0.9655     0.1253     0.2403    26126.74   21085.09     0.0    
   2100     15126.99    0.9735     0.9612     0.1094     0.018     26112.23   21081.14     0.0    
   2200     15849.87    0.9695     0.9552     0.1059     0.2476    26092.84   21062.78     0.0    
   2300     16573.6     0.967      0.9537     0.1245     0.0904    26070.97   21044.99     0.0    
   2400     17297.28    0.963      0.9492     0.1363     0.4562    26047.58   21021.55     0.0    
   2500     18020.66    0.9619     0.9482     0.0968     0.0122    26046.06   21018.95     0.0    
   2600     18744.13    0.9583     0.9443     0.0799     0.008     26047.91   21010.39     0.0    
   2700     19467.72    0.9577     0.9437     0.0784     0.0664    26029.55   20995.76     0.0    
   2800     20191.07    0.9561     0.9429     0.0845     0.0557    26031.61   21003.61     0.0    
   2900     20914.81    0.9563     0.941      0.0609     0.0157    26021.01   20994.22     0.0    
   3000     21638.68    0.9537     0.9397     0.0879     0.4384    26008.86   20987.3      0.0    
   3100     22362.57    0.9527     0.939      0.0919      0.01     25991.2    20975.1      0.0    
   3200     23086.22    0.9533     0.9383     0.1391     0.0125    25986.6    20966.4      0.0    
   3300     23810.05    0.9522     0.9373     0.1143     0.0105    26001.33   20979.36     0.0    
   3400     24534.14    0.9498     0.9368      0.07      0.0108    25989.57   20968.57     0.0    
   3500     25258.07    0.9471     0.934      0.0883     0.0074    25967.86   20953.34     0.0    
   3600     25981.88    0.9489     0.9344     0.0477     0.4077    25973.25   20953.03     0.0    
   3700     26706.0     0.9484     0.9332     0.1008     0.111     25968.49   20948.24     0.0    
   3800     27429.99    0.9468     0.9322     0.0802     0.2202    25959.7    20946.41     0.0    
   3900     28153.88    0.9475     0.9329     0.0952     0.0121    25955.4    20942.55     0.0    
   4000     28877.86    0.9488     0.9342     0.1245     0.0133    25965.36   20944.4      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       550.32     4.1365     4.158      0.3134     1.9693   128899.11  208260.71     0.0    
   200      1369.83     4.0946     4.1155     0.3034     0.7355   122441.19   197784.3     0.0    
   300      2184.01     3.934      3.9543     0.2848     0.492    102805.56  166057.52     0.0    
   400      2996.68     3.5982     3.6165     0.2553     0.3602    75618.46  122172.01     0.0    
   500      3809.64     3.1209     3.1362     0.2907     0.4455    53754.59   86857.74     0.0    
   600      4620.41     2.616      2.6266     0.2912     0.4182    41002.64   66256.92     0.0    
   700      5427.43     2.1763     2.1832     0.2355     0.2321    34338.01   55509.84     0.0    
   800      6235.66     1.8289     1.8334     0.2622     0.2363    30839.05   49876.05     0.0    
   900       7037.8     1.575      1.5759     0.1528     0.2628    28955.21   46814.83     0.0    
   1000     7841.27     1.3937     1.3911     0.1861     0.0776    27859.36   45050.44     0.0    
   1100     8643.73     1.2633     1.2594     0.1616     0.5595    27231.55   44046.91     0.0    
   1200     9448.99     1.1736     1.1691     0.1997     0.1424    26866.3    43441.88     0.0    
   1300     10258.07    1.117      1.1126     0.1452     0.1269    26634.8    43066.17     0.0    
   1400     11061.53    1.075      1.0709     0.0852     0.3974    26460.69   42808.84     0.0    
   1500     11871.78    1.0457     1.0411     0.1763     0.6469    26347.89   42620.16     0.0    
   1600     12684.01    1.026      1.0195     0.1096     0.2118    26293.89   42540.04     0.0    
   1700     13489.43    1.0093     1.0021     0.1492     0.1918    26231.96   42417.43     0.0    
   1800     14298.73    0.9946     0.9884     0.0797     0.2576    26174.68   42346.38     0.0    
   1900     15105.13    0.9876     0.9809     0.2079     0.7048    26169.39   42323.36     0.0    
   2000     15906.05    0.9799     0.9727     0.1169     0.5578    26154.31   42289.18     0.0    
   2100     16707.76    0.974      0.9667     0.1251     0.6145    26117.38   42235.42     0.0    
   2200     17512.56    0.9723     0.9662     0.1048     1.0513    26105.32   42229.92     0.0    
   2300     18317.47    0.9727     0.9674     0.1405     1.9744    26143.64   42275.98     0.0    
   2400     19122.05    0.9798     0.973      0.1417     5.3461    26153.0    42299.59     0.0    
   2500     19927.8     1.1926     1.1844     0.177     59.2955    26274.8    42479.04     0.0    
   2600     20734.85    1.2165     1.2181     0.2199    189.0361     nan        nan        0.0    
   2700     21542.05    1.0396     1.0339     0.2297     95.512      nan        nan        0.0    
   2800     22349.5     1.0323     1.0244     0.1283    19.7644      nan        nan        0.0    
   2900     23156.68    2.115      2.0975     0.2155    484.5222     nan        nan        0.0    
   3000     23963.69    1.0048     0.9962     0.1479    52.9517      nan        nan        0.0    
   3100     24771.78    0.9717     0.9632     0.1068     6.594       nan        nan        0.0    
   3200     25582.88    0.9631     0.9554     0.0916     2.3929      nan        nan        0.0    
   3300     26393.82    0.957      0.9495     0.0797     0.8125      nan        nan        0.0    
   3400     27204.32    0.9544     0.9486     0.0661     0.4678      nan        nan        0.0    
   3500     28014.83    0.9535     0.9462     0.095      0.5493      nan        nan        0.0    
   3600     28826.32    0.9512     0.9431     0.0866     0.4939      nan        nan        0.0    
   3700     29634.49    0.9523     0.9442     0.1055     0.3252      nan        nan        0.0    
   3800     30442.38    0.9516     0.9431     0.0722     0.2607      nan        nan        0.0    
   3900     31249.84     0.95      0.9418     0.0636     0.2286      nan        nan        0.0    
   4000     32055.42    0.9482     0.9396     0.0657     0.2767      nan        nan        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       556.33     4.1433     4.1573     0.3115     2.6459   129125.11  311704.74     0.0    
   200      1462.53     4.1081     4.1228     0.3065     1.1907   123754.99  298772.26     0.0    
   300      2367.96     3.9606     3.9758     0.2868     0.745    105181.34  254038.17     0.0    
   400      3268.75     3.6378     3.6523     0.2617     1.1516    77764.45   187982.7     0.0    
   500      4167.36     3.1672     3.1808     0.2748     7.2346    55127.56  133354.58     0.0    
   600      5063.63     2.7091     2.7222     0.2854    307.7866     nan        nan        0.0    
   700      5956.48     2.2576     2.2674     0.2387    60.9021      nan        nan        0.0    
   800      6847.75     1.9458     1.9524     0.2438    113.5288     nan        nan        0.0    
   900      7739.57     1.6606     1.6669     0.2367    92.0276      nan        nan        0.0    
   1000     8627.23     1.4388     1.4437     0.1998     2.9733      nan        nan        0.0    
   1100     9518.94      1.3       1.3025     0.1526     0.5551      nan        nan        0.0    
   1200     10412.93    1.2053     1.205      0.2055     2.0934      nan        nan        0.0    
   1300     11305.83    1.1384     1.1368     0.1472      0.7        nan        nan        0.0    
   1400     12203.75    1.0926     1.091      0.1624     1.7478      nan        nan        0.0    
   1500     13102.01    1.0927     1.0906     0.2275    13.5202      nan        nan        0.0    
   1600     13992.49    1.0371     1.0339     0.1981    32.2983      nan        nan        0.0    
   1700     14887.85    1.0184     1.0142     0.1301     1.6421      nan        nan        0.0    
   1800     15781.59    1.0008     0.9956     0.1037     2.3133      nan        nan        0.0    
   1900     16674.73    1.0103     1.0052     0.1703    10.7846      nan        nan        0.0    
   2000     17565.81    1.0585     1.0509     0.2611    316.0471     nan        nan        0.0    
   2100     18457.83    0.9791     0.9729     0.1354     5.9406      nan        nan        0.0    
   2200     19347.52    0.9718     0.9658     0.1365     1.5384      nan        nan        0.0    
   2300     20234.51    0.9691     0.9622     0.1216     0.6629      nan        nan        0.0    
   2400     21127.71    0.964      0.9567     0.1524     0.4667      nan        nan        0.0    
   2500     22021.35    0.9597     0.9528     0.1011     0.2883      nan        nan        0.0    
   2600     22914.35    0.9554     0.9488      0.12      0.2745      nan        nan        0.0    
   2700     23807.95    0.9537     0.946      0.0655     0.1921      nan        nan        0.0    
   2800     24698.33    0.9499     0.9431     0.1024     0.1059      nan        nan        0.0    
   2900     25588.0     0.9451     0.9391     0.0983     0.1436      nan        nan        0.0    
   3000     26477.57    0.9438     0.9378     0.0637     0.3798      nan        nan        0.0    
   3100     27369.04    0.9436     0.9366     0.0975     0.0665      nan        nan        0.0    
   3200     28253.36    0.9418     0.9356     0.1272     0.1317      nan        nan        0.0    
   3300     29141.57    0.9419     0.9347     0.1341     0.1582      nan        nan        0.0    
   3400     30030.78    0.9383     0.9323     0.0994     0.0863      nan        nan        0.0    
   3500     30920.67    0.9392     0.9314     0.0764     0.0546      nan        nan        0.0    
   3600     31810.81    0.9369     0.9296     0.0957     0.0571      nan        nan        0.0    
   3700     32700.47    0.9341     0.927      0.1011     0.1663      nan        nan        0.0    
   3800     33591.74    0.9349     0.9278     0.1402     0.0099      nan        nan        0.0    
   3900     34489.04    0.9336     0.926      0.1188     0.0286      nan        nan        0.0    
   4000     35385.31    0.9343     0.9258     0.061      0.0114      nan        nan        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       554.9      4.1465     4.1588     0.315    3361.4733     nan        nan        0.0    
   200      1538.18     4.1155     4.1277     0.302    1138.6754     nan        nan        0.0    
   300      2521.12     3.983      3.9947     0.2854    64.3415      nan        nan        0.0    
   400      3506.77     3.6845     3.6954     0.264     31.3323      nan        nan        0.0    
   500      4489.64     3.2217     3.2318     0.2943    37.9262      nan        nan        0.0    
   600      5466.67     2.7274     2.7362     0.2522    55.8988      nan        nan        0.0    
   700      6440.76     2.2596     2.2679     0.2358    105.3408     nan        nan        0.0    
   800      7413.12     1.9464     1.9522     0.2478    448.4846     nan        nan        0.0    
   900      8383.63     1.7155     1.6941     0.1375    251.9033     nan        nan        0.0    
   1000     9352.65     1.4808     1.4799     0.1948    122.9279     nan        nan        0.0    
   1100     10318.69    1.3105     1.3146     0.2755    23.7843      nan        nan        0.0    
   1200     11283.67    1.256      1.2607     0.2138    22.4868      nan        nan        0.0    
   1300     12248.45    1.1562     1.1601     0.1701    34.0477      nan        nan        0.0    
   1400     13214.7     1.1716     1.1595     0.1382    41.1425      nan        nan        0.0    
   1500     14184.22    1.0657     1.0666     0.1438    35.7859      nan        nan        0.0    
   1600     15152.95    1.0403     1.0391     0.1524     9.0516      nan        nan        0.0    
   1700     16123.06    1.0208     1.0198     0.1473    54.2953      nan        nan        0.0    
   1800     17092.23    1.0059     1.0039     0.1947     25.63       nan        nan        0.0    
   1900     18061.24    1.0004     0.9958     0.0922    25.0131      nan        nan        0.0    
   2000     19031.69    0.9892     0.9871     0.1241    11.1043      nan        nan        0.0    
   2100     20000.77    0.9839     0.9809     0.1022     7.1196      nan        nan        0.0    
   2200     20969.49    0.9747     0.9708     0.153     24.0966      nan        nan        0.0    
   2300     21938.03    0.9712     0.9666     0.0935     23.657      nan        nan        0.0    
   2400     22902.45    0.9736     0.9656     0.1417    173.9351     nan        nan        0.0    
   2500     23867.16    0.9656     0.9612     0.1388    92.0118      nan        nan        0.0    
   2600     24834.03    0.9639     0.9606     0.1007    214.0822     nan        nan        0.0    
   2700     25804.46    0.9777     0.9732     0.1455    113.0301     nan        nan        0.0    
   2800     26773.72    0.994      0.9995     0.0946    965.6464     nan        nan        0.0    
   2900     27739.37    1.2352     1.2388     0.0863   8881.2045     nan        nan        0.0    
   3000     28705.98    1.2351     1.248      0.1244   22225.0143    nan        nan        0.0    
   3100     29676.16    1.1496     1.2063     0.1001   2824.6601     nan        nan        0.0    
   3200     30640.52    0.9859     0.9846     0.1065    214.7628     nan        nan        0.0    
   3300     31610.05    0.9936     1.0009     0.1152    26.4507      nan        nan        0.0    
