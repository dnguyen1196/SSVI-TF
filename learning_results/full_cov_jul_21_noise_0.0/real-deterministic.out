Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  5.2939558029174805
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       202.05    175.4684   175.193     3.1622     0.1647   
   200       409.29    127.2881   126.6866    2.6066     0.054    
   300       616.88    115.6689   115.4908    2.1317     0.0141   
   400       824.03    112.1471   111.8755    1.7107     0.0231   
   500      1033.04    107.8594   108.2092    1.6094     0.0087   
   600      1238.99    105.838    106.243     1.1242     0.0086   
   700      1447.34    105.6639   105.3799    0.9665     0.0092   
   800       1654.9    105.1666   104.7558    0.8698     0.0052   
   900      1862.31    105.5267   105.4304    0.9627     0.0041   
   1000     2068.88    105.9967   105.6051    1.0478     0.0053   
   1100     2276.83    105.6199   105.419     1.0039     0.0036   
   1200     2483.37    106.008    105.6254    1.0737     0.0035   
   1300     2691.56    105.9321   105.6853    0.9859     0.0027   
   1400     2898.34    105.2716   105.0586    0.8227     0.0025   
   1500     3105.95    105.2684   104.6648    0.8365     0.0018   
   1600     3313.53    105.205    104.8798    0.8798     0.0019   
   1700     3522.86    106.2524   105.3243    0.9876     0.0022   
   1800     3729.79    105.5029   104.5888    1.0221     0.0013   
   1900     3938.03    105.5869   105.0488    0.7699     0.0017   
   2000      4143.8    105.9399   105.2891    0.9803     0.0012   
   2100     4352.52    105.0233   104.5461    0.7937     0.001    
   2200     4559.42    105.2863   104.6777    0.8068     0.0013   
   2300     4766.87    104.5283   104.4123    0.7247     0.001    
   2400     4973.08    105.5529   104.792     0.6484     0.0009   
   2500     5179.84    105.3842   104.7242    0.9267     0.0008   
   2600     5385.28    105.1736   104.8479    0.9301     0.0009   
   2700     5591.03    104.9129   104.5992    0.8003     0.0008   
   2800     5798.42    104.7703   104.1954    0.7814     0.0007   
   2900      6004.9    104.6273   104.1116    0.5681     0.0006   
   3000     6209.23    104.9352   104.4583    0.724      0.0006   
   3100      6414.7    105.2105   104.6547    0.7193     0.0006   
   3200     6620.89    104.8384   104.506     0.7921     0.0005   
   3300     6828.43    104.8173   104.2229    0.5213     0.0004   
   3400     7034.77    104.6479   104.3769    0.7565     0.0005   
   3500     7239.16    104.6677   104.0553    0.7103     0.0005   
   3600     7444.92    104.9869   104.3322    0.8095     0.0004   
   3700     7652.62    104.5021   103.9452    0.591      0.0005   
   3800     7857.95    104.8904   104.4033    0.545      0.0004   
   3900     8063.91    104.2734   103.7914    0.5948     0.0004   
   4000     8270.46    104.9812   104.4916    0.574      0.0004   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       201.89    168.3328   169.2768    3.1622     0.4936   
   200       408.15    123.0777   123.6822    2.6542     0.0535   
   300       613.77    114.1172   114.1059    2.4435     0.0254   
   400       819.73     109.94    109.3259    1.705      0.0176   
   500      1025.38    108.5655   108.037     1.3134     0.0125   
   600      1231.45    106.8368   106.4416    1.2358     0.0125   
   700      1438.82    105.4567   104.8495    1.104      0.0069   
   800      1644.75    106.2898   105.5834    1.013      0.0073   
   900      1851.46    106.142    105.3915    0.8389     0.0059   
   1000     2058.22    105.5777   104.8555    0.8962     0.0045   
   1100     2263.93    105.4638   104.881     0.7796     0.0049   
   1200      2469.8    105.8451   105.1447    1.026      0.0029   
   1300     2676.45    105.7959   105.4369    0.8811     0.0031   
   1400     2883.94    106.1038   105.3467    0.8252     0.003    
   1500      3089.8    105.2415   104.4889    0.779      0.0025   
   1600     3297.49    105.3071   104.722     0.7307     0.0026   
   1700     3504.09    105.5682   104.9668    0.9035     0.002    
   1800     3711.01    105.201    105.086     0.8072     0.002    
   1900     3919.55    105.8899   105.2092    0.7076     0.0019   
   2000     4126.11    104.9082   104.6254    0.7612     0.0016   
   2100     4333.65    105.5389   104.9669    0.8019     0.0011   
   2200     4540.64    104.9307   104.2986    0.7334     0.0012   
   2300     4747.46    105.6248   104.6833    0.7969     0.0014   
   2400     4953.82    104.3662   103.8902    0.6505     0.0011   
   2500     5160.03    105.4405   104.4646    0.6279     0.0009   
   2600      5367.4    105.4894   104.789     0.7245     0.001    
   2700     5572.05    104.7545   104.0719    0.6739     0.0009   
   2800     5778.35    104.9815   104.3237    0.771      0.001    
   2900     5984.78    105.2205   104.6287    0.9423     0.0008   
   3000     6190.82    104.8251   104.2498    0.6119     0.0007   
   3100     6398.31    104.9563   104.3929    0.6714     0.0006   
   3200     6603.91    104.3514   103.9877    0.5134     0.0006   
   3300     6810.09    104.816    104.338     0.7149     0.0007   
   3400     7016.17    105.0722   104.4162    0.7954     0.0005   
   3500     7220.75    104.8818   104.2605    0.9034     0.0005   
   3600     7428.87    104.6662   104.1953    0.6903     0.0005   
   3700     7633.51    105.2463   104.4884    0.6744     0.0006   
   3800     7840.87    105.0099   104.4349    0.6834     0.0004   
   3900     8046.74    105.3049   104.6008    0.6642     0.0004   
   4000     8252.59    104.6856   104.0241    0.6582     0.0003   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       201.26    171.1382   170.6316    3.162      0.4953   
   200       411.29    125.2718   125.3262    2.7848     0.0706   
   300       618.66    116.0032   115.6412    2.2581     0.0334   
   400       824.45    108.5841   108.3868    1.7735     0.0305   
   500      1032.01    106.9397    106.32     1.409      0.018    
   600      1237.98    106.8696   106.2435    1.6222     0.0129   
   700      1445.35    106.5935   106.4243    1.1571     0.0099   
   800      1651.46    105.623    105.4559    1.2289     0.0077   
   900      1857.78    105.3964   105.1976    1.0354     0.0065   
   1000     2065.91    104.9296   104.6693    0.7824     0.0056   
   1100     2273.55    105.9723   105.2723    0.8239     0.0045   
   1200     2481.32    105.1453   104.7429    0.8487     0.003    
   1300     2687.39    105.5556   105.0964    0.9392     0.0029   
   1400     2895.59    105.5586   105.5534    1.088      0.0029   
   1500      3103.1    105.9257   105.4801    0.8625     0.0021   
   1600     3311.66    105.2984   104.7763    0.8302     0.0024   
   1700     3519.13    105.6991   105.0794    0.8343     0.0018   
   1800     3727.01    105.2616   104.6259    0.8027     0.0017   
   1900     3934.03    106.3385   105.5905    0.8862     0.0013   
   2000     4141.08    105.4949   105.055     0.7834     0.0014   
   2100     4349.97    104.895    104.4946    0.7236     0.0011   
   2200     4556.65    105.1254   104.6766    0.8192     0.0012   
   2300     4766.02    104.535    104.2357    0.5283     0.0011   
   2400      4972.5    105.066    104.6791    0.5465     0.0008   
   2500     5179.36    104.6794   104.2279    0.7453     0.0009   
   2600     5387.28    104.8545   104.4338    0.5364     0.0007   
   2700     5593.82    104.5123   104.1855    0.7415     0.0008   
   2800      5802.4    104.8955   104.4165    0.7737     0.0007   
   2900     6008.78    104.8643   104.3805    0.6633     0.0007   
   3000     6216.12    104.4977   104.1937    0.6281     0.0005   
   3100     6423.75    105.0258   104.5581    0.7354     0.0006   
   3200     6631.38    105.375    104.9576    0.8248     0.0005   
   3300     6839.73    104.6906   104.376     0.6754     0.0005   
   3400     7045.32    104.5176   104.2391    0.6184     0.0005   
   3500     7252.28    104.6523   104.0792    0.6043     0.0004   
   3600     7459.45    104.2561   104.0815    0.4991     0.0004   
   3700     7666.87    104.6476   104.1908    0.632      0.0004   
   3800     7873.68    104.3537   104.0583    0.5452     0.0004   
   3900     8081.07    104.119    103.8592    0.6483     0.0003   
   4000     8287.11    104.6768   104.2264    0.6283     0.0004   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       199.1     166.4073   167.1034    3.1608     0.3013   
   200       408.02    127.0724   127.1217    2.3643     0.0934   
   300       614.39    115.6225   115.6382    2.2985     0.0463   
   400       822.8     108.1606   108.3472    1.6138     0.0172   
   500      1030.83    107.8715   107.768     1.6577     0.0206   
   600      1239.73    107.061     106.8      1.0332     0.0105   
   700      1448.73    105.6475   105.5771    0.9571     0.0122   
   800      1656.03    105.9652   105.5755    0.8198     0.0064   
   900      1862.64    105.9439   105.9842    1.0976     0.0051   
   1000     2069.27    105.4449   105.5699    0.9027     0.0043   
   1100     2277.75    106.2207   106.1733    1.2732     0.0049   
   1200     2486.03    105.3567   105.3065    0.7423     0.0027   
   1300      2695.1    105.6594   105.5754    1.0059     0.0031   
   1400     2902.59    105.4256   105.1834    0.8701     0.0025   
   1500      3110.9    106.0185   105.6857    0.9035     0.0022   
   1600     3317.91    105.6704   105.5432    0.7492     0.0018   
   1700     3524.78    104.9834   105.2679    0.593      0.0015   
   1800     3734.12    105.0379   105.0186    0.554      0.0015   
   1900     3942.91    105.124    104.9718    0.7404     0.0014   
   2000     4149.74    104.6308   104.572     0.9543     0.0007   
   2100     4358.31    105.321    105.5509    0.8545     0.001    
   2200     4566.55    105.328     105.63     0.8211     0.0009   
   2300     4774.86    105.2311   105.0378    0.8563     0.0006   
   2400     4983.38    104.7922   104.903     0.7218     0.0008   
   2500     5192.37    105.0593   105.3787    0.7784     0.0008   
   2600     5399.27    104.9796   104.9304    0.9102     0.0007   
   2700     5607.58    104.2589   104.6989    0.4695     0.0007   
   2800     5813.81    105.084    105.3168    0.9334     0.0006   
   2900     6022.49    104.8644   105.2129    0.723      0.0006   
   3000     6231.87    104.6629   104.8411    0.5347     0.0008   
   3100     6437.91    105.1964   105.2097    0.743      0.0006   
   3200     6645.12    104.6184   104.6603    0.6157     0.0004   
   3300     6852.72    104.9602   104.8404    0.5652     0.0005   
   3400     7059.65    104.5368   104.5972    0.6662     0.0004   
   3500     7265.73    104.5898   104.6014    0.5706     0.0004   
   3600     7474.36    104.5503   104.5566    0.4832     0.0004   
   3700     7681.02    105.3555   105.398     0.8033     0.0004   
   3800     7888.94    104.9056   104.9086    0.6911     0.0004   
   3900     8095.52    104.8954   105.0429    0.6486     0.0003   
   4000     8301.62    104.9797   104.818     0.6585     0.0003   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       201.36    169.4671   169.4497    3.1622     0.1646   
   200       412.24    124.9356   125.3655    2.7283     0.0524   
   300       620.31    113.3864   113.4503    1.9772     0.0173   
   400       828.22    109.0051   109.225     1.9327     0.0069   
   500       1037.5    107.5282   107.838     1.8183     0.0111   
   600      1246.88    105.6794   105.8546    1.0322     0.0077   
   700      1456.06    105.9796   106.193     0.8785     0.0047   
   800      1665.16    106.3113   106.1132    0.9247     0.0018   
   900      1873.62    106.6354   106.3152    1.3337     0.0021   
   1000     2082.65    107.0592   106.9627    1.2523     0.0046   
   1100     2291.99    106.2651   106.1591    1.2377     0.0019   
   1200     2501.47    105.7094   105.8532    0.8557     0.0013   
   1300      2710.0    106.2278   106.3053    0.9409     0.0016   
   1400     2917.85    105.6022   105.9325    0.7785     0.0015   
   1500     3125.44    105.2192   105.394      0.81      0.0014   
   1600     3334.37    105.0133   105.1299    0.8013     0.0011   
   1700     3542.05    105.1809   105.3719    0.8815     0.0013   
   1800     3750.42    105.8204   105.8745    0.7978     0.001    
   1900     3958.28    104.8357   104.9904    0.8422     0.0009   
   2000     4165.79    105.4248   105.7034    0.9712     0.0009   
   2100     4374.16    104.9928   105.2715    0.7418     0.0008   
   2200     4581.88    105.0196   105.2456    0.7446     0.0006   
   2300     4789.34    105.1127   105.3905    0.7391     0.0009   
   2400     4999.55    105.3172   105.3401    0.7531     0.0008   
   2500     5207.78    104.7672   104.8561    0.6584     0.0006   
   2600     5416.92    104.9308   104.9661    0.6941     0.0005   
   2700     5622.51    105.2967   105.6502    0.7901     0.0004   
   2800     5827.74    105.2661   105.0742    0.8384     0.0005   
   2900     6036.28    104.6287   104.8385    0.5872     0.0005   
   3000     6242.61    104.9395   105.0971    0.8279     0.0004   
   3100     6450.67    105.1155   105.4526    0.724      0.0004   
   3200     6659.97    104.797    104.8792    0.7059     0.0004   
   3300     6867.55    104.6702   104.6031    0.5893     0.0004   
   3400     7076.69    104.9778   104.9395    0.8449     0.0004   
   3500     7286.39    104.1987   104.5632    0.7379     0.0004   
   3600     7495.18    104.7629   104.9398    1.0228     0.0003   
   3700     7704.57    104.5852   104.8068    0.8305     0.0004   
   3800     7911.18    104.5396   104.6995    0.6406     0.0002   
   3900     8118.71    105.261    105.499     0.6726     0.0002   
   4000     8327.26    104.6078   104.9822    0.5858     0.0003   
