Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  7.72295618057251
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       241.62     4.2028     4.2364     0.3136     0.2923   127950.36  103720.78    0.0002  
   200       665.78     4.1544     4.1868     0.295      0.0264   119160.86   96555.96    0.0001  
   300       1091.2     3.9667     3.9981     0.2825     0.0255    97334.65   78818.05     0.0    
   400      1515.91     3.5706     3.5984     0.2631     0.0168    70898.53   57382.24     0.0    
   500      1941.12     3.057      3.0787     0.2645     0.0101    51288.47   41498.6      0.0    
   600      2366.49     2.5687     2.5866     0.1651     0.0119    40121.63   32450.28     0.0    
   700      2791.72     2.1863     2.1964     0.1817     0.0122    34418.83   27810.83    0.0001  
   800      3216.69     1.8875     1.8929     0.2394     0.0113    31277.58   25269.96     0.0    
   900      3640.88     1.6631     1.6662     0.1951     0.0104    29481.92   23813.36     0.0    
   1000     4066.36     1.4937     1.4936     0.1633     0.0097    28395.0    22925.94     0.0    
   1100     4489.02     1.3702     1.3674     0.1956     0.0097    27715.27   22380.93     0.0    
   1200     4913.96     1.2809     1.2751     0.103      0.0088    27313.23   22050.65     0.0    
   1300      5337.1     1.2164     1.2087     0.1014     0.0083    27061.07   21846.68     0.0    
   1400     5759.89     1.1656     1.1556     0.1089     0.0079    26860.83   21677.48     0.0    
   1500     6183.27     1.1226     1.1135     0.1185     0.0077    26692.14   21548.14     0.0    
   1600     6607.31     1.0936     1.0829     0.1683     0.007     26602.8    21476.72     0.0    
   1700     7030.21     1.0715     1.0609     0.1008     0.0068    26518.33   21406.61     0.0    
   1800     7451.34     1.0549     1.0419     0.149      0.0063    26456.94   21359.59     0.0    
   1900     7871.52     1.0415     1.0285     0.0579     0.0063    26418.02   21318.82     0.0    
   2000     8291.23     1.0283     1.0162     0.0657     0.0055    26371.59   21283.44     0.0    
   2100     8711.91     1.0205     1.008      0.139      0.0053    26354.06   21267.42     0.0    
   2200     9132.07     1.0161     1.0034     0.1959     0.005     26360.95   21270.38     0.0    
   2300     9553.51     1.0106     0.9968     0.0931     0.0047    26341.81   21251.77     0.0    
   2400     9974.73     1.0072     0.9939     0.111      0.0045    26342.18   21259.68     0.0    
   2500     10395.43    1.0013     0.9868     0.0652     0.0045    26311.08   21229.1      0.0    
   2600     10817.0     0.997      0.9844     0.1686     0.004     26297.05   21225.18     0.0    
   2700     11238.05    0.9945      0.98      0.1198     0.0039    26287.5    21209.26     0.0    
   2800     11658.14    0.9944     0.9812     0.1341     0.0038    26287.29   21214.67     0.0    
   2900     12080.38    0.9878     0.9747     0.0795     0.0036    26256.0    21192.83     0.0    
   3000     12502.28    0.9856     0.9725     0.0644     0.0035    26239.77   21182.53     0.0    
   3100     12922.87    0.985      0.971      0.1254     0.0035    26249.38   21194.14     0.0    
   3200     13344.5     0.9828     0.9684     0.0636     0.0033    26238.18   21176.98     0.0    
   3300     13763.95    0.9824     0.9693     0.0913     0.0031    26234.16   21176.99     0.0    
   3400     14185.87    0.9865     0.9759     0.1266     0.0028    26265.87   21202.78     0.0    
   3500     14609.28    0.9853     0.9738     0.0781     0.0032    26259.84   21196.74     0.0    
   3600     15030.9     0.9861     0.9756     0.1206     0.0026    26270.61   21203.45     0.0    
   3700     15451.47    0.9789     0.9662     0.1456     0.0028    26226.5    21163.06     0.0    
   3800     15873.07    0.9804     0.969      0.1076     0.0026    26246.03   21187.74     0.0    
   3900     16295.59    0.9815     0.9705     0.0818     0.0028    26250.2    21185.71     0.0    
   4000     16716.88     0.98      0.968      0.0791     0.0023    26236.17   21170.8      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       240.63     4.2204     4.243      0.3128     0.2504   128715.92  207951.39    0.0002  
   200       740.93     4.1754     4.1975     0.297      0.0216   121102.75  195589.24     0.0    
   300      1240.42     3.9901     4.0111     0.291      0.0213    99555.66   160726.1     0.0    
   400      1741.07     3.5924     3.6111     0.2654     0.0134    72052.57  116325.04     0.0    
   500      2239.35     3.0724     3.0869     0.2272     0.0076    51554.12   83268.88     0.0    
   600      2740.11     2.5914     2.6025     0.2058     0.0089    40316.47   65137.93     0.0    
   700      3238.47     2.2064     2.2141     0.2187     0.0091    34518.19   55794.75     0.0    
   800      3735.61     1.9089     1.9134     0.1229     0.0098    31327.22   50636.4      0.0    
   900      4233.84     1.6823     1.6835     0.2653     0.0101    29482.91   47659.3      0.0    
   1000      4733.1     1.5132     1.5126     0.0894     0.0099    28376.18   45873.46     0.0    
   1100     5230.71     1.3832     1.3817     0.2268     0.0095    27657.81   44712.49     0.0    
   1200     5727.64     1.2843     1.2831     0.1386     0.009     27171.62   43955.7      0.0    
   1300     6224.89     1.214      1.2099     0.1787     0.0086    26885.33   43471.61     0.0    
   1400     6722.26     1.1573     1.1534     0.0926     0.0081    26651.11   43104.27     0.0    
   1500      7220.8     1.1186     1.1139     0.1816     0.0077    26518.73   42899.8      0.0    
   1600     7719.96     1.0829     1.0787     0.1444     0.0073    26384.58   42678.16     0.0    
   1700     8218.16     1.0574     1.0536     0.1544     0.007     26310.86   42560.14     0.0    
   1800     8716.07     1.039      1.0321     0.0997     0.0066    26237.73   42440.84     0.0    
   1900     9216.95     1.0228     1.0172     0.1406     0.0062    26200.92   42387.6      0.0    
   2000     9715.02     1.0099     1.0044     0.1665     0.0059    26154.81   42313.62     0.0    
   2100     10213.14    0.9983     0.993      0.182      0.0056    26124.08   42260.72     0.0    
   2200     10712.58    0.9904     0.9838     0.132      0.0054    26092.41   42222.98     0.0    
   2300     11211.85    0.9821     0.976      0.1092     0.0051    26072.44   42180.11     0.0    
   2400     11710.83    0.9769     0.9703      0.07      0.0048    26058.2    42154.07     0.0    
   2500     12211.45    0.9719     0.9651     0.1086     0.0046    26042.61   42134.33     0.0    
   2600     12712.53     0.97      0.9641     0.0973     0.0044    26041.9    42132.83     0.0    
   2700     13211.11    0.9634     0.9566     0.1076     0.0042    26011.63   42080.61     0.0    
   2800     13709.28    0.9614     0.9549     0.0824     0.004     26004.53   42079.92     0.0    
   2900     14206.45    0.958      0.9533     0.0835     0.0039    25990.92   42053.83     0.0    
   3000     14704.47    0.9578     0.9523     0.1668     0.0037    25995.33   42071.02     0.0    
   3100     15203.38    0.9552     0.9472     0.1041     0.0036    25978.24   42020.91     0.0    
   3200     15702.76    0.9552     0.9476     0.1094     0.0035    25976.27   42024.67     0.0    
   3300     16202.94    0.9534     0.947      0.0913     0.0032    25989.1    42038.49     0.0    
   3400     16702.1     0.9508     0.9445     0.1105     0.0032    25969.1    42017.03     0.0    
   3500     17200.93    0.9488     0.9437     0.0721     0.0031    25960.32   42002.08     0.0    
   3600     17700.91    0.9474     0.9429     0.104      0.003     25975.16   42005.62     0.0    
   3700     18199.34    0.9494     0.9432     0.123      0.0028    25980.67   42022.28     0.0    
   3800     18699.39    0.9451     0.9402     0.0822     0.0028    25946.24   41969.52     0.0    
   3900     19198.87    0.9464     0.9403     0.0553     0.0026    25946.72   41977.52     0.0    
   4000     19699.35    0.947      0.9413     0.0846     0.0026    25965.13   41994.19     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       241.32     4.2267     4.2418     0.3133     0.2299   128879.97  311064.66    0.0001  
   200       824.63     4.1811     4.1961     0.2996     0.0197   121485.51   293184.2     0.0    
   300      1406.66     3.992      4.0067     0.2917     0.0193    99754.39   240746.9     0.0    
   400      1990.95     3.5893     3.6031     0.2683     0.0119    71810.67  173403.42     0.0    
   500      2578.36     3.0666     3.079      0.2684     0.0069    51206.24  123768.83     0.0    
   600      3161.58     2.5866     2.5978     0.2478     0.0078    40094.89   96956.15     0.0    
   700      3744.75     2.2072     2.2173     0.1649     0.0091    34429.59   83284.2      0.0    
   800      4329.97     1.9142     1.922      0.2218     0.0099    31302.66   75723.93     0.0    
   900      4912.94     1.6868     1.6928     0.1295     0.0101    29455.45   71238.69     0.0    
   1000     5497.85     1.5127     1.518      0.1394     0.0099    28310.96   68483.51     0.0    
   1100     6078.66     1.3803     1.3846     0.199      0.0097    27591.53   66713.55     0.0    
   1200     6662.95     1.2841     1.2866     0.0882     0.0093    27131.57   65598.53     0.0    
   1300     7246.17     1.2067     1.208      0.1738     0.0088    26785.5    64773.59     0.0    
   1400      7826.7     1.1504     1.1516     0.1529     0.0084    26575.74   64270.46     0.0    
   1500     8409.41     1.1069     1.1062     0.1027     0.0081    26406.73   63839.74     0.0    
   1600     8990.93     1.0748     1.0732     0.1376     0.0077    26304.12   63578.72     0.0    
   1700     9570.78     1.0492     1.0469     0.1083     0.0072    26213.06   63366.03     0.0    
   1800     10151.84    1.0279     1.0265     0.1291     0.0069    26146.29   63214.68     0.0    
   1900     10733.85    1.0137     1.0113     0.1244     0.0065    26127.31   63141.42     0.0    
   2000     11314.72    1.0001     0.9969     0.1063     0.0063    26070.37   63020.97     0.0    
   2100     11896.14    0.9898     0.986      0.1049     0.006     26030.37   62921.83     0.0    
   2200     12477.78    0.9811     0.9768     0.0916     0.0056    26002.82   62848.61     0.0    
   2300     13058.65    0.9737     0.9684     0.1023     0.0054    25983.65   62799.13     0.0    
   2400     13638.58    0.9691     0.9626     0.101      0.0051    25972.53   62754.68     0.0    
   2500     14221.18    0.9639     0.9569     0.1527     0.0049    25974.45   62747.51     0.0    
   2600     14803.53    0.9572     0.953      0.0964     0.0047    25939.53   62691.84     0.0    
   2700     15383.98    0.9548     0.9484     0.088      0.0045    25920.25   62642.54     0.0    
   2800     15961.4     0.9525     0.9464     0.0592     0.0043    25911.49   62633.0      0.0    
   2900     16540.16    0.9494     0.9444     0.1146     0.0041    25914.61   62635.64     0.0    
   3000     17120.04    0.9487     0.9415     0.1225     0.004     25907.17   62610.01     0.0    
   3100     17696.99    0.9457     0.939      0.1171     0.0038    25893.04   62571.91     0.0    
   3200     18275.29    0.9432     0.9372     0.1452     0.0037    25893.61   62558.69     0.0    
   3300     18854.14    0.9431     0.937      0.1234     0.0036    25895.12   62571.85     0.0    
   3400     19433.66    0.9403     0.9336     0.0783     0.0034    25884.76   62540.54     0.0    
   3500     20014.17    0.9391     0.9333     0.0861     0.0034    25873.75   62533.76     0.0    
   3600     20592.94    0.9383     0.9318     0.1006     0.0032    25867.43   62503.26     0.0    
   3700     21173.68    0.9395     0.9329     0.1076     0.0032    25883.18   62550.08     0.0    
   3800     21752.12    0.9368     0.9308     0.0939     0.003     25864.88   62503.45     0.0    
   3900     22332.56    0.9369     0.9301     0.1041     0.0029    25866.8    62495.29     0.0    
   4000     22912.93    0.937      0.9305     0.0859     0.0028    25865.29   62506.22     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       240.11      4.23      4.2429     0.3142     0.2438   129050.54  415191.52    0.0001  
   200       900.13     4.1889     4.2015     0.3018     0.0182   122440.37  393876.16     0.0    
   300      1557.89     4.0095     4.0216     0.2894     0.0183    101480.0  326465.97     0.0    
   400      2216.65     3.6192     3.6313     0.2749     0.0114    73414.99  236332.68     0.0    
   500      2877.05     3.1021     3.1127     0.2659     0.0064    52206.29   168237.7     0.0    
   600      3537.86     2.6197     2.629      0.2054     0.0072    40652.42  131086.17     0.0    
   700      4198.82     2.2316     2.2387     0.2177     0.0089    34675.83  111853.49     0.0    
   800       4857.8     1.9376     1.943      0.1836     0.0096    31478.18  101533.56     0.0    
   900      5516.38     1.7123     1.7166     0.1534      0.01     29601.21   95514.36     0.0    
   1000     6173.07     1.5387     1.5418     0.2198     0.0097    28428.82   91687.9      0.0    
   1100     6830.11     1.412      1.4137     0.1605     0.0094    27703.07   89361.05     0.0    
   1200     7488.35     1.3108     1.3114     0.1378     0.009     27185.61   87693.01     0.0    
   1300     8147.15     1.2361     1.2355     0.1513     0.0087    26860.82   86664.22     0.0    
   1400     8804.23     1.1797     1.1792     0.1655     0.0083    26641.48   85965.19     0.0    
   1500     9462.18     1.1341     1.1318     0.0973     0.0079    26457.85   85335.91     0.0    
   1600     10119.36    1.0973     1.0955     0.1941     0.0076    26350.88   84969.26     0.0    
   1700     10778.69    1.0683     1.0661     0.131      0.0072    26225.34   84605.22     0.0    
   1800     11435.36    1.0465     1.0431     0.1375     0.0068    26160.77   84369.05     0.0    
   1900     12094.72    1.029      1.0266     0.1028     0.0066    26109.99   84251.9      0.0    
   2000     12751.92    1.0156     1.0121     0.1027     0.0063    26068.62   84093.53     0.0    
   2100     13407.97    1.0027     0.9983     0.0977     0.0059    26031.27   83943.6      0.0    
   2200     14065.57    0.9913     0.987      0.1419     0.0057    25990.83   83813.58     0.0    
   2300     14723.36    0.9827     0.9774     0.0848     0.0055    25960.7    83708.08     0.0    
   2400     15380.75    0.9759     0.9712     0.1054     0.0053    25939.09   83676.72     0.0    
   2500     16039.73    0.9717     0.9652     0.1033     0.005     25929.86   83624.19     0.0    
   2600     16696.26    0.964      0.9588     0.0951     0.0048    25903.97   83545.6      0.0    
   2700     17355.9     0.9614     0.954      0.1324     0.0047    25905.7    83520.61     0.0    
   2800     18013.89    0.9574      0.95      0.114      0.0045    25896.46   83484.87     0.0    
   2900     18671.12    0.9552     0.9476     0.1298     0.0044    25876.52   83450.24     0.0    
   3000     19329.97    0.9514     0.9445     0.1341     0.0042    25864.82   83431.5      0.0    
   3100     19986.6     0.9486     0.9415     0.131      0.004     25854.66   83382.91     0.0    
   3200     20644.3     0.9461     0.938      0.0973     0.0039    25852.67   83357.5      0.0    
   3300     21305.07    0.9447     0.9374     0.0904     0.0037    25840.38   83317.76     0.0    
   3400     21965.94    0.9424     0.9365     0.1585     0.0036    25843.22   83348.06     0.0    
   3500     22626.13    0.9404     0.9342     0.0718     0.0035    25828.45   83316.59     0.0    
   3600     23286.12    0.939      0.9326     0.0726     0.0034    25817.05   83277.27     0.0    
   3700     23945.43    0.9375     0.9306     0.0748     0.0033    25814.27   83253.19     0.0    
   3800     24610.12    0.9373     0.9293     0.0868     0.0032    25812.33   83233.79     0.0    
   3900     25275.19    0.9377     0.9293     0.0583     0.0031    25819.04   83248.64     0.0    
   4000     25938.06    0.9362     0.9284     0.1115     0.003     25818.8    83237.86     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       245.1      4.2317     4.2509     0.3123     0.2199   129058.36  520195.96    0.0001  
   200      1007.07     4.1887     4.2074     0.302      0.017    122287.52  492795.38     0.0    
   300       1766.5     4.0031     4.0211     0.2945     0.0162   100847.96  406337.65     0.0    
   400      2526.23     3.6044     3.6209     0.2729     0.0111    72509.27  292267.63     0.0    
   500      3288.53     3.0902     3.1044     0.3077     0.0061    51755.64  208741.32     0.0    
   600      4049.52     2.6098     2.621      0.2038     0.0071    40402.54  162993.41     0.0    
   700      4811.42     2.2248     2.2341     0.1961     0.0089    34557.03  139456.27     0.0    
   800      5573.83     1.9355     1.9422     0.209      0.0095    31435.22  126827.51     0.0    
   900      6334.11     1.7099     1.7159     0.1758     0.0097    29556.92  119295.29     0.0    
   1000     7094.28     1.539      1.5434     0.1549     0.0096    28408.17  114630.06     0.0    
   1100     7854.91     1.4029     1.4067     0.2473     0.0093    27635.41  111537.13     0.0    
   1200      8611.5     1.3017     1.3054     0.1058     0.009     27149.68  109574.02     0.0    
   1300     9371.77     1.2276     1.2287     0.1734     0.0086    26815.61  108212.02     0.0    
   1400     10128.49    1.1679     1.1698     0.1208     0.0082    26581.26  107283.95     0.0    
   1500     10885.76    1.1233     1.1259     0.183      0.0078    26406.68   106644.6     0.0    
   1600     11644.36    1.0893     1.0901     0.1841     0.0075    26290.06  106133.83     0.0    
   1700     12402.5     1.0611     1.0613     0.1508     0.0072    26184.11  105712.82     0.0    
   1800     13152.49    1.0388     1.0394     0.1104     0.0069    26124.43  105456.57     0.0    
   1900     13908.85    1.0223     1.023      0.1271     0.0066    26072.89  105278.42     0.0    
   2000     14666.11    1.0099     1.0085     0.1416     0.0062    26029.75  105052.11     0.0    
   2100     15422.37    0.9974     0.9957     0.2011     0.006     25992.43  104891.35     0.0    
   2200     16178.82    0.9872     0.9853     0.0923     0.0058    25959.3   104760.46     0.0    
   2300     16935.69    0.9809     0.9788     0.124      0.0055    25950.01  104709.54     0.0    
   2400     17694.17    0.9731     0.9713     0.1091     0.0053    25914.15  104600.13     0.0    
   2500     18451.02    0.9678     0.966      0.2033     0.0051    25913.81  104613.66     0.0    
   2600     19209.71    0.9632     0.9607     0.1169     0.005     25891.21  104492.26     0.0    
   2700     19967.69    0.9603     0.9567     0.114      0.0048    25892.7   104483.37     0.0    
   2800     20724.35    0.9556     0.9526     0.1655     0.0046    25870.06  104392.44     0.0    
   2900     21481.95    0.9519     0.9482     0.1421     0.0045    25846.5   104325.46     0.0    
   3000     22240.24    0.9474     0.9444     0.0871     0.0043    25835.96  104277.13     0.0    
   3100     22996.75    0.9463     0.9421     0.0573     0.0041    25825.1   104212.46     0.0    
   3200     23754.35    0.9437     0.9398     0.0814     0.004     25824.86  104200.41     0.0    
   3300     24497.21    0.9414     0.9381     0.1108     0.0038    25812.86  104174.87     0.0    
   3400     25235.44    0.9414     0.9374     0.1173     0.0038    25825.81  104192.89     0.0    
   3500     25974.05    0.9398     0.9356     0.1105     0.0037    25809.0   104133.97     0.0    
   3600     26713.06    0.9375     0.9329     0.0926     0.0036    25799.74  104105.37     0.0    
   3700     27452.99    0.9355     0.9316     0.0745     0.0034    25792.76  104087.91     0.0    
   3800     28193.75    0.9336     0.9304     0.1183     0.0034    25783.58   104069.7     0.0    
   3900     28933.28    0.9338     0.9298     0.1252     0.0032    25781.17  104042.28     0.0    
   4000     29673.67    0.9337     0.9297     0.0867     0.0032    25787.03  104058.77     0.0    
