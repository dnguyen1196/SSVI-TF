Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  5.342370986938477
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       201.52    180.5762   179.9708    3.1619     0.539       0.0    
   200       409.73    127.0852   126.6567    2.7663     0.109       0.0    
   300       617.36    114.1579   113.6627    2.0962     0.0427      0.0    
   400       825.83    107.3526   106.7265    1.1634     0.0218      0.0    
   500      1033.45    106.3369   105.8778    0.9484     0.0144      0.0    
   600      1240.61    105.1712   104.9762    1.1142     0.0128      0.0    
   700      1446.85    105.4056   105.383     1.4928     0.0103      0.0    
   800      1654.88    105.8356   105.1921    1.0443     0.0098      0.0    
   900      1861.48    104.8196   104.6126    0.7635     0.0066      0.0    
   1000     2069.09    105.3732   105.0718    0.707      0.0057      0.0    
   1100     2275.58    105.7132   105.2827    0.9187     0.0051      0.0    
   1200     2482.05    105.711    104.9751    0.7982     0.004       0.0    
   1300      2690.4    105.2901   104.5611    1.1581     0.0038      0.0    
   1400     2897.63    104.652    104.391     0.7269     0.0023      0.0    
   1500      3105.5    104.6249   104.1761    0.5801     0.0022      0.0    
   1600      3312.7    104.561    104.3308    0.7232     0.0023      0.0    
   1700      3518.4    104.7739   104.3811    0.7582     0.0015      0.0    
   1800     3724.78    104.8755   104.6743    0.6514     0.0014      0.0    
   1900     3931.85    105.0651   104.4983    0.7809     0.0014      0.0    
   2000     4137.28    104.9986   104.6344    0.8376     0.0014      0.0    
   2100     4344.78    105.0208   104.4621    0.8733     0.0009      0.0    
   2200     4549.34    104.4427   104.0021    0.7021     0.001       0.0    
   2300     4755.57    105.1995   104.7483    0.8509     0.001       0.0    
   2400     4962.98    104.8642   104.3619    0.699      0.0009      0.0    
   2500     5171.64    104.7059   104.1002    0.5377     0.0008      0.0    
   2600     5379.72    104.9404   104.5485    0.5686     0.0007      0.0    
   2700     5586.87    104.9086   104.2232    0.5718     0.0008      0.0    
   2800     5793.42    105.3207   104.8771    0.7158     0.0006      0.0    
   2900     6000.25    104.9425   104.4481    0.8905     0.0007      0.0    
   3000     6206.48    105.1011   104.483     0.6788     0.0005      0.0    
   3100     6412.96    105.1587   104.6313    0.597      0.0006      0.0    
   3200     6619.22    104.4394   103.911     0.6542     0.0006      0.0    
   3300     6824.64    104.5608   104.319     0.5388     0.0005      0.0    
   3400     7032.11    104.5715   104.2048    0.695      0.0005      0.0    
   3500     7238.85    104.9894   104.8304    0.7772     0.0005      0.0    
   3600     7444.61    104.4478   104.067     0.7725     0.0004      0.0    
   3700     7652.45    104.4153   104.012     0.5597     0.0004      0.0    
   3800     7860.41    104.7765   104.2947    0.6017     0.0003      0.0    
   3900     8067.77    104.0936   103.6869    0.6683     0.0003      0.0    
   4000      8273.4    104.4277   103.6921    0.7624     0.0004      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       199.95    168.0815   168.8586    3.1618     0.2243      0.0    
   200       407.14    126.0783   126.284     2.5713     0.0871      0.0    
   300       614.31    116.1697   116.2132    2.3241     0.0305      0.0    
   400       821.75    110.3967   110.2288    1.9383     0.0188      0.0    
   500      1028.61    107.8716   107.745     1.6789     0.0133      0.0    
   600      1235.58    106.5267   106.2956    1.4758     0.0075      0.0    
   700       1440.9    106.1355   105.7208    1.0231     0.0077      0.0    
   800      1647.28    104.9608   104.8453    0.8386     0.0047      0.0    
   900      1855.36    104.7261   104.2307    0.775      0.003       0.0    
   1000     2061.67    105.0413   104.5995    1.0041     0.0035      0.0    
   1100     2268.94    106.1806   105.5244    1.0433     0.0029      0.0    
   1200     2475.61    105.7874   105.0051    0.8752     0.0019      0.0    
   1300     2682.41    106.0429   105.2171    0.8301     0.0017      0.0    
   1400     2889.02    105.3718   104.5158    0.9015     0.0019      0.0    
   1500     3096.66    105.5663   104.8252    0.6568     0.0016      0.0    
   1600     3304.76    105.9205   105.4316    1.1078     0.0016      0.0    
   1700     3512.67    104.9797   104.5723    0.8561     0.0012      0.0    
   1800     3720.21    104.7934   104.2928    0.7146     0.0011      0.0    
   1900     3926.52    104.7867   104.124     0.7641     0.001       0.0    
   2000     4134.37    104.9946   104.3272    0.5676     0.0008      0.0    
   2100     4340.05    104.8971   104.4064    0.7391     0.0008      0.0    
   2200     4547.84    105.0402   104.7234    0.5802     0.0008      0.0    
   2300     4755.05    105.5456   104.7892    0.806      0.0006      0.0    
   2400     4961.97    105.1877   104.6598    1.0374     0.0006      0.0    
   2500     5169.39    105.2042   104.7206    0.8564     0.0006      0.0    
   2600      5375.8    104.7012   104.3739    0.7123     0.0005      0.0    
   2700     5583.65    105.5152   104.6239    0.6247     0.0005      0.0    
   2800     5790.58    104.6348   104.029     0.5775     0.0005      0.0    
   2900     5996.31    104.2363   104.0205    0.5027     0.0004      0.0    
   3000     6204.52    104.9848   104.2875    0.6209     0.0004      0.0    
   3100     6411.96    104.8391   104.1145    0.514      0.0004      0.0    
   3200     6619.23    105.4835   104.8872    1.012      0.0004      0.0    
   3300     6826.53    105.184    104.5545    0.7781     0.0004      0.0    
   3400     7034.87    104.4878   103.8171    0.4779     0.0003      0.0    
   3500      7241.8    105.102    104.2998    0.4905     0.0003      0.0    
   3600     7449.46    104.8393   104.3949    0.5682     0.0002      0.0    
   3700     7655.79    104.4237   104.0185    0.8139     0.0003      0.0    
   3800     7863.91    104.4932   103.9569    0.906      0.0003      0.0    
   3900     8070.22    104.6328   104.2192    0.6276     0.0003      0.0    
   4000     8277.28    104.579    104.0447    0.6765     0.0002      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       200.3     167.3848   167.0308    3.1602     0.1908     0.0001  
   200       409.26    124.6867   124.8765    3.0437     0.0628      0.0    
   300       616.69    114.3363   114.2497    2.4915     0.0301      0.0    
   400       825.01    110.776    110.4428    1.7956     0.0211      0.0    
   500      1033.84    107.6114   106.9988    1.0662     0.0101      0.0    
   600      1241.55    107.2919   106.6682    1.1955     0.0104      0.0    
   700      1450.31    107.5414   106.6555    1.1825     0.0072      0.0    
   800      1656.64    107.4978   106.7739    1.1824     0.0048      0.0    
   900      1865.82    105.7075   105.3648    1.1479     0.0054      0.0    
   1000     2072.95    105.3402   104.7929    0.9443     0.0054      0.0    
   1100     2280.52    105.9458   105.7242    0.8303     0.0028      0.0    
   1200     2488.93    105.8367   105.4058    0.9589     0.0021      0.0    
   1300     2695.73    105.9027   105.5683    0.9329     0.0027      0.0    
   1400     2904.75    105.8463   105.388     1.1129     0.0016      0.0    
   1500     3111.22    105.9157   105.447     0.8515     0.0016      0.0    
   1600     3320.26    105.5186   105.1556    0.7458     0.0014      0.0    
   1700     3528.31    105.5365   105.2084    0.6948     0.0014      0.0    
   1800      3736.9    105.176    104.7015    0.6209     0.0013      0.0    
   1900     3944.59     105.6     105.0973    0.8049     0.0012      0.0    
   2000     4153.36    105.2005   104.8456    1.0358     0.0011      0.0    
   2100     4362.63    105.2887   104.9938    0.715      0.0006      0.0    
   2200     4570.15    105.0653   104.3734    0.9334     0.0009      0.0    
   2300     4779.25    105.1731   104.8192    0.7794     0.0008      0.0    
   2400     4986.97    105.2759   104.981     0.7963     0.0009      0.0    
   2500      5193.8    105.2722   105.0418    0.6196     0.0008      0.0    
   2600     5403.06    105.3799   105.1843    0.7879     0.0005      0.0    
   2700     5610.82    105.7234   105.1735    0.8013     0.0006      0.0    
   2800     5818.78    104.8309   104.5099    0.7453     0.0005      0.0    
   2900     6026.05    105.0115   104.3998    0.6433     0.0004      0.0    
   3000     6234.14    105.0919   104.865     0.6231     0.0005      0.0    
   3100     6441.51    104.8964   104.4519    0.6381     0.0004      0.0    
   3200     6649.37    104.7875   104.3672    0.5666     0.0003      0.0    
   3300     6857.82    105.1702   105.0544    0.7649     0.0004      0.0    
   3400     7066.04    104.9514   104.5232    0.7049     0.0004      0.0    
   3500     7276.66    104.3996   104.0111    0.5798     0.0003      0.0    
   3600     7483.07    104.6201   104.2331    0.5777     0.0004      0.0    
   3700     7691.74    104.5149   104.1402    0.4723     0.0003      0.0    
   3800     7899.15    104.7468   104.3507    0.5811     0.0003      0.0    
   3900      8108.5    105.0302   104.6934    0.6715     0.0002      0.0    
   4000     8317.56    104.9134   104.4813    0.752      0.0002      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       200.43    163.2715   163.606     3.1619     0.5684     0.0001  
   200       408.87    120.1266   120.6585    2.4358     0.0617      0.0    
   300       618.89    112.8045   112.965     1.8715     0.0207      0.0    
   400       828.53    108.8221   109.0728    1.6491     0.0231      0.0    
   500      1035.91    109.0227   109.2132    1.5644     0.0071      0.0    
   600      1244.33    107.2356   107.408     1.549      0.0097      0.0    
   700      1452.71    106.2624   106.673     1.2699     0.0087      0.0    
   800      1661.73    105.1619   105.2516    0.9606     0.0061      0.0    
   900      1871.54    106.5443   106.387     1.0285     0.0031      0.0    
   1000     2079.45    105.8256   105.9394    1.0449     0.0027      0.0    
   1100      2288.4    105.9265   105.9224    0.9061     0.0041      0.0    
   1200     2496.13    105.7565   105.6961    0.8576     0.0015      0.0    
   1300     2705.68    105.2307   105.1924    0.7414     0.0016      0.0    
   1400     2914.42    105.843    105.7516    0.8035     0.002       0.0    
   1500     3121.73    106.0395   105.9721    1.0036     0.0021      0.0    
   1600     3330.11    104.6847   104.8505    0.8858     0.0017      0.0    
   1700     3537.97    105.7358   105.6134    0.9928     0.0013      0.0    
   1800     3747.37    105.3484   105.4143    1.0147     0.0011      0.0    
   1900     3955.64    105.1056   105.2275    0.715      0.0011      0.0    
   2000     4163.95    106.2807   106.1999    1.0592     0.0012      0.0    
   2100     4371.91    105.0223   104.9538    0.8826     0.0012      0.0    
   2200     4581.32    105.0014   105.1923    0.7111     0.0009      0.0    
   2300      4790.1    104.6305   104.7334    0.6027     0.0007      0.0    
   2400     5000.05    105.5409   105.523     0.9734     0.0008      0.0    
   2500     5208.91    105.3862   105.197     0.7069     0.0008      0.0    
   2600      5416.7    104.978    105.2093    0.6881     0.0007      0.0    
   2700     5626.07    105.0737   105.2231    0.7421     0.0005      0.0    
   2800      5834.1    104.9564   105.2233    0.6363     0.0005      0.0    
   2900      6042.7    105.2337   105.209     0.8248     0.0005      0.0    
   3000     6251.22    104.6402   104.7808    0.889      0.0005      0.0    
   3100     6459.04    104.9413   105.0687    0.6506     0.0004      0.0    
   3200     6667.71    104.2788   104.3857    0.6709     0.0005      0.0    
   3300     6875.26    105.0836   104.9378    0.5247     0.0004      0.0    
   3400     7084.25    105.3218   105.3354    0.8502     0.0003      0.0    
   3500     7292.22    104.914    104.9878    0.5895     0.0004      0.0    
   3600     7502.99    104.4038   104.8215    0.5347     0.0003      0.0    
   3700      7711.5    104.8064   104.9493    0.8336     0.0003      0.0    
   3800      7920.3    105.3047   105.1182    0.8023     0.0003      0.0    
   3900     8129.88    104.3921   104.6835    0.5557     0.0003      0.0    
   4000     8338.56    105.4469   105.2997    0.7901     0.0004      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       200.64    161.8781   162.1727    3.1621     0.5386     0.0001  
   200       408.93    121.4174   121.8139    2.6034     0.0971      0.0    
   300       619.84    112.483    112.3911    1.8333     0.0313      0.0    
   400       829.25    110.4674   110.4102    2.357      0.0244      0.0    
   500      1038.28    107.2331   107.1941    1.3886     0.0111      0.0    
   600      1248.65    106.3782   106.507     1.146      0.0077      0.0    
   700      1457.14    108.9041   108.9482    1.4065     0.007       0.0    
   800       1668.3    105.7781   106.0374    1.2476     0.009       0.0    
   900      1878.11    105.6937   106.1369    1.1618     0.0042      0.0    
   1000     2087.89    106.9462   106.6815    0.9679     0.0029      0.0    
   1100     2297.93    105.9586   105.9304    1.0196     0.0027      0.0    
   1200      2506.2    105.0309   105.403     0.9759     0.0036      0.0    
   1300      2713.9    105.6514   105.9076    1.0127     0.0026      0.0    
   1400     2922.92    105.7406   105.8858    1.0036     0.0024      0.0    
   1500     3133.27    105.6331   105.8122    0.9999     0.0022      0.0    
   1600     3343.26    106.3185   106.3288    0.7656     0.0018      0.0    
   1700     3552.05    105.8181   105.9657    0.9697     0.0019      0.0    
   1800     3762.13    106.4631   106.8681    1.1264     0.0015      0.0    
   1900     3970.96    105.0219   105.2842    0.7522     0.0006      0.0    
   2000     4180.06    104.9687   105.0629    0.9664     0.0007      0.0    
   2100     4390.03    105.1565   105.4322    0.6907     0.001       0.0    
   2200     4599.84    105.287    105.4138    0.6697     0.0008      0.0    
   2300     4809.13    104.965    105.4102    1.067      0.0008      0.0    
   2400     5017.92    104.9641   105.3116    0.7592     0.0009      0.0    
   2500     5228.75    105.0927   105.1466    0.884      0.0008      0.0    
   2600     5437.06    105.5068   105.5258    0.5465     0.0007      0.0    
   2700     5647.32    105.2748   105.2944    0.935      0.0007      0.0    
   2800     5856.57    104.8577   104.9351    0.8827     0.0006      0.0    
   2900     6064.07    105.2892   105.4002    0.9642     0.0006      0.0    
   3000     6272.13    105.3276   105.4551    0.7005     0.0005      0.0    
   3100     6480.35    104.8053   104.9234    0.6672     0.0004      0.0    
   3200     6688.88    104.9472   104.8724    0.7489     0.0004      0.0    
   3300     6897.39    104.6198   104.7597    0.7072     0.0004      0.0    
   3400     7106.79    104.9515   104.9531    0.6775     0.0004      0.0    
   3500     7314.84    104.3356   104.4385    0.8368     0.0003      0.0    
   3600     7522.04    104.9406   105.1254    0.757      0.0004      0.0    
   3700     7730.35    104.9414   105.0347    0.9558     0.0003      0.0    
   3800     7938.41    104.6895   104.8696    0.6476     0.0004      0.0    
   3900     8149.38    104.8321   104.7721    0.7892     0.0004      0.0    
   4000      8357.7    104.6679   104.711     0.7937     0.0003      0.0    
