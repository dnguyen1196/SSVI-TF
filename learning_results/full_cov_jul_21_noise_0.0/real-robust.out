Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  5.48630166053772
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       295.79    170.7211   171.0519    3.1623     0.303       0.0    
   200       602.19    125.105    125.1509    2.5023      0.0        0.0    
   300       908.25    114.1932   114.132     2.0331      0.0        0.0    
   400      1214.75    109.0273   108.9451    1.5222      0.0        0.0    
   500      1521.71    106.6778   106.4383    1.5542      0.0        0.0    
   600      1828.88    107.1307   106.8674    1.774       0.0        0.0    
   700       2136.0    105.133    104.9664    1.5595      0.0        0.0    
   800      2443.86    105.4403   105.1314    0.8735      0.0        0.0    
   900      2751.08    104.7339   104.6048    0.9985      0.0        0.0    
   1000     3058.09    104.7861   104.3567    0.7367      0.0        0.0    
   1100     3365.35    104.6836   104.2232    0.7998      0.0        0.0    
   1200     3672.55    104.9818   104.4413    0.732       0.0        0.0    
   1300      3979.6    104.6344   104.1534    0.7461      0.0        0.0    
   1400     4287.14    105.3226    104.72     0.7503      0.0        0.0    
   1500     4591.86    104.7258   104.2971     0.8        0.0        0.0    
   1600     4896.21    105.0325   104.3448    0.6416      0.0        0.0    
   1700     5200.43    104.5253   103.979     0.6628      0.0        0.0    
   1800     5504.69    104.5827   104.3835    0.8194      0.0        0.0    
   1900     5809.12    104.5675   104.2775    0.7641      0.0        0.0    
   2000      6114.2    104.4882   103.8171    0.6224      0.0        0.0    
   2100      6418.5    104.1368   103.7785    0.7371      0.0        0.0    
   2200     6722.74    104.5841   104.3895    0.576       0.0        0.0    
   2300     7027.33    105.0256   104.8122    0.8853      0.0        0.0    
   2400     7331.69    104.6487   104.3543    0.6056      0.0        0.0    
   2500     7636.01    104.5301   104.2339    0.6528      0.0        0.0    
   2600     7940.92    104.2417   103.7692    0.5783      0.0        0.0    
   2700     8245.22    104.705    103.9564    0.6487      0.0        0.0    
   2800     8549.37    104.3621   103.8171    0.7032      0.0        0.0    
   2900     8854.06    104.4501   103.8821    0.6867      0.0        0.0    
   3000     9158.16    104.7953   104.4463    0.7353      0.0        0.0    
   3100     9462.37    104.0454   103.7259    0.5291      0.0        0.0    
   3200     9766.96    104.8472   104.3545    0.4803      0.0        0.0    
   3300     10071.19   104.5797   103.9478    0.4678      0.0        0.0    
   3400     10375.66   104.4378   103.9608    0.4766      0.0        0.0    
   3500     10680.39   104.1452   103.7792    0.5627      0.0        0.0    
   3600     10984.92   104.4266   104.0429    0.6584      0.0        0.0    
   3700     11289.9    104.2253   103.7006    0.6372      0.0        0.0    
   3800     11594.62   104.8996   104.1792    0.7524      0.0        0.0    
   3900     11899.35   104.2483   103.8883    0.6022      0.0        0.0    
   4000     12204.07   104.2843   103.8133    0.6736      0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       294.29    170.1845   170.9874    3.1592     0.6983      0.0    
   200       599.57    122.1922    122.06     2.6009      0.0        0.0    
   300       906.7     111.8304   111.2691    2.0449      0.0        0.0    
   400      1212.37    109.0938   108.6364    2.0298      0.0        0.0    
   500      1518.59    108.2631   107.9048    1.9968      0.0        0.0    
   600      1824.92    106.6994   106.335     1.3208      0.0        0.0    
   700      2131.61    105.8871   105.4713     1.22       0.0        0.0    
   800      2437.81    105.3921   104.9058    0.8961      0.0        0.0    
   900      2744.32    105.1717   104.7217    0.7377      0.0        0.0    
   1000     3050.51    105.3331   104.6114    0.8985      0.0        0.0    
   1100     3356.81    105.678    105.0413    1.0022      0.0        0.0    
   1200     3663.29    105.8763   105.0379    0.6623      0.0        0.0    
   1300     3969.56    105.3601   104.6617    0.6996      0.0        0.0    
   1400     4276.18    105.0105   104.2985    0.867       0.0        0.0    
   1500     4583.14    105.0523   104.2567    0.6792      0.0        0.0    
   1600     4889.49    104.549    104.1527    0.6659      0.0        0.0    
   1700     5195.52    105.7167   105.1974    0.8584      0.0        0.0    
   1800     5501.86    104.9393   104.3729    0.9053      0.0        0.0    
   1900      5808.3    104.4739   104.1186    0.6224      0.0        0.0    
   2000     6114.92    104.6924   104.2524    0.7281      0.0        0.0    
   2100      6422.3    104.5658   104.0759    0.9313      0.0        0.0    
   2200     6728.82    104.6758   104.024     0.7687      0.0        0.0    
   2300     7035.18    104.7964   104.237     0.6553      0.0        0.0    
   2400     7340.65    105.2562   104.6422    1.0833      0.0        0.0    
   2500      7646.0    104.5412   103.975     0.5606      0.0        0.0    
   2600     7951.71    104.9478   104.4065    0.5431      0.0        0.0    
   2700     8257.24    104.9358   104.4462    0.982       0.0        0.0    
   2800      8562.5    104.5075   103.9376    0.8318      0.0        0.0    
   2900     8868.04    104.2191   103.8279    0.5414      0.0        0.0    
   3000      9173.5    104.6308   104.0371    0.6823      0.0        0.0    
   3100     9478.76    104.3711   103.9643    0.5921      0.0        0.0    
   3200     9784.22    104.2975   103.7536    0.6353      0.0        0.0    
   3300     10089.94   104.8372   104.1608    0.5867      0.0        0.0    
   3400     10395.38   104.6141   104.4086    0.6256      0.0        0.0    
   3500     10700.78   104.5017   104.0921    0.7173      0.0        0.0    
   3600     11007.09   104.6774   104.0997    0.6026      0.0        0.0    
   3700     11313.29   104.0217   103.6412    0.5682      0.0        0.0    
   3800     11619.68   104.453    103.9103    0.6756      0.0        0.0    
   3900     11926.09   104.5777   104.0469    0.8046      0.0        0.0    
   4000     12232.01   104.4696   103.5961    0.6259      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       292.62    177.4782   177.7457    3.1623     0.1049      0.0    
   200       596.79    127.4212   126.9912    2.5106      0.0        0.0    
   300       902.0     114.5649   114.2705    2.4954      0.0        0.0    
   400      1207.39    107.4986   107.1544    1.6384      0.0        0.0    
   500      1513.64    107.2605   106.7785    1.108       0.0        0.0    
   600      1820.13    106.8127   106.2484    1.2424      0.0        0.0    
   700      2125.91    105.6291   105.1806    1.1601      0.0        0.0    
   800      2431.79    104.8467   104.5086    0.8179      0.0        0.0    
   900      2736.81    105.278    104.7644    0.9124      0.0        0.0    
   1000      3041.9    105.8112   105.2568    1.0245      0.0        0.0    
   1100     3347.19    105.5634   105.0048    0.9764      0.0        0.0    
   1200     3651.99    105.448    105.3248    0.909       0.0        0.0    
   1300     3957.16    105.2801   104.8684    0.7793      0.0        0.0    
   1400      4262.1    104.933    104.7473    0.8631      0.0        0.0    
   1500     4567.49    104.7373   104.5009    0.7329      0.0        0.0    
   1600     4873.01    105.2892   104.8863    0.7286      0.0        0.0    
   1700     5178.56    105.0025   104.4983    0.5961      0.0        0.0    
   1800     5484.72    104.5418   104.2777    0.8027      0.0        0.0    
   1900     5789.79    104.4946   104.2397    0.6494      0.0        0.0    
   2000     6095.83    104.9884   104.3589    0.7356      0.0        0.0    
   2100     6401.89    104.3545   104.2927    0.6861      0.0        0.0    
   2200     6708.33    105.0651   104.969     0.6809      0.0        0.0    
   2300     7014.23    104.8004   104.5528    0.6464      0.0        0.0    
   2400     7319.96     104.75    104.3783    0.8461      0.0        0.0    
   2500     7626.27    104.922    104.2368    0.8114      0.0        0.0    
   2600     7932.43    104.5546   104.4064    0.6947      0.0        0.0    
   2700     8238.52    104.3947   103.9296    0.7419      0.0        0.0    
   2800     8544.89    104.7541   104.2907    0.6179      0.0        0.0    
   2900     8850.07    104.5917   104.2359    0.714       0.0        0.0    
   3000     9155.73    104.4098   104.0657    0.7456      0.0        0.0    
   3100     9461.81    104.304    103.9062    0.7325      0.0        0.0    
   3200     9768.42    104.3117   103.9027    0.5398      0.0        0.0    
   3300     10073.87   104.1631   103.8143    0.4449      0.0        0.0    
   3400     10379.38   104.1964   104.0163    0.6292      0.0        0.0    
   3500     10684.95   104.2656   104.1935    0.6301      0.0        0.0    
   3600     10990.64   104.1055   104.0448    0.5084      0.0        0.0    
   3700     11296.57   104.3688   104.075     0.5488      0.0        0.0    
   3800     11602.29   104.1105   103.8764    0.5907      0.0        0.0    
   3900     11909.89   104.4142   103.9421    0.4609      0.0        0.0    
   4000     12216.96   104.6973   104.2259    0.6894      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       294.07    170.6387   170.7687    3.1622     0.1181      0.0    
   200       599.78    122.976    123.3306    2.9557      0.0        0.0    
   300       906.06    111.2646   112.0144    1.8299      0.0        0.0    
   400      1211.93    109.1044   109.6814    2.1375      0.0        0.0    
   500       1519.5    107.4211   107.6637    2.0919      0.0        0.0    
   600      1829.06    106.0938   106.2799    1.3494      0.0        0.0    
   700      2138.04    105.9942   106.1891    1.2661      0.0        0.0    
   800      2446.59    105.6042   105.7144    0.9282      0.0        0.0    
   900      2754.08    105.8164   105.9235    1.3693      0.0        0.0    
   1000     3061.81    105.7422   105.844     1.1066      0.0        0.0    
   1100     3369.92    105.3169   105.5475    0.8102      0.0        0.0    
   1200     3679.88    105.5438   105.4985    0.766       0.0        0.0    
   1300     3989.77    104.8758   104.9792    0.9765      0.0        0.0    
   1400     4296.68    104.7611   104.9802    0.8074      0.0        0.0    
   1500     4604.48    105.4434   105.1331    0.6606      0.0        0.0    
   1600     4912.28    104.9999   105.0827    0.7401      0.0        0.0    
   1700     5220.68    105.1846   105.1103    0.7045      0.0        0.0    
   1800     5527.79    105.1278   105.2421    0.6139      0.0        0.0    
   1900     5836.83    105.1166   105.3773    0.9485      0.0        0.0    
   2000     6147.17    104.5626   104.7607    0.7774      0.0        0.0    
   2100     6453.97    104.5281   104.4768    0.7273      0.0        0.0    
   2200     6759.67    104.4275   104.4737    0.5409      0.0        0.0    
   2300      7068.0    105.1435   104.9301    0.8547      0.0        0.0    
   2400     7375.16    104.3925   104.4932    0.6932      0.0        0.0    
   2500     7680.48    104.7285   104.6737    0.7132      0.0        0.0    
   2600     7985.73    104.5683   104.7603    0.6531      0.0        0.0    
   2700     8290.79    104.7651   104.7284    0.7571      0.0        0.0    
   2800     8595.69    104.0576   104.4286    0.5262      0.0        0.0    
   2900     8900.72    104.8256    104.96     0.9332      0.0        0.0    
   3000     9205.38    105.0182   104.7318    0.6678      0.0        0.0    
   3100      9510.5    104.6695   104.4955    0.5833      0.0        0.0    
   3200     9815.39    104.6735   104.6531    0.7023      0.0        0.0    
   3300     10120.34   104.7069   104.8093    0.669       0.0        0.0    
   3400     10424.88   104.8002    104.76     0.5956      0.0        0.0    
   3500     10729.72    104.31    104.3675    0.5737      0.0        0.0    
   3600     11034.54   104.5668   104.8056    0.5979      0.0        0.0    
   3700     11339.48   104.5373   104.6606    0.7703      0.0        0.0    
   3800     11644.48   104.4828   104.5679    0.5982      0.0        0.0    
   3900     11949.36   104.4617   104.5164    0.6097      0.0        0.0    
   4000     12254.21   104.4824   104.4698    0.4741      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       293.09    169.1807   169.4142    3.1618     0.4982      0.0    
   200       599.35    122.4807   122.0673    2.5686      0.0        0.0    
   300       906.27    113.4387   113.8515    2.0435      0.0        0.0    
   400      1213.46    109.205    109.6355    1.5612      0.0        0.0    
   500      1520.29    105.7693   105.7255    1.0815      0.0        0.0    
   600      1827.46    105.5338   105.6203    1.0687      0.0        0.0    
   700      2135.26    104.8599   104.9762    1.0697      0.0        0.0    
   800      2442.85    105.1049   105.0958    1.0325      0.0        0.0    
   900      2751.29    105.7658   105.6703    1.0094      0.0        0.0    
   1000     3058.89    104.6297   105.1147    0.948       0.0        0.0    
   1100      3365.9    105.2389   105.3561    0.7871      0.0        0.0    
   1200     3673.28    105.6746   105.8215    0.9251      0.0        0.0    
   1300      3980.5    105.0817   105.1382    0.6375      0.0        0.0    
   1400     4287.34    104.859    104.7709    1.1099      0.0        0.0    
   1500     4594.38    105.2414   105.3351    0.7107      0.0        0.0    
   1600     4901.72    104.8385   104.9163    0.8745      0.0        0.0    
   1700     5208.65    104.6205   104.803     0.7652      0.0        0.0    
   1800     5515.71    104.3052   104.6545    0.6564      0.0        0.0    
   1900     5823.37    105.0749   105.153     0.7672      0.0        0.0    
   2000      6130.5    104.5063   104.6732    0.6034      0.0        0.0    
   2100     6437.66    104.4928   104.7563    0.7285      0.0        0.0    
   2200     6744.78    104.6757   104.8744    0.822       0.0        0.0    
   2300     7052.03    104.2354   104.331     0.6331      0.0        0.0    
   2400     7359.34    104.3998   104.4818    0.6909      0.0        0.0    
   2500     7666.25    104.7556   104.9033    0.6793      0.0        0.0    
   2600     7973.53    104.8515   104.7916    0.5811      0.0        0.0    
   2700     8280.72    104.266    104.3513    0.7009      0.0        0.0    
   2800     8587.92    104.3849   104.542     0.6256      0.0        0.0    
   2900     8894.98    104.4849   104.6415    0.6009      0.0        0.0    
   3000     9202.56    104.3395   104.4033    0.6817      0.0        0.0    
   3100     9511.52    104.4781   104.3909    0.5164      0.0        0.0    
   3200     9818.83    104.4418   104.5319    0.6304      0.0        0.0    
   3300     10126.26   104.6927   104.6842    0.4893      0.0        0.0    
   3400     10433.83   104.6591   104.7336    0.627       0.0        0.0    
   3500     10741.31   104.6489   104.654     0.5849      0.0        0.0    
   3600     11049.04   104.1541   104.3918    0.5212      0.0        0.0    
   3700     11356.63   104.2432   104.3656    0.6633      0.0        0.0    
   3800     11664.63   104.5832   104.7366    0.7165      0.0        0.0    
   3900     11972.38   104.6357   104.6244    0.7602      0.0        0.0    
   4000     12280.22   104.4267   104.5156    0.5648      0.0        0.0    
