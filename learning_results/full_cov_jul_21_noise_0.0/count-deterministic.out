Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  7.595182657241821
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       237.41     4.2406     4.2743     0.3125     0.2415   127987.09  103754.23  
   200       648.32     4.1859     4.2191     0.295      0.0242   118874.11   96341.41  
   300      1058.59     3.988      4.0197     0.2818     0.0241    96820.67   78433.73  
   400       1468.8     3.5805      3.61      0.2631     0.0159    70717.3    57273.85  
   500       1879.6     3.0609     3.0846     0.2434     0.0102    51404.13   41620.84  
   600      2289.68     2.5862     2.6067     0.1776     0.0109    40543.82   32813.1   
   700      2697.37     2.2056     2.2215     0.1534     0.0124    34780.33   28135.5   
   800      3108.04     1.9103     1.9219     0.1811     0.0115    31593.07   25541.73  
   900      3517.57     1.6857     1.6929     0.1636     0.0121    29745.02   24036.26  
   1000     3929.04     1.5151     1.5162     0.1819     0.0102    28626.24   23119.47  
   1100     4338.35     1.3852     1.3839     0.1868     0.0096    27869.74   22509.3   
   1200     4746.61     1.2848     1.2817      0.09      0.0091    27407.3    22129.28  
   1300     5155.27     1.2105     1.2054     0.1041     0.0086    27081.35   21864.05  
   1400     5563.81     1.1557     1.1476     0.1328     0.0079    26887.14   21698.72  
   1500     5973.17     1.1093     1.1018     0.0844     0.0076    26700.45   21554.38  
   1600     6380.03     1.0775     1.0667     0.1563     0.0071    26598.61   21462.19  
   1700     6787.63     1.0501     1.0391     0.0816     0.0066    26500.82   21389.03  
   1800     7191.89     1.0309     1.0184     0.1689     0.006     26459.75   21344.72  
   1900     7599.17     1.0144     1.0019     0.1453     0.0055    26418.93   21317.15  
   2000     8005.85     1.0006     0.9869     0.1254     0.0055    26350.56   21270.44  
   2100     8412.36     0.9897     0.9771     0.0937     0.0051    26318.55   21241.6   
   2200     8819.17     0.9835     0.9676     0.1354     0.0049    26312.84   21232.83  
   2300     9225.64     0.9755      0.96      0.2031     0.0051    26293.48   21213.35  
   2400     9632.06     0.9705     0.9562     0.0934     0.005     26270.11   21200.59  
   2500     10039.61    0.9669     0.9496     0.1254     0.004     26276.14   21193.95  
   2600     10445.4     0.9623     0.9458     0.1439     0.0042    26248.73   21176.69  
   2700     10852.41    0.958      0.9437     0.1019     0.0039    26251.96   21174.47  
   2800     11258.35    0.9562     0.9411     0.1103     0.0036    26249.16   21177.18  
   2900     11665.72    0.9528     0.9395     0.1082     0.0035    26237.33   21166.7   
   3000     12072.82    0.9535     0.9372     0.0846     0.0034    26243.52   21171.87  
   3100     12478.25    0.9489     0.9332     0.1252     0.0031    26221.1    21155.84  
   3200     12884.23    0.9487     0.933      0.0569     0.0032    26213.39   21152.78  
   3300     13290.18    0.9476     0.9328     0.124      0.0032    26222.33   21154.65  
   3400     13699.05    0.9444     0.9297     0.159      0.0029    26203.74   21149.45  
   3500     14105.07    0.9426     0.9281     0.0852     0.0028    26205.25   21143.94  
   3600     14510.57    0.9442     0.9263     0.0773     0.003     26215.99   21145.92  
   3700     14917.03    0.9431     0.9292     0.0506     0.0026    26211.88   21143.52  
   3800     15322.53    0.9421     0.9263     0.0851     0.0025    26213.67   21149.84  
   3900     15729.91    0.941      0.9265     0.115      0.0023    26198.26   21139.21  
   4000     16135.86     0.94      0.9241     0.0882     0.0023    26201.3    21146.22  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       237.31     4.2422     4.2649     0.3137     0.2285   128708.72  207934.77  
   200       720.25     4.1928     4.2151     0.2969     0.0211   120889.11  195232.03  
   300      1200.37     4.0006     4.0217     0.2886     0.0198    99155.05  160074.03  
   400      1683.64     3.5933     3.6118     0.2644     0.0124    71690.57  115759.56  
   500      2164.76     3.073      3.0891     0.2126     0.0086    51536.45   83265.74  
   600      2647.91     2.597      2.609      0.2262     0.0088    40456.57   65378.2   
   700      3129.12     2.2229     2.2308     0.2262     0.0099    34753.62   56163.08  
   800      3612.12     1.9315     1.9363     0.1367     0.0104    31578.68   51044.67  
   900      4094.46     1.7059     1.7089     0.2189     0.0103    29703.79   48014.57  
   1000     4575.18     1.5324     1.5348     0.2471      0.01     28510.99   46120.61  
   1100     5057.26     1.4011     1.401      0.1705     0.0097    27765.5    44911.97  
   1200     5538.55     1.2985     1.2972     0.2357     0.0093    27264.38   44092.43  
   1300     6019.75     1.2219     1.2198     0.1219     0.0089    26927.35   43558.77  
   1400     6500.07     1.164      1.1609     0.1446     0.0084    26700.86   43188.44  
   1500     6981.02     1.1185     1.1146     0.103      0.008     26532.63   42914.99  
   1600     7461.23     1.0853     1.0795     0.2198     0.0075    26433.59   42743.78  
   1700      7942.0     1.0546      1.05      0.1135     0.0071    26313.91   42568.39  
   1800     8424.66     1.0313     1.0277     0.1428     0.0068    26246.14   42463.36  
   1900      8905.1     1.0157     1.0101     0.0776     0.0064    26189.98   42374.39  
   2000     9387.15     1.0023     0.9958     0.1444     0.0061    26152.86   42313.98  
   2100     9868.19     0.9901     0.9839     0.0981     0.0057    26112.2    42249.75  
   2200     10350.06    0.9816     0.9739     0.095      0.0055    26100.58   42206.4   
   2300     10830.42    0.9746     0.968      0.0729     0.0051    26071.65   42172.68  
   2400     11312.4     0.9679     0.9621     0.1614     0.0049    26063.5    42156.19  
   2500     11794.77    0.9633     0.9561     0.1309     0.0046    26042.96   42121.65  
   2600     12278.75    0.9591     0.9526     0.0797     0.0044    26025.5    42100.57  
   2700     12761.93    0.9558     0.9478     0.0903     0.0042    26015.42   42080.67  
   2800     13243.79    0.9512     0.9446     0.1317     0.004     26007.05   42069.14  
   2900     13726.98    0.9491     0.9409     0.0706     0.0038    25996.93   42057.7   
   3000     14210.05    0.9466     0.9392     0.0948     0.0037    25992.98   42038.9   
   3100     14693.52    0.9441     0.9367     0.101      0.0034    25983.02   42027.65  
   3200     15176.72    0.9426     0.9354     0.0695     0.0033    25981.87   42026.29  
   3300     15660.17    0.9404     0.9335     0.1162     0.0033    25973.19   42023.11  
   3400     16143.46    0.939      0.9319     0.0512     0.0031    25970.7    42012.13  
   3500     16629.47    0.9374     0.931      0.0762     0.003     25966.74   42007.42  
   3600     17113.49    0.9365     0.9301     0.0831     0.0028    25973.18   42005.19  
   3700     17599.2     0.9366     0.9296     0.1316     0.0027    25962.62   41997.3   
   3800     18083.87    0.9365     0.9286     0.1165     0.0025    25953.83   41982.42  
   3900     18568.86    0.9357     0.9271     0.0736     0.0025    25954.63   41981.87  
   4000     19055.53    0.934      0.9277     0.0845     0.0024    25955.16   41988.11  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       237.55     4.243      4.2582     0.3137     0.2417   128956.32  311241.64  
   200       796.51     4.1985     4.2134     0.2981     0.019    122020.93  294447.25  
   300      1351.71     4.0146     4.0288     0.2929     0.0175   100874.62  243389.47  
   400      1909.98     3.6196     3.633      0.2797     0.0114    73132.67  176532.12  
   500       2469.3     3.1018     3.115      0.2294     0.0069    52259.48  126269.88  
   600      3027.43     2.6188     2.6293     0.2243     0.0076    40721.83   98423.15  
   700      3591.72     2.2319     2.2405     0.1744     0.0092    34766.57   84046.31  
   800      4150.75     1.9282     1.9346     0.2425     0.0099    31457.34   76045.52  
   900      4710.18     1.7019     1.706      0.1154     0.0102    29574.93   71479.67  
   1000     5270.17     1.5298     1.5341     0.1727     0.0101    28433.48   68727.75  
   1100     5828.52     1.3982     1.4001     0.0925     0.0097    27681.89   66893.99  
   1200     6390.14     1.2981     1.2984     0.1381     0.0093    27199.92   65718.36  
   1300     6956.94     1.2231     1.2224     0.1659     0.0088    26864.0    64924.75  
   1400     7523.57     1.1644     1.1634     0.1105     0.0084    26624.87   64352.21  
   1500     8089.97     1.1219     1.1197     0.1902     0.008     26470.02   63964.01  
   1600      8657.2     1.0855     1.0832     0.1419     0.0077    26334.31   63644.92  
   1700     9224.96     1.0583     1.0561     0.136      0.0072    26250.4    63434.11  
   1800     9791.26     1.0378     1.0343     0.1245     0.0069    26186.12   63279.89  
   1900     10379.97    1.0216     1.0169     0.0886     0.0065    26145.71   63149.67  
   2000     11024.21    1.0075     1.0024     0.1116     0.0063    26097.44   63044.39  
   2100     11648.28    0.9943     0.9894     0.1089     0.0059    26054.33   62951.44  
   2200     12273.88    0.9855     0.9799     0.1239     0.0057    26027.79   62884.36  
   2300     12921.77    0.9767     0.9708     0.1037     0.0054    25997.7    62806.13  
   2400     13554.75    0.9693     0.9638     0.152      0.0052    25989.68   62779.33  
   2500     14194.22    0.9627     0.9577     0.0917     0.0049    25960.06   62723.98  
   2600     14792.38    0.9585     0.9527     0.1474     0.0048    25950.42   62703.29  
   2700     15336.55    0.9541     0.9485     0.0886     0.0046    25930.15   62655.44  
   2800     15882.85    0.9511     0.9455     0.1518     0.0043    25920.78   62647.84  
   2900     16431.34    0.9484     0.9423     0.0693     0.0042    25918.91   62612.14  
   3000     16979.06    0.9452     0.9396     0.0794     0.004     25914.91   62597.46  
   3100     17523.69    0.9435     0.9372     0.1195     0.0038    25903.89   62581.95  
   3200     18055.48    0.9408     0.9343     0.1363     0.0037    25904.2    62569.91  
   3300     18585.38    0.9391     0.9324     0.116      0.0036    25894.68   62561.17  
   3400     19115.69    0.9379     0.9315     0.1144     0.0034    25893.65   62547.37  
   3500     19645.13    0.9361     0.9294     0.0914     0.0033    25883.63   62533.51  
   3600     20177.59    0.9354     0.9283      0.08      0.0032    25880.07   62520.88  
   3700     20707.7     0.9352     0.9276     0.0962     0.0031    25884.0    62519.98  
   3800     21238.76    0.9331     0.9262     0.1069     0.003     25873.04   62496.93  
   3900     21770.35    0.932      0.9261     0.0927     0.0028    25880.02   62508.89  
   4000     22299.73    0.9322     0.925      0.1144     0.0028    25867.76   62485.43  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       227.36     4.2433     4.2562     0.3139     0.1918   129059.05  415216.81  
   200       825.53     4.2004     4.213      0.3062     0.0238   122477.65  393939.96  
   300      1424.12     4.0199     4.0317     0.2926     0.0188   101583.63  326664.88  
   400      2025.39     3.6251     3.6361     0.2882     0.0115    73439.05  236286.64  
   500      2622.94     3.109      3.1191     0.2314     0.0068    52364.36  168691.17  
   600      3222.93     2.6335     2.6412     0.2224     0.0078    40883.26  131760.87  
   700      3821.41     2.2552     2.261      0.2398     0.0093    34993.15  112792.04  
   800      4420.72     1.9546     1.9605     0.2414      0.01     31654.25   102103.0  
   900      5019.38     1.7235     1.728      0.1427      0.01     29682.92   95754.68  
   1000     5616.89     1.5504     1.5531     0.1887     0.0097    28514.4    91968.93  
   1100     6213.51     1.4141     1.4173     0.1545     0.0095    27722.72   89447.37  
   1200     6810.53     1.3118     1.3137     0.1699     0.009     27217.66   87834.46  
   1300     7406.46     1.2341     1.2341     0.1206     0.0087    26871.65   86680.31  
   1400      8004.1     1.1752     1.175      0.1256     0.0083    26636.69   85932.57  
   1500     8600.99     1.1281     1.1272     0.0775     0.0079    26449.08   85331.78  
   1600     9196.54     1.091      1.0904     0.1354     0.0076    26318.32   84916.56  
   1700     9793.81     1.0615     1.0604     0.1207     0.0072    26229.77   84612.88  
   1800     10390.23    1.0398     1.0379     0.1556     0.0069    26152.91   84375.45  
   1900     10985.17    1.0225     1.019      0.1134     0.0066    26100.19   84178.51  
   2000     11583.26    1.0079     1.0039     0.2114     0.0063    26072.51   84084.72  
   2100     12181.88    0.9942     0.9908     0.1237     0.0061    26014.22   83913.3   
   2200     12778.47    0.9843     0.9799     0.1202     0.0058    25978.94   83793.57  
   2300     13375.22    0.9769     0.9716     0.1222     0.0056    25966.52   83746.53  
   2400     13971.77    0.9689     0.9639     0.0998     0.0053    25933.16   83649.49  
   2500     14568.62    0.9642     0.9594     0.1118     0.0052    25930.88   83637.9   
   2600     15166.2     0.9585     0.9525     0.0884     0.0049    25908.07   83553.93  
   2700     15762.72    0.9542     0.9484     0.0675     0.0047    25892.66   83512.01  
   2800     16361.15    0.9506     0.945      0.0882     0.0045    25880.87   83487.14  
   2900     16958.1     0.9469     0.9413     0.1417     0.0043    25870.06   83447.32  
   3000     17554.55    0.9448     0.9381     0.1402     0.0042    25870.2    83412.11  
   3100     18152.15    0.9434     0.9368     0.0882     0.0041    25868.9    83412.6   
   3200     18749.93    0.9413     0.9343     0.1008     0.0039    25866.96   83392.19  
   3300     19347.65    0.9396     0.9325     0.0993     0.0038    25844.91   83352.73  
   3400     19973.24    0.9381     0.9306     0.0996     0.0036    25844.51   83342.26  
   3500     20621.96    0.937      0.929      0.0941     0.0036    25836.19   83309.14  
   3600     21273.11    0.936      0.9281     0.0731     0.0034    25836.44   83321.51  
   3700     21924.25    0.9349     0.9278     0.1147     0.0033    25831.5    83311.92  
   3800     22575.36    0.9345     0.9267     0.1028     0.0032    25831.63   83296.71  
   3900     23225.08    0.9326     0.9251     0.0966     0.0032    25821.91   83262.04  
   4000     23877.4     0.9321     0.9241     0.081      0.0031    25823.24   83238.11  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       241.02     4.2434     4.2626     0.3146     0.2089   129101.01  520366.92  
   200       968.79     4.2008     4.2197     0.3015     0.0172   122626.55  494168.72  
   300      1696.26     4.0192     4.0372     0.2945     0.0192   101666.01  409626.56  
   400      2423.54     3.6249     3.6415     0.2686     0.011     73456.72  296057.14  
   500       3150.9     3.1072     3.1211     0.2445     0.0065    52262.84  210769.79  
   600       3881.2     2.634      2.6452     0.2793     0.0078    40860.3    164875.0  
   700      4613.51     2.2531     2.263      0.2212     0.0089    34931.86  140983.24  
   800      5343.72     1.9517     1.9598     0.1285     0.0097    31594.93  127512.79  
   900      6073.82     1.7227     1.7295     0.1902     0.0099    29666.91  119739.33  
   1000     6805.16     1.5439     1.5508     0.1382     0.0097    28431.28  114807.53  
   1100     7535.95     1.4123     1.4182     0.1794     0.0094    27683.84  111764.32  
   1200     8266.47     1.3095     1.3156     0.1731     0.0091    27174.91  109721.35  
   1300     8994.92     1.2333     1.2378     0.2208     0.0087    26837.3   108377.05  
   1400     9725.02     1.1756     1.1801     0.1428     0.0083    26614.67  107455.85  
   1500     10456.46    1.1301     1.1334     0.1211     0.008     26445.45  106752.32  
   1600     11186.35    1.0935     1.0958     0.1214     0.0076    26304.46   106201.2  
   1700     11915.21    1.0646     1.0658     0.1182     0.0073    26212.3   105796.68  
   1800     12647.11    1.0408     1.0414     0.0862     0.0069    26132.23  105480.14  
   1900     13375.72    1.0226     1.0224     0.0919     0.0066    26085.33  105262.33  
   2000     14102.82    1.0093     1.0089     0.1453     0.0063    26048.81  105128.14  
   2100     14830.45    0.9962     0.9947     0.1248     0.0061    25995.88  104926.65  
   2200     15559.23    0.9856     0.9848     0.1478     0.0058    25962.66  104797.94  
   2300     16287.96    0.9775     0.9759     0.122      0.0056    25945.11  104727.95  
   2400     17015.76    0.9709     0.9684     0.1261     0.0054    25920.63  104627.98  
   2500     17742.86    0.9636     0.9622     0.0889     0.0051    25898.16  104540.08  
   2600     18471.9     0.959      0.9563     0.0908     0.005     25876.14  104453.08  
   2700     19200.78    0.9545     0.951      0.095      0.0048    25862.39  104378.71  
   2800     19928.47    0.9511     0.9476     0.0973     0.0046    25857.47  104352.43  
   2900     20655.96    0.9456     0.9434     0.0727     0.0045    25838.58  104293.49  
   3000     21383.27    0.943      0.9406     0.1374     0.0043    25826.52  104257.55  
   3100     22112.03    0.9415     0.9388     0.1096     0.0042    25826.54  104247.36  
   3200     22840.13    0.9388     0.9357     0.1094     0.004     25824.25  104213.61  
   3300     23567.92    0.9373     0.9343     0.1214     0.004     25816.61  104185.71  
   3400     24293.5     0.9349     0.9322     0.0693     0.0038    25813.1   104158.36  
   3500     25020.04    0.9354     0.9313     0.0605     0.0037    25803.41  104140.98  
   3600     25747.84    0.9347     0.9306     0.0768     0.0036    25807.83  104126.35  
   3700     26473.62    0.932      0.9287     0.0768     0.0035    25793.39  104085.29  
   3800     27202.17    0.9308     0.9275     0.0664     0.0034    25790.52  104062.71  
   3900     27929.07    0.931      0.9272     0.0957     0.0033    25789.16  104069.67  
   4000     28656.44    0.9304     0.9262     0.0685     0.0032    25790.39  104057.26  
