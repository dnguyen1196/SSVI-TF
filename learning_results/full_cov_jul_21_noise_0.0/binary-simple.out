Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  5.465836048126221
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       226.17     0.7693     0.7305     3.1199     0.2557     0.0001  
   200       565.01     0.707      0.6598     2.3576     0.1165     0.0001  
   300       902.96     0.6607     0.5841      2.19      0.1081      0.0    
   400      1242.97     0.6476     0.565      2.2093     0.1142      0.0    
   500      1583.12     0.6265     0.5263     1.8193     0.0527      0.0    
   600       1924.2     0.606      0.4923     2.4082     0.0527      0.0    
   700      2264.51     0.6005     0.4654     1.9268     0.0508      0.0    
   800      2604.56     0.5972     0.4752     2.1853     0.0457      0.0    
   900      2945.79     0.6015     0.4779     2.3596     0.0399      0.0    
   1000      3287.9     0.5965     0.4654     1.8711     0.0452      0.0    
   1100     3630.74     0.6007     0.4705     2.2267     0.0358      0.0    
   1200     3970.03     0.6087     0.4815     2.2407     0.0401      0.0    
   1300     4310.69     0.6097     0.4962     2.1761     0.0396      0.0    
   1400     4651.02     0.6177     0.5097     2.461      0.0457      0.0    
   1500     4992.35     0.6125     0.4903     2.6999     0.039       0.0    
   1600     5332.87     0.6094     0.4938     2.3756     0.0419      0.0    
   1700     5671.55     0.5954     0.4764     2.4189     0.0388      0.0    
   1800     6008.82     0.5886     0.4641     2.4781     0.0355      0.0    
   1900      6346.3     0.588      0.4693     2.4772     0.0452      0.0    
   2000     6680.63     0.5794     0.4506     2.5603     0.0492      0.0    
   2100     7014.29     0.583      0.4591     2.2436     0.0416      0.0    
   2200     7347.32     0.5906     0.4658     2.812      0.058       0.0    
   2300     7679.85     0.5719     0.4432     2.2045     0.0482      0.0    
   2400     8009.92     0.5694     0.4264     2.1887     0.0499      0.0    
   2500      8340.4     0.5624     0.428      2.1682     0.0476      0.0    
   2600      8670.4     0.5638     0.4308     2.2182     0.046       0.0    
   2700     9000.62     0.5571     0.424      2.3534     0.0602      0.0    
   2800      9330.3     0.5609     0.4252     1.9838     0.0565      0.0    
   2900     9659.85     0.5538     0.4142     2.2507     0.0582      0.0    
   3000     9989.07     0.5614     0.4276     1.844      0.0515      0.0    
   3100     10316.02    0.5637     0.4368     2.1105     0.0554      0.0    
   3200     10642.73    0.5636     0.4276     1.9691     0.0504      0.0    
   3300     10971.15    0.5675     0.4327     2.2139     0.0702      0.0    
   3400     11299.16    0.5522     0.404      1.7351     0.0607      0.0    
   3500     11624.98    0.5599     0.4133     1.7049     0.0639      0.0    
   3600     11951.71    0.5631     0.414      1.7361     0.0668      0.0    
   3700     12278.39    0.5641     0.4079     1.7013     0.0602      0.0    
   3800     12603.17    0.5638     0.4133     1.8975     0.0626      0.0    
   3900     12929.83    0.5737     0.4354     1.8184     0.0483      0.0    
   4000     13254.64    0.5664     0.4186     1.8303     0.0589      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       224.28     0.8076     0.7865     3.1416     0.1865     0.0002  
   200       612.45     0.7468     0.6991     2.5415     0.0775     0.0001  
   300      1000.99     0.6595     0.6129     2.2628     0.0706     0.0001  
   400       1389.3     0.6271     0.5739     1.6976     0.0592      0.0    
   500      1776.98     0.6093     0.5485     1.9359     0.0474      0.0    
   600      2165.53     0.6032     0.5398     2.5251     0.0457      0.0    
   700       2556.2     0.6007     0.5254     1.7798     0.0424      0.0    
   800      2946.36     0.5799     0.509      1.6439     0.0344      0.0    
   900      3335.61     0.5784     0.4989     2.1651     0.0355      0.0    
   1000     3724.47     0.5738     0.4907     1.7418     0.0337      0.0    
   1100     4113.13     0.5701     0.4916     1.6059     0.0311      0.0    
   1200     4499.26     0.5581     0.464      1.3267     0.0312      0.0    
   1300     4885.61     0.5584     0.4612     1.474      0.0302      0.0    
   1400     5272.04     0.5556     0.4628     1.7122     0.028       0.0    
   1500     5656.23     0.5506     0.4481     1.2588     0.0267      0.0    
   1600      6043.2     0.5544     0.4559     1.413      0.026       0.0    
   1700     6429.04     0.5468     0.4371     1.575      0.0275      0.0    
   1800     6813.34     0.5427     0.4236     1.2742     0.0287      0.0    
   1900      7199.9     0.5448     0.4323     1.3063     0.0272      0.0    
   2000     7584.25     0.5511     0.433      1.3927     0.0245      0.0    
   2100     7968.84     0.5445     0.4204     1.3357     0.0247      0.0    
   2200     8352.32     0.5408     0.429      1.4854     0.0245      0.0    
   2300     8735.56     0.5316     0.4123     1.4627     0.0242      0.0    
   2400     9120.81     0.5273     0.417      1.3439     0.0249      0.0    
   2500      9504.1     0.5296     0.4287     1.4697     0.0244      0.0    
   2600     9887.96     0.5325     0.4183     1.5236     0.0244      0.0    
   2700     10269.38    0.5302     0.4239     1.2787     0.0225      0.0    
   2800     10651.42    0.5231     0.4084     1.2296     0.0239      0.0    
   2900     11033.15    0.5326     0.4329     1.5534     0.0239      0.0    
   3000     11415.6     0.5414     0.4267     1.4468     0.0233      0.0    
   3100     11797.21    0.5392     0.4297     1.5149     0.024       0.0    
   3200     12178.7     0.539      0.4443     1.9177     0.0244      0.0    
   3300     12560.72    0.5392     0.4399     1.8758     0.0247      0.0    
   3400     12942.8     0.5543     0.4677     2.1677     0.0225      0.0    
   3500     13323.82    0.5512     0.4669     2.2925     0.0286      0.0    
   3600     13705.0     0.5594     0.5031     2.5959     0.0287      0.0    
   3700     14086.14    0.563      0.4914     2.4798     0.0325      0.0    
   3800     14467.34    0.5678     0.5027     2.5752     0.038       0.0    
   3900     14848.62    0.5708     0.5051     2.6326     0.0423      0.0    
   4000     15232.24    0.5741     0.5197     2.5236     0.0373      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       223.29     0.8075     0.7909     3.1421     0.1293     0.0003  
   200       665.2      0.7014     0.6753     2.4361     0.0618     0.0001  
   300      1108.11     0.6627     0.6305     1.8288     0.0573     0.0001  
   400      1549.32     0.6449     0.6027     1.9879     0.051       0.0    
   500      1988.43     0.6159     0.5822     2.0507     0.0407      0.0    
   600      2425.44     0.6124     0.5676     1.9654     0.0348      0.0    
   700      2863.59     0.5843     0.5408     1.5904     0.032       0.0    
   800      3297.64     0.5842     0.5455     2.1127     0.037       0.0    
   900      3732.51     0.565      0.5208     1.4541     0.0276      0.0    
   1000     4165.89     0.5657     0.5125     1.6328     0.032       0.0    
   1100     4598.22     0.5604     0.5069     1.6193     0.0245      0.0    
   1200     5028.51     0.5613     0.495      1.4668     0.0403      0.0    
   1300     5460.55     0.5367     0.4803     1.4008     0.0255      0.0    
   1400     5893.42     0.542      0.487      1.3329     0.0262      0.0    
   1500     6326.22     0.5392     0.4794     1.602      0.0207      0.0    
   1600     6757.83     0.5347     0.482      1.5433     0.0181      0.0    
   1700     7189.02     0.5269     0.4695     1.543      0.0203      0.0    
   1800     7618.25     0.5287     0.4648     1.4749     0.0193      0.0    
   1900     8048.13     0.5281     0.4603     1.2103     0.0217      0.0    
   2000     8476.92     0.529      0.4627     1.3523     0.0162      0.0    
   2100     8907.05     0.5158     0.4511     1.1891     0.0199      0.0    
   2200     9338.77     0.5141     0.4444     1.189      0.0184      0.0    
   2300     9771.68     0.5097     0.4458     1.2139     0.0192      0.0    
   2400     10203.96    0.5144     0.4459     1.1268     0.0161      0.0    
   2500     10635.36    0.5171     0.4515     1.2455     0.0197      0.0    
   2600     11063.79    0.5075     0.4426     1.1053     0.0158      0.0    
   2700     11496.6     0.5041     0.4382     1.0018     0.0179      0.0    
   2800     11928.56    0.4993     0.4293     1.316      0.0175      0.0    
   2900     12359.43    0.4982     0.4246     1.1185     0.0196      0.0    
   3000     12790.01    0.5044     0.432      1.1379     0.0129      0.0    
   3100     13221.18    0.4925     0.4236     1.0129     0.0143      0.0    
   3200     13653.96    0.498      0.4236     1.1053     0.0132      0.0    
   3300     14082.3     0.5009     0.4257     1.1456     0.0142      0.0    
   3400     14511.36    0.497      0.4219     1.1917     0.0176      0.0    
   3500     14943.52    0.4953     0.4162     0.9587     0.0132      0.0    
   3600     15374.77    0.505      0.432      1.3991     0.0144      0.0    
   3700     15803.28    0.4899     0.4125     0.8502     0.0129      0.0    
   3800     16233.88    0.4954     0.4181     0.892      0.0135      0.0    
   3900     16666.43    0.4933     0.4142     0.9721     0.0126      0.0    
   4000     17097.31    0.4889     0.4096     0.9677     0.012       0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       226.33     0.7745     0.7594     3.1348     0.1002     0.0003  
   200       721.62     0.7331     0.7121     2.1095     0.0518      0.0    
   300      1220.76     0.6767     0.6543     1.8874     0.049      0.0001  
   400      1720.31     0.6361     0.6039     1.8814     0.0402      0.0    
   500      2215.19     0.6097     0.5815     1.6534     0.0353      0.0    
   600      2709.76     0.6085     0.5714     1.9472     0.0372      0.0    
   700      3198.54     0.5914     0.5648     2.0427     0.0289      0.0    
   800      3686.44     0.5734     0.5381     1.6257     0.0393      0.0    
   900      4172.02     0.565      0.5315     1.5256     0.0268      0.0    
   1000     4655.69     0.5581     0.5207     1.7681     0.0283      0.0    
   1100     5137.77     0.5589      0.51      1.5262     0.0232      0.0    
   1200     5616.53     0.5524     0.5046     1.4011     0.025       0.0    
   1300     6094.79     0.5486     0.4979     1.6892     0.0243      0.0    
   1400     6571.54     0.5393     0.4955     1.4632     0.0246      0.0    
   1500     7046.98     0.5383     0.4897     1.9556     0.0196      0.0    
   1600     7522.24     0.5337     0.4915     1.4383     0.0235      0.0    
   1700     7995.23     0.5287     0.4754     1.2084     0.0189      0.0    
   1800     8470.16     0.5263     0.4742     1.196      0.0213      0.0    
   1900     8944.62     0.527      0.4769     1.2951     0.0186      0.0    
   2000     9419.85     0.5208     0.4749     1.353      0.0193      0.0    
   2100     9892.27     0.5226     0.4741     1.3176     0.0179      0.0    
   2200     10367.17    0.5244     0.475      1.3266     0.0178      0.0    
   2300     10839.66    0.5155     0.4617     1.1024     0.0205      0.0    
   2400     11311.44    0.5144     0.4578     1.189      0.016       0.0    
   2500     11784.6     0.5017     0.4502     1.1247     0.0195      0.0    
   2600     12258.74    0.5094     0.4533     1.067      0.0139      0.0    
   2700     12730.07    0.5017     0.4438     0.9732     0.0136      0.0    
   2800     13201.17    0.5083     0.4493     1.2153     0.0126      0.0    
   2900     13673.05    0.4983     0.4395     1.0318     0.0161      0.0    
   3000     14144.54    0.4967     0.4484     0.9977     0.0135      0.0    
   3100     14617.96    0.4948     0.446      1.0344     0.0138      0.0    
   3200     15088.32    0.4923     0.4382     1.0318     0.0135      0.0    
   3300     15559.71    0.4941     0.4337      0.95      0.0134      0.0    
   3400     16032.44    0.4879     0.4323     1.0081     0.0143      0.0    
   3500     16504.49    0.4985     0.4411     0.8898     0.0106      0.0    
   3600     16975.71    0.483      0.4268     1.0244     0.0145      0.0    
   3700     17447.64    0.4876     0.4282     1.0278     0.0107      0.0    
   3800     17918.53    0.4818     0.4265     0.9889     0.014       0.0    
   3900     18388.96    0.4837     0.4273     0.9186     0.0118      0.0    
   4000     18860.96    0.4808     0.4195     0.893      0.0135      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       226.9      0.9313     0.9148     3.1332     0.0681      0.0    
   200       766.06     0.7303     0.7126     2.2734     0.0431     0.0001  
   300      1309.53     0.6981     0.6731     1.8641     0.0352     0.0001  
   400      1853.82     0.6628     0.6406     1.8051     0.0314     0.0001  
   500      2393.56     0.6286     0.598      1.9881     0.0273      0.0    
   600      2927.92     0.6082     0.5861     1.7669     0.0247      0.0    
   700      3457.73     0.5946     0.5676     1.9807     0.0295      0.0    
   800      3986.95     0.5851     0.559      1.6386     0.0235      0.0    
   900      4514.65     0.5698     0.5389     1.7119     0.022       0.0    
   1000     5040.71     0.5689     0.5382     1.5561     0.0231      0.0    
   1100     5565.18     0.549      0.5223     1.5999     0.0316      0.0    
   1200     6089.14     0.5426     0.5147     1.5034     0.0244      0.0    
   1300     6613.89     0.5334     0.5071     1.495      0.033       0.0    
   1400     7136.42     0.5279     0.4992     1.3486     0.0221      0.0    
   1500     7658.74     0.5378     0.5079     1.5874     0.0237      0.0    
   1600     8181.08     0.5223     0.4915     1.365      0.0184      0.0    
   1700     8704.23     0.5261     0.4822     1.391      0.025       0.0    
   1800     9228.82     0.522      0.4863     1.3272     0.0196      0.0    
   1900      9754.6     0.5063     0.4817     1.613      0.0187      0.0    
   2000     10279.77    0.5101     0.4791     1.2793     0.0169      0.0    
   2100     10804.47    0.5094     0.4733     1.2256     0.0192      0.0    
   2200     11328.68    0.5108     0.4704     1.3652     0.0154      0.0    
   2300     11851.21    0.516      0.4701     1.1594     0.017       0.0    
   2400     12373.83    0.513      0.4671     1.3453     0.0138      0.0    
   2500     12894.98    0.5133     0.4641     1.1313     0.0135      0.0    
   2600     13417.94    0.5031     0.457      1.3478     0.0123      0.0    
   2700     13941.3     0.5026     0.4626     1.0474     0.0112      0.0    
   2800     14463.5     0.4961     0.4533     1.1385     0.0148      0.0    
   2900     14987.2     0.493      0.4534     0.9984     0.0114      0.0    
   3000     15511.53    0.4876     0.4461     0.9943     0.0123      0.0    
   3100     16033.65    0.4953     0.4455     1.3652     0.0101      0.0    
   3200     16556.53    0.4889     0.4464     1.039      0.0134      0.0    
   3300     17081.36    0.4802     0.4365     0.8951     0.0104      0.0    
   3400     17602.56     0.48      0.4419     0.9193     0.0094      0.0    
   3500     18123.48    0.4876     0.442      1.0693      0.01       0.0    
   3600     18645.03    0.484      0.4409     1.1637     0.0089      0.0    
   3700     19167.68    0.4838     0.436      0.925      0.0084      0.0    
   3800     19693.99    0.4888     0.4369     1.0575     0.0097      0.0    
   3900     20220.56    0.4729     0.4287     0.8614     0.0103      0.0    
   4000     20745.18    0.4714     0.4231     0.9905     0.0094      0.0    
