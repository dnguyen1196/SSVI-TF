Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  5.38735818862915
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       225.44     0.7668     0.7279     3.1239     0.2316   
   200       454.82     0.7346     0.6892     2.6882     0.1203   
   300       682.85     0.6906     0.6106     2.0299     0.091    
   400       910.27     0.6614     0.5741     2.3325     0.0697   
   500      1138.42     0.6447     0.5474     1.8157     0.0563   
   600      1367.59     0.627      0.524      2.0237     0.0457   
   700      1596.94     0.6151     0.4982     1.8023     0.0565   
   800      1824.51     0.6025     0.4812     1.9356     0.0382   
   900      2052.38     0.6056     0.4994     1.6583     0.0343   
   1000     2281.03     0.5899     0.4632     1.6241     0.0311   
   1100     2509.27     0.5946     0.4578     1.7341     0.0327   
   1200     2735.85     0.5842     0.4379     2.0045     0.0314   
   1300     2963.07     0.5806     0.4429     1.8408     0.0293   
   1400     3190.41     0.5819     0.4427     1.6503     0.0309   
   1500     3419.52     0.5814     0.429      1.7762     0.0302   
   1600     3647.05     0.5801     0.432      1.7287     0.0289   
   1700      3875.5     0.5981     0.4545     1.687       0.03    
   1800     4101.29     0.6004     0.4756     2.2254     0.0309   
   1900     4327.71     0.5907     0.4652     1.8693     0.0333   
   2000     4555.05     0.6545     0.5771     2.6462     0.0341   
   2100     4784.49     0.6321     0.5494     2.8051     0.0389   
   2200      5011.2     0.6472     0.5812     2.7719     0.0431   
   2300     5240.08     0.6223     0.5242     2.6529     0.0418   
   2400     5467.72     0.6146     0.5246     2.6484     0.0469   
   2500     5693.65     0.6239     0.5448     2.6698     0.0479   
   2600     5918.07     0.5905     0.5036     2.8908     0.0458   
   2700     6142.08     0.5849     0.4881     2.4692     0.0473   
   2800     6366.63     0.581      0.4829     2.4978     0.0582   
   2900     6589.14     0.583      0.4897     2.5035     0.053    
   3000     6814.59     0.5756     0.4754     2.6075     0.0493   
   3100     7037.39     0.5698     0.4589     2.1711     0.0456   
   3200      7260.5     0.5758     0.4673     2.2408     0.0594   
   3300     7484.68     0.5698     0.4615     1.9844     0.0692   
   3400     7708.15     0.5777      0.48      2.3003     0.0564   
   3500     7930.93     0.561      0.4556     1.9875     0.0558   
   3600     8153.13     0.5471     0.4303     1.7635     0.0638   
   3700     8376.25     0.5576     0.4427     1.8997      0.07    
   3800     8598.64     0.562      0.4554     1.7662     0.0656   
   3900      8822.0      0.56      0.4423     1.9678     0.0537   
   4000     9041.21     0.5518     0.4423     1.9875     0.0603   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       223.12     0.8135     0.7884     3.1347     0.1574   
   200       449.22     0.7234     0.6789     2.3972     0.0805   
   300       676.99     0.6514     0.6086     2.1762     0.0637   
   400       905.7      0.6452     0.5973     1.8328     0.0595   
   500      1133.25     0.6215     0.5651     1.9167     0.0512   
   600      1360.61     0.6175     0.5546     1.9386     0.0446   
   700      1586.69     0.6013     0.5356     1.8608     0.0381   
   800      1814.58     0.5957     0.5238     1.7465     0.037    
   900      2040.85     0.572      0.5042     1.6309     0.0348   
   1000      2268.8     0.5702     0.4997     1.8783     0.0346   
   1100     2494.66     0.5596     0.4863     1.8237     0.0284   
   1200      2721.0     0.5512     0.4731     1.2785     0.0255   
   1300      2948.0     0.5708     0.488      1.9321     0.0321   
   1400     3173.59     0.5485     0.4611     1.1492     0.0287   
   1500     3399.96     0.5455     0.4552     1.5411     0.0206   
   1600     3624.69     0.5435     0.4695     1.7488     0.0259   
   1700     3851.98     0.549      0.464      1.3008     0.0226   
   1800     4078.33     0.5314     0.4392     1.0681     0.0197   
   1900     4304.33     0.5356     0.4422     1.3265     0.0177   
   2000     4529.26     0.5305     0.4261     1.1595     0.0171   
   2100      4754.0     0.5375     0.4295     1.2926     0.0198   
   2200     4979.42     0.5393     0.4383     1.4395     0.0204   
   2300     5205.27     0.5298     0.4195     1.152      0.0178   
   2400     5430.87     0.5246     0.4239     1.3605     0.0178   
   2500     5656.42     0.5362     0.4195     1.1707     0.0193   
   2600     5884.83     0.5285     0.4142     1.1923     0.0172   
   2700     6110.68     0.5341     0.4181     1.0631     0.0169   
   2800     6336.64     0.5273     0.4101     1.1082     0.0174   
   2900     6561.64     0.5273     0.4171     1.3383     0.0159   
   3000     6788.65     0.5225     0.4048     0.9841     0.0162   
   3100     7015.61     0.5178     0.4079     0.9194     0.0167   
   3200     7240.92     0.5126     0.3917     0.9539     0.0154   
   3300     7468.39     0.5151     0.3976     0.9165     0.0156   
   3400     7692.68     0.5143     0.4057     1.3504     0.0152   
   3500     7916.64     0.5075     0.4014     1.4159     0.0152   
   3600     8142.36     0.5066     0.3929     0.8731     0.016    
   3700     8367.46     0.5116     0.4129     1.0449     0.0153   
   3800     8591.98     0.5039     0.3874     0.9953     0.0149   
   3900     8817.36     0.5094     0.3863     1.3485     0.0155   
   4000     9041.65     0.5088     0.3897     1.2113     0.0151   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       220.63     0.7975     0.7827     3.1388     0.1148   
   200       450.23     0.7266     0.7067     2.0175     0.0588   
   300       678.6      0.6823     0.6542     1.8802     0.0483   
   400       907.43     0.6694     0.625      1.8828     0.042    
   500      1136.12     0.6332     0.5904     1.8914     0.0365   
   600      1366.03     0.6148     0.5745     1.9753     0.0417   
   700      1596.11     0.588      0.5497     1.6414     0.0296   
   800      1825.98     0.5906     0.5441     1.7455     0.0303   
   900      2054.77     0.5698     0.5215     1.5795     0.0293   
   1000     2283.91     0.5606     0.5104     1.423      0.0323   
   1100     2511.72     0.5518     0.5038     1.5938     0.0223   
   1200     2741.23     0.5466     0.4987     1.3526     0.0251   
   1300     2969.96     0.5402     0.4913     1.4107     0.0232   
   1400      3198.4     0.538      0.4835     1.2298     0.0182   
   1500     3425.93     0.554      0.4908     1.3513     0.0228   
   1600     3653.73     0.5301     0.4773     1.2393     0.0228   
   1700     3881.87     0.5305     0.4724     1.4285     0.0227   
   1800     4109.74     0.5163     0.4614     1.1709     0.0249   
   1900      4337.6     0.5169     0.465      1.4619     0.0154   
   2000     4565.85     0.5192     0.4607     1.1422     0.0151   
   2100     4794.18     0.5246     0.4543     1.1647     0.0211   
   2200      5021.2     0.5157     0.4597     1.103      0.0142   
   2300     5247.98     0.5241     0.4539     1.1265     0.0155   
   2400     5472.84     0.5211     0.453      1.2036     0.0127   
   2500     5700.09     0.5137     0.4492     1.1629     0.0161   
   2600     5926.73     0.5108     0.4404     0.9815     0.0121   
   2700     6153.61     0.5108     0.4477     1.2798     0.0146   
   2800     6380.73     0.5105     0.4395     0.9715     0.0177   
   2900     6608.43     0.4991     0.4374     0.9515     0.0162   
   3000     6834.55     0.4954     0.4297     0.8805     0.0124   
   3100     7062.39     0.4923     0.428      1.3579     0.0146   
   3200     7288.68     0.501      0.4259     1.0227     0.0147   
   3300     7515.21     0.493      0.4239     1.1022     0.0108   
   3400     7741.57     0.4985     0.4293     1.1562     0.0122   
   3500     7966.41     0.4961     0.4324     0.9587     0.0127   
   3600     8194.47     0.4906     0.4188     0.9495     0.0117   
   3700     8419.43     0.4994     0.421      0.9245     0.0096   
   3800     8644.68     0.4855     0.4173     0.9444     0.0104   
   3900     8869.62     0.4797     0.4098     0.8602     0.0104   
   4000     9095.95     0.4845     0.4163     0.9349      0.01    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       220.32     0.7807     0.7605     3.1553     0.0832   
   200       448.02     0.7188     0.6957     2.3504     0.0542   
   300       675.53     0.6582     0.6407     1.9342     0.049    
   400       903.28     0.6497     0.6229     1.8425     0.0449   
   500      1129.78     0.6175     0.5851     1.9671     0.0387   
   600      1356.17     0.5816     0.5551     1.5226     0.0368   
   700      1583.85     0.5795     0.5486     1.5389     0.036    
   800      1812.64     0.5706     0.543      1.6758     0.0315   
   900      2039.84     0.5665     0.5275     1.4138     0.0284   
   1000      2266.9     0.562      0.5237     1.3991     0.0323   
   1100      2493.0     0.556      0.5137     1.7453     0.027    
   1200     2718.43     0.5362     0.4952     1.2957     0.0212   
   1300     2944.75     0.5521     0.5113     1.4525     0.0377   
   1400     3169.09     0.5335     0.4944     1.3946     0.025    
   1500     3396.18     0.529      0.4878     1.2925     0.0214   
   1600     3622.24     0.5356     0.4946     1.2625     0.0225   
   1700     3849.33     0.5228     0.4782     1.2046     0.0184   
   1800     4076.01     0.5241     0.4812     1.6038     0.0211   
   1900     4304.41     0.5198     0.4768     1.1886     0.025    
   2000     4530.17     0.5157     0.467      1.2947     0.0165   
   2100     4757.48     0.5183     0.4595     1.4459     0.0163   
   2200     4983.61     0.5218     0.4666     1.2985     0.019    
   2300      5211.3     0.5042     0.4556     0.9631     0.0149   
   2400     5436.07     0.508      0.4596     1.2752     0.0167   
   2500     5663.46     0.509      0.4546     1.1861     0.0149   
   2600      5889.5     0.5049     0.455      0.9186     0.0139   
   2700      6114.9     0.5063     0.4489     1.0593     0.0135   
   2800     6340.66     0.4927     0.4432     0.9706     0.0158   
   2900     6567.64     0.5025     0.4528     1.0612     0.0148   
   3000     6795.07     0.492      0.4393     0.942      0.0208   
   3100     7019.95     0.4914     0.442      1.3207     0.0178   
   3200     7246.54     0.4954     0.4362      0.98      0.0181   
   3300     7471.74     0.4972     0.4432     1.1214     0.0131   
   3400     7697.16     0.4848     0.4369     1.0637     0.0128   
   3500     7922.31     0.4891     0.4351     0.8785     0.0115   
   3600     8149.91     0.4897     0.4353     0.9887     0.015    
   3700     8375.54     0.4858     0.4309     1.0536     0.011    
   3800     8602.39     0.4922     0.4331     0.8403     0.0159   
   3900     8828.49     0.485      0.4319     0.9184     0.0122   
   4000     9054.11     0.4878     0.4295     0.9253     0.0158   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       223.08     0.8515     0.8367     3.1351     0.0672   
   200       451.97     0.7241     0.706      2.1198     0.0438   
   300       682.85     0.6969     0.6713     1.9208     0.0395   
   400       913.58     0.648      0.6293     1.7216     0.0331   
   500      1143.64     0.6345     0.6148     2.0129     0.0402   
   600      1374.85     0.6077     0.5853     1.9178     0.0277   
   700      1605.24     0.5902     0.5594     1.6734     0.0267   
   800      1836.68     0.5836     0.5454     1.6586     0.0307   
   900      2066.39     0.572      0.5394     1.7356     0.028    
   1000     2299.12     0.567      0.532      1.6803     0.0211   
   1100     2527.89     0.565      0.5281     1.6134     0.032    
   1200     2757.55     0.5528     0.5255     1.4752     0.0208   
   1300     2990.26     0.5417     0.5108     1.585      0.0231   
   1400     3220.74     0.5426     0.5022     1.3237     0.0201   
   1500      3450.2     0.5426     0.5015     1.2788     0.0217   
   1600     3679.53     0.5375     0.5002     1.298      0.0219   
   1700     3909.11     0.5355     0.4894     1.3634     0.0243   
   1800     4141.02     0.5281     0.4816     1.3058     0.0334   
   1900     4373.32     0.5311     0.486      1.3258     0.0202   
   2000     4605.36     0.5163     0.4725     1.1197     0.0168   
   2100     4839.06     0.5104     0.462      1.0913     0.0188   
   2200     5071.08     0.5052     0.4645     1.0374     0.0174   
   2300     5302.08     0.5063     0.4588     1.2238     0.0127   
   2400     5535.87     0.5086     0.4636     1.2358     0.0123   
   2500      5771.3     0.5058     0.4596     1.1196     0.0191   
   2600     6006.91     0.5017     0.4608     1.098      0.0166   
   2700     6243.09     0.4994     0.4594     1.119      0.0221   
   2800     6477.75     0.4957     0.4487     0.9079     0.0132   
   2900     6713.16     0.4982     0.4477     0.9455     0.0141   
   3000     6948.04     0.5017     0.4478     0.9835     0.0197   
   3100     7182.24     0.4957     0.4428     0.9622     0.014    
   3200     7417.34     0.484      0.4389     0.9664     0.0102   
   3300     7652.48     0.4897     0.4377     1.0101     0.0181   
   3400     7886.02     0.4812     0.434      1.2888     0.0108   
   3500     8120.85     0.484      0.439      1.1115     0.012    
   3600     8355.92     0.4815     0.4304     0.9596     0.0091   
   3700     8591.83     0.4767     0.4256     0.8559     0.0098   
   3800     8825.75     0.4843     0.4341     1.2558     0.0211   
   3900      9061.0     0.4845     0.432      1.2378     0.0152   
   4000     9293.68     0.475      0.4296     0.9045     0.0102   
