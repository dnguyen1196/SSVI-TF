Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  5.468010902404785
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       254.37     0.7712     0.7608     3.1044     0.5913     0.002   
   200       651.21     0.7586     0.7404     2.103      6.5472     0.0026  
   300       1044.1     0.7402     0.7314     1.7832    21.1711     0.0069  
   400      1434.49     0.746      0.7382     1.8506    274.2149    0.0065  
   500      1824.44     0.7557     0.7474     3.1454    1588.692    0.0093  
   600      2214.01     0.7358     0.7221     1.1949    47.2213     0.0046  
   700      2603.24     0.7386     0.7258     1.9612   6619.1254    0.0151  
   800      2991.93     0.7355     0.7154     1.3192    342.1548    0.0003  
   900      3378.84     0.7435     0.7284     1.1683    196.5183    0.0144  
   1000     3765.21     0.7477     0.7377     3.1622   1358.4712    0.0063  
   1100     4150.98     0.7341     0.7136     1.2003    105.3138    0.0023  
   1200     4535.98     0.7404     0.7211     1.0947    27.4634     0.0054  
   1300     4917.53     0.7321     0.7129     0.9692    53.6244     0.0028  
   1400     5298.86     0.7416     0.7196     1.3056   1551.5559    0.0181  
   1500     5680.73     0.7322     0.7237     1.0427    601.6654    0.0033  
   1600     6063.33     0.745      0.7259     0.9327    96.3272     0.0029  
   1700      6443.9     0.7403     0.7212     0.9511    38.7279     0.0064  
   1800     6824.53     0.7449     0.7182     0.9082    14.0931     0.0052  
   1900     7205.42     0.7436     0.7217     1.0175    16.6626     0.0034  
   2000     7586.19     0.7542     0.7393     0.9376    53.2682     0.0065  
   2100      7966.6     0.7782     0.7651     0.9128    198.1051    0.0015  
   2200     8347.51     0.7818     0.7675     0.9526   2138.5528    0.0014  
   2300     8727.58     0.7809     0.7664     1.3262   3825.2731    0.0051  
   2400     9107.55     0.7755     0.7605     0.9369   1416.2858    0.0025  
   2500     9487.67     0.7726     0.7595     0.993     987.6185    0.0014  
   2600     9867.15     0.7622     0.7487     0.8843    347.626     0.0069  
   2700     10246.12    0.7566     0.7408     0.8514    89.6954     0.0022  
   2800     10625.01    0.7529     0.7385     0.8591    478.9873    0.0078  
   2900     11003.93    0.7532     0.742      1.6272    9794.853    0.0102  
   3000     11384.07    0.7476     0.7324     2.0069   4640.5069    0.0017  
   3100     11762.98    0.7789     0.7729     0.8705    209.058     0.0051  
   3200     12141.81    0.7651     0.7595     0.7907    79.8071     0.0022  
   3300     12521.5     0.7697     0.7593     0.6099     25.378     0.0025  
   3400     12900.72    0.7582     0.7587     0.7493     22.169     0.001   
   3500     13280.53    0.7629     0.7523     0.824      9.6007     0.0016  
   3600     13659.4     0.7557     0.7505     0.6972     6.9153     0.0031  
   3700     14038.0     0.7522     0.7404     0.6782      8.54      0.0033  
   3800     14416.76    0.7511     0.7428     0.6801    11.9532     0.0027  
   3900     14795.95    0.7562     0.7363     0.5701    20.1116     0.0018  
   4000     15174.3     0.7508     0.7308     0.6364    33.0922     0.0044  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       245.84     1.4176     1.415      3.1617   3697.2538    0.0004  
   200       666.5      1.4081     1.4102     1.6529   5313.0317    0.0002  
   300      1089.52     1.4084     1.416      0.821     701.0303    0.0001  
   400      1513.39     1.4156     1.4161     1.2113   1672.1937    0.0001  
   500      1936.87     1.4162     1.4168     1.0231   11672.8534   0.0001  
   600      2360.63     1.4063     1.4132     0.6916   12671.2932   0.0001  
   700      2782.71     1.4148     1.4098     0.6355   2987.8295     0.0    
   800      3205.41     1.4165     1.4128     0.4156    324.3813    0.0001  
   900       3628.3     1.4035     1.4193     0.6402   18565.5613   0.0001  
   1000     4051.18     1.4118     1.4171     0.7036   2170.6055     0.0    
   1100     4474.25     1.4139     1.4171     0.385     396.1582     0.0    
   1200     4897.04     1.4171     1.415      2.9336    6870.12      0.0    
   1300     5320.71     1.4063     1.4048     1.0587   77881.3252    0.0    
   1400     5743.69     1.4007     1.4045     1.1951   1416.4151     0.0    
   1500     6166.43     1.4045     1.406      0.4919    220.2247     0.0    
   1600     6588.92     1.3997     1.4035     0.7245    2058.464     0.0    
   1700     7011.68     1.3689     1.3616     0.4846    214.4803     0.0    
   1800     7434.13     1.1329     1.1283     3.1137    133.3566     0.0    
   1900     7856.19     1.108      1.0991     2.7365    154.8169     0.0    
   2000     8278.61     1.0913     1.0823     0.6643    34.8078      0.0    
   2100     8701.46      0.95      0.947      2.1214    116.6728     0.0    
   2200     9123.99     0.9365     0.9296     2.2001    171.2503     0.0    
   2300     9546.24     0.8545     0.8465     0.473     126.9322     0.0    
   2400     9969.39     0.7965     0.7811     0.4735    39.8862      0.0    
   2500     10392.36    0.8694     0.8703     3.0017    1055.708     0.0    
   2600     10814.3     1.327      1.3236     0.8544   45189.879     0.0    
   2700     11236.34    1.4028     1.3979     0.6304   19396.3967    0.0    
   2800     11659.75    1.388      1.3954     0.3427   2258.5948     0.0    
   2900     12082.78    1.3573     1.3574     0.2318    636.4066     0.0    
   3000     12506.01    1.1931     1.1884     0.6889   20096.4543    0.0    
   3100     12928.92    0.9411     0.9405     0.5903   2244.8115     0.0    
   3200     13351.77    0.8197     0.8062     0.6869    335.7779     0.0    
   3300     13774.53    0.7699     0.7764     2.5292    146.9393     0.0    
   3400     14197.99    0.7724     0.7615     0.3145    42.1163      0.0    
   3500     14621.35    0.7593     0.7542     0.3095    27.9556      0.0    
   3600     15044.62    0.7622     0.7602     0.3613    52.4696      0.0    
   3700     15468.59    0.7854     0.7733     0.3467    115.3834     0.0    
   3800     15892.78    0.8234     0.8142     0.5616    354.4778     0.0    
   3900     16316.99    0.9053     0.9049     0.6615   1743.2596     0.0    
   4000     16741.21    0.9933     0.9865     0.5402   4380.3379     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       246.0      1.418      1.4153     3.1608   15037.9486   0.0001  
   200       724.04     1.4079     1.4145     1.4122   1941.5011    0.0001  
   300      1208.98     1.4164     1.4109     1.0207    643.4778     0.0    
   400      1695.92     1.4164     1.4108     0.7998   1028.8639     0.0    
   500      2182.66     1.4147     1.4179     0.6665    141.2106     0.0    
   600      2670.28     1.4158     1.4188     0.4718    235.0743     0.0    
   700      3157.82     1.4143     1.4171     1.0706   4802.2289     0.0    
   800      3646.49     1.4112     1.4155     0.6885    170.1575     0.0    
   900       4137.1     1.4152     1.4073     1.0629   37500.5676    0.0    
   1000     4627.91     1.4113     1.4123     0.6579    510.3367     0.0    
   1100     5118.25     1.3999     1.4009     0.4374    999.2159     0.0    
   1200      5607.7     1.182      1.1741     0.7697   1459.3219     0.0    
   1300     6098.16     0.7423     0.7258     0.8847    151.1154     0.0    
   1400     6589.38     0.7439     0.7328     0.8535    23.0716      0.0    
   1500     7080.79     0.7461     0.734      1.046      21.736      0.0    
   1600     7575.68     0.7384     0.7309     3.1604    73.3012      0.0    
   1700     8069.77     0.7426     0.7291     2.9136   20791.2365    0.0    
   1800     8564.19     0.7252     0.7121     1.1839   1448.2354     0.0    
   1900     9056.35     0.7252     0.717      0.6784    314.8407     0.0    
   2000     9549.61     0.7354     0.7249     0.7072    34.5573      0.0    
   2100     10043.84    0.732      0.7181     0.6697    61.4304      0.0    
   2200     10543.95    0.7254     0.7175     1.195    2317.6879     0.0    
   2300     11036.54    0.7254     0.7163     0.8215   1947.4819     0.0    
   2400     11530.32    0.7635     0.7563     3.162     179.7541     0.0    
   2500     12021.2     0.7368     0.7243     2.9609    52.5252      0.0    
   2600     12510.35    0.7335     0.7183     3.0879    96.3708      0.0    
   2700     13000.01    0.767      0.7577     3.162     252.107      0.0    
   2800     13489.28    0.7544     0.7442     3.1226    21.0066      0.0    
   2900     13978.98    0.7367     0.7279     3.1511    47.5478      0.0    
   3000     14467.49    0.7405     0.727      3.1577    375.3748     0.0    
   3100     14956.4     0.7373     0.7353     0.6843   1214.3414     0.0    
   3200     15445.65    0.7542     0.7419     2.7053   2590580.1181    0.0    
   3300     15933.34    0.7299     0.7203     0.7635   3732.9093     0.0    
   3400     16421.0     0.723      0.714      0.7163   1033.8931     0.0    
   3500     16910.03    0.7315     0.7212     3.1622   2673.6223     0.0    
   3600     17399.31    0.7242     0.714      0.948     5234.081     0.0    
   3700     17887.57    0.7314     0.7142     0.4159    303.613      0.0    
   3800     18375.36    0.7358     0.7183     0.3961    99.2704      0.0    
   3900     18862.53    0.7229     0.7112     0.467     52.9103      0.0    
   4000     19348.8     0.7439     0.725      3.1621    320.4627     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       243.07     1.4118     1.4171     3.1142   31069.6623   0.0001  
   200       772.35     1.4189     1.4122     1.4584    529.5406     0.0    
   300      1308.81     1.4094     1.409      0.8165    384.4967     0.0    
   400      1848.58     1.4077     1.4155     0.7935    319.6802     0.0    
   500      2389.44     1.4074     1.4152     1.129     571.056      0.0    
   600      2929.52     1.4139     1.412      0.4659    19.3998      0.0    
   700      3470.63     1.4113     1.4122     0.8297    875.1716     0.0    
   800      4011.52     1.4127     1.4133     0.6411    593.9058     0.0    
   900      4553.55     1.4118     1.4142     0.6692     649.81      0.0    
   1000     5096.37     1.4273     1.4102     1.6528    358.6779     0.0    
   1100     5639.61     1.4221     1.4115     2.8964    305.2217     0.0    
   1200     6181.88     1.3574     1.3638     0.5811    150.5517     0.0    
   1300     6725.88     0.9462     0.9482     1.069     572.5762     0.0    
   1400     7269.85     0.7884     0.7787     1.5549   4026.4449     0.0    
   1500      7813.7     0.7506     0.7427     1.4727    224.7354     0.0    
   1600     8357.17     0.728      0.7177      2.95     90.4045      0.0    
   1700      8900.7     0.7292     0.7181     3.1379    26.6383      0.0    
   1800     9444.03     0.7874     0.7825     3.1622    73.9964      0.0    
   1900     9988.86     0.7832     0.7718     3.0428   4260.0857     0.0    
   2000     10530.51    0.7558     0.7475     1.0149    389.3534     0.0    
   2100     11072.41    0.8221     0.8172     3.1621   54853.1991    0.0    
   2200     11613.69    0.7539     0.741      1.158    2157.1141     0.0    
   2300     12156.98    0.7434     0.7338     0.602     554.251      0.0    
   2400     12700.23    0.7444     0.7331     1.6349    239.3485     0.0    
   2500     13245.01    0.7478     0.736      2.6877    53.0304      0.0    
   2600     13788.75    0.7417     0.7243     3.0989    26.4305      0.0    
   2700     14332.34    0.7293     0.7154     2.3173    96.5934      0.0    
   2800     14875.89    0.7401     0.7332     3.0854     21.804      0.0    
   2900     15421.64    0.731      0.7187     3.0383   1401.0016     0.0    
   3000     15966.6     0.7326     0.7138     0.6027    219.8319     0.0    
   3100     16512.15    0.7336     0.7196     3.0943    79.7497      0.0    
   3200     17057.11    0.7318     0.7189     0.5783    187.1251     0.0    
   3300     17599.31    0.7282     0.7139     3.1231    6261.443     0.0    
   3400     18141.33    0.7277     0.7155     0.7064    978.5209     0.0    
   3500     18683.07    0.7619     0.7548     2.1095   19234.0556    0.0    
   3600     19224.44    0.7263     0.7144     2.8463    280.2576     0.0    
   3700     19765.6     0.728      0.7138     0.4328    956.3541     0.0    
   3800     20307.66    0.7316     0.7146     0.6251    113.8101     0.0    
   3900     20849.07    0.7281     0.7171     0.4707    186.2974     0.0    
   4000     21391.53    0.7287     0.715      0.5794    647.5373     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       243.05     1.4187     1.4116     3.1352   11589.4631    0.0    
   200       823.32     1.419      1.4135     1.3514    585.5302     0.0    
   300      1410.45     1.408      1.4156     2.0801   37210.5252    0.0    
   400      1999.91     1.4101     1.4105     3.1509    732.3445     0.0    
   500      2590.62     1.4176     1.4047     3.0575    421.4097     0.0    
   600      3182.54     1.4071     1.4051     1.1041   1207.9753     0.0    
   700      3774.69     1.4199     1.4141     2.3092   133925.4096    0.0    
   800      4366.55     1.4044     1.4085     3.1617    191.3443     0.0    
   900      4960.46     1.4021     1.3953     1.3731   11777.735     0.0    
   1000     5554.59     1.3641     1.3565     0.9499    724.6783     0.0    
   1100     6151.38     0.7822     0.7671     0.9386     181.6       0.0    
   1200     6751.44     0.7832     0.773      0.9789    103.2983     0.0    
   1300     7346.82     0.7531     0.7408     0.8789    64.0889      0.0    
   1400     7943.14     0.7271     0.7193     0.8314    30.9851      0.0    
   1500     8539.26     0.7414     0.7272     3.1587    182.6468     0.0    
   1600     9134.94     0.738      0.7236     3.1243    6701.821     0.0    
   1700     9728.16     0.7449     0.7373     1.0701    600.3629     0.0    
   1800     10321.95    0.7368     0.7251     3.1467    94.6507      0.0    
   1900     10916.03    0.7335     0.7208     2.7384    81.8238      0.0    
   2000     11510.17    0.744      0.7329     3.1094   1296.2104     0.0    
   2100     12105.03    0.7513     0.7408     3.158     309.303      0.0    
   2200     12698.68    0.7252     0.7122     0.7897    330.4676     0.0    
   2300     13292.38    0.7298     0.7118     2.6525   4709.8624     0.0    
   2400     13886.16    0.7549     0.7458     3.0508    124.9967     0.0    
   2500     14479.87    0.7521     0.7429     2.3262   2097.7064     0.0    
   2600     15073.57    0.7785     0.7616     3.1607    324.0575     0.0    
   2700     15667.94    0.7489     0.7382     0.5547   2309.6032     0.0    
   2800     16261.42    0.7419     0.728      2.2219   26539.3187    0.0    
   2900     16853.74    0.7326     0.7221     3.0162   1273.0824     0.0    
   3000     17445.75    0.7357     0.7266     2.3029    214.5739     0.0    
   3100     18037.94    0.7433     0.7324     2.9176   2041.2814     0.0    
   3200     18631.4     0.732      0.7207     3.069    100306.1836    0.0    
   3300     19223.88    0.7383     0.7286     0.7355   1849.8398     0.0    
   3400     19817.03    0.7289     0.7173     0.5004    306.0387     0.0    
   3500     20411.13    0.7405     0.7336     3.1242    423.5902     0.0    
   3600     21005.09    0.7453     0.7326     0.6837    518.8529     0.0    
   3700     21599.19    0.7297     0.7204     2.9867   3359.0455     0.0    
   3800     22194.58    0.731      0.7211     3.1622   2017.2792     0.0    
   3900     22789.31    0.7315     0.7216     0.4791    248.3884     0.0    
   4000     23382.75    0.724      0.7163     2.8594    105.6206     0.0    
