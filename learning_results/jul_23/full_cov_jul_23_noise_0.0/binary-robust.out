Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  6.156703233718872
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       364.82     0.8364     0.8296     3.1135     0.0278     0.0025  
   200       863.92     0.7586     0.7518     1.826      0.0434     0.0004  
   300      1361.16     0.7356     0.7225     1.8312     0.0281     0.0036  
   400       1857.7     0.7475     0.732      1.6104     0.023      0.0014  
   500      2352.77     0.7469     0.7298     1.6092     0.0201     0.002   
   600      2842.34     0.7274     0.7161     1.2882     0.0169     0.0005  
   700      3333.88     0.7297     0.7168     1.3098     0.0159      0.0    
   800      3824.23     0.7331     0.723      1.1513     0.0128     0.0002  
   900      4316.43     0.7355     0.7207     1.2474     0.0114     0.0019  
   1000     4808.74     0.7339     0.7197     1.2126     0.0109     0.0002  
   1100     5299.62     0.725      0.7157     0.9806     0.0105     0.0001  
   1200     5796.61     0.7294     0.7134     1.0978     0.0089     0.0012  
   1300     6291.57     0.7344     0.713      1.1957     0.0084     0.0006  
   1400     6786.54     0.7277     0.7158     1.0319     0.0082     0.0016  
   1500      7280.0     0.7291     0.7122     0.939      0.0081     0.0004  
   1600     7772.25     0.737      0.7193     0.9283     0.0078     0.0007  
   1700     8264.35     0.7379     0.7221     0.9011     0.0074     0.0017  
   1800     8757.53     0.7344     0.7137     0.8462     0.0076     0.0008  
   1900     9247.32     0.7303     0.7165      1.0       0.007      0.0001  
   2000     9719.11     0.7327     0.7207     0.9249     0.0066     0.0021  
   2100     10185.18    0.7356     0.728      0.8036     0.0057     0.0012  
   2200     10646.99    0.7324     0.7192     0.9503     0.0063     0.0002  
   2300     11090.9     0.7376     0.7197     0.8436     0.0056     0.0005  
   2400     11528.67    0.7333     0.7178     0.924      0.0054     0.0017  
   2500     11965.0     0.7252     0.721      0.962      0.0056     0.0009  
   2600     12401.01    0.7327     0.7118     0.7939     0.0055     0.0003  
   2700     12879.75    0.7333     0.7175     0.8614     0.0051     0.0007  
   2800     13369.19    0.7295     0.7222     0.7196     0.0049     0.0006  
   2900     13858.94    0.7341     0.7137     0.6782     0.0045     0.0004  
   3000     14347.42    0.7334     0.7172     0.7216     0.0051     0.0014  
   3100     14836.73    0.7417     0.7239     0.6834     0.0049     0.0006  
   3200     15326.67    0.7334     0.7225     0.724      0.0047     0.0009  
   3300     15805.73    0.7363     0.7225     0.7311     0.0042     0.001   
   3400     16243.67    0.7342     0.7208     0.7884     0.0039     0.0026  
   3500     16681.33    0.7298     0.7207     0.8266     0.0042     0.0012  
   3600     17117.55    0.7331     0.7179     0.5934     0.004      0.0002  
   3700     17553.32    0.7294     0.7199     0.6439     0.0037     0.0001  
   3800     17989.5     0.7303     0.7111     0.6625     0.0035     0.0006  
   3900     18426.07    0.7292     0.7161     0.6305     0.004      0.0001  
   4000     18862.59    0.7356     0.7183     0.751      0.0036     0.0009  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       328.58     0.8006     0.797      3.1584     0.2744     0.0004  
   200       817.36     0.7575     0.7387     1.5623     0.3107     0.0003  
   300      1304.61     0.7391     0.7296     1.2871     0.3529     0.0002  
   400      1790.92     0.7363     0.7277     1.2436     0.4003     0.0002  
   500      2275.61     0.7363     0.7226     1.3657     0.4107     0.0002  
   600      2760.75     0.7404     0.7259     1.0208     0.4879     0.0002  
   700       3245.1     0.7369     0.7228     0.8357     0.5671     0.0002  
   800      3729.64     0.7347     0.7281     0.8593     0.6937     0.0002  
   900      4211.94     0.7379     0.7256     0.856      0.7299     0.0002  
   1000     4694.55     0.7378     0.7289     0.8095     0.8473     0.0002  
   1100     5177.56     0.7381     0.7261     0.7637     1.012      0.0002  
   1200     5660.11     0.7365     0.7255     0.7352     1.0941     0.0002  
   1300     6142.74     0.7376     0.7235     0.7015     1.4255     0.0002  
   1400     6625.04     0.7405     0.7245     0.6257     1.7457     0.0002  
   1500     7109.65     0.7356     0.7201     0.6773     1.9223     0.0002  
   1600      7592.0     0.7455     0.7288     0.6212     2.5917     0.0001  
   1700     8075.11     0.7389     0.7237     0.5186     2.6342     0.0001  
   1800     8556.53     0.7393     0.7335     0.5814     3.3934     0.0002  
   1900     9040.09     0.7392     0.7361     0.5686     4.2075     0.0001  
   2000     9520.84     0.7379     0.7279     0.6012     4.7038     0.0001  
   2100     10002.43    0.7364     0.7308     0.6052     6.2223     0.0001  
   2200     10485.21    0.7484     0.7329     0.6058     7.7411     0.0001  
   2300     10967.93    0.7422     0.7322     0.4522    10.5868     0.0002  
   2400     11449.54    0.7439     0.7383     0.6317    12.9813     0.0001  
   2500     11932.36    0.7531     0.7337     0.6232    12.4148     0.0001  
   2600     12414.69    0.7492     0.7363     0.5934    21.7293     0.0001  
   2700     12896.97    0.7548     0.7377     0.5407    27.7354     0.0001  
   2800     13378.45    0.7556     0.7411     0.6013    46.9949     0.0001  
   2900     13859.73    0.7597     0.7432     0.5437    55.2844     0.0002  
   3000     14340.19    0.7671     0.7497     0.6476    113.5375    0.0001  
   3100     14822.49    0.7802     0.7796     0.7023    168.4773    0.0001  
   3200     15304.03    0.8088     0.8015     0.6716    199.2223    0.0001  
   3300     15784.82    0.8742     0.8572     0.7358    336.3202    0.0001  
   3400     16265.94    0.9498     0.9505     0.8321    695.9451    0.0001  
   3500     16747.16    1.1009     1.084      0.761    1297.4981    0.0001  
   3600     17229.64    1.2353     1.2315     0.9094   4509.5219    0.0001  
   3700     17710.94    1.3401     1.3388     1.0917   13310.8176   0.0001  
   3800     18192.08    1.2365     1.2366     1.4162   415343.5027   0.0001  
   3900     18673.25    0.7417     0.7299     1.3855   56381.8847    0.0    
   4000     19153.89    0.7311     0.7169     1.0059   8122.3863    0.0001  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       327.95     1.0848     1.0777     3.1543     0.2912     0.0001  
   200       864.99     0.754      0.7491     1.3964     1.375      0.0001  
   300      1402.51     0.7616     0.7507     1.1383     9.8468     0.0001  
   400       1938.5     0.8477     0.8355     1.032     246.2183    0.0001  
   500      2473.46     1.1429     1.1433     1.0717    68.4176     0.0001  
   600      3008.41     1.4054     1.4051     1.1711   1159.5022    0.0001  
   700      3545.09     1.4133     1.4169     1.084    3265.5228    0.0001  
   800      4081.34     1.4201     1.4124     1.0594   1712.2011    0.0001  
   900      4616.46     1.4162     1.4137     1.2018   6172.3227     0.0    
   1000     5154.97     1.4205     1.4131     0.9925    8224.873     0.0    
   1100     5692.28     1.4223     1.4142     1.0477   1183.4854     0.0    
   1200     6230.34     1.414      1.4146     0.8718    58.7407      0.0    
   1300     6768.16     1.4156     1.4191     0.824     18.1583      0.0    
   1400     7305.86     1.4109      1.41      0.7837     10.957      0.0    
   1500     7843.38     1.4123     1.4147     0.8303     4.994       0.0    
   1600     8382.36     1.4139     1.4181     0.8707     3.1713      0.0    
   1700     8920.21     1.4144     1.4157     0.7197     2.1473      0.0    
   1800     9458.58     1.4119     1.4148     0.7847     1.8644      0.0    
   1900     9996.63     1.4215     1.4175     0.8163     1.431       0.0    
   2000     10534.61    1.4167     1.4106     0.7613     1.0141      0.0    
   2100     11073.7     1.4183     1.4149     0.8122     0.934       0.0    
   2200     11612.63    1.4134     1.414      0.7511     0.7715      0.0    
   2300     12150.52    1.411      1.4122     0.7346     0.6052      0.0    
   2400     12686.94    1.4191     1.4136     0.6754     0.4262      0.0    
   2500     13224.5     1.4105     1.4093     0.6786     0.382       0.0    
   2600     13762.37    1.411      1.4146     0.7586     0.4383      0.0    
   2700     14299.82    1.4149     1.4106     0.6426     0.3848      0.0    
   2800     14836.8     1.4142     1.4131     0.707      0.2622      0.0    
   2900     15375.01    1.4164     1.4119     0.6415     0.2525      0.0    
   3000     15916.35    1.4173     1.4094     0.7041     0.2669      0.0    
   3100     16477.51    1.4125     1.4153     0.6713     0.2189      0.0    
   3200     17038.74    1.4181     1.4163     0.6354     0.199       0.0    
   3300     17595.02    1.4062     1.4177     0.6108     0.1792      0.0    
   3400     18151.1     1.4202     1.4092     0.6173     0.1485      0.0    
   3500     18707.6     1.4124     1.4111     0.6422     0.1715      0.0    
   3600     19245.23    1.4215     1.4163     0.6025     0.1387      0.0    
   3700     19782.53    1.4182     1.4171     0.6507     0.134       0.0    
   3800     20319.98    1.4133     1.4197     0.5678     0.1142      0.0    
   3900     20858.74    1.4157     1.4132     0.6443     0.1303      0.0    
   4000     21396.08    1.4202     1.4124     0.5605     0.1159      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       329.85     1.4125     1.4137     3.1587     0.3736     0.0001  
   200       925.61     1.4157     1.4144     1.403      0.1923      0.0    
   300      1534.61     1.4139     1.4126     1.3589     0.1547      0.0    
   400      2146.98     1.4148     1.413      1.3359     0.1355      0.0    
   500      2756.55     1.4086     1.4144     1.2028     0.1188      0.0    
   600       3380.7     1.4169     1.4183     1.1131     0.1072      0.0    
   700      4012.61     1.4118     1.4131     1.0676     0.1026      0.0    
   800       4644.2     1.4201     1.416      1.1932     0.0972      0.0    
   900      5276.05     1.4087     1.4153     1.0315     0.0934      0.0    
   1000      5908.2     1.4209     1.4156     1.1376     0.0894      0.0    
   1100     6539.03     1.4147     1.4163     1.0721     0.0889      0.0    
   1200     7171.23     1.4131     1.4164     0.9695     0.0839      0.0    
   1300     7804.41     1.4101     1.4129     0.9808     0.0817      0.0    
   1400      8421.5     1.4092     1.4196     0.9961     0.084       0.0    
   1500      9027.0     1.4139     1.4132     1.0176     0.0801      0.0    
   1600     9655.28     1.417      1.4138     0.9662     0.0782      0.0    
   1700     10269.56    1.4091     1.4185     1.1561     0.0798      0.0    
   1800     10886.28    1.414      1.4147     0.9461     0.0821      0.0    
   1900     11502.56    1.4161     1.4134     0.9064     0.0776      0.0    
   2000     12111.2     1.4116     1.411      0.9298     0.074       0.0    
   2100     12721.37    1.4138     1.4147     0.8607     0.0756      0.0    
   2200     13334.45    1.4232     1.4146     0.8696     0.0775      0.0    
   2300     13963.22    1.4135     1.4133     0.8297     0.0759      0.0    
   2400     14586.37    1.4075     1.4131     0.8635     0.0743      0.0    
   2500     15197.77    1.4195     1.4126     0.8634     0.077       0.0    
   2600     15815.71    1.4197     1.4128     0.8328     0.0765      0.0    
   2700     16425.52    1.4143     1.411      0.8691     0.0754      0.0    
   2800     17048.64    1.4192     1.4113     0.7997     0.0802      0.0    
   2900     17677.46    1.4102     1.4156     0.8027     0.0776      0.0    
   3000     18307.36    1.405      1.4168     0.8384     0.0795      0.0    
   3100     18938.8     1.4165     1.4141     0.8103     0.0778      0.0    
   3200     19576.35    1.4153     1.4121     0.8338     0.0783      0.0    
   3300     20209.35    1.4093     1.4139     0.7852     0.0778      0.0    
   3400     20825.33    1.4156     1.4127     0.773      0.0774      0.0    
   3500     21437.45    1.4142     1.4172     0.7728     0.0792      0.0    
   3600     22063.54    1.4113     1.4162     0.8102     0.0849      0.0    
   3700     22691.81    1.4132     1.4177     0.7476     0.0839      0.0    
   3800     23309.89    1.4177     1.4165     0.8014     0.0833      0.0    
   3900     23924.13    1.4168     1.4178     0.7822     0.0809      0.0    
   4000     24532.47    1.4058     1.4156     0.7797     0.0849      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       337.19     1.421      1.4142     3.157      0.4949     0.0001  
   200       995.9      1.4147     1.4127     1.4178     0.3373      0.0    
   300      1653.67     1.4166     1.4135     1.3452     0.2795      0.0    
   400      2312.78     1.4086     1.4184     1.3006     0.2453      0.0    
   500      2972.49     1.4066     1.4117     1.2915     0.2304      0.0    
   600      3630.75     1.4166     1.4145     1.2539     0.2323      0.0    
   700      4289.13     1.4153     1.4139     1.233      0.2306      0.0    
   800      4950.91     1.4147     1.414      1.1733     0.2254      0.0    
   900      5613.68     1.414      1.4181     1.2401     0.2355      0.0    
   1000     6274.57     1.4107     1.415      1.1997     0.2349      0.0    
   1100      6933.3     1.4119     1.4181     1.1341     0.2402      0.0    
   1200     7591.71     1.4149     1.4159     1.2574     0.2386      0.0    
   1300     8253.92     1.4032     1.4132     1.1258     0.2506      0.0    
   1400     8934.37     1.4146     1.4173     1.0828     0.2657      0.0    
   1500     9619.63     1.4041     1.415      1.0806     0.2697      0.0    
   1600     10284.6     1.4126     1.415      1.0813     0.301       0.0    
   1700     10938.78    1.421      1.4169     1.0581     0.3146      0.0    
   1800     11594.02    1.4097     1.4129     1.0419     0.3258      0.0    
   1900     12250.87     1.42      1.4138     1.1475     0.3568      0.0    
   2000     12908.21    1.4093     1.4151     1.1216     0.3725      0.0    
   2100     13565.17    1.414      1.4159     1.1306     0.4107      0.0    
   2200     14217.73    1.4097     1.4156     1.1111     0.463       0.0    
   2300     14873.2     1.4203     1.4181     1.0886     0.4831      0.0    
   2400     15529.52    1.416      1.416      1.0697     0.5327      0.0    
   2500     16182.46    1.423      1.4147     1.0702     0.7634      0.0    
   2600     16835.67    1.4144     1.4134     1.1185     1.0768      0.0    
   2700     17487.7     1.4104     1.4168     1.1174     2.0171      0.0    
   2800     18139.46    1.413      1.4128     1.1055     3.4733      0.0    
   2900     18792.78    1.4211     1.4141     1.1023    34.5734      0.0    
   3000     19443.61    1.4202     1.4165     1.1515   7186.7995     0.0    
   3100     20096.36    1.4088     1.4151     1.1113   1910.4562     0.0    
   3200     20747.37    1.4205     1.4128     1.0274    948.3228     0.0    
   3300     21398.93    1.4159     1.4148     1.0485    66.1821      0.0    
   3400     22052.91    1.4154     1.4137      1.01     106.7975     0.0    
   3500     22710.69    1.4199     1.411      1.066     193.9605     0.0    
   3600     23371.51    1.4085     1.412      1.169    58061.1412    0.0    
   3700     24031.73    1.4208     1.4174     1.0998   17241.5289    0.0    
   3800     24683.66    1.4109     1.4144     1.1151    638.1392     0.0    
   3900     25335.61    1.4224     1.4169     1.1585   8553.0951     0.0    
   4000     25988.07    1.4194     1.417      1.1006   3190.0677     0.0    
