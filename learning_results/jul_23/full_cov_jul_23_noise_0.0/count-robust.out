Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  9.216201543807983
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       647.29      4.12      4.1528     0.3101     2.2168   128335.82  104053.85     0.0    
   200      1484.87     4.0752     4.1078     0.2926     2.3813   120919.37   98048.85     0.0    
   300       2321.8     3.9145     3.9448     0.2879     0.9751   100578.77   81550.37     0.0    
   400      3154.19     3.5759     3.6055     0.2573     0.7151    73675.67   59717.48     0.0    
   500      3989.64     3.0937     3.1187     0.2181     0.5314    52548.35   42568.51     0.0    
   600       4823.3     2.5796     2.5978     0.2341     0.1894    40221.85   32543.45     0.0    
   700       5657.3     2.1282     2.1399     0.2182     1.0171    33836.66   27342.23     0.0    
   800      6494.86     1.7737     1.7805     0.1966     0.9326    30573.6    24686.22     0.0    
   900      7325.57     1.5232     1.5254     0.1277     0.7178    28795.61   23248.5      0.0    
   1000     8150.04     1.3444     1.3422     0.1123     0.3111    27863.07   22485.19     0.0    
   1100      8971.6     1.2293     1.2215     0.1547     0.2795    27313.64   22039.95     0.0    
   1200     9772.44     1.1525     1.1447     0.1589     0.8707    26978.9    21771.44     0.0    
   1300     10526.83    1.1013     1.0912     0.1525     0.5898    26780.48   21619.19     0.0    
   1400     11278.41    1.0646     1.0536     0.1831     0.7021    26638.54   21507.46     0.0    
   1500     12060.27    1.0417     1.0324     0.152      0.0053    26552.99   21433.19     0.0    
   1600     12871.42    1.022      1.0102     0.0863     0.0354    26482.94   21372.72     0.0    
   1700     13683.45    1.0096     0.998      0.1029     0.0427    26439.82   21337.52     0.0    
   1800     14491.33    1.001      0.987      0.1296     0.0277    26401.63   21308.01     0.0    
   1900     15258.9     0.9905     0.9758     0.1463      0.46     26352.35   21270.16     0.0    
   2000     15998.98    0.9806     0.9683     0.1297     0.6013    26337.09   21258.02     0.0    
   2100     16730.31    0.9781     0.9648     0.1069     0.1806    26320.27   21252.83     0.0    
   2200     17447.85    0.9724     0.9608     0.1092     0.0064    26302.18   21234.9      0.0    
   2300     18172.0     0.9718     0.9564     0.125      0.2259    26274.62   21212.56     0.0    
   2400     18901.65    0.9683     0.9544     0.1349     0.3173    26248.09   21187.86     0.0    
   2500     19632.97    0.9656     0.951      0.0998     0.3185    26242.27   21181.33     0.0    
   2600     20369.18    0.9624     0.9487     0.081      0.0526    26239.8    21170.45     0.0    
   2700     21119.71    0.9624     0.9477     0.0767     0.0159    26222.85   21157.17     0.0    
   2800     21865.97    0.9593     0.9464     0.0852     0.0006    26225.28   21165.06     0.0    
   2900     22617.43    0.9599     0.946      0.0622     0.0006    26215.01   21156.56     0.0    
   3000     23365.34    0.9587     0.9452     0.0896     0.0006    26202.71   21148.53     0.0    
   3100     24098.36    0.9555     0.9449     0.0938     0.0065    26189.94   21140.18     0.0    
   3200     24817.33    0.9595     0.9427     0.1432     0.0017    26186.94   21133.17     0.0    
   3300     25538.4     0.9558     0.9416     0.1173     0.0051    26197.83   21143.44     0.0    
   3400     26255.39    0.9541     0.9412      0.07      0.0145    26184.89   21131.36     0.0    
   3500     26974.62    0.952      0.9384     0.0849     0.0036    26164.08   21116.74     0.0    
   3600     27701.55    0.9526     0.9395     0.0471     0.005     26168.6    21116.25     0.0    
   3700     28422.68    0.9517     0.9384     0.1088     0.2596    26164.85   21112.96     0.0    
   3800     29143.24    0.952      0.9377     0.0783     0.0005    26154.97   21110.2      0.0    
   3900     29866.14    0.9514     0.9386     0.096      0.0005    26150.69   21106.55     0.0    
   4000     30597.83    0.9527     0.9389     0.1209     0.1101    26153.59   21104.08     0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       578.55     4.1374     4.159      0.311      2.5562   128676.05  207871.24     0.0    
   200      1383.14     4.0993     4.1202     0.3038     1.1223   122467.27  197801.45     0.0    
   300      2188.32     3.942      3.9623     0.2865     0.7004   102498.16  165543.11     0.0    
   400      2991.07     3.6032     3.6217     0.2596     0.6562    74932.65  121046.38     0.0    
   500      3792.28     3.1183     3.1335     0.2902     0.2022    53103.15   85796.74     0.0    
   600      4590.34     2.6064     2.6176     0.2863     0.3894    40572.2    65565.54     0.0    
   700      5387.22     2.1631     2.1711     0.2454     0.2816    34099.76   55133.0      0.0    
   800      6186.46     1.8151     1.8196     0.2591     0.0578    30744.47   49726.2      0.0    
   900      6980.36     1.5639     1.565      0.1547     0.0905    28967.27   46837.96     0.0    
   1000     7771.75     1.3866     1.3826     0.1861     0.5357    27896.64   45118.08     0.0    
   1100     8565.17     1.2576     1.2525     0.1616     0.7357    27285.21   44141.89     0.0    
   1200     9356.08     1.1693     1.166      0.1915     0.027     26942.54   43575.47     0.0    
   1300     10146.05    1.1142     1.1104     0.1478     0.3186    26728.05   43228.85     0.0    
   1400     10935.71    1.0732     1.0689     0.0855     0.3557    26567.99   42990.78     0.0    
   1500     11727.86    1.0461     1.0403     0.1764     0.233     26462.92   42811.99     0.0    
   1600     12519.15    1.0252     1.0193     0.1036     0.0607    26402.19   42721.46     0.0    
   1700     13308.56    1.0083     1.0014     0.1546     0.0476    26343.65   42604.51     0.0    
   1800     14098.22    0.9939     0.9877     0.0829     0.0311    26286.23   42532.33     0.0    
   1900     14888.72    0.9856     0.9799     0.209      0.4232    26284.28   42514.16     0.0    
   2000     15679.66    0.9802     0.972      0.1145     0.1474    26262.2    42469.72     0.0    
   2100     16468.92    0.9719     0.9661     0.1277     0.3103    26215.39   42400.73     0.0    
   2200     17258.81    0.9707     0.9644     0.109      0.046     26191.44   42376.1      0.0    
   2300     18046.79    0.9681     0.9616     0.1414     0.1068    26212.06   42394.82     0.0    
   2400     18837.48    0.9638     0.9579     0.1489     0.8054    26176.47   42347.31     0.0    
   2500     19626.92    0.9602     0.9536     0.1232     0.6165    26150.48   42295.37     0.0    
   2600     20416.72    0.9563     0.9511     0.0582     0.0051    26138.67   42274.37     0.0    
   2700     21206.78    0.957      0.9505     0.1576     0.357     26142.9    42295.88     0.0    
   2800     21997.27    0.9531     0.9458     0.1207     0.0068    26137.59   42264.68     0.0    
   2900     22790.22    0.9534     0.9469     0.0896     0.0034    26123.44   42259.04     0.0    
   3000     23590.14    0.9513     0.9443     0.0665     0.0105    26112.73   42241.52     0.0    
   3100     24408.44    0.9503     0.9435     0.1145     0.0223    26109.35   42247.29     0.0    
   3200     25229.08    0.9508     0.9427     0.0955     0.0222    26096.11   42216.93     0.0    
   3300     26056.67    0.9489      0.94      0.0827     0.0656    26089.23   42201.02     0.0    
   3400     26854.68    0.945      0.9409     0.0689     0.0026    26085.64   42209.44     0.0    
   3500     27643.34    0.947      0.9409     0.1003     0.0182    26083.17   42203.71     0.0    
   3600     28434.85    0.9447     0.9377     0.0856     0.2671    26073.58   42180.49     0.0    
   3700     29228.3     0.9478     0.9385     0.1204     0.1739    26077.11   42177.57     0.0    
   3800     30026.9     0.9467     0.9386     0.073      0.0127    26067.73   42169.21     0.0    
   3900     30847.99    0.9444     0.937      0.0669     0.0012    26057.73   42148.1      0.0    
   4000     31666.48    0.9425     0.9351     0.0719     0.0361    26051.95   42134.8      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       610.23     4.1438     4.1577     0.305      1.5668   129004.23  311405.28     0.0    
   200      1549.42     4.1127     4.1274     0.3041     1.7396   123986.93  299315.73     0.0    
   300      2489.42     3.9703     3.9853     0.2904     0.7473   105447.12  254651.51     0.0    
   400      3424.34     3.6474     3.662      0.2661     0.5969    77745.17  187900.48     0.0    
   500      4358.46     3.1673     3.1807     0.2569     0.498     54721.1   132358.13     0.0    
   600      5275.13     2.6508     2.6635     0.2886     0.2675    41320.04   99993.76     0.0    
   700      6143.92     2.1899     2.2005     0.2273     1.3687    34342.62   83089.38     0.0    
   800      7066.52     1.8329     1.8405     0.1656     0.5921    30763.05   74400.6      0.0    
   900      7960.86     1.5702     1.5749     0.1698     0.9112    28856.81   69777.11     0.0    
   1000     8852.57     1.3839     1.3898     0.2017     0.1662    27786.61   67185.54     0.0    
   1100     9732.43     1.2609     1.2638     0.1508     0.8348    27209.04   65789.61     0.0    
   1200     10602.61    1.1768     1.1765     0.2065     1.9581    26896.16   64997.32     0.0    
   1300     11501.24    1.1163     1.1146     0.1535     0.1393    26612.7    64315.64     0.0    
   1400     12383.44    1.0737     1.0712     0.1642     0.2492    26473.82   63996.56     0.0    
   1500     13243.48    1.044      1.0405     0.1423     0.5035    26342.27   63647.12     0.0    
   1600     14120.18    1.0217     1.0174     0.1666     0.0265    26264.01   63476.59     0.0    
   1700     15041.15    1.0063     1.0014     0.1326     0.0679    26218.79   63371.91     0.0    
   1800     15965.75    0.9894     0.9847     0.1073     0.2658    26164.55   63198.43     0.0    
   1900     16883.56    0.9803     0.9762     0.1738     0.7218    26123.44   63115.62     0.0    
   2000     17788.51    0.9734     0.9681     0.1489     0.1462    26116.73   63108.45     0.0    
   2100     18667.17    0.9664     0.9616     0.139      0.0201    26080.79   63025.01     0.0    
   2200     19564.54    0.9624     0.9565     0.1393     0.1051    26071.29   62991.49     0.0    
   2300     20461.58    0.9632     0.9556     0.1297     0.2155    26043.98   62918.61     0.0    
   2400     21355.84    0.9582     0.9511     0.1553     0.1657    26025.96   62867.38     0.0    
   2500     22246.78    0.9543     0.9484      0.1       0.002     26013.39   62839.44     0.0    
   2600     23149.15    0.9513     0.9447     0.1242     0.4437    25999.6    62825.17     0.0    
   2700     24050.27    0.9507     0.9442     0.065      0.0139    25998.29   62809.29     0.0    
   2800     24950.62    0.9464     0.9401     0.1021     0.0021    25993.08   62797.13     0.0    
   2900     25843.62    0.9438     0.9363     0.0935     0.0168    25986.25   62807.23     0.0    
   3000     26738.41    0.9434     0.9366     0.0632     0.0676    25976.29   62769.15     0.0    
   3100     27626.82    0.9421     0.9364     0.0974     0.1391    25943.71   62686.59     0.0    
   3200     28516.07    0.9409     0.9357     0.1276     0.0328    25939.26   62682.35     0.0    
   3300     29404.53    0.9404     0.9348     0.1349     0.2209    25949.21   62686.01     0.0    
   3400     30292.45    0.9386     0.9328     0.0983     0.0016    25944.07   62683.26     0.0    
   3500     31191.09    0.9398     0.9319     0.0771     0.1569    25940.28   62660.92     0.0    
   3600     32086.84    0.9377     0.9314     0.0979     0.0013    25930.33   62644.34     0.0    
   3700     32978.38    0.9353     0.9275     0.1031     0.0561    25903.14   62571.82     0.0    
   3800     33872.62    0.9382     0.9297     0.1419     0.0013    25898.86   62571.79     0.0    
   3900     34774.09    0.9368     0.9284     0.1183     0.002     25891.19   62548.38     0.0    
   4000     35659.51    0.9362     0.9274     0.0591     0.0042    25885.42   62537.64     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       586.33     4.1438     4.1563     0.3105     3.1752   129025.74  415081.71     0.0    
   200      1566.69     4.1113     4.1231     0.3042     1.494    123875.87  398390.14     0.0    
   300      2559.97     3.9662     3.9776     0.2894     1.0844   105322.25  338644.94     0.0    
   400      3546.73     3.6427     3.6525     0.265      0.7206    77662.32  249769.78     0.0    
   500      4527.12     3.161      3.1698     0.2966     0.4843    54632.16   175855.7     0.0    
   600      5505.84     2.6517     2.6585     0.2502     1.7617    41368.47   133275.4     0.0    
   700      6485.33     2.2027     2.2086     0.2406     0.7631    34466.14  111130.06     0.0    
   800       7462.7     1.8536     1.8576     0.1729     1.0692    30953.03   99852.56     0.0    
   900       8436.4     1.5893     1.5928     0.1443     0.8334    28978.06   93508.87     0.0    
   1000     9371.61     1.4044      1.41      0.194      0.9689    27931.0    90163.18     0.0    
   1100     10305.1     1.2792     1.2827     0.2769     1.0453    27298.99   88103.85     0.0    
   1200     11239.31    1.1966     1.197      0.2096     0.6441    26937.23   86924.4      0.0    
   1300     12171.67    1.1318     1.1338     0.1671     0.662     26679.47   86117.47     0.0    
   1400     13108.17    1.0886     1.0883     0.1426     0.0752    26512.22   85551.54     0.0    
   1500     14040.34    1.0557     1.0555     0.1383     0.8554    26388.3    85150.58     0.0    
   1600     14972.39    1.0326     1.0298     0.1514     0.0109    26323.34   84930.7      0.0    
   1700     15904.45    1.0123     1.0115     0.1273     0.4669    26259.29   84737.84     0.0    
   1800     16837.25    1.0017     0.9972     0.1994     0.6265    26254.41   84719.94     0.0    
   1900     17768.39    0.9935     0.9899     0.0956     0.3162    26168.82   84419.32     0.0    
   2000     18700.75    0.9834     0.9786     0.1235     0.2579    26132.59   84314.16     0.0    
   2100     19631.29    0.9784     0.9728     0.0983     0.0046    26102.03   84197.85     0.0    
   2200     20563.55    0.9678     0.9642     0.1521     0.0266    26075.01   84136.65     0.0    
   2300     21495.03    0.9623     0.9572     0.0917     0.0639    26070.76   84101.94     0.0    
   2400     22424.75    0.9613     0.9557     0.1466     0.0721    26048.16   84012.84     0.0    
   2500     23357.26    0.9555     0.9487     0.1364     0.4201    25992.0    83833.54     0.0    
   2600     24289.61    0.9511     0.9441     0.0988     0.0066    25970.03   83756.69     0.0    
   2700     25220.55    0.9523     0.9455     0.1437     0.086     25969.9    83762.72     0.0    
   2800     26151.93    0.9475     0.9399     0.1006     0.6425    25926.07   83618.79     0.0    
   2900     27083.62    0.944      0.9379     0.0815     0.002     25908.8    83569.9      0.0    
   3000     28014.42    0.9424     0.9367     0.1059     0.0149    25901.95   83553.55     0.0    
   3100     28946.64    0.9427     0.9359     0.1056     0.0034    25911.81   83580.98     0.0    
   3200     29878.24    0.9404     0.9349     0.1003     0.0014    25897.85   83528.3      0.0    
   3300     30811.25    0.9383     0.9324     0.1109     0.003     25871.77   83437.64     0.0    
   3400     31743.8     0.9367     0.9304     0.0725     0.2428    25867.39   83417.54     0.0    
   3500     32675.17    0.9357     0.9294     0.0911     0.0023    25859.68   83405.94     0.0    
   3600     33605.09    0.9378     0.9303     0.1243     0.015     25856.95   83365.85     0.0    
   3700     34536.44    0.9325     0.9259     0.1426     0.0256    25832.77   83299.71     0.0    
   3800     35468.64    0.9348     0.9274     0.0973     0.0102    25833.62   83309.12     0.0    
   3900     36400.56    0.9346     0.9263     0.0943     0.0023    25824.62   83253.86     0.0    
   4000     37331.7     0.9326     0.9258     0.075      0.479     25817.4    83231.89     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
   100       583.69     4.1461     4.1646     0.3034     2.0037   129091.34   520318.7     0.0    
   200       1650.6     4.1212     4.1388     0.3061     1.5621   125013.63  503707.71     0.0    
   300      2693.76     3.992      4.009      0.2905     0.7702   107924.44  434645.78     0.0    
   400      3713.02     3.6865     3.7023     0.2759     0.5965    80520.3   324226.85     0.0    
   500      4728.83     3.215      3.2287     0.2983     0.3953    56503.2   227672.15     0.0    
   600      5741.18     2.6983     2.7079     0.1977     1.7586    42296.33   170450.6     0.0    
   700      6751.47     2.2361     2.2415     0.2445     0.4877    34886.54  140601.02     0.0    
   800      7761.34     1.8711     1.8756     0.2337     0.8244    31095.89  125381.54     0.0    
   900      8769.47     1.6025     1.6047     0.211      0.9971    29071.25  117295.91     0.0    
   1000     9777.31     1.4123     1.416      0.2603     0.1962    27961.72  112825.73     0.0    
   1100     10782.95    1.282      1.2845     0.101      0.6078    27299.83  110211.39     0.0    
   1200     11812.85    1.1932     1.1958     0.1512     0.5768    26907.1    108616.4     0.0    
   1300     12870.36    1.127      1.1312     0.1419     0.5697    26642.18  107598.78     0.0    
   1400     13898.97    1.084      1.0848     0.1164     0.4901    26484.22  106875.49     0.0    
   1500     14905.09    1.0537     1.0554     0.1497     0.2665    26389.25  106589.12     0.0    
   1600     15909.99    1.0258     1.0279     0.1657     0.3811    26287.21  106145.45     0.0    
   1700     16915.71    1.0096     1.0097     0.082      0.3662    26235.1   105933.33     0.0    
   1800     17919.75    0.9961     0.9974     0.1027     0.2473    26188.69  105717.13     0.0    
   1900     18924.65    0.9864     0.9861     0.1429     0.0103    26190.64  105718.84     0.0    
   2000     19929.62    0.9779     0.977      0.0996     0.0372    26144.92  105529.81     0.0    
   2100     20934.83    0.9717     0.9707     0.1046     0.011     26114.35  105432.08     0.0    
   2200     21940.86    0.966      0.9642     0.0972     0.0215    26070.7   105216.43     0.0    
   2300     22945.7     0.9607     0.9581     0.1013     0.1028    26047.72  105136.31     0.0    
   2400     23950.46    0.9583     0.9581     0.1059     0.5481    26037.03   105119.3     0.0    
   2500     24955.42    0.9532     0.9519     0.1451     0.0036    26021.07  105037.76     0.0    
   2600     25959.43    0.9507     0.9501     0.1556     0.1125    26013.31  105010.44     0.0    
   2700     26964.47    0.9498     0.9479     0.1081     0.0103    26013.94  104981.97     0.0    
   2800     27969.97    0.9468     0.9452     0.1492     0.1825    25993.3   104900.69     0.0    
   2900     28974.01    0.9458     0.9427     0.0957     0.0068    25988.89  104894.87     0.0    
   3000     29977.92    0.9447     0.9418     0.1072     0.0365    25965.09  104812.27     0.0    
   3100     30984.21    0.9421      0.94      0.1089     0.0036    25965.53  104823.79     0.0    
   3200     31989.13    0.9417     0.9404     0.0814     0.0029    25967.32  104818.03     0.0    
   3300     32993.2     0.9423     0.9379     0.0633     0.4602    25959.59  104800.75     0.0    
   3400     33998.12    0.9405     0.9378     0.1387     0.0306    25952.24  104743.46     0.0    
   3500     35003.35    0.9397     0.9371     0.1122     0.0567    25943.77  104706.34     0.0    
   3600     36008.0     0.9402     0.9373     0.0899     0.0106    25943.44  104714.77     0.0    
   3700     37010.45    0.9384     0.9333     0.0895     0.0211    25922.05   104603.1     0.0    
   3800     38014.9     0.9387     0.9346     0.1203     0.0051    25924.62  104626.24     0.0    
   3900     39019.97    0.9353     0.9335     0.0946     0.1831    25908.14   104533.4     0.0    
   4000     40023.4     0.9345     0.9313     0.0799     0.0116    25901.21  104520.54     0.0    
