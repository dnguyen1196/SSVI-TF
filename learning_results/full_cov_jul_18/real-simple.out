Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.4839184284210205
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       83.05     374.8893   374.1494    3.1623     4.4719     0.0001  
   100       174.67    207.3954   208.3424    3.1618     0.3001      0.0    
   150       266.28    173.9327   172.8013    3.0743     0.1877      0.0    
   200       357.51    161.7229   160.6904    2.8246     0.1478      0.0    
   250       448.66    155.8759   156.062     2.1492     0.1095      0.0    
   300       539.94    153.8567   153.0647    1.9318     0.0466      0.0    
   350       631.27    149.2413   149.8136    1.7478     0.0267      0.0    
   400       722.35    151.1036   150.4569    1.6564     0.0177      0.0    
   450       813.71    150.2249   149.6781    2.2931     0.0225      0.0    
   500       905.31    149.5321   149.2471    1.7266     0.0196      0.0    
   550       996.49    147.984    147.8654    1.4984     0.0122      0.0    
   600      1087.91    147.7801   147.1695    1.1261     0.0103      0.0    
   650       1179.1    148.0816   148.2596    1.2415     0.0076      0.0    
   700      1270.29    149.3227   148.5248    1.3988     0.0092      0.0    
   750      1361.57    147.3284   147.3344    1.3695     0.0094      0.0    
   800      1452.87    148.0902   147.4654    1.3606     0.0068      0.0    
   850      1544.02    147.1436   147.0048    1.2275     0.0056      0.0    
   900       1635.0    149.1827   148.4083    1.3345     0.0057      0.0    
   950      1725.89    147.4826   146.8717    1.1779     0.0064      0.0    
   1000      1816.9    147.2448   146.8202    1.0825     0.0043      0.0    
   1050     1907.91    147.1926   147.1396    1.1899     0.0045      0.0    
   1100     1999.01    147.4193   146.5301    1.3156     0.004       0.0    
   1150     2090.14    146.1324   146.1364    0.8217     0.0047      0.0    
   1200     2181.13    146.6396   146.0416    1.0936     0.0034      0.0    
   1250     2272.08    147.5833   147.0841    1.114      0.0039      0.0    
   1300     2363.05    146.5909   146.2438    0.9835     0.0028      0.0    
   1350     2454.07    146.4321   146.3231     0.92      0.0025      0.0    
   1400      2545.0    146.9901   146.6881    1.1316     0.0026      0.0    
   1450     2635.99    146.9641   146.9672    1.2378     0.0023      0.0    
   1500     2726.88    146.8007   146.4118    0.9998     0.002       0.0    
   1550     2817.92    146.6701   146.1899    0.9734     0.0027      0.0    
   1600     2909.22    146.4159   146.1306    0.8531     0.0019      0.0    
   1650     3000.27    146.5735   145.9534    0.8686     0.0019      0.0    
   1700      3091.5    146.3207   145.9936    1.1224     0.0018      0.0    
   1750     3182.68    146.2469   145.9558    0.9276     0.0014      0.0    
   1800     3273.67    146.1962   145.6856    1.1295     0.0016      0.0    
   1850     3364.75    146.6665   146.1055    0.9904     0.0014      0.0    
   1900     3455.83    147.3597   146.8944    0.8627     0.0015      0.0    
   1950     3546.79    146.3285   145.9486    0.8875     0.0015      0.0    
   2000     3637.79    145.7368   145.4199    0.6177     0.0013      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       83.01     298.4204   298.5487    3.0672     4.4679      0.0    
   100       175.26    177.828    177.8962    2.8261     0.3763      0.0    
   150       267.12    159.9522   159.606     2.3429     0.1431      0.0    
   200       358.95    154.0906   153.7329    1.991      0.0629      0.0    
   250       450.97    152.9151   152.2543    2.1479     0.0384      0.0    
   300       542.85    150.5201   150.459     2.0311     0.0278      0.0    
   350       634.71    150.8023   149.6709    1.7594     0.0193      0.0    
   400       726.77    148.6085   148.7656    1.4138     0.0165      0.0    
   450       818.75    150.1536   149.4143    1.7927     0.0127      0.0    
   500       910.93    148.5505   148.4461    1.5319     0.0099      0.0    
   550      1003.15    147.2065   147.5431    1.2545     0.0081      0.0    
   600      1095.24    148.644    148.1228    1.5606     0.0078      0.0    
   650      1187.23    147.6841   147.3098    1.3915     0.0061      0.0    
   700      1279.52    146.7211   146.3045    0.995      0.0057      0.0    
   750      1371.56    147.7239   147.1316    0.9523     0.004       0.0    
   800      1463.23    147.3875   147.1259    1.001      0.0041      0.0    
   850      1554.91    145.7619   146.0156    1.0997     0.0039      0.0    
   900      1646.67    147.3132   146.8802    1.1547     0.0034      0.0    
   950      1738.28    147.7768   147.2831    1.2838     0.0028      0.0    
   1000     1829.84    147.7247   147.4594    1.8437     0.0029      0.0    
   1050     1921.45    147.2907   146.7067    1.3047     0.0025      0.0    
   1100     2013.16    146.5668   145.9194    0.9409     0.0021      0.0    
   1150     2104.99    146.0424   145.7484    0.8451     0.0021      0.0    
   1200     2197.02    146.5005   146.4965    0.9399     0.0021      0.0    
   1250      2288.7    147.4629   147.2792    1.0366     0.0018      0.0    
   1300     2380.43    147.542    147.2177    1.1794     0.0016      0.0    
   1350     2472.27    147.0622   146.8237    0.9799     0.0014      0.0    
   1400     2564.01    146.6295   146.3505    1.0801     0.0015      0.0    
   1450     2655.77    147.1354   146.6117    0.9848     0.0012      0.0    
   1500     2747.55    146.7967   146.5139    0.9932     0.0013      0.0    
   1550     2839.24    146.6279   146.2771    1.0684     0.0013      0.0    
   1600      2931.0    147.4129   147.0377    0.9005     0.0012      0.0    
   1650     3022.89    146.7999   146.5808    1.4783     0.0011      0.0    
   1700     3114.71    146.7673   146.5399    0.9587     0.0009      0.0    
   1750     3206.34    146.8766   146.3067    0.9779     0.0012      0.0    
   1800     3298.11    146.288    146.1112    0.8443     0.0009      0.0    
   1850     3389.94    146.3397   146.0989    0.9445     0.0009      0.0    
   1900     3481.59    146.0586   145.7971    1.1952     0.0008      0.0    
   1950     3573.53    145.8185   145.9061    0.8303     0.0007      0.0    
   2000     3665.48    146.3983   146.2077    1.0763     0.0007      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       82.42     288.3394   286.6755    2.9108     4.4666      0.0    
   100       174.49    176.8211   175.5899    2.6711     0.3798      0.0    
   150       266.64    158.0335   156.8843    2.1447     0.1209      0.0    
   200       358.82    150.9864   150.3532    1.5684     0.0728      0.0    
   250       451.29     149.23    148.6316    1.3364     0.0487      0.0    
   300       544.09    147.2495   146.9464    1.0641     0.0309      0.0    
   350       636.48    147.7312   147.1324    1.2145     0.022       0.0    
   400       728.87    146.6011   146.4191    1.0949     0.0163      0.0    
   450       821.45    147.041    146.3291    1.1803     0.014       0.0    
   500       913.9     147.2113   146.9725    1.3103     0.0118      0.0    
   550      1006.27    146.6635   146.3197    1.154       0.01       0.0    
   600       1098.6    147.7507   147.0965    1.4011     0.0087      0.0    
   650      1191.03    145.703    145.465     0.9928     0.0066      0.0    
   700      1283.31    146.0936   145.4733    0.6927     0.0065      0.0    
   750      1375.82    146.2702   145.8818    0.9449     0.0051      0.0    
   800       1468.3    146.6633   146.2034    0.8665     0.0047      0.0    
   850      1560.71    147.6919   147.0975    1.4334     0.0037      0.0    
   900      1653.01    146.2922   146.2487    1.2149     0.0043      0.0    
   950      1745.32    146.5041   146.0257    1.2386     0.0035      0.0    
   1000     1837.59    146.0486   145.8563    1.1535     0.003       0.0    
   1050     1929.94    146.5955   146.2753    1.1232     0.0029      0.0    
   1100     2022.28    146.5777   146.1252    1.3813     0.0026      0.0    
   1150     2114.57    147.2667   146.6711    1.144      0.0023      0.0    
   1200     2206.83    146.459    145.8948    1.3878     0.0023      0.0    
   1250     2299.25    146.0725   145.7787    0.9417     0.0021      0.0    
   1300      2391.5    145.5579   145.101     0.8133     0.0018      0.0    
   1350     2483.75    145.5841   145.1191    0.9615     0.0016      0.0    
   1400     2576.17    146.2139   145.9136    0.7116     0.0017      0.0    
   1450     2668.39    145.7687   145.6758    0.9541     0.0015      0.0    
   1500     2760.64    146.753    146.0731    1.3667     0.0014      0.0    
   1550     2853.09    146.3987   146.0733    0.9831     0.0014      0.0    
   1600     2945.48    145.9008   145.6789    0.9252     0.0012      0.0    
   1650     3037.69    145.6182   145.1034    0.8752     0.0012      0.0    
   1700     3130.01    145.4819   145.301     0.755      0.0012      0.0    
   1750     3222.29    145.5456   144.9721    0.7376     0.001       0.0    
   1800     3314.36    146.2191   145.6017    0.9537     0.0011      0.0    
   1850     3406.68    145.8835   145.4551    1.0092     0.0009      0.0    
   1900      3499.0    145.969    145.643     0.6658     0.001       0.0    
   1950     3591.23    146.1169   145.4842    0.7376     0.0009      0.0    
   2000     3683.84    146.0001   145.4839    0.7788     0.0008      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       83.08     280.9534   280.0357    2.7719     4.4658      0.0    
   100       176.12    181.4172   180.3937    2.3456     0.3811      0.0    
   150       269.18    159.8528   159.5269    1.7232     0.1389      0.0    
   200       362.52    152.8138   152.8476    1.0774     0.0758      0.0    
   250       455.73    150.2032   150.0223    1.1599     0.0509      0.0    
   300       548.82    147.2634   147.1937    1.4189     0.0368      0.0    
   350       642.09    147.0025   147.3781    1.3147     0.0276      0.0    
   400       735.35    146.8467   147.0476    1.5081     0.0198      0.0    
   450       828.5     146.8847   146.9257    1.5019     0.0148      0.0    
   500       921.8     146.7764   146.9085    1.3638     0.0135      0.0    
   550       1015.3    146.3212   146.6179    1.3183     0.0101      0.0    
   600      1108.69    145.3766   145.574      1.18      0.009       0.0    
   650      1201.85    145.8365   146.1795    0.851      0.008       0.0    
   700      1295.29    145.865    145.998     1.0662     0.0056      0.0    
   750      1388.92    145.4191   145.4578    0.9289     0.0058      0.0    
   800      1482.56    145.6397   145.6955    1.0249     0.005       0.0    
   850       1576.1    145.9772   146.3556    0.858      0.005       0.0    
   900      1669.34    146.2859   146.1657    0.8135     0.0037      0.0    
   950      1762.97    145.7547   146.1086    1.2633     0.0035      0.0    
   1000     1856.36    145.7263   145.7527    0.8714     0.0034      0.0    
   1050     1949.78    144.9739   145.3148    0.8633     0.0031      0.0    
   1100     2043.18    145.8763   146.1579    1.0337     0.003       0.0    
   1150     2136.69    146.8338   146.7089    1.0331     0.0026      0.0    
   1200     2230.11    145.4586   145.7997    0.9783     0.0024      0.0    
   1250     2323.34    145.907    146.0791    0.7927     0.002       0.0    
   1300     2416.67    145.7417   145.6786    0.7332     0.0019      0.0    
   1350     2510.01    145.6758   145.6895    0.7866     0.0018      0.0    
   1400      2603.4    145.8183   145.7745    0.7368     0.0018      0.0    
   1450     2696.84    145.2873   145.648     0.8349     0.0017      0.0    
   1500     2790.25    146.2823   145.9304    0.8248     0.0016      0.0    
   1550     2883.49    145.8927   146.006     0.7404     0.0014      0.0    
   1600     2976.69    145.5274   145.6895    0.7946     0.0013      0.0    
   1650     3069.89    145.4835   145.5035    0.882      0.0013      0.0    
   1700     3163.07    146.3303   146.0082    0.7462     0.0012      0.0    
   1750     3256.24    145.4048   145.6708    0.8904     0.0011      0.0    
   1800     3349.26    145.7393   145.6548    0.8954     0.0011      0.0    
   1850     3442.34    145.5962   145.5481    0.6436     0.0011      0.0    
   1900     3535.43    145.7656   146.0261    0.815      0.0009      0.0    
   1950     3628.63    145.4425   145.4997    0.9896     0.0009      0.0    
   2000      3721.8    145.7638   145.8953    0.865      0.0009      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       82.78     288.3812   286.8033    2.6518     4.4652      0.0    
   100       176.42    188.5558   187.2665    2.6289     0.3458      0.0    
   150       269.98    163.4827   162.5256    1.325      0.1488      0.0    
   200       363.54    153.1007   152.858     1.2043     0.0674      0.0    
   250       457.12    149.8223   149.493     0.8838     0.0518      0.0    
   300       550.92    148.026    147.8547    0.9742     0.0287      0.0    
   350       644.81    147.2054   146.8559    0.9747     0.0205      0.0    
   400       738.7     146.2513   146.3698    0.9532     0.0171      0.0    
   450       832.51    146.7455   146.5089    1.3649     0.0135      0.0    
   500       926.45    145.8122   145.7818    1.4047     0.0102      0.0    
   550      1020.44    146.0933   145.8643    0.9687     0.0088      0.0    
   600       1114.3    146.5833   146.4727    1.1484     0.0083      0.0    
   650      1208.11    145.7399   145.6548    1.2037     0.0063      0.0    
   700      1301.99    145.5266   145.5108    0.9149     0.0063      0.0    
   750      1396.07    145.278    145.3592    0.9616     0.0054      0.0    
   800      1489.98    145.1847   145.0754    0.7703     0.0048      0.0    
   850      1583.63    145.363    145.2731    1.0513     0.0041      0.0    
   900      1677.55    145.0031   145.2199    0.7375     0.0037      0.0    
   950      1771.54    145.5368   145.3794    0.7211     0.0035      0.0    
   1000     1865.36    145.4519   145.6127    0.6759     0.0029      0.0    
   1050     1959.04    145.1457   145.3909    0.6842     0.0032      0.0    
   1100     2052.81    145.5911   145.5544    0.7131     0.0023      0.0    
   1150     2146.51    144.7548   144.9085    0.7851     0.0023      0.0    
   1200     2240.22    145.2148   145.2568    0.6465     0.0022      0.0    
   1250     2333.91    145.3345   145.3866    0.604      0.002       0.0    
   1300     2427.76    145.7477   145.6593    0.5677     0.0019      0.0    
   1350     2521.82    145.2277   145.2568    0.697      0.0018      0.0    
   1400     2615.63    144.9947   145.1812    0.5465     0.0018      0.0    
   1450     2709.32    145.2511   145.2403    0.6599     0.0015      0.0    
   1500     2803.18    145.2106   145.2504    0.9211     0.0014      0.0    
   1550     2897.32    145.3644   145.2143    0.8703     0.0015      0.0    
   1600     2991.04    145.5283   145.7134    0.9542     0.0012      0.0    
   1650     3084.65    145.4823   145.3846    0.8181     0.0012      0.0    
   1700     3178.36    145.9973   146.0713    0.9943     0.0011      0.0    
   1750      3272.3    145.9109   145.7099    1.296      0.0012      0.0    
   1800     3366.12    144.9157   144.9376    0.7536     0.0011      0.0    
   1850     3459.91    145.5769   145.5777    0.6314     0.0009      0.0    
   1900      3554.0    145.2867   145.2984    0.8379     0.0009      0.0    
   1950     3647.98    145.4368   145.4449    0.8519     0.0009      0.0    
   2000     3741.73    145.4702   145.4139    1.0701     0.0008      0.0    
