Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.283201217651367
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       149.85     1.2441      1.21      3.1623     1.0239     0.013   
   100       440.98     0.7865     0.7694     3.103      0.6574     0.0007  
   150       730.91     0.7452     0.7281     2.2678     1.8765     0.0034  
   200      1020.01     0.7445     0.7336     2.1065    13.3958     0.0058  
   250      1307.84     0.7476     0.7269     2.0154    58.5807     0.0075  
   300      1595.19     0.7463     0.735      1.8013    28.0035     0.0032  
   350      1882.23     0.7475     0.7309     1.6143    17.9807     0.0046  
   400      2169.04     0.7748     0.7575     1.945     512.5755    0.0178  
   450      2455.68     0.7549      0.74      1.9452    376.0666    0.0027  
   500      2741.65     0.745      0.7301     1.6488    315.3111    0.007   
   550      3027.13     0.7343     0.7155     2.1533   5244.1535    0.0164  
   600      3311.94     0.743      0.7298     1.943    4695.3438    0.0144  
   650      3596.55     0.7447     0.7225     1.908     4077.93     0.0013  
   700      3880.65     0.7328     0.7225     1.8599   5106.4765    0.0041  
   750      4164.13     0.7381     0.7251     2.028    2315.7428    0.0069  
   800      4447.65     0.7254     0.7127     1.6921   1212.5322    0.0004  
   850      4731.11     0.7378     0.7219     1.5308    818.9269    0.0179  
   900      5013.95     0.7311     0.7123     1.8012   2172.6331    0.0045  
   950      5295.84     0.7291     0.7192     1.7936    776.1833    0.0081  
   1000     5577.66     0.7286     0.7105     1.4567    198.4653    0.0092  
   1050     5859.46     0.7321     0.7175     1.715     83.4157     0.0019  
   1100     6140.57     0.7231     0.705      1.5597    195.1602    0.0048  
   1150     6420.78     0.7194     0.6971     1.6338   1931.2836    0.0018  
   1200     6700.73     0.7287     0.7063     1.8408    3048.013    0.0005  
   1250     6980.35     0.7224     0.6981     1.7676   2088.5818    0.0005  
   1300     7259.91     0.7179     0.7001     1.6536    235.4231    0.0006  
   1350     7539.17     0.7202     0.6953     1.9854    323.1024    0.0027  
   1400     7818.24     0.7109     0.6875     1.5779    498.492     0.0016  
   1450     8096.79     0.7102     0.6912     1.7858    42.0013     0.0051  
   1500     8375.64     0.7109     0.6922     1.7893   1124.4835    0.0007  
   1550     8654.43     0.7094     0.6877     2.0259   5235.1592    0.0094  
   1600     8933.36     0.7123     0.6984     1.9812   2811.8492    0.0058  
   1650     9212.95     0.712      0.6851     1.7899    581.5052    0.0021  
   1700     9491.72     0.7073     0.6872     1.4826    38.3429     0.0003  
   1750     9769.86     0.7097     0.6818     1.658     27.2678     0.0002  
   1800     10047.62    0.7022     0.6854     1.5061    12.5892     0.0015  
   1850     10326.99    0.7052     0.6813     1.3231    21.0211     0.0031  
   1900     10605.47    0.714      0.6905     1.1873    13.6215     0.0026  
   1950     10883.5     0.7057     0.6902     1.1999     8.8238     0.0044  
   2000     11165.29    0.707      0.6853     1.1917     8.816      0.0018  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       151.36     1.4226     1.4146     1.0046     0.0055     0.0019  
   100       498.99     1.4048     1.4157     1.1034     0.0064     0.0001  
   150       847.88     1.3912     1.3948     1.1013     0.0069     0.0001  
   200      1198.93     1.0697     1.0754     1.4817     0.0076     0.0006  
   250      1548.14     0.7347     0.725      1.5541     0.0314     0.0015  
   300       1896.3      0.76      0.7461     1.4145     0.059      0.0002  
   350       2243.7     0.7416     0.7338     1.4114     0.055      0.0003  
   400      2591.29     0.7384     0.7348     1.378      0.0641     0.0023  
   450      2938.12     0.7541     0.7423     1.248      0.0665     0.0037  
   500      3285.18     0.7318     0.7208     1.1873     0.0625     0.0012  
   550      3626.89     0.7344     0.7217     1.1839     0.0687     0.0055  
   600      3964.01     0.726      0.7173     1.1695     0.0691     0.0041  
   650      4300.88     0.7318     0.718      0.9494     0.0722     0.0027  
   700      4637.46     0.7292     0.7208     1.0832     0.0756     0.0005  
   750      4974.61     0.7485     0.7374     1.0115     0.0765     0.0019  
   800      5310.39     0.7306     0.719      0.9898     0.0783     0.0012  
   850      5646.92     0.7292     0.7188     1.1552     0.0862     0.0035  
   900      5982.77     0.7323     0.7207     0.9067     0.1002     0.0017  
   950      6318.12     0.7239     0.7148     0.9954     0.1034     0.0011  
   1000     6653.83     0.7339     0.7208     1.129      0.1194     0.0072  
   1050     6988.93     0.7343     0.7183     0.9238     0.1189     0.0024  
   1100     7323.28     0.7303     0.7153     1.0883     0.1144     0.002   
   1150     7659.01     0.7233     0.7144     0.9472     0.1449     0.0002  
   1200     7993.99     0.7331     0.7262     0.9925     0.1525     0.0017  
   1250      8328.7     0.7343     0.718      0.8274     0.1764     0.0043  
   1300      8666.1     0.7386     0.7207     0.8197     0.1697     0.0022  
   1350      9000.5     0.7311     0.7226     0.9084     0.1937     0.0014  
   1400     9334.87     0.7429     0.7296     0.8586     0.2326     0.0018  
   1450     9668.54     0.7368     0.7239     1.0282     0.2126     0.0009  
   1500     10002.68    0.7242     0.7172     0.8655     0.2252     0.0012  
   1550     10338.1     0.7271     0.7199     0.7585     0.2658     0.0006  
   1600     10672.73    0.7341     0.7244     0.9263     0.323      0.0028  
   1650     11006.83    0.733      0.7185     0.8498     0.3473     0.001   
   1700     11340.3     0.7302     0.723      0.7878     0.4443     0.003   
   1750     11675.2     0.7298     0.721      0.6951     0.4177     0.0027  
   1800     12009.8     0.7357     0.7224     0.8335     0.5132     0.0005  
   1850     12343.05    0.7281     0.7223     0.8189     0.5918     0.0006  
   1900     12676.13    0.7356     0.719      0.8618     0.6846     0.0006  
   1950     13009.03    0.7345     0.7216     0.8023     0.7319     0.001   
   2000     13341.43    0.7335     0.7231     0.9475     0.8627     0.003   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       149.08     1.4151     1.415      0.6776     0.0037     0.0002  
   100       549.32     1.4141     1.4123     0.8438     0.004      0.0005  
   150       950.34     1.4166     1.4153     0.9097     0.004      0.0002  
   200       1351.5     1.4166     1.4119     0.6357     0.0041     0.0014  
   250      1752.41     1.4121     1.4081     0.7155     0.004      0.0007  
   300      2155.45     1.4191     1.4096     0.7599     0.0041     0.0002  
   350      2557.97     1.3177     1.3135     0.9055     0.0042     0.0004  
   400       2959.8     0.849      0.8393     0.9042     0.0067     0.0003  
   450       3362.3     0.7415     0.7309     0.8508     0.0218     0.0013  
   500      3763.48     0.7318     0.7167     0.745      0.0409     0.0028  
   550      4165.05     0.7394     0.7252     0.7322     0.057      0.0018  
   600      4565.61     0.723      0.7111     0.686      0.062      0.0031  
   650      4967.04     0.7435     0.7292     0.6883     0.0626     0.0008  
   700      5366.96     0.7377     0.7248     0.6171     0.0704     0.0023  
   750      5767.46     0.7331     0.7265     0.6269     0.0845     0.0017  
   800      6168.36     0.7308     0.7236     0.5781     0.085      0.0054  
   850      6568.71     0.7288     0.721      0.6556     0.091      0.0011  
   900      6968.84     0.7446     0.7313     0.6176     0.094      0.0005  
   950      7366.79     0.7351     0.7207     0.5573     0.1203     0.0002  
   1000      7768.5     0.7342     0.7257     0.5576     0.1125     0.0008  
   1050     8168.13     0.7308     0.7195     0.5269     0.1135     0.0003  
   1100     8568.62     0.7298     0.7247     0.4539     0.1392     0.0021  
   1150     8968.33     0.7334     0.7243     0.5155     0.1316     0.0036  
   1200     9367.93     0.7269     0.7163     0.5348     0.1601     0.0037  
   1250     9766.58     0.7334     0.7241     0.5096     0.1866     0.0021  
   1300     10163.95    0.7316     0.7204     0.5062     0.2061     0.0046  
   1350     10564.25    0.7327     0.7207     0.4645     0.2434     0.0023  
   1400     10964.7     0.7353     0.7222     0.494      0.2647     0.0025  
   1450     11364.16    0.7284     0.7176     0.5332     0.3366     0.003   
   1500     11764.47    0.7293     0.7147     0.486      0.312      0.0002  
   1550     12164.52    0.7281     0.7206     0.5152     0.3762     0.0046  
   1600     12564.79    0.7333     0.7172     0.491      0.4666     0.0001  
   1650     12965.62    0.7295     0.7193     0.4506     0.4544     0.0029  
   1700     13366.5     0.7364     0.7254     0.498      0.6246     0.0029  
   1750     13768.16    0.7388     0.7248     0.6005     0.7513     0.0099  
   1800     14169.6      0.73      0.7267     0.6838     1.0057     0.002   
   1850     14570.14    0.7377     0.7203     0.7187     0.9041     0.0019  
   1900     14970.94    0.7305     0.7191     0.8374     1.386      0.0052  
   1950     15371.24    0.7294     0.7187     0.7609     1.2892     0.0005  
   2000     15772.56    0.7274     0.714      0.9653     1.7279     0.0015  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       149.8      1.4108     1.4174     0.4968     0.0026     0.0025  
   100       606.26     1.4089     1.4119     0.5527     0.0029      0.0    
   150      1066.11     1.4113     1.4122     0.6072     0.0031      0.0    
   200      1527.62     1.4189     1.4143     0.6871     0.0031     0.0006  
   250      1987.44     1.412      1.4144     0.6908     0.0032     0.0005  
   300      2447.46     1.4174     1.4113     0.6024     0.0031     0.0004  
   350      2910.31     1.4142     1.4153     0.5508     0.003      0.0004  
   400      3372.99     1.416      1.4115     0.6305     0.0031     0.0003  
   450      3836.82     1.4039     1.4127     0.6095     0.003      0.0003  
   500      4299.88     1.4135     1.4106     0.6116     0.003      0.0003  
   550      4761.66     1.4054     1.4091     0.5277     0.003      0.0006  
   600      5223.65     1.3921     1.3827     0.6892     0.0032     0.0001  
   650      5687.31     1.1644     1.1643     0.9839     0.0032     0.0003  
   700      6150.04     0.7405     0.731      1.0726     0.008      0.0014  
   750      6611.89     0.7375     0.7244     1.0148     0.0271     0.0006  
   800      7074.17     0.7346     0.7248     0.9842     0.0333     0.0065  
   850      7535.01     0.7328     0.7242     0.9915     0.0341     0.0043  
   900      7993.52     0.7359     0.7203     0.9713     0.0359     0.007   
   950      8451.82     0.7267     0.7136     0.8791     0.0427     0.0045  
   1000     8909.72     0.7292     0.713      0.8349     0.0429     0.002   
   1050     9367.24     0.7319     0.7242     0.9126     0.0454     0.0069  
   1100     9823.01     0.7302     0.7194     0.8172     0.041      0.0004  
   1150     10279.59    0.7322     0.7173     0.8287     0.0479     0.0011  
   1200     10736.48    0.7307     0.7163     0.775      0.0498     0.0046  
   1250     11193.03    0.7312     0.7169     0.9215     0.0534     0.0037  
   1300     11648.87    0.7239     0.7114     0.8106     0.0491     0.0017  
   1350     12105.05    0.7315     0.7226     0.8591     0.0546     0.0042  
   1400     12560.4     0.7305     0.7147     0.7668     0.0529     0.0003  
   1450     13016.41    0.7321     0.7197     0.8817     0.0607     0.0035  
   1500     13472.16    0.7329     0.7194     0.768      0.0622     0.0019  
   1550     13926.62    0.7264     0.7167     0.8518     0.0545     0.0056  
   1600     14381.32    0.7292     0.7165     0.8316     0.076      0.0013  
   1650     14837.11    0.7264     0.7119     0.9209     0.0682     0.0019  
   1700     15290.64    0.7295     0.7138     0.9465     0.0636     0.0118  
   1750     15742.01    0.7242     0.7166     0.8085     0.0671     0.0021  
   1800     16195.26    0.7285     0.7165     1.0887     0.0667     0.003   
   1850     16648.12    0.7228     0.7088     0.9254     0.0673     0.0016  
   1900     17101.19    0.7294     0.7148     1.0643     0.0737     0.0026  
   1950     17553.72    0.7197     0.7103     1.101      0.0719     0.0044  
   2000     18005.11    0.7292     0.7184     0.8966     0.0728     0.0053  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       150.08     1.4209     1.4114     0.4458     0.0021     0.0017  
   100       665.91     1.4131     1.4118     0.3942     0.0027     0.0001  
   150      1186.13     1.4079     1.4163     0.4086     0.0025     0.0007  
   200      1706.85     1.4084     1.4141      0.4       0.0028     0.001   
   250      2228.89     1.4128     1.4136     0.474      0.0026     0.0003  
   300      2752.02     1.4145     1.4149     0.6099     0.0025     0.0001  
   350      3276.01     1.4175     1.4162     0.528      0.0024     0.0002  
   400      3799.11     1.4164     1.4115     0.5004     0.0025     0.0006  
   450      4321.88     1.4053     1.4174     0.4307     0.0024      0.0    
   500      4845.79      1.41      1.4126     0.4973     0.0025     0.0005  
   550      5370.44     1.4126     1.4174     0.5156     0.0026     0.0005  
   600      5895.63     1.4072     1.4058     0.4716     0.0026     0.0006  
   650      6419.67     1.3866     1.381      0.6437     0.0026     0.0001  
   700      6944.52     1.1092     1.1119     0.8108     0.003      0.001   
   750      7471.11     0.7459     0.7366     0.8236     0.0074     0.0006  
   800      7993.75     0.7356     0.7295     0.736      0.025      0.0003  
   850      8514.31     0.7391     0.7251     0.725      0.0288     0.002   
   900      9035.09     0.7311      0.72      0.7298     0.0354     0.0051  
   950      9555.13     0.7308     0.7136     0.6972     0.0349     0.0082  
   1000     10075.13    0.7381     0.7256     0.6772     0.0463     0.0055  
   1050     10597.3     0.7298     0.7157     0.7109     0.0481     0.0047  
   1100     11116.18     0.73      0.7205     0.6243     0.0527     0.0061  
   1150     11635.04    0.7318     0.7209     0.5989     0.0504     0.0047  
   1200     12156.23    0.7341     0.7201     0.5701     0.049      0.005   
   1250     12676.01    0.7305     0.7216     0.6029     0.0671     0.0024  
   1300     13195.83    0.7401     0.7287     0.585      0.0785     0.0018  
   1350     13716.08    0.7341     0.7212     0.5004     0.0695     0.0028  
   1400     14236.14    0.7266     0.7162     0.6114     0.0734     0.0014  
   1450     14755.32    0.7297     0.7162     0.5338     0.0702     0.0074  
   1500     15276.32    0.7321     0.7209     0.5464     0.0659     0.0036  
   1550     15796.14    0.7323     0.719      0.4821     0.0826     0.0047  
   1600     16316.33    0.7298     0.7165     0.5599     0.0809     0.0021  
   1650     16835.07    0.7282     0.7197     0.5186     0.0904     0.0003  
   1700     17354.26    0.727      0.713       0.39      0.1005     0.0072  
   1750     17874.99    0.7295     0.715      0.4794     0.0964     0.006   
   1800     18394.16    0.7311     0.7184     0.4823     0.1017     0.0134  
   1850     18914.11    0.7322     0.7189     0.5434     0.1427     0.0008  
   1900     19434.92    0.728      0.7175     0.5165     0.1308     0.0072  
   1950     19953.37    0.7243     0.7171     0.4492     0.1149     0.0012  
   2000     20472.18    0.7345     0.7177     0.3945     0.155      0.0033  
