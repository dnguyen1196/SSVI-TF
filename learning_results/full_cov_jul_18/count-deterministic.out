Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  7.5960681438446045
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       102.2      4.2449     4.2785     0.3162     4.4199   128892.15  104489.81  
   100       396.86     4.2406     4.2742     0.3124     0.2421   127982.58  103750.83  
   150       690.08     4.2244     4.2578     0.3042     0.0488   124989.29  101315.39  
   200       982.31     4.1857     4.2188     0.2942     0.0242   118835.47   96306.98  
   250      1274.38     4.1108     4.1439     0.286      0.0259   109159.29   88452.7   
   300      1566.99     3.9889     4.0199     0.2823     0.022     96894.4    78497.55  
   350      1859.41     3.8104     3.8407     0.2758     0.0186    83528.64   67655.75  
   400      2151.49     3.5833     3.6122     0.2607     0.0161    70839.28   57375.39  
   450      2443.81     3.3261     3.3538     0.2541     0.0132    59975.83   48577.68  
   500      2736.81     3.0667     3.0907     0.2181     0.0107    51552.99   41740.21  
   550      3029.13     2.8138     2.8352     0.2344     0.011     45138.68   36540.14  
   600      3321.17     2.588      2.6068     0.181      0.0127    40568.3    32824.25  
   650       3613.1     2.3872     2.4022     0.1933     0.0114    37238.21   30119.65  
   700      3904.43     2.2058     2.2197     0.1599     0.0126    34774.65   28117.68  
   750      4195.84     2.0486     2.0604     0.1219     0.0121    32962.51   26650.14  
   800      4487.14     1.9144     1.9238     0.1764     0.0114    31632.74   25562.72  
   850      4778.46     1.7951     1.8031     0.2054     0.011     30609.96   24726.91  
   900      5068.64     1.689      1.6945     0.2582     0.0106    29765.64   24049.92  
   950       5359.1     1.596      1.6001     0.1178     0.0105    29131.7    23535.11  
   1000     5650.14     1.515      1.5166     0.1867      0.01     28614.15   23108.75  
   1050     5940.97     1.445      1.4452     0.1933      0.01     28199.88   22770.71  
   1100     6231.58     1.384      1.3817     0.1041     0.0097    27871.43   22506.41  
   1150      6521.9     1.3316     1.3299     0.0796     0.0094    27615.94   22299.99  
   1200     6812.67     1.2851     1.2809     0.094      0.0091    27401.56   22126.08  
   1250     7103.57     1.2448     1.2394     0.1103     0.009     27235.43   21984.21  
   1300     7394.43     1.2111     1.2042     0.1927     0.0088    27094.72   21877.06  
   1350     7685.43     1.1807     1.1747     0.1601     0.0083    26976.58   21781.28  
   1400     7975.87     1.153      1.1458     0.1564     0.008     26878.36   21692.0   
   1450     8266.82     1.1302     1.1223     0.1396     0.0076    26801.12   21629.68  
   1500     8557.55     1.1087     1.0998     0.1745     0.0074    26700.76   21556.44  
   1550     8847.97     1.0917     1.0838     0.1181     0.0072    26647.78   21516.06  
   1600     9139.71     1.0772     1.067      0.2048     0.0069    26598.8    21471.4   
   1650     9432.12     1.0633     1.0515     0.1679     0.0067    26547.77   21431.82  
   1700     9723.08     1.0516     1.0392     0.141      0.0065    26509.06   21394.54  
   1750     10013.68    1.0405     1.0291     0.1042     0.0062    26473.58   21371.46  
   1800     10303.97    1.0309     1.0183     0.1115     0.0061    26465.78   21355.04  
   1850     10595.84    1.0241     1.0106     0.1098     0.0058    26437.03   21333.86  
   1900     10887.54    1.0158     1.0036     0.1276     0.0057    26402.87   21314.32  
   1950     11179.2     1.0077     0.9955     0.0852     0.0056    26380.42   21293.7   
   2000     11471.2     1.0011     0.9902     0.1174     0.0054    26376.15   21288.17  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       100.77     4.1999     4.2224     0.058      3.6181    87709.58  141753.91  
   100       476.8      4.2351     4.2571     0.0289     0.8463   110766.87  179068.04  
   150       853.47      4.24      4.2624     0.0275     0.3973   117925.98  190602.25  
   200      1230.15     4.2416     4.2639     0.025      0.2473   121140.01  195778.35  
   250      1605.67     4.2426     4.2653     0.0254     0.1771   122916.27  198638.71  
   300      1981.21     4.2429     4.2657     0.0278     0.1342    124045.1  200454.38  
   350      2356.19     4.243      4.2661     0.0284     0.1064   124809.94  201685.18  
   400      2730.45     4.2435     4.2662     0.0294     0.0876   125366.83   202580.7  
   450      3104.02     4.2437     4.2664     0.0312     0.0731   125787.62  203257.56  
   500      3477.46     4.2437     4.2664     0.0316     0.0632   126115.58  203784.95  
   550      3850.52     4.2438     4.2667     0.0324     0.055    126377.42  204205.99  
   600       4223.7     4.2439     4.2667     0.0302     0.0484   126591.37  204549.97  
   650       4597.7     4.244      4.2668     0.032      0.0424    126767.8  204833.58  
   700      4970.12     4.244      4.2667     0.0314     0.0385   126913.63  205067.95  
   750      5344.39     4.244      4.2669     0.032      0.0345   127035.37  205263.63  
   800      5717.28     4.2439     4.2668     0.0329     0.0317   127136.23  205425.63  
   850      6091.24     4.2441     4.2667     0.0369     0.0289   127218.79  205558.22  
   900      6465.97     4.244      4.2667     0.0363     0.0265   127286.21   205666.4  
   950       6839.4     4.2439     4.2667     0.0389     0.0242   127337.51  205748.69  
   1000     7213.33     4.2435     4.2665     0.0363     0.0228   127374.47  205807.81  
   1050     7586.36     4.2437     4.2664     0.0428     0.0213   127395.18  205840.83  
   1100     7960.81     4.2434     4.2662     0.0416     0.0196   127399.52  205847.43  
   1150      8338.2     4.2432     4.266      0.0432     0.0185    127383.6  205821.32  
   1200     8711.54     4.2428     4.2656     0.0452     0.0171   127344.94  205758.64  
   1250     9085.49     4.2424     4.2651     0.049      0.0162   127276.96  205648.46  
   1300     9458.92     4.2416     4.2645     0.0519     0.0153   127174.04  205481.68  
   1350      9832.8     4.2407     4.2634     0.0552     0.0145   127022.82  205237.11  
   1400     10206.33    4.2394     4.2623      0.06      0.0138   126803.85   204882.8  
   1450     10580.35    4.2377     4.2604     0.0672     0.0129   126500.73  204392.25  
   1500     10955.23    4.2354     4.2579     0.0747     0.0126   126075.34  203704.29  
   1550     11330.8     4.232      4.2546     0.0879     0.012    125475.61  202735.63  
   1600     11704.72    4.2271     4.2496     0.0949     0.0118   124614.14  201341.43  
   1650     12077.08    4.2199     4.2423     0.1078     0.0116   123373.07  199331.64  
   1700     12449.7     4.2088     4.2311     0.1187     0.0113    121555.4  196388.55  
   1750     12822.27    4.192      4.2139     0.1348     0.0117   118907.79  192104.64  
   1800     13196.14    4.1661     4.1883     0.1678     0.0127   115158.57  186040.59  
   1850     13570.9     4.1274     4.1494     0.1752     0.0139   110071.95  177829.22  
   1900     13946.3     4.0706     4.092      0.1827     0.0162   103486.13  167189.33  
   1950     14323.54    3.9901     4.0118     0.197      0.0188    95588.06  154445.57  
   2000     14702.1     3.8835     3.9045     0.1935     0.0199    86899.09  140415.73  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       100.88     4.1798     4.1945     0.0663     3.4551    80191.57  193723.24  
   100       559.25     4.2292     4.2437     0.0312     0.9197   104494.58  252452.16  
   150       1016.8     4.2369     4.2527     0.0247     0.4202   114004.27  275346.97  
   200      1474.63     4.2402     4.2552     0.0224     0.2672   118426.36  285981.36  
   250      1932.28     4.2411     4.2565     0.0205     0.1895    120903.0  291932.25  
   300      2390.85     4.242      4.2572     0.0215     0.145    122466.91  295689.09  
   350      2848.69     4.2429     4.258      0.0219     0.1159   123537.55  298260.12  
   400      3306.08     4.243      4.2583     0.0247     0.0954   124309.51  300113.02  
   450      3762.02     4.2434     4.2585     0.0234     0.0785    124893.3  301514.65  
   500      4219.55     4.2433     4.2586     0.0219     0.0672   125351.28  302613.72  
   550      4677.53     4.2437     4.2589     0.0245     0.0587   125717.29  303492.46  
   600      5134.73     4.2437     4.259      0.023      0.0522   126018.16  304214.46  
   650      5591.91     4.244      4.2591     0.024      0.046    126266.77  304811.05  
   700      6050.23     4.244      4.2592     0.023      0.0421   126477.19  305315.84  
   750      6506.81     4.2442     4.2592     0.0232     0.0373   126657.67  305748.76  
   800      6963.98     4.2441     4.2593     0.0243     0.0342   126812.87  306121.22  
   850      7421.35     4.2441     4.2593     0.0243     0.0315   126947.22   306443.7  
   900      7877.48     4.2441     4.2595     0.0247     0.029    127065.61  306727.81  
   950      8334.31     4.244      4.2594     0.0239     0.0268   127168.89   306975.6  
   1000     8790.28     4.2443     4.2594     0.0245     0.0247   127259.52  307193.07  
   1050     9247.05     4.2443     4.2595     0.0246     0.0235   127338.74  307383.13  
   1100     9703.94     4.2441     4.2594     0.0254     0.0217   127408.05  307549.37  
   1150     10160.04    4.2443     4.2595     0.0262     0.0207   127468.23  307693.72  
   1200     10615.99    4.2442     4.2595     0.0269     0.0192   127520.13  307818.11  
   1250     11072.29    4.2441     4.2594     0.0273     0.0182   127563.43   307921.8  
   1300     11529.46    4.2441     4.2593     0.0294     0.0172   127598.93  308006.68  
   1350     11986.51    4.2441     4.2591     0.0302     0.0162   127625.45  308070.01  
   1400     12443.58    4.2438     4.2591     0.0328     0.0154   127642.61  308110.87  
   1450     12900.67    4.2437     4.2589     0.0293     0.0148   127648.88  308125.46  
   1500     13357.93    4.2435     4.2587     0.0308     0.014    127642.19  308108.83  
   1550     13814.99    4.2433     4.2586     0.0357     0.0133   127619.46   308053.5  
   1600     14272.44    4.2429     4.2581     0.0382     0.0128   127576.22  307948.49  
   1650     14729.82    4.2425     4.2577     0.0408     0.0123   127507.68  307782.88  
   1700     15186.93    4.2418     4.257      0.048      0.0118   127400.55  307524.23  
   1750     15644.07    4.2408     4.2561     0.0561     0.0115   127241.78  307141.01  
   1800     16102.42    4.2394     4.2548     0.0576     0.0112   127005.15  306569.77  
   1850     16559.92    4.2375     4.2527     0.0603     0.0107    126648.2  305708.44  
   1900     17018.44    4.2344     4.2496     0.0695     0.0106   126104.62   304395.5  
   1950     17476.57    4.2298     4.2449     0.0868     0.0104   125277.84  302402.74  
   2000     17933.94    4.2224     4.2375     0.0893     0.0105   123998.84  299317.79  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       101.2      4.1669     4.1814     0.0925     3.3579    77436.49  249360.02  
   100       642.11     4.2252     4.2386      0.04      1.0365   101688.67  327451.34  
   150      1182.81     4.2358     4.2488     0.0306     0.419    112128.05  360983.41  
   200      1724.81     4.2396     4.2526     0.0247     0.2787   117090.77  376897.66  
   250      2265.57     4.2413     4.2538     0.0249     0.1944   119897.33  385893.57  
   300      2804.99     4.2419     4.2549     0.0216     0.1505   121676.06  391597.42  
   350      3344.55     4.2424     4.2554     0.0249     0.1176   122894.87  395503.18  
   400      3883.04     4.243      4.2558     0.0214     0.0971   123778.94  398337.16  
   450      4422.15     4.2432     4.2561     0.0224     0.0819   124448.48  400482.35  
   500      4962.09     4.2434     4.2562     0.0211     0.0706   124970.24  402154.59  
   550      5501.45     4.2437     4.2565     0.0229     0.0609   125387.32  403490.34  
   600      6041.55     4.2436     4.2567     0.0265     0.054    125727.82  404580.99  
   650      6580.37     4.2437     4.2568     0.0234     0.0477   126011.48  405489.44  
   700      7119.26     4.2441     4.2568     0.023      0.0426   126251.23  406257.42  
   750      7659.59     4.2441     4.2568     0.0246     0.0393   126455.74  406912.48  
   800      8199.84     4.2441     4.257      0.0249     0.0353   126633.19  407480.84  
   850      8739.45     4.2443     4.2569     0.0225     0.0325   126787.27   407974.3  
   900      9278.81     4.2443     4.257      0.0222     0.0298   126922.44  408407.07  
   950      9819.18     4.2443     4.2571     0.0221     0.0276   127041.03   408786.8  
   1000     10358.3     4.2442     4.2572     0.0217     0.0259   127146.12  409123.31  
   1050     10896.7     4.2444     4.2572     0.023      0.0244   127239.14  409421.05  
   1100     11435.63    4.2445     4.2572     0.0228     0.0226   127321.58  409684.76  
   1150     11975.38    4.2442     4.2572     0.0235     0.0213   127394.37   409917.8  
   1200     12515.4     4.2444     4.2572     0.0244     0.0198   127458.28  410122.24  
   1250     13056.38    4.2443     4.2571     0.0251     0.0192   127514.09  410300.67  
   1300     13597.43    4.2442     4.2572     0.0232     0.018    127561.63  410452.61  
   1350     14138.2     4.2442     4.2571     0.0252     0.0172   127601.53  410579.98  
   1400     14678.46    4.2441     4.257      0.0251     0.0161   127632.28  410678.21  
   1450     15216.67    4.2439     4.2569     0.0292     0.0153    127654.0  410747.11  
   1500     15756.34    4.2438     4.2568     0.0298     0.0148   127666.02  410784.97  
   1550     16294.64    4.2437     4.2567     0.0302     0.014    127665.82   410783.6  
   1600     16832.5     4.2435     4.2564     0.0372     0.0133   127649.21  410729.21  
   1650     17370.08    4.2432     4.2561     0.0387     0.013    127613.67  410613.95  
   1700     17907.4     4.2428     4.2556     0.0399     0.0124   127552.23  410415.54  
   1750     18444.48    4.2422     4.2551     0.0417     0.012    127454.87  410100.97  
   1800     18981.42    4.2413     4.2542     0.0523     0.0116   127304.83  409617.52  
   1850     19519.64     4.24      4.2528     0.0565     0.0115   127075.09  408878.05  
   1900     20056.5     4.2378     4.2508     0.0665     0.0111   126715.16  407720.02  
   1950     20591.93    4.2349     4.2477     0.0741     0.0111   126151.84  405907.71  
   2000     21127.35    4.2296     4.2426     0.0897     0.011    125252.47  403015.26  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       100.4      4.1621     4.1807     0.0885     3.3187    76047.17   306553.6  
   100       722.45     4.2242     4.2425     0.0498     0.964     100262.9  404270.27  
   150      1343.61     4.235      4.2545     0.0353     0.4397    111114.3  447998.26  
   200      1962.62     4.2389     4.2581     0.0289     0.284    116396.78  469268.08  
   250      2583.48     4.241      4.2598     0.0266     0.1994   119371.08  481248.05  
   300       3205.6     4.242      4.2609     0.0258     0.1541   121268.13   488886.7  
   350       3825.8     4.2423     4.2615     0.0222     0.1208   122564.43   494104.1  
   400      4446.41     4.2429     4.2621     0.0224     0.0983   123501.38  497875.96  
   450      5066.73     4.2434     4.2623     0.0231     0.0825    124207.6  500718.01  
   500      5687.72     4.2436     4.2625     0.0232     0.0724    124760.1   502941.3  
   550      6309.64     4.2436     4.2628     0.0232     0.062    125204.63  504730.43  
   600      6928.89     4.2436     4.2629     0.0218     0.055    125568.29  506193.92  
   650      7548.37     4.2439     4.263      0.0223     0.0492   125871.83  507414.81  
   700      8167.77     4.2438     4.2632     0.0246     0.0438   126128.19  508446.39  
   750      8787.17     4.244      4.2631     0.0207     0.0398   126347.75  509329.39  
   800      9406.79     4.2441     4.2632     0.0218     0.0361   126537.04  510090.46  
   850      10025.23    4.2441     4.2633     0.0255     0.0329   126701.55  510752.17  
   900      10643.5     4.2441     4.2633     0.024      0.0309   126845.55  511331.26  
   950      11262.48    4.2443     4.2634     0.0212     0.0284   126972.93  511843.77  
   1000     11882.09    4.2443     4.2634     0.0233     0.0263    127085.3  512295.67  
   1050     12501.87    4.2443     4.2635     0.0212     0.0245   127184.82  512695.81  
   1100     13121.17    4.2444     4.2635     0.0236     0.0232   127273.83  513053.73  
   1150     13740.47    4.2443     4.2634     0.0215     0.0215   127352.92  513371.53  
   1200     14361.16    4.2442     4.2635     0.0246     0.0204   127423.11  513653.58  
   1250     14980.37    4.2444     4.2635     0.0266     0.0192    127484.5  513900.39  
   1300     15600.51    4.2444     4.2635     0.0262     0.0184   127537.88  514114.83  
   1350     16221.03    4.2443     4.2635     0.0268     0.0173   127582.79  514295.26  
   1400     16840.13    4.2442     4.2634     0.0239     0.0165   127619.65  514443.41  
   1450     17458.61    4.2441     4.2633     0.0274     0.0159   127647.71  514555.54  
   1500     18076.59    4.244      4.2632     0.0293     0.0153   127665.37  514625.92  
   1550     18693.62    4.2438     4.2632     0.0298     0.0145   127669.77  514642.86  
   1600     19312.96    4.2436     4.2628     0.0336     0.0138   127661.26  514607.53  
   1650     19931.91    4.2433     4.2626     0.0356     0.0134   127631.57  514487.09  
   1700     20550.16    4.2429     4.2621     0.0357     0.0128   127578.16  514271.11  
   1750     21169.86    4.2423     4.2615     0.0436     0.0124   127489.29  513911.98  
   1800     21790.44    4.2415     4.2607     0.0471     0.0121   127349.03   513346.6  
   1850     22409.07    4.2402     4.2595     0.0584     0.0118   127122.94  512435.05  
   1900     23028.79    4.2382     4.2574     0.0701     0.0116   126765.85  510992.36  
   1950     23648.15    4.235      4.2542     0.0786     0.0115   126177.63  508616.22  
   2000     24267.66    4.2297     4.2488     0.0966     0.0115   125221.08  504756.39  
