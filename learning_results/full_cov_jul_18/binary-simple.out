Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.301404237747192
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       95.23      1.2336     1.1834     3.1623     3.9773     0.001   
   100       332.01     0.7709     0.7386     3.1362     0.2472     0.0001  
   150       565.4      0.7595     0.7211     2.6266     0.1663     0.0001  
   200       797.71     0.7134     0.6542     2.2909     0.1253      0.0    
   250      1030.55     0.6787     0.6161     2.1539     0.1054      0.0    
   300      1263.24     0.6622     0.595      2.1162     0.0956      0.0    
   350      1496.22     0.6462     0.5711     2.2251     0.0893      0.0    
   400      1728.87     0.6407     0.5561     2.0437     0.0652      0.0    
   450       1962.1     0.6592     0.5506     1.9308     0.0617      0.0    
   500      2194.88     0.617      0.528      1.8647     0.0816      0.0    
   550      2428.22     0.6278     0.5085     2.1836     0.0512      0.0    
   600      2661.91     0.6148     0.501      1.9446     0.0582      0.0    
   650       2896.2     0.6127     0.4905     1.7598     0.0459      0.0    
   700      3130.11     0.6095     0.4883     1.8022     0.0478      0.0    
   750      3363.75     0.602      0.4746     1.7967     0.0452      0.0    
   800      3597.93     0.6076     0.4671     1.6641     0.0417      0.0    
   850      3832.06     0.5928     0.4643     2.0194     0.0485      0.0    
   900      4065.41     0.5917     0.4488     2.2157     0.0386      0.0    
   950      4298.77     0.5896     0.4488     2.0618     0.0371      0.0    
   1000     4532.37     0.5997     0.4626     1.8259     0.0376      0.0    
   1050     4765.56     0.5971     0.4667     2.2236     0.0384      0.0    
   1100      4998.4     0.5968     0.4514     2.0397     0.0346      0.0    
   1150      5231.8     0.6076     0.479      2.2081     0.0368      0.0    
   1200     5463.15     0.607      0.4737     2.3429     0.035       0.0    
   1250     5693.35     0.6023     0.4932     2.3242     0.0363      0.0    
   1300     5923.93     0.6091     0.4735     2.3435     0.034       0.0    
   1350     6154.16      0.6       0.4815     2.2412     0.0345      0.0    
   1400     6383.72     0.6081     0.4919     2.8188     0.0356      0.0    
   1450      6612.7     0.5918     0.4779     2.4717     0.0396      0.0    
   1500     6841.83     0.6099     0.4881     2.4232     0.0398      0.0    
   1550      7070.9     0.6053     0.5171     2.7678     0.0395      0.0    
   1600     7299.91     0.5997     0.4767     2.5661     0.0388      0.0    
   1650     7528.98     0.5991     0.5034     2.4066     0.0493      0.0    
   1700     7757.98     0.6053     0.4938     2.777      0.0441      0.0    
   1750     7985.64     0.5911     0.4934     2.5511     0.0509      0.0    
   1800     8213.08     0.5985     0.4781     2.3948     0.0456      0.0    
   1850     8439.57     0.586      0.4831     2.5255     0.0399      0.0    
   1900     8666.05     0.5902     0.4707     2.4621     0.0374      0.0    
   1950     8891.64     0.5799     0.4675     2.4916     0.0426      0.0    
   2000     9116.98     0.5854     0.4697     2.5191     0.0481      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       94.27      1.4148     1.4117     0.2988     1.2169      0.0    
   100       387.57     1.4137     1.4145     0.1753     0.793      0.0001  
   150       680.95      1.41      1.4114     0.1431     0.5016     0.0001  
   200       975.25     1.417      1.4169     0.1392     0.317      0.0001  
   250      1267.97     1.4201     1.4114     0.1863     0.2117     0.0001  
   300      1561.01     1.4099     1.4122     0.1089     0.1535     0.0001  
   350      1856.24     1.4166     1.4129     0.0841     0.1155     0.0001  
   400      2151.28     1.4123     1.419      0.1118     0.0914     0.0001  
   450       2446.7     1.4039     1.4143     0.131      0.073       0.0    
   500       2743.9     1.3985     1.3981     0.1482     0.0601      0.0    
   550      3041.11     1.3765     1.376      0.2148     0.0512      0.0    
   600       3340.3     1.2798     1.2787     0.3497     0.0452      0.0    
   650      3636.69     0.9187     0.9126     0.7344     0.0508      0.0    
   700       3929.6     0.7369     0.7326     0.9031     0.0935      0.0    
   750       4218.3     0.7177     0.7089     0.8824     0.0976      0.0    
   800      4505.21     0.735      0.7196     0.7464     0.0613      0.0    
   850      4791.13     0.7321     0.7194     0.6367     0.0343      0.0    
   900      5076.76     0.7306     0.7174     0.6566     0.0254      0.0    
   950       5360.9     0.7357     0.7252     0.6706     0.0223      0.0    
   1000     5644.62     0.7306     0.7181     0.6525     0.0185      0.0    
   1050     5929.08     0.7384     0.7278     0.6887     0.0172      0.0    
   1100     6211.68     0.7176     0.7083     0.6696     0.0159      0.0    
   1150     6496.56     0.7258     0.7198     0.6733     0.0151      0.0    
   1200     6780.04     0.718      0.7099     0.5261     0.0138      0.0    
   1250     7064.47     0.736      0.7312     0.6238     0.0135      0.0    
   1300     7349.37     0.7363     0.7197     0.5537     0.0122      0.0    
   1350     7634.16     0.7229     0.7135     0.5644     0.0143      0.0    
   1400     7919.67     0.7278     0.7114     0.5347     0.0122      0.0    
   1450     8204.95     0.7142     0.7051     0.5428     0.0108      0.0    
   1500     8490.13     0.7252     0.7139     0.6989     0.0118      0.0    
   1550      8776.2     0.7209     0.7062     0.4939     0.0108      0.0    
   1600     9062.83     0.7203     0.7102     0.3492     0.0094      0.0    
   1650     9349.91     0.725      0.7063     0.3505     0.0095      0.0    
   1700      9636.3     0.7149     0.7029     0.4582     0.0104      0.0    
   1750     9923.01     0.7054     0.6892     0.5496     0.0088      0.0    
   1800     10210.57    0.7064     0.6918     0.5397     0.0096      0.0    
   1850     10499.79    0.7067     0.6896     0.4537     0.0086      0.0    
   1900     10787.05    0.7009     0.6828     0.5038     0.0095      0.0    
   1950     11075.6     0.7028     0.6837     0.3862     0.0078      0.0    
   2000     11362.37    0.6962     0.6828     0.4793     0.0084      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       94.09      1.4127     1.4171     0.1705     0.9933      0.0    
   100       446.1      1.4092     1.4095     0.156      0.712       0.0    
   150       799.38     1.4174     1.4118     0.1284     0.5028      0.0    
   200      1152.59     1.4186     1.4194     0.1618     0.3485      0.0    
   250      1504.72     1.4154     1.4105     0.0974     0.2447      0.0    
   300      1857.71     1.406      1.4113     0.0958     0.1776      0.0    
   350       2211.6     1.408      1.4131     0.0832     0.1353      0.0    
   400      2564.24     1.4142     1.4128     0.083      0.1061      0.0    
   450      2916.84     1.4129     1.4121     0.1011     0.0861      0.0    
   500       3270.1     1.4184     1.4119     0.0892     0.0713      0.0    
   550      3623.55     1.4135     1.411      0.1019     0.0601      0.0    
   600      3976.92     1.4017     1.3984     0.1319     0.0516      0.0    
   650      4331.58     1.3783     1.3831     0.1768     0.0454      0.0    
   700      4686.98     1.3328     1.3285     0.2068     0.0407      0.0    
   750      5041.66     1.1216     1.1236     0.4791     0.0385      0.0    
   800      5397.77     0.7724     0.7677     0.8514     0.0645      0.0    
   850      5745.47     0.7224     0.7165     0.9965     0.1008      0.0    
   900      6089.13     0.7207     0.7104     0.704      0.0877      0.0    
   950      6432.64     0.7358     0.7268     0.6838     0.0415      0.0    
   1000     6775.19     0.7223     0.7134     0.6015     0.031       0.0    
   1050     7116.18     0.7348     0.723      0.7496     0.0269      0.0    
   1100     7457.33     0.7187     0.7097     0.7409     0.0239      0.0    
   1150      7798.2     0.7272     0.7111     0.6947     0.0195      0.0    
   1200     8138.42     0.7217     0.7128     0.7141     0.0188      0.0    
   1250     8478.82     0.7163     0.7042     0.3742     0.0171      0.0    
   1300     8819.01     0.7202     0.7098     0.6243     0.0153      0.0    
   1350     9160.44     0.7324     0.7212     0.7081     0.0152      0.0    
   1400     9500.86     0.7129     0.6954     0.5713     0.0158      0.0    
   1450     9841.71     0.7289     0.7199     0.606      0.0137      0.0    
   1500     10183.02    0.7157     0.7037     0.708      0.013       0.0    
   1550     10524.33    0.7089     0.6953     0.4374     0.012       0.0    
   1600     10867.06    0.7168     0.7038     0.6366     0.0123      0.0    
   1650     11210.57    0.7113     0.7019     0.4739     0.0125      0.0    
   1700     11554.66    0.7125     0.7024     0.4861     0.0115      0.0    
   1750     11899.12    0.7061     0.6927     0.4967     0.0118      0.0    
   1800     12243.66    0.6977     0.6864     0.379      0.0099      0.0    
   1850     12587.57    0.702      0.6866     0.5299     0.0108      0.0    
   1900     12931.74    0.6836     0.6718     0.5867     0.0102      0.0    
   1950     13276.86    0.6848     0.6704     0.5139     0.0088      0.0    
   2000     13621.35    0.6737     0.6582     0.3071     0.0092      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       94.16      1.4171     1.4175     0.2212     0.9011      0.0    
   100       506.78     1.4199     1.4179     0.1694     0.6758      0.0    
   150       917.52     1.413      1.4206     0.1499     0.4993      0.0    
   200      1331.37     1.4086     1.4129      0.14      0.3579      0.0    
   250      1743.07     1.4152     1.4123     0.1161     0.2572      0.0    
   300      2158.05     1.416      1.4164     0.1227     0.1885      0.0    
   350       2573.2     1.414      1.4177     0.1097     0.1436      0.0    
   400      2986.77     1.4203     1.4102     0.0934     0.1127      0.0    
   450      3400.21     1.4147     1.4127     0.0922     0.0911      0.0    
   500      3812.72     1.4121     1.4103     0.0748     0.0748      0.0    
   550      4226.65     1.4114     1.4141     0.0873     0.064       0.0    
   600      4640.14     1.4122     1.4074     0.102      0.0545      0.0    
   650       5051.8     1.3999     1.4042     0.1212     0.0476      0.0    
   700      5463.48     1.3799     1.3805     0.1645     0.0423      0.0    
   750       5876.2     1.3153     1.3127     0.2679     0.0383      0.0    
   800      6290.34     0.9324     0.9315     0.5972     0.0455      0.0    
   850      6698.83     0.7294     0.7181     0.6877     0.0923      0.0    
   900      7103.39     0.727      0.713      0.6814     0.0919      0.0    
   950      7507.41     0.7261     0.7128     0.5219     0.0568      0.0    
   1000     7911.79     0.7346     0.722      0.6382     0.0293      0.0    
   1050     8317.38     0.7298     0.7165      0.65      0.0216      0.0    
   1100      8722.8     0.7295     0.7157     0.4991     0.0202      0.0    
   1150     9130.82     0.7332     0.7218     0.5553     0.0185      0.0    
   1200     9537.06     0.7305     0.7138     0.5235     0.0181      0.0    
   1250     9945.18     0.7218     0.7093     0.4443     0.0175      0.0    
   1300     10352.32    0.7309     0.7173     0.4317     0.0163      0.0    
   1350     10760.41    0.7206     0.7073     0.3459     0.0157      0.0    
   1400     11169.23    0.7408     0.7299     0.3569     0.015       0.0    
   1450     11577.88    0.7241     0.7091     0.4137     0.0148      0.0    
   1500     11985.1     0.7262     0.714      0.3982     0.0137      0.0    
   1550     12393.05    0.7256      0.71      0.3415     0.0134      0.0    
   1600     12801.05    0.7248     0.7114     0.3744     0.0134      0.0    
   1650     13209.31    0.727      0.7104     0.4748     0.0123      0.0    
   1700     13619.13    0.735      0.7203     0.4217     0.0118      0.0    
   1750     14029.68    0.7218     0.7079     0.3977     0.0121      0.0    
   1800     14439.16    0.7182     0.7056     0.3924     0.0111      0.0    
   1850     14849.69    0.7196     0.705      0.3962     0.0105      0.0    
   1900     15259.66    0.7188     0.7041     0.2753     0.0102      0.0    
   1950     15671.04    0.7285     0.7128     0.328      0.0097      0.0    
   2000     16081.66    0.7307     0.7118     0.3685     0.0099      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50        94.4      1.4115     1.4162     0.3016     0.8627      0.0    
   100       566.56     1.4135     1.4149     0.2616     0.6567      0.0    
   150      1040.48     1.4114     1.415      0.1844     0.4914      0.0    
   200      1514.91     1.421      1.4164     0.1732     0.3607      0.0    
   250      1987.25     1.4165     1.4178     0.1397     0.2645      0.0    
   300       2459.0     1.416      1.4136     0.1301     0.1971      0.0    
   350      2930.73     1.4171     1.4138     0.148      0.1495      0.0    
   400      3403.59     1.409      1.4134     0.0976     0.1175      0.0    
   450      3874.44     1.4121     1.4181     0.1074     0.0947      0.0    
   500      4345.86     1.4077     1.4128     0.0929     0.0782      0.0    
   550      4817.48     1.4184     1.4139     0.0994     0.0659      0.0    
   600      5289.48     1.4117     1.4105     0.0918     0.0559      0.0    
   650      5761.93     1.4135     1.4076     0.1133     0.0488      0.0    
   700      6234.48     1.3943     1.3962     0.1158     0.043       0.0    
   750      6707.45     1.3751     1.3683     0.1768     0.0386      0.0    
   800      7180.88     1.2614     1.2569     0.3135     0.0358      0.0    
   850       7654.4     0.8365     0.8329     0.6665     0.0487      0.0    
   900      8121.53     0.726      0.7142     0.7479     0.0972      0.0    
   950      8585.91     0.7232     0.7113     0.7243     0.0914      0.0    
   1000     9047.54     0.7546     0.7388     0.6157     0.043       0.0    
   1050     9507.38     0.7287     0.7199     0.583      0.0281      0.0    
   1100     9967.99     0.7437     0.7327     0.5412     0.0244      0.0    
   1150     10428.33    0.7475     0.7426     0.6318     0.0223      0.0    
   1200     10888.98    0.7368     0.7255     0.7177     0.0216      0.0    
   1250     11350.16    0.7299     0.7157     0.5157     0.0194      0.0    
   1300     11809.52    0.7228     0.7122     0.4972     0.0181      0.0    
   1350     12270.46    0.7263     0.7136     0.4523     0.0173      0.0    
   1400     12731.21    0.7193     0.706      0.4511     0.0164      0.0    
   1450     13192.35    0.7177     0.7044     0.4149     0.0155      0.0    
   1500     13653.92    0.7171     0.7042     0.5178     0.0147      0.0    
   1550     14116.66    0.7229     0.7098     0.3459     0.0141      0.0    
   1600     14578.47    0.7304     0.7142     0.4249     0.0138      0.0    
   1650     15040.15    0.7178     0.7029     0.4321     0.0135      0.0    
   1700     15502.29    0.7173     0.7039     0.3444     0.0124      0.0    
   1750     15963.8     0.7177     0.7032     0.3626     0.012       0.0    
   1800     16425.44    0.7181     0.7055     0.4222     0.0129      0.0    
   1850     16887.85    0.7198     0.7131     0.3217     0.0114      0.0    
   1900     17350.8     0.7152     0.6991     0.3773     0.0114      0.0    
   1950     17812.61    0.709      0.7008     0.3503     0.0112      0.0    
   2000     18275.09    0.7177     0.7028     0.3255     0.0106      0.0    
