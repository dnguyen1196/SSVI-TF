Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.432996511459351
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       168.13    361.0716   362.617     3.1623     4.4721      0.0    
   100       345.09    191.3754   192.1535    3.1623     0.2069      0.0    
   150       522.9     167.6278   167.582     2.8845      0.0        0.0    
   200       700.91    160.2231   159.484     2.6523      0.0        0.0    
   250       878.97    155.0211   154.3849    2.3185      0.0        0.0    
   300      1057.12    151.3789   151.1281    2.0788      0.0        0.0    
   350      1235.29    149.6503   149.328     1.6998      0.0        0.0    
   400      1413.38    147.8986   147.3924    1.3092      0.0        0.0    
   450      1591.57    146.9017   146.8251    1.1975      0.0        0.0    
   500      1769.61    147.0375   146.7422    1.1806      0.0        0.0    
   550      1947.66    147.352    146.9827    1.2067      0.0        0.0    
   600      2125.79    147.3521   146.9611    1.2732      0.0        0.0    
   650      2303.81    146.6537   146.6474    1.4006      0.0        0.0    
   700      2481.93    146.3807   145.9902    1.332       0.0        0.0    
   750      2660.05    146.4721   146.201     0.8164      0.0        0.0    
   800      2838.21    147.1585   146.6462    1.0483      0.0        0.0    
   850      3016.33    146.7284   146.5673    1.2278      0.0        0.0    
   900      3194.49    146.7389   146.4529    1.1919      0.0        0.0    
   950       3372.5    146.6276   146.4002    1.0322      0.0        0.0    
   1000     3550.57    146.256    145.7002    1.0394      0.0        0.0    
   1050     3728.78    145.7151   145.7061    0.9614      0.0        0.0    
   1100     3907.02    147.2506   146.5417    0.8785      0.0        0.0    
   1150     4085.19    146.543    146.3915    1.1631      0.0        0.0    
   1200      4263.3    146.5019   146.0626    1.0281      0.0        0.0    
   1250     4441.41    145.8385   145.4521    1.1164      0.0        0.0    
   1300     4619.55    146.1593   145.9264    1.0173      0.0        0.0    
   1350     4797.83    146.8611   146.1568    1.088       0.0        0.0    
   1400     4976.09    145.8995   145.6987    0.8789      0.0        0.0    
   1450     5154.32    145.8365   145.4777    0.7686      0.0        0.0    
   1500     5332.58    146.4155   146.0817    0.8638      0.0        0.0    
   1550     5510.82    146.4746   145.8128    0.9295      0.0        0.0    
   1600     5689.09    145.8275   145.614     0.9597      0.0        0.0    
   1650     5867.42    145.7297   145.6901    1.0713      0.0        0.0    
   1700     6045.16    145.8696   145.5965    0.946       0.0        0.0    
   1750      6222.6    146.3561   145.8105    0.7509      0.0        0.0    
   1800     6399.87    145.5031   145.404     0.7367      0.0        0.0    
   1850     6577.13    145.8201   145.3319    0.674       0.0        0.0    
   1900     6754.49    146.2479   145.8208    0.8181      0.0        0.0    
   1950     6931.94    145.9223   145.883     0.833       0.0        0.0    
   2000     7109.29    146.0731   145.5418    0.833       0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       168.24    303.2906   302.8966    3.0279     4.4721      0.0    
   100       345.91    177.3034   177.0036    3.0906      0.0        0.0    
   150       524.04    161.9109   160.9195    2.1479      0.0        0.0    
   200       702.29    154.5262   154.3104    2.3044      0.0        0.0    
   250       880.52    152.2929   152.1152    2.1927      0.0        0.0    
   300      1058.74    151.5245   150.9239    2.0987      0.0        0.0    
   350      1237.07    150.3386   149.7194    1.7717      0.0        0.0    
   400      1415.39    147.8934   148.0226    1.5884      0.0        0.0    
   450      1593.75    146.8571   146.4136    1.2796      0.0        0.0    
   500      1772.11    147.3553   147.0004    1.2586      0.0        0.0    
   550      1950.41    147.2235   146.5252    1.0021      0.0        0.0    
   600      2128.81    147.2419   147.3557    1.3264      0.0        0.0    
   650      2307.05    147.1704   146.6675    1.6075      0.0        0.0    
   700      2485.36    146.4232   146.2442    1.6644      0.0        0.0    
   750       2663.6    146.8706   146.6139    1.4021      0.0        0.0    
   800      2841.81    146.3302   146.1239    0.9218      0.0        0.0    
   850      3020.04    147.149    146.6961    1.1996      0.0        0.0    
   900      3198.27    146.4909   146.4571    1.2237      0.0        0.0    
   950      3376.53    146.3968   146.1384    1.1887      0.0        0.0    
   1000     3554.77    146.2503   145.7725    1.0681      0.0        0.0    
   1050     3733.08    145.9493   145.7588    1.0213      0.0        0.0    
   1100      3911.7    146.7705   146.3047    0.8534      0.0        0.0    
   1150     4090.25    146.1808   145.6618    0.8519      0.0        0.0    
   1200     4268.93    145.8752   145.6215    0.9987      0.0        0.0    
   1250     4447.06    145.5376   145.4954    1.0011      0.0        0.0    
   1300     4625.49    145.9328   145.522     0.9672      0.0        0.0    
   1350     4803.78    146.1403   145.6187    0.7901      0.0        0.0    
   1400     4982.27    145.9013   145.5122    0.816       0.0        0.0    
   1450     5160.66    146.6447   146.0992    1.0589      0.0        0.0    
   1500     5339.11    146.3167   146.2476    1.2927      0.0        0.0    
   1550     5517.46    146.106    145.8293    0.9465      0.0        0.0    
   1600     5695.95    145.854    145.6082    1.0376      0.0        0.0    
   1650     5874.32    146.3422   146.0305    1.1872      0.0        0.0    
   1700     6052.72    146.4101   145.9072    0.9374      0.0        0.0    
   1750     6231.05    145.2698   144.9889    0.9978      0.0        0.0    
   1800     6409.52    145.3154   145.2388    0.9149      0.0        0.0    
   1850     6587.91    145.8139   145.6333    0.7366      0.0        0.0    
   1900     6766.48     145.85    145.783     1.019       0.0        0.0    
   1950     6944.97    145.9727   145.7185    0.8904      0.0        0.0    
   2000     7123.49    145.6774   145.6553    0.9769      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       168.68    290.2659   288.9922    2.8786     4.4721      0.0    
   100       347.55    175.4512   174.3703    2.6916      0.0        0.0    
   150       526.68    158.6565   157.7927    1.6801      0.0        0.0    
   200       705.64    151.124    150.5018    1.5706      0.0        0.0    
   250       884.68    147.4749   147.2273    1.4964      0.0        0.0    
   300       1063.6    147.248    146.5481    1.1566      0.0        0.0    
   350      1242.53    146.6337   146.126     1.1165      0.0        0.0    
   400      1421.52    146.7932   146.2724    1.5835      0.0        0.0    
   450      1600.54    146.8662   146.3349    1.5065      0.0        0.0    
   500      1779.56    146.5413   146.0673     1.23       0.0        0.0    
   550      1958.53    146.401    146.1112    1.2964      0.0        0.0    
   600      2137.57    146.4639   146.2714    1.509       0.0        0.0    
   650      2316.47    146.3985   146.2098    1.5739      0.0        0.0    
   700      2495.39    146.5319   146.173     1.1311      0.0        0.0    
   750      2674.17    146.2044   145.5392    1.1997      0.0        0.0    
   800       2853.0    145.7345   145.6251    1.1205      0.0        0.0    
   850      3031.82    146.2571   145.721     0.9901      0.0        0.0    
   900      3210.57    145.955    145.4033    1.0126      0.0        0.0    
   950      3389.41    145.5426   145.4105    0.813       0.0        0.0    
   1000      3568.3    145.8629   145.473     0.9192      0.0        0.0    
   1050     3747.08    145.6279   145.3521    0.9228      0.0        0.0    
   1100     3925.98    145.9666   145.5592    1.0705      0.0        0.0    
   1150     4104.93    146.3277   146.0436    0.9075      0.0        0.0    
   1200     4283.63    145.5025   145.127     0.9519      0.0        0.0    
   1250     4462.34    145.8309   145.4139    0.7754      0.0        0.0    
   1300     4641.06    145.6484   145.4613    0.9466      0.0        0.0    
   1350     4819.89    145.6079   145.0547    0.9857      0.0        0.0    
   1400     4998.97    145.3898   145.2045    0.8877      0.0        0.0    
   1450     5177.89    146.1419   145.5983    1.0415      0.0        0.0    
   1500     5356.65    145.4684   145.2426    0.6994      0.0        0.0    
   1550     5537.56    145.858    145.2686    0.8715      0.0        0.0    
   1600     5716.31    146.2231   145.7079    0.8347      0.0        0.0    
   1650     5895.23    145.2771   145.2419    0.8139      0.0        0.0    
   1700     6074.14    145.8709   145.6087    0.7549      0.0        0.0    
   1750     6253.28    145.9965   145.5632    0.8688      0.0        0.0    
   1800     6432.71    145.396    144.9753    0.9094      0.0        0.0    
   1850     6611.85    145.5293   145.3984    0.8547      0.0        0.0    
   1900     6791.25    145.3851   144.9494    0.7355      0.0        0.0    
   1950      6970.0    145.0943   144.7074    0.6127      0.0        0.0    
   2000     7149.12    144.9845   144.9573    0.9066      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       168.17    284.5703   283.4564    2.755      4.4721      0.0    
   100       347.4     179.1812   178.1401    2.6435      0.0        0.0    
   150       526.43    159.5609   159.0838     2.01       0.0        0.0    
   200       705.49    151.9939   152.0084    1.2501      0.0        0.0    
   250       884.55    148.7638   148.5162    1.0587      0.0        0.0    
   300      1064.29    146.6003   146.8226    0.9376      0.0        0.0    
   350      1243.65    146.7918   146.8648    0.9691      0.0        0.0    
   400      1422.89    145.9391   146.1405    0.8561      0.0        0.0    
   450      1602.22    145.8257   145.8412    1.1627      0.0        0.0    
   500      1781.37    146.2003   146.2723    1.0019      0.0        0.0    
   550      1960.68    146.2828   146.4958    1.2235      0.0        0.0    
   600      2140.17    146.1731   145.9854    1.0086      0.0        0.0    
   650      2319.76    145.9558   145.8674    0.8433      0.0        0.0    
   700      2499.11    145.7844   145.9183    1.2084      0.0        0.0    
   750      2678.37    145.9604   145.9404    1.2882      0.0        0.0    
   800      2857.59    145.2578   145.3743    0.7958      0.0        0.0    
   850      3036.78    146.3645   145.9295    0.9927      0.0        0.0    
   900      3215.94    145.8795   145.973     0.9956      0.0        0.0    
   950      3394.99    145.6981   145.6041    0.9936      0.0        0.0    
   1000     3574.26    145.6836   145.6343    1.2034      0.0        0.0    
   1050     3753.34    145.4112   145.5265    0.9362      0.0        0.0    
   1100      3932.5    145.7514   146.0486    0.9355      0.0        0.0    
   1150     4111.56    145.2682   145.6171    0.8808      0.0        0.0    
   1200     4290.74    144.9753   145.2622    0.9087      0.0        0.0    
   1250     4469.88    145.0325   144.991     0.7167      0.0        0.0    
   1300     4648.95    145.9947   146.068     0.8307      0.0        0.0    
   1350     4828.07    145.8489   146.1053    1.3649      0.0        0.0    
   1400     5007.19    145.7787   145.8319    1.1301      0.0        0.0    
   1450     5186.39    145.2528   145.3856    0.8547      0.0        0.0    
   1500     5365.56    145.6501   145.6706    0.8486      0.0        0.0    
   1550     5544.82    145.3236   145.618     0.8532      0.0        0.0    
   1600     5723.86    145.5887   145.4963    0.7098      0.0        0.0    
   1650     5903.06    145.1545   145.3537    0.5613      0.0        0.0    
   1700     6082.22    145.8908   145.5774    0.9026      0.0        0.0    
   1750     6261.35    144.9084    145.2      0.7337      0.0        0.0    
   1800      6440.5    145.5865   145.4972    0.7025      0.0        0.0    
   1850     6619.74    144.9129   145.1973    0.8388      0.0        0.0    
   1900     6798.98    145.3915   145.4726    0.9596      0.0        0.0    
   1950     6978.17    145.2528   145.5517    0.9099      0.0        0.0    
   2000     7157.45    144.8833   145.1974    0.9262      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       168.1     285.1432   284.0419    2.6398     4.4721      0.0    
   100       347.38    186.1255   184.6143    2.4241      0.0        0.0    
   150       526.94    162.6336   161.9969    1.6933      0.0        0.0    
   200       706.64    152.9247   152.3795    1.1979      0.0        0.0    
   250       886.44    149.8185   149.3393    1.0048      0.0        0.0    
   300      1066.17     147.31    147.3515    1.2759      0.0        0.0    
   350       1246.2    146.4349   146.3623    1.0958      0.0        0.0    
   400      1426.08    146.3334   146.4244    1.1121      0.0        0.0    
   450       1605.9    145.3935   145.4729    1.1372      0.0        0.0    
   500      1785.81    145.3741   145.4624    0.708       0.0        0.0    
   550      1965.66    145.7505   145.527     0.895       0.0        0.0    
   600      2145.58    144.7529   144.9609    0.9443      0.0        0.0    
   650      2325.49    145.5704   145.5074    0.7902      0.0        0.0    
   700      2505.43    145.265    145.3435    0.8149      0.0        0.0    
   750      2685.26    145.3693   145.4785    0.7911      0.0        0.0    
   800      2865.21    145.4389   145.3744    1.0327      0.0        0.0    
   850      3045.08    145.6437   145.5473    0.9785      0.0        0.0    
   900      3224.98    145.3752   145.5078    0.9531      0.0        0.0    
   950      3404.85    145.4613   145.4712    0.7113      0.0        0.0    
   1000     3584.74    145.6575   145.4153    0.869       0.0        0.0    
   1050     3764.72    145.0246   145.3199    0.8632      0.0        0.0    
   1100     3944.55    145.0978   145.0271    0.7923      0.0        0.0    
   1150      4124.5    145.7285   145.8282    0.9542      0.0        0.0    
   1200     4304.39    144.7472   144.8683    0.8673      0.0        0.0    
   1250     4484.36    144.9623   144.9965    0.6311      0.0        0.0    
   1300     4664.29    145.4675   145.3924    0.5073      0.0        0.0    
   1350     4844.17    145.0578   144.9848    0.709       0.0        0.0    
   1400     5023.81    144.7753   145.0074    0.6298      0.0        0.0    
   1450     5203.84    145.0847   145.1118    0.7661      0.0        0.0    
   1500     5383.77    145.1572   145.0234    0.8401      0.0        0.0    
   1550     5563.67    145.4034   145.2306    0.914       0.0        0.0    
   1600     5743.48    145.0551   145.2187    0.8581      0.0        0.0    
   1650     5923.29    145.3943   145.3664    0.7774      0.0        0.0    
   1700     6103.21    145.3463   145.3636    0.782       0.0        0.0    
   1750     6283.16    144.9846   145.1331    0.7898      0.0        0.0    
   1800     6462.95    145.0866   144.9331    0.7096      0.0        0.0    
   1850     6642.91    144.8522   144.902     0.6235      0.0        0.0    
   1900     6822.73    145.2096   145.1814    0.6548      0.0        0.0    
   1950     7002.45    144.7687   144.7621     0.64       0.0        0.0    
   2000     7182.45    144.7317   144.873     0.6706      0.0        0.0    
