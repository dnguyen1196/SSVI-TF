Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.401811122894287
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       140.99     0.3897     0.3587     3.1623     3.969      0.0009  
   100       412.34     0.149      0.1378     3.1507     0.322      0.0002  
   150       681.67     0.1518     0.1355     2.686      0.1691      0.0    
   200       951.12     0.1357     0.114      2.397      0.1387      0.0    
   250      1220.09     0.1182     0.0951     2.2656     0.1148     0.0001  
   300       1488.4     0.1106     0.083      1.8992     0.0865      0.0    
   350       1759.0     0.1068     0.0789     2.2966     0.0847      0.0    
   400      2028.73     0.1058     0.0764     1.9843     0.0827      0.0    
   450      2298.34     0.0977     0.0701     2.1085     0.0596      0.0    
   500      2567.42     0.0972     0.0648     1.8552     0.0553      0.0    
   550      2836.82     0.0964     0.0679     1.6669     0.052       0.0    
   600      3107.85     0.0954     0.0626     1.873      0.0483      0.0    
   650      3378.54     0.0961     0.0594     1.8511     0.0493      0.0    
   700      3648.32     0.0919     0.056      1.5425     0.0389      0.0    
   750      3916.97     0.0916     0.0528     1.9686      0.05       0.0    
   800      4186.61     0.0907     0.0539     1.7315     0.035       0.0    
   850      4457.05     0.0933     0.0538     2.0488     0.0342      0.0    
   900      4726.77     0.0905     0.0538     1.7514     0.0409      0.0    
   950      4994.35     0.0919     0.0507     1.8429     0.0301      0.0    
   1000     5262.69      0.09      0.0499     2.1902     0.0286      0.0    
   1050     5531.52     0.0862     0.049      2.1017     0.0333      0.0    
   1100     5801.05     0.0852     0.0478     1.7324     0.0504      0.0    
   1150     6069.55     0.0863     0.0461     1.6978     0.0334      0.0    
   1200     6338.38     0.0852     0.0432     1.7652     0.0344      0.0    
   1250     6606.29     0.0903     0.0468     2.1518     0.0245      0.0    
   1300     6875.54     0.0853     0.0452     1.6495     0.0255      0.0    
   1350     7144.76     0.0886     0.0453     1.7129     0.0269      0.0    
   1400      7412.6     0.0862     0.0457     1.6287     0.024       0.0    
   1450     7681.33     0.0933     0.053      2.1684     0.0276      0.0    
   1500     7950.61     0.0856     0.0456     2.021      0.0251      0.0    
   1550     8219.19     0.0864     0.042      1.8586     0.0258      0.0    
   1600     8486.31     0.0838     0.0434     1.919      0.0276      0.0    
   1650     8754.29     0.0874     0.0436     1.8999     0.024       0.0    
   1700     9024.57     0.0832     0.0457     2.017      0.0243      0.0    
   1750     9292.85     0.089      0.0458     1.8887     0.022       0.0    
   1800     9554.95     0.0826     0.0444     1.8976     0.022       0.0    
   1850     9800.08     0.0902     0.0468     2.1718     0.027       0.0    
   1900     10045.57    0.0866     0.0523     2.3146     0.0241      0.0    
   1950     10290.73    0.0922     0.0522     2.4264     0.0257      0.0    
   2000     10535.89    0.089      0.0506     2.4589     0.0244      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       134.65     0.4951     0.5017     0.8265     1.2163      0.0    
   100       433.8      0.4953     0.5014     0.7138     0.7839      0.0    
   150       754.13     0.505      0.5064     0.7898     0.4898     0.0001  
   200      1056.33     0.4944     0.4965      0.66      0.3082     0.0001  
   250      1360.41     0.5015     0.4962     0.3906     0.2052     0.0001  
   300       1680.9     0.4989     0.4936     0.4563     0.1478     0.0001  
   350      2007.28     0.4668     0.4659     0.5909     0.1119     0.0001  
   400      2332.72     0.1743     0.1723     0.7626     0.1297      0.0    
   450      2651.36     0.1328     0.1328     0.7552     0.153       0.0    
   500       2967.4     0.137      0.1347      0.69      0.122       0.0    
   550       3279.3     0.139      0.1376     0.6561     0.0504      0.0    
   600      3590.07     0.1424     0.1388     0.6748     0.0456      0.0    
   650      3898.99     0.1335     0.1336     0.6567     0.043       0.0    
   700      4209.21     0.1318     0.1313     0.4871     0.0396      0.0    
   750      4520.74     0.1343     0.1343     0.5321     0.0353      0.0    
   800      4830.77     0.1353     0.1331     0.4912     0.0344      0.0    
   850      5142.31     0.136      0.1356     0.4553     0.0308      0.0    
   900      5453.33     0.1388     0.1367     0.5408     0.0263      0.0    
   950      5764.32     0.1306     0.1317     0.5407     0.0268      0.0    
   1000     6075.05     0.1349     0.1331     0.4176     0.0236      0.0    
   1050     6385.91     0.1298     0.1306     0.4277     0.0226      0.0    
   1100     6686.97     0.1398     0.1363     0.4376     0.0213      0.0    
   1150     6981.48     0.1324     0.132      0.4637     0.0206      0.0    
   1200     7276.65     0.1322     0.1321     0.3926     0.0195      0.0    
   1250     7576.64     0.1345     0.1333     0.3849     0.0171      0.0    
   1300     7890.35     0.1335     0.133      0.3999     0.0175      0.0    
   1350     8213.05     0.1383     0.1352     0.4113     0.016       0.0    
   1400     8530.12     0.1312     0.1308     0.5061     0.017       0.0    
   1450     8831.11     0.1302     0.1302     0.3725     0.0153      0.0    
   1500     9135.84     0.1329     0.132      0.4055     0.0146      0.0    
   1550      9441.0     0.1316     0.1308     0.5711     0.0166      0.0    
   1600     9745.02     0.1326     0.1327     0.443      0.0136      0.0    
   1650     10049.6     0.134      0.1319     0.3627     0.0136      0.0    
   1700     10353.75    0.135      0.1349     0.326      0.015       0.0    
   1750     10657.45    0.1329     0.1333     0.4938     0.0121      0.0    
   1800     10962.83    0.1337     0.1321     0.5307     0.0123      0.0    
   1850     11265.92    0.133      0.1326     0.4222     0.0121      0.0    
   1900     11573.65    0.1303     0.1292     0.3204     0.012       0.0    
   1950     11884.15    0.1333     0.1297     0.4215     0.0123      0.0    
   2000     12194.16    0.1325     0.1293     0.3863     0.0151      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       139.05     0.4972     0.502      0.9349     0.9877      0.0    
   100       508.66     0.4978     0.501      1.0975     0.7091      0.0    
   150       877.95     0.5018     0.5019     0.8953     0.502       0.0    
   200      1247.88     0.4983     0.4987     0.7151     0.344       0.0    
   250      1618.64     0.5024      0.5       0.6067     0.2413      0.0    
   300      1992.34     0.4988     0.4986     0.5713     0.1751      0.0    
   350       2365.9     0.4974     0.4999     0.4661     0.1321      0.0    
   400      2738.21     0.4731     0.4654     0.5538     0.1051      0.0    
   450       3111.5     0.1579     0.1558     0.7931     0.129       0.0    
   500      3480.83     0.1347     0.1322     0.7826     0.1548      0.0    
   550      3846.34     0.1337     0.1325     0.7084     0.1186      0.0    
   600      4209.32     0.1376     0.135      0.7033     0.053       0.0    
   650      4572.13     0.1334     0.1326     0.6771     0.0458      0.0    
   700      4925.14     0.1334     0.1329     0.6946     0.0434      0.0    
   750      5278.45     0.1408     0.1369     0.5288     0.0382      0.0    
   800      5636.62     0.1358     0.1362     0.4979     0.0381      0.0    
   850      5998.29     0.1381     0.1351     0.676      0.0331      0.0    
   900       6360.1     0.1298     0.1306     0.5548     0.0315      0.0    
   950      6720.94     0.1399     0.1356     0.632      0.0296      0.0    
   1000     7082.19     0.131       0.13      0.4827     0.0286      0.0    
   1050     7443.72     0.1356     0.1343     0.4938     0.0255      0.0    
   1100      7804.8     0.1354     0.1326     0.4521     0.0243      0.0    
   1150     8164.94     0.1328     0.132      0.3963     0.0232      0.0    
   1200     8524.56     0.1269     0.1252     0.4502     0.0214      0.0    
   1250     8882.13     0.1264     0.1252     0.606      0.0213      0.0    
   1300     9233.07     0.1259     0.123      0.4811     0.0214      0.0    
   1350     9582.12     0.1334     0.1282     0.4903     0.0225      0.0    
   1400     9931.32     0.1258     0.1257     0.7066     0.023       0.0    
   1450     10280.54    0.1238     0.1185     0.5943     0.0343      0.0    
   1500     10628.2     0.1234     0.1191     0.6389     0.0269      0.0    
   1550     10981.67    0.1229     0.1186     0.6926     0.0223      0.0    
   1600     11337.95    0.1232     0.119      0.7271     0.0207      0.0    
   1650     11694.1     0.1229     0.1188     0.673      0.0215      0.0    
   1700     12050.88    0.1209     0.1174     0.678      0.0168      0.0    
   1750     12406.69    0.1208     0.1166     0.6426     0.0234      0.0    
   1800     12762.21    0.1222     0.1182     0.6969     0.0168      0.0    
   1850     13118.97    0.1198     0.1158     0.609      0.0152      0.0    
   1900     13473.25    0.1204     0.1152     0.568      0.0134      0.0    
   1950     13830.18    0.1168     0.1134     0.6337     0.0145      0.0    
   2000     14187.25    0.1228     0.1166     0.6132     0.0104      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       138.98     0.4982     0.5006     1.1039     0.9007      0.0    
   100       560.13     0.4957      0.5       1.1536     0.6713      0.0    
   150       981.12     0.4986     0.5011     1.0853     0.4919      0.0    
   200      1403.06     0.501      0.4985     0.7813     0.3538      0.0    
   250       1825.1     0.4981     0.502      0.7376     0.2558      0.0    
   300      2246.03     0.4964     0.5026     0.6834     0.1886      0.0    
   350      2667.28     0.4968     0.4978     0.6538     0.1416      0.0    
   400      3086.74     0.4501      0.45      0.7002     0.1135      0.0    
   450      3500.63     0.1418     0.1418     0.8629      0.15       0.0    
   500      3910.45     0.1359     0.1352     0.7399     0.1511      0.0    
   550      4318.48     0.1434     0.1444     0.7086     0.1032      0.0    
   600      4726.62     0.1336     0.1345     0.7027     0.0567      0.0    
   650      5136.06     0.1358     0.1366     0.6849     0.0506      0.0    
   700      5544.72     0.1434     0.1432     0.5462     0.0479      0.0    
   750       5951.9     0.1302     0.132      0.552      0.0444      0.0    
   800      6361.55     0.1359     0.1386     0.6114     0.0395      0.0    
   850      6770.24     0.1362     0.136      0.4693     0.0362      0.0    
   900      7178.61     0.1322     0.134      0.4467     0.0339      0.0    
   950      7585.83     0.1338     0.1328     0.458      0.0326      0.0    
   1000     7992.16     0.1364     0.1367     0.5135     0.0305      0.0    
   1050     8401.18     0.1336     0.1357     0.5713     0.0269      0.0    
   1100     8809.53     0.1334     0.135      0.5431     0.0254      0.0    
   1150     9217.85     0.1411     0.1403     0.3772     0.023       0.0    
   1200     9624.89     0.1321     0.1335     0.4949     0.0231      0.0    
   1250     10031.55    0.1348     0.1346     0.3734     0.0203      0.0    
   1300     10439.05    0.1328     0.1334     0.3861     0.0202      0.0    
   1350     10845.75    0.1337     0.1354     0.4159     0.0177      0.0    
   1400     11252.61    0.1346     0.1352     0.4272     0.0181      0.0    
   1450     11659.69    0.1398     0.1372     0.3382     0.0163      0.0    
   1500     12067.3     0.1308     0.133      0.4185     0.0151      0.0    
   1550     12474.95    0.1368     0.1378     0.4325     0.0155      0.0    
   1600     12882.73    0.1317     0.1326     0.3741     0.0149      0.0    
   1650     13290.52    0.1334     0.134      0.416      0.0148      0.0    
   1700     13698.18    0.1367     0.1355     0.3247     0.0134      0.0    
   1750     14107.29    0.1336     0.134      0.3461     0.0121      0.0    
   1800     14515.99    0.1376     0.1374     0.4132     0.0117      0.0    
   1850     14925.63    0.133      0.1339     0.3656     0.0119      0.0    
   1900     15332.72    0.1331     0.133      0.2813     0.0116      0.0    
   1950     15742.47    0.1338     0.1329     0.3339     0.0119      0.0    
   2000     16151.63    0.131      0.1327     0.433      0.0137      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       137.25     0.4912     0.4975     1.0179     0.8531      0.0    
   100       604.41     0.4935     0.4987     1.2779     0.6478      0.0    
   150      1071.87     0.4994     0.5002     1.0569     0.4875      0.0    
   200      1541.41     0.4988     0.5006     0.8503     0.3587      0.0    
   250      2003.79     0.5005     0.4989     0.6778     0.2638      0.0    
   300      2456.27     0.5045     0.497      0.6539     0.1948      0.0    
   350      2907.39     0.4944     0.4982     0.5343     0.1473      0.0    
   400      3359.21     0.491      0.4883     0.6402     0.116       0.0    
   450      3811.07     0.1611     0.1673     0.8092     0.1292      0.0    
   500      4257.09     0.1366     0.1379     0.7802     0.1574      0.0    
   550      4700.52     0.1318     0.1329     0.7061     0.1277      0.0    
   600      5142.75     0.1343     0.1366     0.7071     0.0619      0.0    
   650      5586.41     0.1392     0.139      0.7028     0.0526      0.0    
   700       6029.7     0.1348     0.135      0.6146     0.0485      0.0    
   750      6473.47     0.1356     0.1362     0.5919     0.0459      0.0    
   800      6915.93     0.1446     0.145      0.5661     0.0408      0.0    
   850      7358.27     0.1352     0.1366     0.584      0.038       0.0    
   900      7801.74     0.139      0.1386     0.4717     0.0349      0.0    
   950       8244.9     0.1306     0.1335     0.5211     0.0317      0.0    
   1000     8687.07     0.1335     0.1339     0.4297     0.0291      0.0    
   1050     9129.57     0.1362     0.137      0.4464     0.0272      0.0    
   1100     9572.55     0.1333     0.134      0.4397     0.0253      0.0    
   1150     10016.02    0.1367     0.136      0.4491     0.0253      0.0    
   1200     10459.8     0.1338     0.136      0.5801     0.0224      0.0    
   1250     10903.46    0.1343     0.1363     0.4509     0.0218      0.0    
   1300     11346.03    0.1351     0.1349     0.4951     0.0204      0.0    
   1350     11789.25    0.1317     0.1326     0.4337     0.021       0.0    
   1400     12231.87    0.135      0.1363     0.4602     0.0176      0.0    
   1450     12674.92    0.1313     0.1343     0.4709     0.0174      0.0    
   1500     13118.61    0.1322     0.1328     0.3727     0.0164      0.0    
   1550     13561.98    0.1315     0.135      0.3321     0.0158      0.0    
   1600     14004.84    0.1319     0.133      0.3158     0.0139      0.0    
   1650     14447.9     0.1378     0.1389     0.3625     0.0137      0.0    
   1700     14892.38    0.1302     0.1328     0.3804     0.0134      0.0    
   1750     15337.01    0.1336     0.1347     0.2896     0.0142      0.0    
   1800     15775.04    0.1293     0.1322     0.4281     0.0119      0.0    
   1850     16217.34    0.1319     0.1336     0.4252     0.014       0.0    
   1900     16647.34    0.127      0.1288     0.4979     0.0119      0.0    
   1950     17076.04    0.1256     0.1265     0.5046     0.0144      0.0    
   2000     17503.84    0.1234     0.1254     0.6339     0.0216      0.0    
