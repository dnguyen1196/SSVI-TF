Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  8.070757865905762
max_count =  22  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll  
    50       147.26     2.8591     2.889      0.3162     4.4326   132405.75  107317.79  
   100       478.14     2.8602     2.8897     0.2753     0.3657   131305.87  106437.55  
   150       809.05     2.8595     2.8896     0.2896     0.1892   130131.55  105489.43  
   200      1141.12     2.8502     2.8801     0.3027     0.1244   127389.59  103281.16  
   250      1472.34     2.8173     2.8469     0.3056     0.1393   121322.13   98387.28  
   300      1807.09     2.7437     2.7732     0.3028     0.1387   111640.04   90559.46  
   350      2138.24     2.6221     2.6504     0.297      0.1234    99387.02   80635.46  
   400      2468.48     2.4506     2.4761     0.3036     0.1039    86056.16   69828.82  
   450       2800.5     2.2273     2.2512     0.3046     0.0757    73333.81   59514.82  
   500      3132.61     2.0056     2.0277     0.2966     0.0694    62815.39   50963.5   
   550      3465.64     1.811      1.8327     0.2795     0.053     54926.43   44545.55  
   600      3797.98     1.6506     1.6729     0.2789     0.0534    49201.83   39887.27  
   650      4130.99     1.5351     1.5567     0.2491     0.0454    45193.48   36616.75  
   700      4463.49     1.444      1.4632     0.225      0.0417    42381.71   34306.28  
   750       4796.1     1.3677     1.3857     0.2758     0.0361    40227.79   32561.62  
   800      5128.41     1.3034     1.3222     0.2144     0.0363    38590.88   31212.3   
   850      5467.27     1.2495     1.2666     0.2881     0.0362    37315.14   30135.06  
   900      5800.31     1.1974     1.2154     0.2679     0.032     36250.98   29280.46  
   950      6133.26     1.1539     1.1698     0.2183     0.0271    35387.61   28548.19  
   1000     6465.06     1.114      1.1274     0.2209     0.0267    34700.12   27955.93  
   1050     6797.38     1.0785     1.094      0.157      0.0268    34090.36   27465.93  
   1100     7131.35     1.0478     1.0614     0.194      0.0263    33581.09   27032.99  
   1150      7463.1     1.0187     1.0321     0.1488     0.0261    33141.31   26669.47  
   1200      7794.6     0.9954     1.008      0.1787     0.0255    32811.25   26368.53  
   1250     8126.31     0.9722     0.983      0.1809     0.0251    32464.49   26094.21  
   1300     8458.33     0.9528     0.9633     0.1872     0.024     32181.15   25859.37  
   1350      8789.3     0.9329     0.9436     0.1646     0.0233    31971.11   25668.46  
   1400      9123.6     0.9193     0.9278     0.1636     0.0227    31729.06   25472.43  
   1450     9454.94     0.9035     0.9105     0.1091     0.0225    31555.48   25325.42  
   1500      9765.5     0.8873     0.8934     0.1655     0.0216    31382.56   25170.66  
   1550     10074.31    0.8771     0.8832     0.2288     0.021     31234.66   25048.56  
   1600     10383.95    0.8656     0.8728     0.1685     0.0204    31112.92   24935.8   
   1650     10693.18    0.8546     0.8615     0.1378     0.0197    30982.31   24837.3   
   1700     11007.48    0.8449     0.8496     0.1741     0.0191    30873.47   24732.37  
   1750     11334.56    0.8373     0.8396     0.1216     0.0183    30765.61   24650.32  
   1800     11652.48    0.8302     0.8328     0.1302     0.0179    30675.71   24568.57  
   1850     11975.24    0.8236     0.8255     0.2061     0.0171    30606.27   24519.69  
   1900     12306.31    0.8164     0.819      0.1262     0.0168    30523.73   24439.49  
   1950     12634.51    0.8126     0.8144     0.1679     0.0163    30450.15   24404.24  
   2000     12961.01    0.8057     0.8075     0.1647     0.0158    30395.15   24357.07  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll  
    50       144.29     2.8789     2.9027     0.0782     3.8827    91441.21  147557.34  
   100       548.14     2.8738     2.8959     0.0348     0.8124   114326.03  184636.23  
   150       953.79     2.8688     2.891      0.0254     0.3631   121472.66  196165.42  
   200      1359.01     2.8668     2.8888     0.0219     0.218    124661.32   201295.1  
   250      1763.51     2.8653     2.8874     0.0232     0.1507   126420.84  204122.55  
   300      2165.75     2.8646     2.8861     0.0199     0.1166    127541.5  205922.93  
   350      2575.94     2.8637     2.8857     0.018      0.0885   128309.06  207155.99  
   400      2981.06     2.8631     2.8852     0.0149     0.0702   128864.67  208048.55  
   450      3386.49     2.8629     2.8849     0.0181     0.0594   129284.77  208723.13  
   500      3793.38     2.8624     2.8845     0.0172     0.0486    129611.5  209247.49  
   550      4180.49     2.8622     2.8843     0.0166     0.0418   129874.33  209669.22  
   600      4569.58     2.8622     2.8842     0.0154     0.0362    130088.6  210013.26  
   650      4961.01     2.8621     2.8835     0.014      0.032    130267.66  210300.55  
   700      5366.51     2.8621     2.8836     0.0138     0.0268   130418.52  210542.63  
   750      5769.73     2.8619     2.8837     0.013      0.0239   130546.68  210748.28  
   800       6168.9     2.862      2.8834     0.0159     0.0213   130658.45  210927.63  
   850      6571.79     2.8615     2.8834     0.0133      0.02    130755.21  211082.86  
   900      6977.22     2.8614     2.8833     0.0147     0.0177   130839.49  211218.04  
   950      7382.65     2.8614     2.8829     0.0124     0.0163   130913.35  211336.55  
   1000     7787.68     2.8612     2.8832     0.0134     0.0151   130980.35  211444.06  
   1050     8192.73     2.8612     2.8831     0.0138     0.0136   131040.64  211540.79  
   1100     8598.25     2.8613     2.8833     0.0131     0.0126   131093.95  211626.31  
   1150     9003.56     2.861      2.883      0.0132     0.0114   131142.26  211703.73  
   1200     9409.09     2.861      2.883      0.0152     0.0108   131185.57  211773.23  
   1250     9813.65     2.8611     2.8827     0.0144     0.0103   131225.32  211837.06  
   1300     10219.54    2.8611     2.8828     0.0135     0.0095    131261.8  211895.56  
   1350     10625.38    2.8613     2.8828     0.0125     0.0089   131294.95  211948.76  
   1400     11030.86    2.8609     2.8827     0.0159     0.0088   131324.92   211996.8  
   1450     11436.61    2.8606     2.8827     0.0118     0.0078   131352.56  212041.13  
   1500     11841.51    2.861      2.8828     0.0135     0.0077   131378.03  212081.95  
   1550     12248.19    2.861      2.8827     0.0176     0.0067   131401.33   212119.3  
   1600     12653.73    2.861      2.8827     0.0142     0.0069   131422.81  212153.75  
   1650     13060.54    2.8608     2.8827     0.0119     0.0059   131442.57  212185.44  
   1700     13466.05    2.8609     2.8828     0.0127     0.0059   131461.45  212215.71  
   1750     13862.52    2.8609     2.8826     0.0128     0.0056   131478.65  212243.32  
   1800     14266.29    2.8609     2.8824     0.0129     0.0054   131494.65  212269.04  
   1850     14661.73    2.8608     2.8825     0.013      0.0052   131509.09  212292.15  
   1900     15056.53    2.8609     2.8825     0.0124     0.005    131523.01  212314.43  
   1950     15457.65    2.8607     2.8826     0.013      0.0047   131535.47  212334.38  
   2000     15855.46    2.8608     2.8824     0.0125     0.0041   131547.16  212353.09  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll  
    50       146.33     2.8807     2.9021     0.0844     3.7248    84069.55  203103.69  
   100       633.4      2.8775     2.8969     0.0521     0.9104    108655.8  262809.33  
   150      1120.04     2.8727     2.8913     0.0484     0.3915   117843.19  285047.47  
   200      1607.35     2.8694     2.8881     0.0291     0.2474   122194.26  295559.28  
   250      2093.25     2.8674     2.8862      0.03      0.1726   124646.22  301476.19  
   300      2577.36     2.8658     2.8846     0.0278     0.1289   126187.31  305194.24  
   350      3050.13     2.8649     2.8839     0.026      0.1036   127248.02  307753.65  
   400      3526.48     2.8642     2.8827     0.0266     0.0835   128017.81  309609.22  
   450      3996.51     2.8638     2.882      0.0283     0.0691   128599.61  311011.43  
   500      4459.68     2.8638     2.8818     0.0247     0.0576   129055.83  312111.19  
   550      4935.13     2.863      2.8814     0.0225     0.049    129422.75  312995.98  
   600      5410.43     2.8629     2.8812     0.0208     0.0431   129722.58  313718.66  
   650      5885.87     2.8628     2.8808     0.0236     0.039     129971.7  314319.26  
   700      6360.05     2.862      2.8806     0.0203     0.0345   130184.28  314831.52  
   750      6834.28     2.8619     2.8806     0.021      0.0306   130363.97   315264.5  
   800      7318.08     2.8619     2.8801     0.0238     0.0273   130520.55  315641.76  
   850      7804.69     2.8618     2.8803     0.0186     0.0248   130656.92  315970.55  
   900      8291.04     2.8616     2.8801     0.0202     0.0233   130777.62   316261.3  
   950      8778.96     2.8615     2.8801     0.0197     0.0211   130884.81  316519.64  
   1000     9266.66     2.8614     2.8798     0.0215     0.0186   130981.01  316751.53  
   1050     9743.63     2.8615     2.8798     0.022      0.0174    131066.6  316957.66  
   1100     10229.82    2.8612     2.8795     0.0175     0.0163   131143.25  317142.38  
   1150     10716.91    2.861      2.8797     0.0174     0.0151   131213.17  317310.86  
   1200     11204.99    2.8609     2.8795     0.0177     0.0137   131276.28  317463.01  
   1250     11693.09    2.8609     2.8796     0.0174     0.0131   131334.41  317603.14  
   1300     12180.75    2.8609     2.8793     0.0166     0.0124   131387.59  317731.36  
   1350     12666.09    2.8611     2.8793     0.0175     0.0113    131436.3  317848.81  
   1400     13151.2     2.8612     2.8794     0.0206     0.0109   131481.47  317957.67  
   1450     13637.62    2.8608     2.8791     0.0159     0.0105   131523.24  318058.25  
   1500     14123.58    2.8608     2.8793     0.0161     0.0097   131562.13  318152.04  
   1550     14609.52    2.8607     2.8791     0.0186     0.0092   131598.03  318238.48  
   1600     15096.23    2.8607     2.8792     0.0218     0.0085   131631.35  318318.78  
   1650     15583.18    2.8609     2.8792     0.0169     0.0082   131662.26   318393.3  
   1700     16069.76    2.8606     2.8791     0.0171     0.0079   131691.03  318462.61  
   1750     16556.37    2.8606     2.879      0.0161     0.0073   131718.29  318528.32  
   1800     17042.43    2.8606     2.8791     0.0173     0.0074   131743.25  318588.55  
   1850     17529.25    2.8607     2.8789     0.0158     0.0068   131766.71  318645.08  
   1900     18015.94    2.8605     2.879      0.0194     0.0069   131788.34  318697.14  
   1950     18502.46    2.8604     2.8789     0.0184     0.0062    131808.8  318746.46  
   2000     18990.57    2.8606     2.879      0.0167     0.0061   131828.07  318792.92  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll  
    50       146.18     2.8822     2.8932     0.1104     3.6244    81163.39  260416.91  
   100       709.79     2.8788     2.8896     0.0745     0.9543   105719.92  339384.79  
   150      1273.61     2.8737     2.8836     0.0567     0.4081   115937.98  372206.42  
   200      1836.79     2.8708     2.8804     0.0425     0.2575   120899.52  388132.84  
   250      2400.17     2.8681     2.8781     0.0449     0.1833   123691.65  397092.76  
   300      2963.83     2.8664     2.8766     0.0363     0.1358   125460.97  402767.33  
   350      3525.57     2.8656     2.8753     0.0396     0.1084   126681.31  406680.42  
   400      4087.16     2.8646     2.8746     0.0315     0.0896   127557.68  409489.83  
   450      4648.52     2.8647     2.874      0.0285     0.0736   128226.01  411632.63  
   500      5210.33     2.8635     2.8734     0.0312     0.062    128749.44   413310.8  
   550      5773.24     2.8633     2.873      0.0272     0.055    129169.13  414655.76  
   600      6335.16     2.8629     2.8727     0.0272     0.0479   129511.71  415754.06  
   650      6897.08     2.8625     2.8723     0.0264     0.0425   129798.29  416672.68  
   700      7499.19     2.862      2.872      0.0241     0.0379   130041.65   417452.6  
   750      8132.18     2.8621     2.8718     0.0269     0.033    130248.74   418116.3  
   800       8764.3     2.8621     2.8718     0.0241     0.0303   130428.48  418692.66  
   850      9396.95     2.8616     2.8715     0.0262     0.0277   130586.02  419197.75  
   900      10031.27    2.8616     2.8716     0.0303     0.025    130724.76  419642.48  
   950      10663.84    2.8612     2.8712     0.0251     0.0233    130847.2  420035.05  
   1000     11296.05    2.8614     2.8712     0.0235     0.021    130956.94  420386.76  
   1050     11928.22    2.8613     2.8709     0.0273     0.0199   131055.63  420703.27  
   1100     12503.37    2.8611     2.8708     0.0216     0.0183   131144.19  420987.33  
   1150     13069.12    2.8611     2.8708     0.0245     0.0171    131224.5  421244.83  
   1200     13581.42    2.8611     2.8708     0.0266     0.0157   131297.58  421479.06  
   1250     14094.02    2.8612     2.8708     0.025      0.0146    131364.4  421693.24  
   1300     14605.73    2.861      2.8707     0.0228     0.014    131426.24   421891.5  
   1350     15116.61    2.8608     2.8707     0.0271     0.0133   131482.66   422072.4  
   1400     15628.5     2.8608     2.8706     0.0199     0.0126   131535.01  422240.29  
   1450     16141.26    2.8609     2.8706     0.0204     0.0122   131583.15  422394.62  
   1500     16652.79    2.8608     2.8706     0.0222     0.0112   131628.28  422539.32  
   1550     17164.29    2.8605     2.8704     0.0227     0.0106   131669.86  422672.68  
   1600     17676.7     2.8607     2.8704     0.0216     0.0101   131708.38  422796.17  
   1650     18189.12    2.8607     2.8702     0.0205     0.0096   131744.51  422912.06  
   1700     18701.72    2.8607     2.8705     0.0208     0.0092    131778.9  423022.32  
   1750     19214.3     2.8605     2.8704     0.0217     0.0082   131810.75  423124.45  
   1800     19726.36    2.8605     2.8703     0.0201     0.0084   131840.61  423220.25  
   1850     20238.27    2.8604     2.8703     0.0233     0.008    131868.64  423310.02  
   1900     20749.9     2.8605     2.8702     0.0197     0.0076   131894.77   423393.8  
   1950     21261.67    2.8604     2.8701     0.0229     0.0075   131918.96  423471.39  
   2000     21775.33    2.8605     2.8702     0.022      0.0069   131941.89  423544.91  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll  
    50       137.36     2.8846     2.898      0.1416     3.6041    79885.62  320919.57  
   100       723.06     2.8805     2.8951     0.0824     0.995    104387.06  419843.93  
   150      1309.89     2.8738     2.8895     0.0616     0.4256   115092.14  462951.59  
   200      1896.57     2.8708     2.8856     0.0519     0.2673   120275.84  483788.45  
   250      2482.76     2.8688     2.8831     0.0458     0.1892   123215.57  495594.84  
   300      3067.73     2.8674     2.8816     0.0487     0.1404   125098.57  503153.53  
   350      3652.12     2.8662     2.8805     0.0359     0.1117   126386.83  508327.57  
   400      4235.09     2.8652     2.8796     0.0367     0.093    127323.03  512084.35  
   450      4819.18     2.8643     2.8789     0.0352     0.0763   128033.72  514935.75  
   500      5403.71     2.8638     2.8785     0.0331     0.0659   128590.86  517172.15  
   550      5988.15     2.8635     2.8778     0.0324     0.0569   129037.63  518964.66  
   600      6573.13     2.863      2.8775     0.033      0.0495   129403.19  520431.88  
   650      7157.52     2.8627     2.8772     0.0286     0.0438   129709.44  521660.59  
   700       7741.3     2.8626     2.8772     0.0278     0.0409   129968.15  522699.22  
   750      8326.71     2.8624     2.8767     0.0293     0.0344   130189.79  523588.68  
   800      8937.62     2.862      2.8766     0.026      0.0329   130382.46  524362.11  
   850      9602.21     2.8618     2.8764     0.0287     0.0295   130549.45   525032.4  
   900      10273.13    2.8617     2.8762     0.0252     0.0263   130697.33  525625.87  
   950      10938.64    2.8614     2.8759     0.0301     0.0246   130829.19  526154.75  
   1000     11602.44    2.8615     2.8758     0.0313     0.0222   130946.56  526625.72  
   1050     12272.14    2.8612     2.8758     0.0275     0.0216   131052.69   527051.6  
   1100     12938.23    2.8615     2.8756     0.0311     0.0192   131147.48  527431.85  
   1150     13604.36    2.8611     2.8756     0.0277     0.0182   131234.17  527779.73  
   1200     14274.45    2.8612     2.8755     0.0264     0.0169   131313.44  528097.64  
   1250     14941.12    2.861      2.8755     0.0282     0.0164   131385.38  528386.41  
   1300     15607.98    2.8609     2.8753     0.0236     0.0149   131451.67  528652.39  
   1350     16273.21    2.8609     2.8753     0.026      0.0142   131512.46  528896.25  
   1400     16938.91    2.8607     2.8752     0.0241     0.0133   131568.08  529119.25  
   1450     17607.32    2.8607     2.8751     0.0288     0.0129   131619.18   529324.3  
   1500     18271.47    2.8607     2.8752     0.0235     0.0122   131667.41  529517.92  
   1550     18935.07    2.8607     2.8751     0.0277     0.0116   131712.12  529697.39  
   1600     19605.11    2.8604     2.875      0.0265     0.0109   131753.65  529864.01  
   1650     20271.8     2.8605     2.875      0.0241     0.0103   131792.74  530020.85  
   1700     20938.39    2.8606     2.875      0.0217      0.01    131829.02  530166.41  
   1750     21568.52    2.8605     2.875      0.0253     0.0096   131862.36  530300.34  
   1800     22209.84    2.8606     2.8749     0.0218     0.0088   131893.22  530424.26  
   1850     22855.63    2.8603     2.8748     0.0256     0.0086   131922.63  530542.28  
   1900     23496.73    2.8606     2.8749     0.0273     0.0084   131949.23   530649.0  
   1950     24138.75    2.8605     2.8749     0.0207     0.0078   131974.52  530750.42  
   2000     24783.89    2.8604     2.8747     0.0243     0.0077   131997.88  530844.23  
