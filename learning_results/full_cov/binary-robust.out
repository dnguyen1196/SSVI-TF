Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.3944411277771
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       151.95     0.3422     0.3286     3.1623     1.0775     0.0181  
   100       447.85     0.1905     0.1842     3.102      0.5525     0.0047  
   150       743.54     0.1682     0.1688     2.6279     0.8285     0.0069  
   200      1036.45     0.1693      0.17      2.5818     1.2394     0.0088  
   250      1328.72     0.1664     0.1669     2.332      1.6266     0.0038  
   300      1621.37     0.1638     0.1643     2.633      3.5179     0.003   
   350      1912.88     0.1709     0.1696     2.2171     7.4013     0.0145  
   400      2204.15     0.164      0.1655     2.3124    19.7647     0.0057  
   450      2494.65     0.1691     0.1694     2.2839    487.7538    0.0154  
   500      2786.93     0.1662     0.1668     2.4879    348.9163    0.0048  
   550      3078.34     0.1694     0.1655     2.3786    59.5452     0.0019  
   600      3369.19     0.1678     0.1675     2.1739    27.6344     0.0057  
   650      3661.03     0.1672     0.1708      2.01     77.8554     0.0081  
   700      3952.61      0.17      0.1699     2.0627    431.5726    0.0053  
   750      4244.44     0.1647     0.1655     2.0204   1480.3761    0.0108  
   800      4536.02     0.1697     0.1708     1.8543    618.4503    0.0119  
   850       4827.2     0.1658     0.1667     1.8296    119.2669    0.0035  
   900      5118.28     0.1688     0.165      1.5524    54.0216     0.0053  
   950      5409.31     0.1641     0.163      1.4485    31.4836     0.0031  
   1000     5700.44     0.1647     0.1641     1.5595    17.4837     0.005   
   1050     5991.83     0.1672     0.1664     1.5112    11.2768     0.006   
   1100     6283.34     0.1634     0.165      1.4806    15.9986     0.0068  
   1150     6573.84     0.1702     0.1714     1.4856    19.9469     0.0041  
   1200      6863.2     0.1652     0.1664     1.2039     27.688     0.0098  
   1250     7151.85     0.1651     0.1652     1.3001     65.32      0.0068  
   1300      7440.3     0.1648     0.1658     1.4691    119.6942    0.0131  
   1350     7730.06     0.1685     0.1682      1.51     274.3454    0.0131  
   1400     8019.68     0.1656     0.1674     1.5088    934.5221    0.0218  
   1450     8309.09     0.1703     0.1719     2.0575   4836.9837    0.0045  
   1500     8598.36     0.1705     0.1738     2.4294   7502.0459    0.0417  
   1550     8887.77     0.168      0.166      1.9163   3169.7638    0.0029  
   1600     9176.52     0.1777      0.18      2.693    13957.3729   0.0045  
   1650     9465.49     0.1807     0.1819     2.3033   11758.1898   0.0038  
   1700     9753.98     0.205      0.202      2.631    36600.4183   0.0173  
   1750     10040.99    0.1789     0.1778     2.2867   41298.2269   0.0128  
   1800     10328.15    0.1708     0.1734     1.8646   4395.3805    0.0371  
   1850     10614.56    0.168      0.1698     1.9473   8525.6024    0.0023  
   1900     10900.44    0.164      0.1649     1.5158   18401.7523   0.009   
   1950     11186.44    0.1592     0.162      1.0409   5323.5356    0.0009  
   2000     11472.15    0.1627     0.1601     0.8113    696.1545    0.0061  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       151.09     0.4976     0.4955     0.7019     0.0039     0.003   
   100       494.75     0.5008     0.4969     0.6422     0.0051     0.0013  
   150       840.49     0.4937     0.4982     0.8096     0.0058     0.0006  
   200      1186.44     0.4962     0.5004     0.8436     0.0063     0.0007  
   250       1532.5     0.5008     0.4977     0.8883     0.0063      0.0    
   300      1879.89     0.4926     0.5033     0.8963     0.0062     0.0003  
   350       2227.5     0.4911     0.494      0.9118     0.0064     0.0003  
   400      2574.59     0.4918     0.4902     0.8356     0.0063     0.0001  
   450      2922.16     0.4842     0.4904     0.8109     0.0065     0.0001  
   500       3270.2     0.474      0.4738     0.915      0.0062     0.0002  
   550      3617.48     0.4436     0.441      0.9967     0.006       0.0    
   600      3965.66     0.2849     0.2874     1.0218     0.0065     0.0005  
   650      4313.76     0.1717     0.1714     1.1972     0.0125     0.0006  
   700      4660.57     0.1661     0.1654     1.1097     0.0335     0.0013  
   750      5006.32     0.1696     0.1665     1.0973     0.048      0.0002  
   800      5352.74     0.1616     0.165      0.9522     0.0473     0.0035  
   850      5698.09     0.1707     0.1675     1.1579     0.0496     0.0008  
   900      6042.38     0.1628     0.1628     1.097      0.0478     0.0004  
   950      6386.18     0.1656     0.1651     1.0341     0.0492     0.0013  
   1000      6729.8     0.1646     0.1641     1.2366     0.0522     0.0004  
   1050     7073.12     0.1677     0.1666     1.1243     0.0511     0.0006  
   1100     7416.65     0.1611     0.1617     1.086      0.0485     0.0021  
   1150      7759.5     0.1658     0.1656     1.0455     0.0452     0.0022  
   1200     8102.29     0.1638     0.1622     1.1537     0.0482     0.001   
   1250     8446.66     0.1615     0.1635     1.2156     0.049      0.0028  
   1300     8788.32     0.1637     0.1611     1.1505     0.0503     0.0013  
   1350     9129.83     0.1643     0.1617     1.183      0.0482     0.0013  
   1400     9470.87     0.1649     0.1644     0.8905     0.0453     0.001   
   1450     9812.52     0.1651     0.1648     0.6785     0.0444     0.0002  
   1500     10155.76    0.1652     0.1652     0.882      0.0516     0.0026  
   1550     10496.75    0.1658     0.1652     0.8348     0.0573     0.0007  
   1600     10837.72    0.1644     0.1626     0.9804     0.0514     0.0003  
   1650     11176.9     0.164      0.163      1.0852     0.0481     0.0015  
   1700     11516.04    0.1638     0.1618     0.8267     0.0519      0.0    
   1750     11855.6     0.1614     0.161      1.1501     0.0513     0.0003  
   1800     12195.4     0.1631     0.1614     0.9451     0.0555     0.0003  
   1850     12535.91    0.1627     0.1623     1.0277     0.0463     0.0006  
   1900     12876.17    0.1638     0.163      1.0347     0.0509     0.0007  
   1950     13216.13    0.1649     0.1635     0.9065     0.0624     0.0011  
   2000     13555.56    0.1645     0.1623     1.079      0.063      0.0002  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       151.36     0.4996     0.5011     0.7208     0.0039     0.0006  
   100       555.33     0.4985     0.5002     0.8246     0.004       0.0    
   150       960.73     0.5054     0.4962     0.7533     0.0039     0.0001  
   200      1366.69     0.5006     0.4984     0.7073     0.0044      0.0    
   250      1774.11     0.5022     0.4997     0.7519     0.0039     0.0001  
   300      2180.67     0.4964     0.4985     0.7087     0.0042     0.0003  
   350       2589.2     0.5006     0.4982     0.778      0.0039     0.0003  
   400      2996.46     0.4954     0.4933     0.6843     0.0039     0.0002  
   450      3403.58     0.498      0.4937     0.747      0.004      0.0005  
   500      3811.58     0.499      0.4938     0.7006     0.0038     0.0002  
   550      4220.92     0.4883     0.4912     0.8219     0.0039     0.0002  
   600      4629.13     0.4838     0.484      0.7645     0.0041     0.0006  
   650      5036.41     0.4662     0.4617     0.9218     0.0038     0.0003  
   700      5443.99     0.3512     0.339      1.1084     0.004      0.0004  
   750      5852.19     0.1748     0.1733     1.2784     0.0069     0.0004  
   800       6260.1     0.1662     0.1667     1.1895     0.0201     0.0005  
   850      6667.25     0.1664     0.1667     1.2245     0.0272     0.0017  
   900      7072.84     0.1671     0.1649     1.2394     0.0307     0.0024  
   950      7477.99     0.1662     0.1642     1.1846     0.0308     0.0002  
   1000      7884.2     0.1665     0.1648     1.0141     0.028      0.0004  
   1050     8288.67     0.1651     0.1676     1.1266     0.0289     0.0012  
   1100      8692.4     0.1656     0.1632     1.0929     0.0304     0.0003  
   1150      9096.2     0.1674     0.1686     1.0086     0.0333     0.0028  
   1200     9499.02     0.1652     0.1628     1.079      0.0339     0.0014  
   1250     9900.33     0.1652     0.1652     0.9081     0.0403     0.0013  
   1300     10301.67    0.1626     0.1629     0.9005     0.0388     0.0004  
   1350     10703.64    0.1624     0.1617     0.8836     0.0338     0.0021  
   1400     11104.47    0.1627     0.1637     0.9969     0.0338     0.012   
   1450     11505.47    0.165      0.1659     0.7793     0.0357     0.0005  
   1500     11904.34    0.1638     0.1636     0.7826     0.0396     0.0009  
   1550     12301.19    0.1626     0.1627     0.7475     0.0388     0.0014  
   1600     12699.8     0.1649     0.1631     0.6845     0.0414     0.0019  
   1650     13096.97    0.1628     0.1624     0.7794     0.039      0.0021  
   1700     13493.27    0.1628     0.1626     0.7932     0.044      0.0024  
   1750     13889.72    0.1635     0.1621     0.693      0.0423     0.0005  
   1800     14286.62    0.1626     0.1617     0.7537     0.0415     0.0009  
   1850     14682.94    0.1622     0.1615     0.8098     0.0435     0.0015  
   1900     15078.88    0.162      0.1623     0.7057     0.0438     0.0015  
   1950     15473.8     0.1636     0.1616     0.7209     0.0415     0.0058  
   2000     15869.97    0.1619     0.1627     0.9256     0.0493     0.001   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       152.04     0.5062     0.4999     0.5573     0.0029     0.0014  
   100       612.74     0.4952     0.5017     0.7779     0.003      0.0002  
   150      1074.44     0.4984     0.5015      0.78      0.0031      0.0    
   200      1537.58     0.4965     0.4986     0.6822     0.0032     0.0001  
   250      2000.68     0.501      0.5034     0.6446     0.003       0.0    
   300      2465.38     0.4968     0.502      0.6395     0.0032     0.0001  
   350      2929.54     0.4941     0.4952     0.6725     0.0031     0.0001  
   400      3394.11     0.4987     0.4997     0.6864     0.0031     0.0003  
   450      3859.41     0.4967     0.4971     0.6784     0.003      0.0002  
   500       4324.5     0.5036     0.4967     0.6609     0.003      0.0006  
   550      4789.63     0.4934     0.4967     0.5456     0.0029     0.0004  
   600      5256.25     0.4906     0.4914     0.639      0.0028     0.0002  
   650      5720.83     0.4752     0.4753     0.7343     0.0032     0.0003  
   700      6187.94     0.3733     0.3736     0.8246     0.0032     0.0008  
   750       6655.9     0.1812     0.1823     1.0093     0.005      0.0001  
   800      7122.57     0.1696     0.1676     1.022      0.0171     0.0024  
   850      7588.88     0.1704     0.1698     0.9652     0.0284     0.0014  
   900      8050.73     0.1731     0.171      0.9928     0.0293     0.0008  
   950      8512.62     0.1687     0.1674     0.9946     0.0297     0.0003  
   1000     8973.94     0.1657     0.1639     0.898      0.0334     0.0004  
   1050     9434.62     0.1694     0.1678     0.9289     0.034      0.0035  
   1100     9895.43     0.1618     0.1636     0.9059     0.0313     0.0021  
   1150     10357.44    0.1618     0.1639     0.8303     0.0317     0.0006  
   1200     10816.76    0.1651     0.1651     0.8037     0.0348     0.0019  
   1250     11276.65    0.1615     0.1618     0.7864     0.0281     0.0011  
   1300     11736.2     0.162      0.162      0.7705     0.0291     0.0023  
   1350     12195.99    0.161      0.1625     0.8913     0.0361     0.001   
   1400     12655.24    0.1628     0.1619     0.6862     0.0349     0.0003  
   1450     13113.16    0.1633     0.1624     0.756      0.0332     0.0017  
   1500     13571.36    0.1622     0.1624     0.5907     0.0417     0.0001  
   1550     14030.41    0.1607     0.1617     0.5971     0.0426     0.0021  
   1600     14486.62    0.1637     0.1643     0.7265     0.0469     0.0053  
   1650     14943.12    0.1603     0.1621     0.7518     0.0432     0.0015  
   1700     15400.34    0.1622     0.1619     0.6285     0.0428     0.0009  
   1750     15855.72    0.1634     0.1632     0.6273     0.0408     0.0017  
   1800     16311.8     0.1644     0.1624     0.5733     0.0428     0.0064  
   1850     16767.48    0.1632     0.1638     0.5667     0.0492     0.0011  
   1900     17223.24    0.1652     0.1628     0.6561     0.0516     0.0019  
   1950     17678.28    0.1634     0.1641     0.6828     0.0499     0.0028  
   2000     18132.95    0.1648     0.1637     0.624      0.0505     0.0007  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       151.73     0.4972     0.4956     0.473      0.002      0.0009  
   100       667.24     0.501      0.5009     0.5145     0.0023     0.0008  
   150      1187.52     0.5017     0.4986     0.6389     0.0026     0.0005  
   200       1707.5     0.4965     0.4992     0.5983     0.0026     0.0002  
   250      2227.89     0.497      0.5006     0.5389     0.0025     0.0004  
   300       2747.9     0.4992     0.4972     0.7025     0.0025     0.0003  
   350       3269.3     0.5028     0.5006     0.5961     0.0025     0.001   
   400      3790.96     0.4968     0.497      0.6622     0.0025     0.0002  
   450      4314.14     0.4984     0.4973     0.5667     0.0025     0.0001  
   500       4838.4     0.4946     0.4996     0.5812     0.0025     0.0004  
   550      5361.85     0.4979     0.4945     0.6031     0.0025     0.0008  
   600      5886.23     0.4985     0.4965     0.5936     0.0024     0.0011  
   650      6412.11     0.4939     0.4904     0.5308     0.0025     0.0007  
   700      6936.57     0.473      0.4776     0.6215     0.0025     0.0004  
   750       7464.2     0.3487     0.3441     0.8582     0.0028     0.0002  
   800      7990.34     0.1731     0.1733     0.8308     0.0053     0.0006  
   850      8515.42     0.1665     0.1676     0.771      0.0131     0.0013  
   900      9038.71     0.1643     0.164      0.7771     0.0314     0.0018  
   950       9563.5     0.1671     0.1683     0.8371     0.0281     0.0012  
   1000     10085.72    0.1616     0.1653     0.7655     0.0349     0.0103  
   1050     10604.22    0.1609     0.1628     0.7913     0.0333     0.0044  
   1100     11122.21    0.1629     0.1626     0.7086     0.0321     0.002   
   1150     11641.78    0.164      0.1657     0.6906     0.0335     0.0068  
   1200     12158.81    0.1627     0.1618     0.7029     0.0409     0.0027  
   1250     12676.96    0.162      0.1632     0.6261     0.0392     0.0032  
   1300     13193.04    0.164      0.1639     0.5959     0.0432     0.0003  
   1350     13713.42    0.1643     0.1652     0.609      0.0406     0.0004  
   1400     14236.28    0.165      0.1664     0.6614     0.0384     0.0021  
   1450     14756.64    0.1608     0.1616     0.8229     0.0441     0.0009  
   1500     15276.63    0.1615     0.1624     0.7026     0.0512     0.0015  
   1550     15797.8     0.1608     0.1622     0.6363     0.0408     0.0037  
   1600     16319.6     0.1624     0.1622     0.7307     0.0446     0.0026  
   1650     16838.17    0.1638     0.1631     0.863      0.0419     0.0002  
   1700     17354.2     0.1636     0.1632     1.0215     0.0508     0.0035  
   1750     17867.78    0.1636     0.1615     0.9299     0.0474     0.0031  
   1800     18381.07    0.1645     0.163      0.7713     0.0462     0.0002  
   1850     18892.74    0.1628     0.161      0.942      0.0484     0.002   
   1900     19406.49    0.1642     0.1628     0.7814     0.0449     0.0003  
   1950     19933.26    0.1633     0.1614     1.0127     0.0474     0.0007  
   2000     20470.78    0.1616     0.1624     0.9392     0.0501      0.0    
