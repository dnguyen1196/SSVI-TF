Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.5983569622039795
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       173.0      0.1472     0.1471     3.1623     4.4721      0.0    
   100       354.86     0.0807     0.0812     3.1622     1.2615      0.0    
   150       538.19     0.0692     0.0699     3.1536      0.0        0.0    
   200       721.78     0.0665     0.0658     2.9992      0.0        0.0    
   250       905.53     0.0628     0.063      2.1991      0.0        0.0    
   300      1089.45     0.0617     0.0615     2.1883      0.0        0.0    
   350      1273.18     0.0603     0.0604     1.4467      0.0        0.0    
   400      1456.99     0.0603     0.0602     1.3735      0.0        0.0    
   450      1640.85     0.0599     0.0599     1.5931      0.0        0.0    
   500      1824.54     0.0595     0.0594     1.3933      0.0        0.0    
   550      2008.29     0.0595     0.0593     1.1617      0.0        0.0    
   600       2192.1     0.0595     0.0594     0.9604      0.0        0.0    
   650      2375.15     0.0595     0.0593     0.9253      0.0        0.0    
   700      2558.13     0.0592     0.0592     0.7726      0.0        0.0    
   750      2740.99     0.0595     0.0595     0.944       0.0        0.0    
   800       2924.0     0.0594     0.0592     1.0934      0.0        0.0    
   850      3106.58     0.0592     0.0591     0.8572      0.0        0.0    
   900      3289.02     0.0593     0.0593     0.7656      0.0        0.0    
   950      3471.53     0.0593     0.0592     1.0978      0.0        0.0    
   1000     3654.62     0.0593     0.0593     1.3659      0.0        0.0    
   1050     3837.39     0.0591     0.0591     1.1513      0.0        0.0    
   1100     4019.63     0.0591     0.059      0.9415      0.0        0.0    
   1150     4201.81     0.0589     0.0589     0.6992      0.0        0.0    
   1200     4384.23     0.0594     0.0591     0.7483      0.0        0.0    
   1250     4566.97     0.0595     0.0593     0.893       0.0        0.0    
   1300     4748.81     0.0591     0.0592     0.6924      0.0        0.0    
   1350     4930.73     0.0593     0.0592     1.0057      0.0        0.0    
   1400     5113.01     0.0592     0.0591     0.8083      0.0        0.0    
   1450      5294.9     0.0591     0.0591     1.0643      0.0        0.0    
   1500     5477.02     0.0596     0.0593     1.3482      0.0        0.0    
   1550     5658.95     0.0593     0.0591     1.1023      0.0        0.0    
   1600     5840.89     0.0591     0.0591     0.8005      0.0        0.0    
   1650     6023.08     0.0589     0.0589     0.8924      0.0        0.0    
   1700     6206.57     0.0591     0.0591     1.0678      0.0        0.0    
   1750     6388.76     0.059      0.0589     0.5694      0.0        0.0    
   1800     6569.23     0.0592     0.0592     0.7993      0.0        0.0    
   1850     6749.82     0.0589     0.0588     1.1078      0.0        0.0    
   1900     6930.21     0.0595     0.0594     1.2786      0.0        0.0    
   1950     7110.62     0.0597     0.0594     1.0544      0.0        0.0    
   2000     7291.09     0.0593     0.0592     0.9365      0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       171.16     0.1157     0.1163     3.0252     4.4721      0.0    
   100       352.28     0.0734     0.0733     3.0211     0.0002      0.0    
   150       533.88     0.0664     0.0667     2.188       0.0        0.0    
   200       715.71     0.0642     0.0641     2.5617      0.0        0.0    
   250       897.57     0.0619     0.0618     2.0438      0.0        0.0    
   300       1079.4     0.0618     0.0617     1.812       0.0        0.0    
   350       1261.3     0.0608     0.0606     1.7247      0.0        0.0    
   400      1443.14     0.061      0.0608     1.8479      0.0        0.0    
   450      1624.99     0.0604     0.0601     1.7925      0.0        0.0    
   500      1806.77     0.0603      0.06      1.7564      0.0        0.0    
   550      1988.74     0.0595     0.0594     1.3622      0.0        0.0    
   600      2170.63     0.0601     0.0599     1.258       0.0        0.0    
   650      2352.49     0.0596     0.0595     1.5709      0.0        0.0    
   700       2534.4     0.0598     0.0597     1.4712      0.0        0.0    
   750      2716.33     0.0596     0.0596     1.1468      0.0        0.0    
   800      2898.27     0.0596     0.0596     1.1595      0.0        0.0    
   850      3080.25     0.0597     0.0597     1.332       0.0        0.0    
   900      3262.31     0.0593     0.0593     1.4504      0.0        0.0    
   950      3444.25     0.0595     0.0593     1.292       0.0        0.0    
   1000     3626.96     0.0597     0.0597     1.1938      0.0        0.0    
   1050     3808.87     0.0597     0.0597     1.0418      0.0        0.0    
   1100     3990.67     0.0596     0.0596     1.2623      0.0        0.0    
   1150     4172.54     0.0594     0.0592     0.9388      0.0        0.0    
   1200     4354.72     0.0592     0.059      0.8434      0.0        0.0    
   1250      4536.8     0.0589     0.0589     0.9485      0.0        0.0    
   1300     4718.94     0.0591     0.059      0.8909      0.0        0.0    
   1350     4901.02     0.0592     0.0593     0.9433      0.0        0.0    
   1400      5083.1     0.0593     0.0593     1.0021      0.0        0.0    
   1450     5265.19     0.0593     0.0592     1.0472      0.0        0.0    
   1500     5449.09     0.0592     0.0592     0.9402      0.0        0.0    
   1550     5631.16     0.0594     0.0594     1.0209      0.0        0.0    
   1600     5813.33     0.0592     0.0591     1.0122      0.0        0.0    
   1650     5995.38     0.0595     0.0595     0.9709      0.0        0.0    
   1700     6177.54     0.0589     0.059      1.1069      0.0        0.0    
   1750     6359.56     0.0593     0.0592     0.9907      0.0        0.0    
   1800      6541.5     0.0592     0.0591     0.9555      0.0        0.0    
   1850     6723.41     0.0594     0.0593     0.9662      0.0        0.0    
   1900     6905.37     0.0591     0.0589     0.8228      0.0        0.0    
   1950      7087.4     0.0592     0.0592     0.8448      0.0        0.0    
   2000     7271.23     0.059      0.0589     0.8589      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       171.46     0.1091     0.1101     2.901      4.4721      0.0    
   100       352.94     0.072      0.0727     2.5611     0.0003      0.0    
   150       534.73     0.066      0.0664     2.1641      0.0        0.0    
   200       716.63     0.062      0.0622     1.5738      0.0        0.0    
   250       898.42     0.0602     0.0603     1.4795      0.0        0.0    
   300      1080.18     0.0605     0.0606     1.5562      0.0        0.0    
   350      1262.02     0.0602     0.0603     1.2828      0.0        0.0    
   400      1443.98     0.0593     0.0595     1.2827      0.0        0.0    
   450      1625.93     0.0597     0.0598     1.2916      0.0        0.0    
   500       1809.1     0.0595     0.0598     1.2393      0.0        0.0    
   550      1990.88     0.0599     0.0599     1.6061      0.0        0.0    
   600       2172.7     0.0594     0.0597     1.3768      0.0        0.0    
   650      2354.52     0.0595     0.0597     1.1642      0.0        0.0    
   700      2536.33     0.0593     0.0594     1.019       0.0        0.0    
   750       2718.2     0.0592     0.0593     1.0472      0.0        0.0    
   800      2900.12     0.0594     0.0596     0.9921      0.0        0.0    
   850      3082.15     0.0593     0.0595     1.3301      0.0        0.0    
   900      3263.95     0.0597     0.0598     1.3151      0.0        0.0    
   950      3446.14     0.0593     0.0596     1.3108      0.0        0.0    
   1000     3628.32     0.0593     0.0594     1.1674      0.0        0.0    
   1050     3810.32     0.0595     0.0596     0.9836      0.0        0.0    
   1100      3992.3     0.0589     0.0592     0.8878      0.0        0.0    
   1150     4174.31     0.0591     0.0594     0.7388      0.0        0.0    
   1200     4356.33     0.059      0.0592     0.8986      0.0        0.0    
   1250     4538.35     0.059      0.0591     0.9942      0.0        0.0    
   1300     4720.29     0.0591     0.0593     0.8881      0.0        0.0    
   1350     4902.25     0.0593     0.0594     0.9641      0.0        0.0    
   1400     5084.37     0.059      0.0594     1.1455      0.0        0.0    
   1450     5268.18     0.0591     0.0592     0.8513      0.0        0.0    
   1500      5450.5     0.0589     0.0591     0.8559      0.0        0.0    
   1550     5632.75     0.0595     0.0594     0.9749      0.0        0.0    
   1600     5814.88     0.059      0.0593     1.2556      0.0        0.0    
   1650     5996.97     0.0589     0.0592     0.9296      0.0        0.0    
   1700     6179.18     0.059      0.0591     0.7802      0.0        0.0    
   1750     6361.06     0.0593     0.0595     0.8644      0.0        0.0    
   1800      6543.2     0.059      0.0591     0.7987      0.0        0.0    
   1850      6725.1     0.0592     0.0595     0.9268      0.0        0.0    
   1900     6907.24     0.0596     0.0598     0.9517      0.0        0.0    
   1950     7090.23     0.059      0.0592     0.8942      0.0        0.0    
   2000      7272.3     0.0591     0.0592     0.9751      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       172.08     0.1066     0.108      2.7838     4.4721      0.0    
   100       354.73     0.0727     0.0733     2.5346     0.0002      0.0    
   150       537.76     0.0658     0.0665     1.6362      0.0        0.0    
   200       720.79     0.0619     0.0621     1.3911      0.0        0.0    
   250       903.85     0.0608     0.061      1.348       0.0        0.0    
   300      1086.88     0.0602     0.0603     1.353       0.0        0.0    
   350      1269.91     0.0595     0.0598     1.5708      0.0        0.0    
   400       1453.0      0.06      0.0601     1.1878      0.0        0.0    
   450      1637.62     0.0596     0.0597     1.3478      0.0        0.0    
   500      1820.61     0.0595     0.0597     1.2558      0.0        0.0    
   550      2003.75     0.0592     0.0594     1.2718      0.0        0.0    
   600      2187.07     0.059      0.0592     1.0896      0.0        0.0    
   650      2370.26     0.0595     0.0595     0.9917      0.0        0.0    
   700      2553.44     0.0591     0.0593     1.0043      0.0        0.0    
   750      2736.62     0.0591     0.0594     0.8189      0.0        0.0    
   800      2919.68     0.0592     0.0595     1.1327      0.0        0.0    
   850      3102.71     0.0591     0.0593     1.0904      0.0        0.0    
   900      3285.83     0.0592     0.0593      1.36       0.0        0.0    
   950      3469.95     0.059      0.0592     1.2606      0.0        0.0    
   1000     3653.21     0.0592     0.0593     0.935       0.0        0.0    
   1050     3836.39     0.0592     0.0593     1.0902      0.0        0.0    
   1100     4019.47     0.0593     0.0594     1.2976      0.0        0.0    
   1150     4202.65     0.0591     0.0593     0.958       0.0        0.0    
   1200     4385.76     0.0591     0.0593     1.0626      0.0        0.0    
   1250     4568.96     0.0591     0.0593     1.1027      0.0        0.0    
   1300     4752.11     0.0591     0.0593     0.8642      0.0        0.0    
   1350     4935.28     0.0589     0.0592     1.0707      0.0        0.0    
   1400     5118.31     0.0589     0.0593     1.1867      0.0        0.0    
   1450     5302.04     0.059      0.0593     0.999       0.0        0.0    
   1500     5485.18     0.0592     0.0593     1.0092      0.0        0.0    
   1550     5668.26     0.0587     0.059      1.0071      0.0        0.0    
   1600      5851.4     0.0589     0.059      0.9415      0.0        0.0    
   1650     6034.45     0.0589     0.0591     0.7195      0.0        0.0    
   1700     6217.49     0.0589     0.0591     0.8739      0.0        0.0    
   1750     6400.77     0.0591     0.0592     1.0456      0.0        0.0    
   1800     6583.86     0.059      0.0592     1.031       0.0        0.0    
   1850     6766.88     0.0592     0.0594     1.0185      0.0        0.0    
   1900     6949.99     0.059      0.0591     0.8516      0.0        0.0    
   1950     7133.79     0.0589     0.059      0.9521      0.0        0.0    
   2000     7316.77     0.0591     0.0593     1.2131      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       171.84     0.1067     0.108      2.7339     4.4721      0.0    
   100       354.72     0.0745     0.0753     2.3125     0.0003      0.0    
   150       537.79     0.0674     0.068      1.2703     0.0001      0.0    
   200       720.96     0.0628     0.0633     1.139       0.0        0.0    
   250       904.26     0.0613     0.0615     1.1343      0.0        0.0    
   300      1087.41     0.0603     0.0607     1.4567      0.0        0.0    
   350      1270.74     0.0596     0.0597     1.2522      0.0        0.0    
   400      1456.09     0.0593     0.0596     1.3818      0.0        0.0    
   450      1640.31     0.0591     0.0594     1.2531      0.0        0.0    
   500      1823.67     0.059      0.0593     1.0775      0.0        0.0    
   550      2007.12     0.0591     0.0593     0.9214      0.0        0.0    
   600      2190.51     0.0591     0.0592     1.1049      0.0        0.0    
   650      2373.91     0.059      0.0591     0.9557      0.0        0.0    
   700      2557.23     0.059      0.0592     0.9593      0.0        0.0    
   750      2740.48     0.059      0.0592     0.8285      0.0        0.0    
   800      2923.92     0.0589     0.0591     0.7467      0.0        0.0    
   850      3107.18     0.0588     0.059      0.7443      0.0        0.0    
   900      3292.32     0.0589     0.0591     0.7078      0.0        0.0    
   950      3475.75     0.0588     0.059      0.7408      0.0        0.0    
   1000     3659.21     0.0589     0.0592     0.8906      0.0        0.0    
   1050     3842.35     0.0589     0.0592     0.984       0.0        0.0    
   1100     4025.57     0.059      0.0592     1.1333      0.0        0.0    
   1150     4208.86     0.059      0.0593     0.9961      0.0        0.0    
   1200     4392.19     0.059      0.0593     1.0517      0.0        0.0    
   1250     4575.52     0.0588     0.0591     0.9404      0.0        0.0    
   1300     4758.91     0.0587     0.059      0.7886      0.0        0.0    
   1350      4942.2     0.0592     0.0592     0.7758      0.0        0.0    
   1400     5125.67     0.059      0.0592     1.0193      0.0        0.0    
   1450      5309.0     0.0588     0.0589     1.0479      0.0        0.0    
   1500     5492.53     0.059      0.0592     1.0404      0.0        0.0    
   1550     5675.83     0.0587     0.0589     0.7495      0.0        0.0    
   1600     5859.27     0.0588     0.059      0.6789      0.0        0.0    
   1650     6042.72     0.0589     0.0591     0.6622      0.0        0.0    
   1700     6226.18     0.0591     0.0594     0.9435      0.0        0.0    
   1750     6409.65     0.0587     0.0589     0.9282      0.0        0.0    
   1800      6593.2     0.0591     0.0595     0.8486      0.0        0.0    
   1850     6776.81     0.0587     0.0589     0.5857      0.0        0.0    
   1900      6961.0     0.059      0.0591     1.0732      0.0        0.0    
   1950     7144.36     0.0589     0.0591     1.1681      0.0        0.0    
   2000     7327.82     0.0588     0.059      0.6732      0.0        0.0    
