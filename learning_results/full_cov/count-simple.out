Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  8.044231414794922
max_count =  19  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       150.08     2.8073     2.8449     0.3162     4.3917   127611.71  103469.17    0.0002  
   100       501.47     2.8101     2.8475     0.2857     0.3656   126475.33  102536.56     0.0    
   150       854.09     2.8089     2.8471     0.2814     0.1844   125180.83  101480.75     0.0    
   200      1207.21     2.7996     2.8375     0.3019     0.1251    122344.5   99171.47    0.0001  
   250      1560.06     2.7682     2.8056     0.3036     0.1384   116496.29   94413.53     0.0    
   300      1913.86     2.7007     2.7381     0.2957     0.1231   107344.94   86970.92     0.0    
   350      2266.31     2.5952     2.6312     0.2937     0.1285    95697.21   77495.36     0.0    
   400      2616.92     2.4424     2.4761     0.2853     0.1057    82949.41   67115.15     0.0    
   450      2967.17     2.2415     2.2719     0.2778     0.0817    70571.41   57011.5      0.0    
   500      3317.25     2.0228     2.0477     0.3028     0.0576    59913.08   48308.12     0.0    
   550      3667.13     1.8207     1.8391     0.264      0.0521    51828.91   41700.85    0.0001  
   600      4016.77     1.6446     1.6549     0.3062     0.055     45910.94   36828.49     0.0    
   650      4366.55     1.5006     1.5069     0.2763     0.0551    41843.12   33485.55    0.0001  
   700      4715.37     1.3824     1.3847     0.2921     0.0445    38985.75   31151.64    0.0001  
   750      5063.52     1.2855     1.2853     0.2024     0.0357    36980.87   29504.08    0.0001  
   800      5413.34     1.2071     1.2044     0.2154     0.0341    35478.6    28296.01    0.0001  
   850      5762.23     1.142      1.1396     0.2396     0.0265    34407.94   27398.95     0.0    
   900      6110.81     1.087      1.0835     0.2054     0.0241    33558.24   26708.09     0.0    
   950      6459.57     1.0376     1.0331     0.1816     0.0224    32926.77   26179.7      0.0    
   1000     6809.13     0.9993     0.9926     0.2034     0.0226    32423.86   25794.58     0.0    
   1050     7158.97     0.9618     0.9571     0.1809     0.0221    32020.54   25478.29     0.0    
   1100     7507.96     0.9276     0.9213     0.2644     0.0216    31666.32   25177.74     0.0    
   1150     7858.17     0.9028     0.8966     0.1897     0.0218    31409.12   24968.84     0.0    
   1200     8206.67     0.8757     0.8706     0.1797     0.0214    31166.15   24775.2      0.0    
   1250     8555.33     0.8554     0.8496     0.2302     0.0208    30990.48   24625.01     0.0    
   1300      8904.8     0.837      0.8313     0.2073     0.0202    30831.09   24498.34     0.0    
   1350     9254.25     0.8231     0.8169     0.1177     0.0198    30734.51   24413.58     0.0    
   1400     9595.02     0.8126     0.8068     0.2335     0.0191    30643.51   24361.26     0.0    
   1450     9913.37     0.7949     0.788      0.2243     0.0192    30479.48   24222.6      0.0    
   1500     10232.03    0.7806     0.7748     0.1515     0.0186    30393.15   24155.79     0.0    
   1550     10550.06    0.774      0.7685     0.2138     0.0178    30323.87   24112.23     0.0    
   1600     10868.4     0.7673     0.7601     0.1746     0.0179    30288.6    24068.38     0.0    
   1650     11196.56    0.7594     0.753      0.1324     0.0172    30240.81   24029.72     0.0    
   1700     11525.33    0.7556     0.7497     0.1136     0.017     30196.8    23999.86     0.0    
   1750     11847.0      0.75      0.7428     0.154      0.0162    30168.09   23961.43     0.0    
   1800     12183.26    0.7441     0.736      0.1608     0.0155    30120.81   23935.37     0.0    
   1850     12521.97    0.7435     0.7344     0.126      0.0153    30078.03   23910.19     0.0    
   1900     12859.58    0.7373     0.7316     0.1724     0.0152    30038.53   23875.37     0.0    
   1950     13196.71    0.7347     0.7272     0.1631     0.0146    30027.81   23849.47     0.0    
   2000     13534.1     0.7323     0.7241     0.1422     0.0141    30010.78   23839.83     0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       147.39     2.8319     2.8853     0.0732     3.5238    86140.74  140170.67    0.0001  
   100       572.33     2.8179     2.8703     0.0358     0.796     107038.3  174561.25    0.0001  
   150       997.32     2.8141     2.8659     0.0284     0.3779   114779.87  187311.57    0.0001  
   200      1425.86     2.8122     2.8645     0.0237     0.2378   118482.62  193407.43    0.0001  
   250      1850.51     2.811      2.8636     0.0174     0.1584   120613.17  196911.79    0.0001  
   300      2277.82     2.8114     2.8637     0.0203     0.1214    121962.4  199132.51    0.0001  
   350      2703.99     2.8112     2.8631     0.0181     0.098    122891.06  200660.22     0.0    
   400      3128.31     2.8107     2.8634     0.016      0.0798   123563.58  201766.74    0.0001  
   450      3544.55     2.8109     2.8636     0.0198     0.0662   124069.01  202598.17     0.0    
   500      3945.94     2.8113     2.8637     0.0145     0.055    124464.67  203248.97     0.0    
   550      4345.65     2.8111     2.8636     0.0183     0.047    124782.53  203771.79     0.0    
   600      4765.32     2.8113     2.8642     0.0181     0.0413   125041.36  204197.63     0.0    
   650      5197.64     2.8115     2.8641     0.0155     0.0366   125256.72  204551.95     0.0    
   700      5621.75     2.8114     2.8642     0.0155     0.0327   125439.15  204851.92     0.0    
   750      6034.24     2.8117     2.8647     0.0144     0.0279   125595.87  205109.61     0.0    
   800       6446.3     2.8115     2.8645     0.0131     0.0254   125728.99  205328.65     0.0    
   850      6858.53     2.8119     2.8648     0.0154     0.0233   125845.44  205520.15     0.0    
   900      7270.03     2.8118     2.8649     0.0147     0.0208   125947.52   205688.0     0.0    
   950      7680.29     2.8116     2.8648     0.0145     0.0196   126037.88  205836.66     0.0    
   1000     8091.05     2.812      2.8653     0.0138     0.0177   126117.95  205968.38     0.0    
   1050     8508.75     2.8123     2.8652     0.0146     0.0164   126189.44  206085.89     0.0    
   1100     8931.11     2.812      2.8654     0.0146     0.0149   126253.31  206190.94     0.0    
   1150     9354.24     2.8124     2.8653     0.0129     0.0134   126311.42   206286.6     0.0    
   1200     9777.27     2.8123     2.8654     0.0136     0.013    126363.79  206372.62     0.0    
   1250     10199.94    2.8126     2.8656     0.0149     0.0122   126410.83  206449.97     0.0    
   1300     10625.14    2.8127     2.8654     0.0133     0.0116   126454.33   206521.5     0.0    
   1350     11050.27    2.8125     2.8657     0.0123     0.0104   126493.88   206586.6     0.0    
   1400     11475.48    2.8127     2.866      0.0147     0.0101   126530.46  206646.73     0.0    
   1450     11899.76    2.8131     2.8659     0.0126     0.0091   126563.96  206701.76     0.0    
   1500     12324.32    2.8131     2.8661     0.015      0.0089   126594.46  206751.96     0.0    
   1550     12749.84    2.8131     2.866      0.0129     0.0082    126622.6  206798.22     0.0    
   1600     13173.01    2.8131     2.8664     0.0135     0.008    126649.11   206841.8     0.0    
   1650     13593.93    2.8132     2.8661     0.0123     0.0073   126673.01  206881.13     0.0    
   1700     14007.78    2.8132     2.8663     0.0147     0.0068    126694.8  206916.94     0.0    
   1750     14416.3     2.8131     2.8664     0.0135     0.0067   126715.03  206950.24     0.0    
   1800     14827.06    2.8136     2.8663     0.0162     0.0063   126733.51  206980.64     0.0    
   1850     15246.74    2.8135     2.8664     0.0164     0.0061   126750.57  207008.72     0.0    
   1900     15665.79    2.8134     2.8666     0.0156     0.0059   126766.32  207034.57     0.0    
   1950     16084.3     2.8134     2.8666     0.0136     0.0057   126780.74   207058.3     0.0    
   2000     16502.39    2.8132     2.8666     0.0133     0.005    126793.78   207079.7     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       146.31     2.8463     2.883      0.0882     3.3964    80209.86  194871.85     0.0    
   100       652.8      2.8322     2.8735     0.0556     0.9192   102220.75  248862.19     0.0    
   150      1158.93     2.8259     2.867      0.0477     0.4003   111765.17  272268.51     0.0    
   200      1652.49     2.8235     2.8635     0.0417     0.2473   116456.02  283768.94     0.0    
   250      2142.53     2.8216     2.8617     0.0307     0.1809   119148.52  290370.77     0.0    
   300      2634.49     2.8204     2.8605     0.0259     0.1314    120865.4  294580.26     0.0    
   350      3126.78     2.8188     2.8592     0.0296     0.1093    122045.3  297474.35     0.0    
   400       3629.7     2.8183     2.8586     0.025      0.0884   122905.32  299583.32     0.0    
   450      4133.45     2.8177     2.8584     0.0243     0.0729   123556.53  301180.35     0.0    
   500      4638.26     2.8167     2.8579     0.0225     0.0624   124063.87  302424.29     0.0    
   550      5141.88     2.8168     2.8577     0.0238     0.0536    124472.9   303427.1     0.0    
   600      5647.31     2.8172     2.8574     0.0218     0.0468    124808.3  304249.41     0.0    
   650      6151.62     2.8167     2.8574     0.0224     0.041    125085.46  304929.12     0.0    
   700       6657.5     2.8163     2.8567     0.0225     0.0372   125319.17  305502.32     0.0    
   750      7162.58     2.8165     2.8568     0.0252     0.0331   125518.74  305991.66     0.0    
   800      7668.05     2.8163     2.8569     0.0221     0.0304    125692.9  306418.57     0.0    
   850      8173.48     2.8161     2.8566     0.0195     0.0278   125845.28  306792.06     0.0    
   900       8679.5     2.8157     2.8569     0.0197     0.0253   125978.85  307119.51     0.0    
   950      9184.26     2.8158     2.8566     0.0181     0.0224   126096.41  307407.59     0.0    
   1000     9690.72     2.8157     2.8567     0.0202     0.021    126201.94  307666.36     0.0    
   1050     10188.95    2.8156     2.8565     0.0185     0.0197   126296.36  307897.74     0.0    
   1100     10688.09    2.8157     2.8565     0.0194     0.0178    126381.2  308105.69     0.0    
   1150     11186.87    2.8157     2.8565     0.0228     0.0167    126457.9  308293.73     0.0    
   1200     11685.73    2.8158     2.8564     0.0188     0.0159   126528.01  308465.51     0.0    
   1250     12184.72    2.8158     2.8564     0.0199     0.0145   126592.24  308622.94     0.0    
   1300     12683.7     2.8158     2.8563     0.0186     0.0138   126650.67  308766.23     0.0    
   1350     13183.14    2.8155     2.8564     0.0197     0.0126   126704.12   308897.3     0.0    
   1400     13683.05    2.8156     2.8562     0.0178     0.0119   126753.44  309018.14     0.0    
   1450     14181.44    2.8154     2.8564     0.0162     0.0116   126799.01  309129.81     0.0    
   1500     14679.55    2.8156     2.8565     0.0179     0.0107   126840.82  309232.31     0.0    
   1550     15177.85    2.8154     2.8562     0.0188     0.0102    126879.7  309327.57     0.0    
   1600     15676.34    2.8155     2.8563     0.018      0.0096   126916.05  309416.74     0.0    
   1650     16174.68    2.8156     2.8564     0.0192     0.0091   126949.16  309497.84     0.0    
   1700     16672.34    2.8155     2.8563     0.0176     0.0089   126980.42  309574.49     0.0    
   1750     17171.12    2.8157     2.8566     0.0169     0.0084   127009.75  309646.36     0.0    
   1800     17669.47    2.8157     2.8565     0.0177     0.0078   127036.81  309712.67     0.0    
   1850     18168.19    2.8156     2.8565     0.0178     0.0076   127061.89  309774.12     0.0    
   1900     18666.43    2.8156     2.8566     0.019      0.0074   127084.93   309830.6     0.0    
   1950     19165.31    2.8157     2.8565     0.0194     0.007    127106.53  309883.56     0.0    
   2000     19663.88    2.8156     2.8565     0.021      0.0066   127125.92  309931.06     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       145.48     2.8486     2.8871     0.1087     3.2921    78093.22  252826.12     0.0    
   100       727.6      2.8359     2.8768     0.0674     0.9156   100003.49  324513.07     0.0    
   150      1310.62     2.8306     2.8706     0.0544     0.4371   110165.11  357783.86     0.0    
   200      1894.69     2.8265     2.8678     0.0443     0.2524   115303.75   374605.0     0.0    
   250      2477.75     2.8238     2.8658     0.0446     0.1786   118298.77  384406.27     0.0    
   300      3059.65     2.8225     2.8639     0.0397     0.1382   120207.64  390656.31     0.0    
   350      3643.38     2.8215     2.8626     0.0393     0.1125   121539.06  395015.52     0.0    
   400      4226.66     2.8205     2.8622     0.0379     0.093     122500.4  398163.61     0.0    
   450      4808.41     2.8202     2.8615     0.0352     0.077    123227.44  400543.63     0.0    
   500      5384.64     2.8199     2.8607     0.0313     0.066    123796.61  402407.05     0.0    
   550      5947.67     2.8188     2.8605     0.0348     0.0569   124254.16  403904.95     0.0    
   600      6510.04     2.8187      2.86      0.0274     0.0511    124629.7   405133.8     0.0    
   650      7074.28     2.8184     2.8599     0.0277     0.0447   124943.66  406161.35     0.0    
   700      7637.05     2.818      2.8598     0.0258     0.0404   125208.69  407029.07     0.0    
   750      8199.95     2.8177     2.8595     0.0273     0.0355   125434.95  407769.67     0.0    
   800      8761.49     2.8179     2.8592     0.0248     0.0319   125630.97  408411.03     0.0    
   850      9322.97     2.8178     2.8592     0.0233     0.0298    125801.8  408969.97     0.0    
   900      9884.86     2.8177     2.859      0.0254     0.0272   125952.14  409462.03     0.0    
   950      10447.72    2.8176     2.8588     0.0255     0.0249   126085.02  409896.82     0.0    
   1000     11010.75    2.8176     2.8589     0.0242     0.0233   126203.75  410285.45     0.0    
   1050     11573.74    2.8175     2.8588     0.022      0.0213   126309.94   410632.9     0.0    
   1100     12134.47    2.8172     2.8585     0.025       0.02    126406.49  410948.95     0.0    
   1150     12696.1     2.8173     2.8585     0.0231     0.0185   126494.26  411236.19     0.0    
   1200     13259.68    2.8172     2.8586     0.0218     0.0181   126573.81  411496.45     0.0    
   1250     13821.49    2.8172     2.8585     0.0207     0.0162   126646.74  411735.18     0.0    
   1300     14383.51    2.817      2.8585     0.0208     0.0151   126712.93  411951.84     0.0    
   1350     14945.7     2.8167     2.8584     0.0218     0.0146   126774.47   412153.2     0.0    
   1400     15508.0     2.8167     2.8584     0.0216     0.0135    126830.3  412335.91     0.0    
   1450     16069.13    2.817      2.8585     0.0218     0.0127    126882.6  412507.01     0.0    
   1500     16631.3     2.8168     2.8582     0.0251     0.0124    126930.8  412664.63     0.0    
   1550     17193.35    2.8167     2.8583     0.0243     0.0115   126975.16  412809.96     0.0    
   1600     17755.06    2.8167     2.8584      0.02      0.0109   127016.41  412944.94     0.0    
   1650     18315.42    2.8167     2.8583      0.02      0.0102   127054.85  413070.73     0.0    
   1700     18875.24    2.8169     2.8584     0.0212     0.0099   127090.03  413185.83     0.0    
   1750     19433.5     2.817      2.8581     0.0228     0.0096   127123.39  413294.96     0.0    
   1800     19982.12    2.8169     2.8583     0.0206     0.009    127153.87  413394.64     0.0    
   1850     20527.99    2.8169     2.8583     0.0216     0.0085   127182.21  413487.39     0.0    
   1900     21073.6     2.8167     2.858      0.0264     0.0084   127208.22  413572.52     0.0    
   1950     21607.57    2.8165     2.8581     0.0196     0.0081   127231.81  413649.75     0.0    
   2000     22141.15    2.8168     2.858      0.0253     0.0077   127252.57  413717.66     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       136.96     2.8491     2.8836     0.1325     3.2553    76657.31   309778.7     0.0    
   100       746.45     2.8382     2.876      0.0874     0.9516    98532.79  399057.36     0.0    
   150      1355.58     2.8324     2.8696     0.0598     0.431    109162.27  442459.69     0.0    
   200      1965.57     2.8289     2.866      0.061      0.2711   114612.36  464723.64     0.0    
   250      2575.76     2.8257     2.8632     0.0454     0.1861    117790.7  477709.44     0.0    
   300       3185.5     2.8245     2.8617     0.0484     0.1466   119821.32  486004.43     0.0    
   350      3796.61     2.8233     2.8606     0.0412     0.1164   121235.98   491782.8     0.0    
   400      4406.58     2.8221     2.8596     0.0423     0.0957   122264.06  495983.42     0.0    
   450      5016.96     2.8211     2.8588     0.0363     0.0805   123039.94  499152.29     0.0    
   500      5627.31     2.8207     2.8583     0.0331     0.0698   123643.59  501617.97     0.0    
   550       6235.6     2.8204     2.8578     0.0365     0.0594   124131.38  503610.06     0.0    
   600      6909.99     2.8201     2.8576     0.0335     0.0525   124530.39   505239.3     0.0    
   650      7602.81     2.819      2.857      0.0334     0.0458   124864.15  506602.35     0.0    
   700      8298.79     2.819      2.8568     0.0306     0.0417   125143.67  507743.96     0.0    
   750      8993.59     2.8191     2.8566     0.0317     0.0373   125383.28  508722.39     0.0    
   800      9688.17     2.8191     2.8565     0.031      0.0338   125591.24   509571.7     0.0    
   850      10383.92    2.8185     2.8564     0.033      0.0309   125773.76  510317.25     0.0    
   900      11081.21    2.8182     2.8561     0.0295     0.0289   125934.48   510973.3     0.0    
   950      11778.07    2.8187     2.8561     0.0289     0.026    126076.69  511553.96     0.0    
   1000     12473.58    2.818      2.856      0.0289     0.0245   126204.27  512074.84     0.0    
   1050     13168.01    2.8181     2.8559     0.0286     0.0226   126318.19  512540.02     0.0    
   1100     13861.52    2.8181     2.8559     0.0249     0.0212   126421.09  512960.16     0.0    
   1150     14554.3     2.8182     2.8557     0.0261     0.0198   126514.17  513340.19     0.0    
   1200     15247.95    2.8177     2.8558     0.0282     0.0181   126599.04  513686.48     0.0    
   1250     15939.23    2.8179     2.8556     0.0253     0.0174   126676.38  514002.28     0.0    
   1300     16632.39    2.8176     2.8555     0.0233     0.0162   126747.27  514291.69     0.0    
   1350     17324.97    2.8175     2.8555     0.0214     0.0149   126812.47  514558.04     0.0    
   1400     18013.44    2.8176     2.8554     0.0241     0.0142   126872.42   514802.9     0.0    
   1450     18705.33    2.8177     2.8554     0.0244     0.0135   126927.82  515029.27     0.0    
   1500     19352.06    2.8175     2.8554     0.0269     0.0127   126979.13  515238.75     0.0    
   1550     20015.0     2.8176     2.8553     0.0238     0.0123   127027.09  515434.58     0.0    
   1600     20680.42    2.8177     2.8554     0.028      0.0117   127071.72  515616.71     0.0    
   1650     21343.59    2.8176     2.8552     0.0213     0.011    127113.54  515787.46     0.0    
   1700     22007.56    2.8173     2.8554     0.022      0.0104   127152.02  515944.37     0.0    
   1750     22670.52    2.8175     2.855      0.0228      0.01    127187.33  516088.46     0.0    
   1800     23332.75    2.8174     2.8551     0.0231     0.0096   127220.81  516225.09     0.0    
   1850     23995.65    2.8176     2.8551     0.0273     0.0091    127252.2  516353.37     0.0    
   1900     24658.04    2.8175     2.8552     0.0231     0.0089   127280.74  516470.03     0.0    
   1950     25319.22    2.8173     2.8551     0.0272     0.0086   127306.52   516575.4     0.0    
   2000     25982.12    2.8171     2.855      0.0245     0.0084   127330.08  516671.61     0.0    
