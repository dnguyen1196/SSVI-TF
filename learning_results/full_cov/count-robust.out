Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  7.662285566329956
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       282.89     2.0974     2.103      0.3162     4.4721    88499.97   71273.01     0.0    
   100       751.49     2.0955     2.1012     0.2807     2.6694    88570.38   71319.68     0.0    
   150      1220.64     2.0944     2.1014     0.3027     2.6183    88178.53   71009.83     0.0    
   200      1689.28     2.0913     2.0975     0.2988     0.9137    86841.41   69940.79     0.0    
   250      2157.61     2.0733     2.0787     0.2997     2.3708    83654.77   67384.94     0.0    
   300      2624.17     2.0326     2.0397     0.2983     0.9456    77942.61   62798.32     0.0    
   350      3090.26     1.957      1.9674     0.2938     0.6928    69954.32   56374.05     0.0    
   400      3555.46     1.8428     1.8514     0.2864     1.061     60710.36   48932.52     0.0    
   450      4020.08     1.6962     1.707      0.2951     1.3936    51541.92   41559.53     0.0    
   500      4483.69     1.5332     1.5473     0.2465     0.769     43614.92   35177.93     0.0    
   550       4945.6     1.3668     1.381      0.2968     1.9261    37471.66   30230.72     0.0    
   600      5406.15     1.2112     1.2212     0.2478     1.6942    33017.44   26641.77     0.0    
   650      5866.76     1.0784     1.0874     0.2298     2.4173    29918.47   24145.17     0.0    
   700      6329.39     0.9712     0.9829     0.2717     1.0802    27861.72   22478.82     0.0    
   750      6789.31     0.8874     0.8969     0.2219     1.3783    26426.82   21315.23     0.0    
   800       7246.0     0.8285     0.8355     0.2106     1.472     25533.61   20584.18     0.0    
   850      7701.84     0.7771     0.7842     0.2233     1.0473    24827.97   20014.11     0.0    
   900      8158.77     0.7379     0.7419     0.169      0.7352    24372.72   19646.05     0.0    
   950      8615.27     0.7061     0.7097     0.2586     1.7364    24075.79   19393.96     0.0    
   1000     9071.72     0.6805     0.6826     0.1949     0.7394    23822.52   19193.41     0.0    
   1050     9527.78     0.6661     0.6671     0.1322     0.5434    23605.22   19008.12     0.0    
   1100     9983.93     0.6527     0.6529     0.2266     0.6967    23455.11   18884.88     0.0    
   1150     10439.78    0.636      0.6384     0.2448     0.5881    23322.88   18788.01     0.0    
   1200     10895.55    0.6285     0.6303     0.2298     0.8095    23191.25   18687.62     0.0    
   1250     11351.3     0.6164     0.6156     0.209      0.7332    23126.83   18626.18     0.0    
   1300     11806.97    0.6078     0.6092     0.1856     0.5407    23049.32   18557.14     0.0    
   1350     12262.74    0.6027     0.6014     0.1592     0.8387    22958.77   18483.6      0.0    
   1400     12718.49    0.5945     0.5949     0.1962     0.0782    22902.85   18434.94     0.0    
   1450     13173.94    0.5893     0.5867     0.1779     0.0637    22881.55   18405.48     0.0    
   1500     13629.42    0.5837     0.5839     0.1737     0.4824    22862.11   18392.62     0.0    
   1550     14084.52    0.5828     0.5812     0.1623     0.4916    22827.51   18360.03     0.0    
   1600     14539.74    0.5734     0.5746     0.188      0.013     22775.68   18342.76     0.0    
   1650     14994.86    0.5692     0.5676     0.1453     0.1759    22755.98   18314.92     0.0    
   1700     15450.13    0.5683     0.5686     0.1625     0.0785    22715.44   18278.05     0.0    
   1750     15905.04    0.5659     0.5668     0.1184     0.1907    22655.48   18234.69     0.0    
   1800     16360.21    0.5635     0.5642     0.1716     0.4921    22671.06   18252.34     0.0    
   1850     16815.44    0.5633     0.5639     0.1607     0.241     22636.19   18209.96     0.0    
   1900     17270.61    0.5609     0.5602     0.1439     0.0332    22631.89   18208.14     0.0    
   1950     17726.04    0.5575     0.5565     0.1547     0.0045    22631.86   18207.39     0.0    
   2000     18181.41    0.5537     0.5541     0.1347     0.4298    22583.68   18177.34     0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       282.28     2.1017     2.1023     0.1348     4.4721    88594.88  142112.49     0.0    
   100       827.99     2.0999     2.1007     0.0192     2.279     88725.62  142321.62     0.0    
   150      1373.27     2.0986     2.0997     0.0245     1.6921    88784.46  142408.08     0.0    
   200      1918.06     2.0989     2.0992     0.018      1.8456    88802.04   142440.9     0.0    
   250      2462.61     2.0974     2.098      0.019      1.5668    88817.79  142466.22     0.0    
   300      3007.33     2.097      2.0977     0.0222     1.6397    88828.01  142481.25     0.0    
   350      3552.08     2.0965     2.0975     0.0188     1.2674    88834.93  142490.42     0.0    
   400      4096.91     2.097      2.0963     0.0181     1.3893    88848.04  142507.47     0.0    
   450      4641.69     2.0955     2.0968     0.0191     0.9478    88857.88  142520.59     0.0    
   500      5186.69     2.0959     2.0966     0.0194     0.4988    88867.3   142533.45     0.0    
   550      5731.45     2.0958     2.0961     0.0174     0.8771    88871.63  142538.87     0.0    
   600      6276.31     2.0957     2.096      0.0166     0.6957    88875.37  142545.76     0.0    
   650      6820.81     2.0949     2.0961     0.015      0.8905    88881.59  142554.79     0.0    
   700      7365.43     2.0943     2.0953     0.0169     1.2132    88886.05  142561.03     0.0    
   750      7910.01     2.0942     2.0957     0.019      0.7984    88890.01  142565.95     0.0    
   800      8454.31     2.0945     2.0953     0.0162     0.1976    88890.87  142567.12     0.0    
   850      8999.72     2.0945     2.0956     0.0153     0.8286    88892.78  142570.82     0.0    
   900       9543.8     2.0933     2.0949     0.0189     0.4589    88893.46  142571.77     0.0    
   950      10087.58    2.0938     2.0953     0.0165     0.529     88893.88  142572.17     0.0    
   1000     10631.36    2.0946     2.0958     0.0167     0.7909    88893.97  142572.03     0.0    
   1050     11175.55    2.0949     2.0947     0.0147     0.0889    88894.03  142571.96     0.0    
   1100     11719.91    2.0934     2.0948     0.0173     0.7074    88894.85  142573.16     0.0    
   1150     12264.35    2.0933     2.0947     0.015      0.906     88894.72  142572.35     0.0    
   1200     12808.78    2.0945     2.0949     0.017      0.0742    88893.58  142570.25     0.0    
   1250     13352.98    2.0939     2.0949     0.018      0.7196    88891.73  142567.81     0.0    
   1300     13897.2     2.0937     2.0946     0.0186     0.6489    88889.42   142564.0     0.0    
   1350     14442.04    2.0938     2.0943     0.0158     0.1598    88886.4   142559.05     0.0    
   1400     14986.07    2.0934     2.0949     0.0194     0.4565    88882.86  142552.72     0.0    
   1450     15530.26    2.0936     2.0947     0.0172     0.1019    88878.1   142545.12     0.0    
   1500     16075.09    2.0934     2.0946     0.0183     0.2132    88872.75  142536.08     0.0    
   1550     16619.38    2.0935     2.095      0.0186     0.6206    88866.06  142525.27     0.0    
   1600     17163.62    2.0938     2.095      0.0192     0.0961    88857.73  142512.05     0.0    
   1650     17707.92    2.0933     2.0942     0.0196     0.1136    88848.08  142496.42     0.0    
   1700     18251.71    2.0939     2.0944     0.0224     0.3703    88836.49  142478.03     0.0    
   1750     18794.62    2.094      2.0949     0.0235     0.2012    88822.05  142454.72     0.0    
   1800     19338.49    2.094      2.0943     0.0242     0.1374    88802.56  142423.42     0.0    
   1850     19882.41    2.0925     2.0943     0.0265     0.0606    88777.78  142383.48     0.0    
   1900     20426.5     2.0936     2.0938     0.0287     0.4131    88746.62  142333.08     0.0    
   1950     20970.33    2.0928     2.0941     0.0333     0.023     88701.05  142260.13     0.0    
   2000     21514.11    2.0931     2.0936     0.0433     0.1549    88636.23   142156.5     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       281.49     2.1043     2.1021     0.1485     4.4721    88418.48  212617.22     0.0    
   100       905.53     2.101       2.1       0.0285     2.476     88566.57  212975.24     0.0    
   150       1530.0     2.0999     2.0993     0.0385     1.2928    88644.15  213151.78     0.0    
   200      2154.35     2.1002     2.0979     0.026      0.8921    88689.26  213254.71     0.0    
   250      2778.14     2.0991     2.0979     0.0254     1.2243    88743.17  213386.75     0.0    
   300       3402.4     2.0974     2.0971     0.0241     1.3606    88764.85   213434.8     0.0    
   350      4026.79     2.0972     2.0964     0.0279     1.8676    88785.3   213479.77     0.0    
   400      4650.68     2.0967     2.0962     0.0229     0.8985    88797.16   213509.6     0.0    
   450      5274.38     2.0966     2.096      0.0239     0.679     88804.63  213525.71     0.0    
   500       5898.5     2.0962     2.0959     0.0234     1.3368    88822.26  213564.93     0.0    
   550      6523.39     2.096      2.0954     0.0218     0.9593    88833.12  213586.76     0.0    
   600      7148.79     2.0957     2.0956     0.022      0.6798    88845.73  213614.29     0.0    
   650      7773.21     2.0962     2.095      0.0235     0.8315    88855.52  213635.56     0.0    
   700      8397.56     2.0958     2.0945     0.021      0.9015    88861.36  213649.64     0.0    
   750      9023.49     2.0951     2.0949     0.0184     0.2947    88865.42   213658.6     0.0    
   800      9648.91     2.0951     2.0944     0.0205     0.8011    88868.49   213665.0     0.0    
   850      10274.01    2.0949     2.0943     0.0252     0.1339    88871.23  213671.34     0.0    
   900      10898.68    2.0946     2.0947     0.0183     0.7626    88872.76  213675.61     0.0    
   950      11523.52    2.0948     2.0945     0.0211     0.5628    88875.04  213682.13     0.0    
   1000     12148.77    2.0951     2.0945     0.0216     0.2858    88875.96  213683.17     0.0    
   1050     12773.96    2.0948     2.0943     0.0214     0.0407    88875.79  213682.28     0.0    
   1100     13398.32    2.0946     2.0944     0.0186     0.5789    88876.48  213684.25     0.0    
   1150     14023.31    2.0946     2.094      0.0233     0.0565    88876.6   213683.97     0.0    
   1200     14647.95    2.095      2.0937     0.0216     0.4627    88876.24  213682.81     0.0    
   1250     15272.03    2.0944     2.0939     0.0216     0.1511    88875.57  213681.85     0.0    
   1300     15897.34    2.0938     2.0941     0.0252     0.8654    88875.67  213682.39     0.0    
   1350     16522.38    2.0944     2.0939     0.0219     0.3051    88873.74  213677.29     0.0    
   1400     17147.31    2.0944     2.0936     0.0201     0.618     88871.76  213671.61     0.0    
   1450     17771.54    2.0941     2.0937     0.0208     0.0515    88866.99  213660.66     0.0    
   1500     18396.16    2.0945     2.094      0.0245     0.3151    88862.49  213649.77     0.0    
   1550     19020.28    2.0944     2.0937     0.025      0.2249    88856.22  213635.49     0.0    
   1600     19644.79    2.0949     2.0933     0.0247     0.0696    88847.4   213615.12     0.0    
   1650     20270.13    2.094      2.0937     0.0246     0.4234    88837.21  213590.64     0.0    
   1700     20895.06    2.0936     2.0935     0.0267     0.0513    88823.45  213558.63     0.0    
   1750     21519.86    2.0947     2.0936     0.0254     0.5921    88807.07  213519.68     0.0    
   1800     22144.82    2.0937     2.0935     0.0281     0.3959    88784.53  213466.24     0.0    
   1850     22768.63    2.0938     2.0929     0.0307     0.1381    88754.57  213396.38     0.0    
   1900     23392.36    2.0941     2.0926     0.0351     0.487     88711.03  213293.72     0.0    
   1950     24015.7     2.0942     2.0928     0.0464     0.0446    88646.65  213141.93     0.0    
   2000     24639.78    2.0922     2.0924     0.0501     0.047     88549.19   212911.8     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       281.52     2.1057     2.1031     0.191      4.4721    88594.35  283836.59     0.0    
   100       987.12     2.1054     2.102      0.0369     1.5018    88668.62  284079.63     0.0    
   150      1693.36     2.1032     2.0999     0.0452     1.6818    88712.17  284221.47     0.0    
   200      2399.68     2.1022     2.0989     0.0303     2.2183    88753.61  284330.43     0.0    
   250      3106.59     2.1011     2.0981     0.0332     1.3271    88772.66  284375.14     0.0    
   300      3813.75     2.1006     2.0975     0.0316     1.426     88792.3   284435.65     0.0    
   350      4521.31      2.1       2.0968     0.031      1.473     88800.21  284460.38     0.0    
   400      5227.65     2.0995     2.0968     0.028      1.0219    88817.45  284512.44     0.0    
   450      5931.61     2.0988     2.0963     0.0306     0.377     88819.81  284520.45     0.0    
   500      6636.48     2.099      2.0958     0.0299     1.045     88829.9   284548.39     0.0    
   550      7339.61     2.0985     2.096      0.0263     0.657     88833.75  284560.07     0.0    
   600      8043.12     2.0994     2.0955     0.0299     0.8245    88840.16  284579.52     0.0    
   650      8751.13     2.0984     2.0956     0.0247     0.9559    88844.54  284591.47     0.0    
   700       9459.1     2.098      2.0949     0.0317     0.5116    88847.56   284603.0     0.0    
   750      10167.06    2.0987     2.0951     0.0249     0.2388    88849.45  284607.98     0.0    
   800      10875.03    2.0978     2.0953     0.0251     0.9308    88851.76  284615.37     0.0    
   850      11583.5     2.0973     2.0948     0.0236     0.9799    88853.79  284624.32     0.0    
   900      12287.59    2.0969     2.0947     0.0337     0.1343    88854.86  284626.84     0.0    
   950      12991.09    2.0969     2.0946     0.026      0.7077    88856.66  284631.23     0.0    
   1000     13695.08    2.0971     2.0946     0.0257     0.0996    88857.41  284633.44     0.0    
   1050     14404.52    2.0966     2.0946     0.0229     0.7018    88859.07   284635.6     0.0    
   1100     15139.64    2.0975     2.0944     0.0284     0.8685    88858.74   284634.7     0.0    
   1150     15876.03    2.0972     2.0943     0.0228     0.9141    88860.19  284635.96     0.0    
   1200     16621.86    2.0975     2.0944     0.0286     0.9792    88862.72  284641.97     0.0    
   1250     17379.37    2.0972     2.0944     0.0273     0.5237    88862.56  284639.96     0.0    
   1300     18137.94    2.0965     2.0941     0.0278     0.7029    88862.15  284638.38     0.0    
   1350     18895.35    2.0962     2.0935     0.0312     1.2277    88861.7   284639.15     0.0    
   1400     19654.14    2.0973     2.0938     0.0238     0.2073    88860.32  284633.77     0.0    
   1450     20413.38    2.096      2.0936     0.0286     1.2482    88860.45  284628.06     0.0    
   1500     21171.44    2.096      2.0939     0.0269     0.398     88857.23  284619.45     0.0    
   1550     21930.93    2.0965     2.0935     0.0292     0.1575    88852.76  284605.71     0.0    
   1600     22688.16    2.0963     2.0937     0.0243     0.3077    88847.48  284589.75     0.0    
   1650     23448.11    2.096      2.0934     0.0273     0.783     88841.1   284570.56     0.0    
   1700     24206.51    2.0957     2.093      0.0263     0.4108    88833.32  284545.62     0.0    
   1750     24966.15    2.0959     2.0931     0.0266     0.0625    88822.66  284511.91     0.0    
   1800     25725.19    2.0957     2.0931     0.0308     0.0535    88808.58  284468.19     0.0    
   1850     26484.7     2.0958     2.0931     0.0351     0.8219    88792.71   284417.9     0.0    
   1900     27223.0     2.0956     2.0929     0.0305     0.9138    88771.39  284350.83     0.0    
   1950     27942.94    2.0953     2.0927     0.041      0.0271    88739.4   284250.55     0.0    
   2000     28663.68    2.0955     2.093      0.0376     0.8043    88695.3   284111.79     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       289.12     2.1075     2.1068     0.1887     4.4721    88571.53  355018.97     0.0    
   100      1093.31     2.1069     2.1051     0.0376     2.7453    88622.16  355249.56     0.0    
   150      1896.25     2.1065     2.1038     0.0415     0.9493    88666.73  355442.14     0.0    
   200      2698.66     2.1046     2.1037     0.0407     1.9911    88709.1   355605.29     0.0    
   250      3502.64     2.1045     2.1026     0.0391     1.4104    88733.65  355710.52     0.0    
   300      4305.99     2.1042     2.1022     0.0367     0.7028    88762.79  355819.85     0.0    
   350      5109.29     2.1036     2.1021     0.0344     0.863     88778.49  355885.01     0.0    
   400      5912.63     2.103      2.1017     0.0343     1.4363    88794.96  355946.68     0.0    
   450      6716.18     2.1024     2.1004     0.0355     2.0479    88801.37  355981.75     0.0    
   500       7519.0     2.1025     2.1003     0.0364     0.8493    88815.92  356041.54     0.0    
   550      8321.68     2.1017      2.1       0.0338     1.0262    88823.03  356072.55     0.0    
   600      9124.97     2.1015     2.0996     0.0372     0.9009    88829.26  356099.81     0.0    
   650      9928.39     2.1008     2.0992     0.0349     0.7731    88834.65  356125.54     0.0    
   700      10730.58    2.1002     2.0988     0.0374     1.0502    88838.0   356143.27     0.0    
   750      11532.55     2.1       2.0992     0.0338     0.1119    88840.35  356152.32     0.0    
   800      12333.22    2.1005     2.0987     0.033      0.7208    88842.45  356162.81     0.0    
   850      13133.16    2.1008     2.0987     0.038      0.1661    88844.83  356173.44     0.0    
   900      13933.15    2.0999     2.0989     0.0305     0.1495    88846.98   356183.2     0.0    
   950      14732.68    2.1002     2.0982     0.0358     0.9762    88849.3   356197.89     0.0    
   1000     15533.21    2.1002     2.0984     0.0334     0.9479    88852.82  356214.46     0.0    
   1050     16335.77    2.0995     2.0977     0.0307     0.1623    88855.22   356223.5     0.0    
   1100     17137.85    2.1004     2.0985     0.0325     0.0781    88855.56   356225.3     0.0    
   1150     17939.1     2.0992     2.0978     0.0273     0.6637    88855.13  356226.89     0.0    
   1200     18740.55    2.0993     2.0985     0.0314     0.7442    88857.84  356240.99     0.0    
   1250     19543.02    2.0984     2.0977     0.0376     0.1152    88858.67  356245.19     0.0    
   1300     20358.04    2.0994     2.0974     0.0298     0.8723    88857.77  356238.12     0.0    
   1350     21162.64    2.0988     2.0974     0.0339     0.2382    88856.35  356232.38     0.0    
   1400     21967.05    2.0991     2.0974     0.0369     0.2508    88855.42  356231.69     0.0    
   1450     22771.27    2.0983     2.0973     0.0312     0.6208    88853.7   356224.54     0.0    
   1500     23575.23    2.0986     2.0974     0.0343     0.0569    88849.26  356207.71     0.0    
   1550     24378.53    2.0992     2.0974     0.0328     0.2667    88843.91  356187.36     0.0    
   1600     25182.14    2.0985     2.0969     0.0308     0.6749    88837.95  356164.48     0.0    
   1650     25982.89    2.099      2.0973     0.0293     0.4425    88832.1   356141.91     0.0    
   1700     26784.03    2.0987     2.0973     0.0329     0.6791    88822.32  356103.44     0.0    
   1750     27585.25    2.0981     2.0965     0.0329     0.853     88811.62  356063.75     0.0    
   1800     28386.22    2.0981     2.0971     0.0341     0.5595    88798.17  356011.58     0.0    
   1850     29187.21    2.0983     2.0965     0.0347     0.9268    88777.02  355928.51     0.0    
   1900     29988.03    2.0981     2.0964     0.0401     0.4359    88748.56  355816.12     0.0    
   1950     30788.77    2.0977     2.0961     0.0459     0.4258    88708.41   355657.8     0.0    
   2000     31590.27    2.0969     2.0958     0.0518     0.116     88645.1    355404.5     0.0    
