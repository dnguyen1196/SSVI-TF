Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.471373796463013
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       138.29     0.3472     0.3228     3.1623     4.0033   
   100       281.55     0.1576     0.1471     3.1403     0.2327   
   150       425.46     0.1452     0.1308     2.5454     0.1977   
   200       568.97     0.1242     0.1079     2.4126     0.1506   
   250       712.01     0.1158     0.094      2.0593     0.1549   
   300       855.88     0.1086     0.0898     1.9025     0.1137   
   350       999.32     0.1085     0.0856     2.6801     0.0981   
   400      1142.46     0.098      0.0778     1.8187     0.081    
   450      1286.16     0.1009     0.0744     1.9042     0.0783   
   500      1429.35     0.0979     0.0712     1.7771     0.0655   
   550      1572.93     0.0986     0.0724     2.2599     0.0601   
   600      1717.66     0.098      0.072      2.2459     0.0538   
   650      1861.51     0.0935     0.0626     2.199      0.0532   
   700       2005.7     0.0906     0.062      1.817      0.0457   
   750      2149.51     0.0916     0.0589     1.8926     0.0421   
   800      2293.31     0.0928     0.059      2.0996     0.0408   
   850       2436.9     0.089      0.0578     1.7105      0.04    
   900      2580.98     0.0897     0.0585     1.7725     0.0361   
   950       2725.3     0.0901     0.0582     1.8385     0.0435   
   1000     2868.94     0.0861     0.0554     2.1187     0.034    
   1050     3013.35     0.0872     0.0542     1.7425     0.0332   
   1100     3156.71     0.0837     0.0502     1.9378     0.0389   
   1150     3300.69     0.0854     0.0503     1.491      0.0265   
   1200      3446.1     0.0845     0.0538     1.7365     0.0276   
   1250     3590.09     0.0868     0.0526     1.9004     0.0259   
   1300     3733.77     0.0808     0.044      1.6407     0.0256   
   1350     3877.69     0.082      0.0468     1.5739     0.0243   
   1400     4021.91     0.0847     0.046      1.6455     0.023    
   1450     4165.34     0.0789     0.0481     1.692      0.0244   
   1500     4309.13     0.0912     0.0548     2.1392     0.0245   
   1550     4452.45     0.0882     0.0541     1.9942     0.0271   
   1600     4595.44     0.086      0.0506     2.0538     0.0271   
   1650      4739.2     0.0875     0.0556     2.0627     0.0263   
   1700     4881.85     0.088      0.0568     1.9396     0.0247   
   1750     5025.18     0.0883     0.0542     1.986      0.0205   
   1800     5168.28     0.0855     0.0526     1.8768     0.0194   
   1850     5311.89     0.0858     0.054      2.0585     0.0206   
   1900     5455.13     0.0931     0.0596     2.0495     0.0224   
   1950     5598.14     0.0934     0.0668     2.4679      0.02    
   2000     5741.42     0.0926     0.0637     2.4969     0.0285   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       138.51     0.4956     0.497      1.4237     1.3147   
   100       282.6      0.4875     0.4904     1.7258     0.8715   
   150       426.53     0.4066     0.4077     1.4367     0.5386   
   200       570.88     0.1498     0.1488     1.4054     0.3218   
   250       714.55     0.1258     0.1278     1.0846     0.2604   
   300       857.92     0.1378     0.1389     0.8627     0.1907   
   350      1002.12     0.1298     0.1322     0.7039     0.109    
   400      1146.48     0.1447     0.1434     0.6908     0.0888   
   450      1291.07     0.1258     0.1278     0.5705     0.0803   
   500      1435.05     0.1351     0.1356     0.6038     0.0692   
   550      1578.64     0.1258     0.1278     0.6382     0.057    
   600      1721.92     0.1309     0.1315     0.5277     0.0504   
   650      1865.93     0.1327     0.1341     0.5168     0.0515   
   700       2009.2     0.1359     0.1378     0.5475     0.0417   
   750      2153.08     0.1394     0.1386     0.5384     0.0387   
   800      2297.29     0.1248     0.1277     0.4618     0.036    
   850      2440.94     0.1309     0.1314     0.5344     0.0335   
   900       2584.8     0.1308     0.1332     0.3681     0.028    
   950       2728.8     0.1278     0.1308     0.414      0.0255   
   1000     2872.86     0.1256     0.127      0.5775     0.0269   
   1050     3016.76     0.1297     0.1313     0.5274     0.0282   
   1100      3163.3     0.1247     0.127      0.589      0.0246   
   1150      3306.3     0.1253     0.1273     0.6143     0.0223   
   1200     3450.06     0.1251     0.1274     0.5416     0.0216   
   1250     3593.46     0.1237     0.126      0.5958     0.0227   
   1300     3735.08     0.1237     0.1267     0.6456     0.0225   
   1350     3870.54     0.1241     0.1246     0.5631     0.0206   
   1400      4006.0     0.1204     0.1233     0.6671     0.0185   
   1450     4142.38     0.1234     0.1234     0.6213     0.0214   
   1500     4278.87     0.1224     0.1238     0.5888     0.0195   
   1550     4415.35     0.1235     0.1238     0.9051     0.0216   
   1600     4551.41     0.1233     0.1247     0.7705     0.016    
   1650     4687.89     0.1211     0.1196     0.7858     0.0173   
   1700     4824.44     0.1196     0.1185     0.7566     0.0281   
   1750     4960.81     0.1144     0.1137     0.6295     0.021    
   1800     5098.26     0.1138     0.1153     0.8399     0.0189   
   1850     5236.81     0.1096     0.1098     0.8516     0.0191   
   1900     5377.59     0.1119     0.108      0.8538     0.0192   
   1950      5520.4     0.1118      0.11      0.8631     0.0194   
   2000      5660.7     0.1088     0.1063     0.9261     0.0261   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       134.67     0.5012     0.496      1.4526     1.0425   
   100       273.75     0.4082     0.4046     1.6603     0.7665   
   150       416.12     0.1306     0.1319     1.5742     0.5449   
   200       557.9      0.1288     0.1316     1.287      0.396    
   250       702.12     0.1328     0.1332     1.1019     0.2577   
   300       843.93     0.1403     0.1395     0.9408     0.2095   
   350       987.57     0.1337     0.1358     0.9616     0.1744   
   400      1127.55     0.1329     0.1343     0.9527     0.1546   
   450      1268.83     0.134      0.1346     0.7101     0.1194   
   500      1410.11     0.1254     0.1256     0.8333     0.1048   
   550       1550.9     0.1391     0.1367     0.724      0.0893   
   600      1692.64     0.1256     0.1257     0.6252     0.0735   
   650      1832.35     0.1236     0.1229     0.6708     0.0687   
   700      1973.53     0.1233     0.1252     0.5079     0.0579   
   750      2113.99     0.1224     0.1226     0.6774     0.0535   
   800      2254.37     0.1246     0.1253     0.6044     0.0402   
   850      2396.17     0.1234     0.1232     0.8404     0.0587   
   900      2536.68     0.1202     0.1223     0.5882     0.0393   
   950      2678.76     0.1222     0.1216     0.5786     0.0338   
   1000     2819.44     0.121      0.1217     0.581      0.0349   
   1050     2960.78     0.1212     0.1232     0.5774     0.0251   
   1100     3101.59     0.1236     0.1246     0.6318     0.0258   
   1150     3243.02     0.121      0.1226     0.4817     0.0215   
   1200     3384.02     0.1256     0.1267     0.4895     0.0254   
   1250     3524.49     0.1201     0.1227     0.6085     0.0234   
   1300     3666.41     0.1231     0.1254     0.7934     0.0182   
   1350     3806.59     0.1197     0.1226     0.6563     0.0192   
   1400     3947.34     0.1257     0.1248     0.5874     0.0205   
   1450     4088.42     0.1182     0.122      0.5935     0.0169   
   1500     4229.19     0.1217     0.1225     0.4986     0.018    
   1550     4372.75     0.1203     0.1227     0.5249     0.0159   
   1600     4513.13      0.12      0.1218     0.5366     0.015    
   1650     4655.59     0.1199     0.1223     0.5773     0.0136   
   1700     4797.06     0.1204     0.1218     0.5549     0.0158   
   1750     4940.03     0.1164     0.118      0.6855     0.016    
   1800     5080.78     0.1178     0.1203     0.5331     0.0151   
   1850     5221.47     0.1177     0.1184     0.6745     0.0188   
   1900     5364.67     0.115      0.117      0.791      0.0187   
   1950     5505.65     0.1115     0.1137     0.6289     0.0312   
   2000      5644.9     0.1122     0.1109     0.6164     0.0262   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       131.74     0.496      0.4949     1.3396     0.949    
   100       270.31     0.5112     0.4991     1.5312     0.7183   
   150       407.72     0.4941     0.4851     1.3707     0.5335   
   200       548.34     0.4906     0.4829     1.2135     0.3833   
   250       689.44     0.419      0.4166     1.0374     0.2658   
   300       830.66     0.1547     0.1536     1.0429     0.1853   
   350       971.26     0.1258     0.1282     1.0014     0.1933   
   400      1111.13     0.1248     0.1272     0.7955     0.172    
   450       1256.6     0.1309     0.1321     0.7059     0.0907   
   500      1402.02     0.1288     0.1318     0.7002     0.0757   
   550      1544.55     0.1328     0.1333     0.6938     0.0627   
   600      1687.73     0.1258     0.1282     0.5467     0.0601   
   650      1831.26     0.1247     0.1272     0.5409     0.0513   
   700      1974.15     0.1306     0.1325     0.5933     0.0483   
   750      2116.25     0.1359     0.1391     0.5508     0.0459   
   800      2255.43     0.1299     0.1311     0.5541     0.0384   
   850      2396.77     0.1248     0.1272     0.4562     0.0345   
   900      2539.69     0.1314     0.133      0.3803     0.0316   
   950      2682.63     0.1258     0.1282     0.436      0.0292   
   1000     2825.32     0.1258     0.1282     0.3791     0.0295   
   1050     2968.53     0.1258     0.1282     0.5356     0.0271   
   1100     3111.23     0.1248     0.1272     0.5249     0.0265   
   1150     3253.27     0.1319     0.1345     0.3838     0.0248   
   1200     3395.99     0.1248     0.1272     0.5246     0.0229   
   1250     3538.86     0.1328     0.1333     0.3389     0.0213   
   1300     3681.69     0.1299     0.1311     0.4644     0.0193   
   1350      3824.4     0.1248     0.1271     0.4188     0.0174   
   1400     3967.32     0.1299     0.1311     0.4072     0.0164   
   1450     4110.19     0.1295     0.1316     0.4252     0.0173   
   1500     4252.96     0.1248     0.1272     0.4401     0.0166   
   1550     4395.89     0.1257     0.128      0.4344     0.0161   
   1600     4539.25     0.1248     0.1272     0.4366     0.0164   
   1650     4681.78     0.1248     0.1272     0.4381     0.0131   
   1700     4824.36     0.1244     0.1269     0.4573     0.0139   
   1750     4966.68     0.1242     0.1263     0.6009     0.0163   
   1800     5109.96     0.1224     0.1254     0.6481     0.0178   
   1850     5252.57     0.1241     0.1253     0.6565     0.0165   
   1900     5396.15     0.1223     0.1236     0.6005     0.014    
   1950     5539.83     0.1236     0.125      0.6672     0.0159   
   2000     5682.98     0.1213     0.1227     0.7311     0.0161   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       137.59     0.4976     0.4927     1.1856     0.8914   
   100       281.89     0.4788     0.4748     1.4291     0.6951   
   150       426.39     0.2219     0.2193     1.294      0.525    
   200       570.06     0.1258     0.1283     1.2482     0.4022   
   250       713.44     0.1288     0.1319     1.0712     0.3078   
   300       857.28     0.1314     0.1331     0.955      0.2099   
   350      1000.47     0.1417     0.1418     0.9675     0.1734   
   400      1143.65     0.1265     0.1291     0.872      0.1581   
   450      1287.14     0.135      0.1355     0.7985     0.1307   
   500      1430.11     0.1243     0.127      0.7376     0.1182   
   550      1572.74     0.1293     0.1287     0.7361     0.1023   
   600      1715.95     0.1312     0.1323     0.8027     0.0874   
   650      1858.72     0.1248     0.1259     0.6799     0.0751   
   700      2001.99     0.1238     0.1252     0.6758     0.0638   
   750      2145.77     0.1279     0.1303     0.8922     0.0701   
   800      2290.27     0.1218     0.1227     0.6482     0.0499   
   850      2433.66     0.1264     0.1289     0.5915     0.0426   
   900      2576.66     0.127      0.1267     0.6354     0.0408   
   950      2719.92     0.1198     0.1209     0.7349     0.0382   
   1000     2863.23     0.1201     0.1204     0.6171     0.0335   
   1050     3006.92     0.1202     0.1225     0.5584     0.0285   
   1100     3149.35     0.1209     0.1215     0.5617     0.0333   
   1150     3293.26     0.1215     0.1207     0.5802     0.0294   
   1200      3436.5     0.1178     0.1187     0.6606     0.0263   
   1250     3579.92     0.1203     0.1201     0.511      0.0294   
   1300     3723.48     0.123      0.1213     0.5969     0.021    
   1350     3866.81     0.1254     0.124      0.4847     0.0208   
   1400      4008.2     0.1186     0.1208     0.6205     0.0206   
   1450      4149.7     0.1188     0.1194     0.4912     0.0217   
   1500     4290.86     0.118      0.1181     0.5926     0.0191   
   1550     4433.19     0.1188     0.1189     0.4783     0.0178   
   1600     4577.19     0.1167     0.1185     0.5983     0.015    
   1650     4720.05     0.117      0.1178     0.4925     0.015    
   1700     4860.51     0.1165     0.119      0.5621     0.0164   
   1750     5002.07     0.1176     0.1178     0.4411     0.0174   
   1800      5143.4     0.1181     0.1174     0.5011     0.0119   
   1850     5283.61     0.1179     0.1191     0.4575     0.0132   
   1900     5425.27     0.1179     0.1177     0.4459     0.0104   
   1950     5566.43     0.1176     0.1165     0.4656     0.0121   
   2000     5709.25     0.1164     0.1169     0.4673     0.0181   
