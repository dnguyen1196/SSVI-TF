Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.405872821807861
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       127.17     0.1533     0.1521     3.1623     4.4719      0.0    
   100       259.49     0.0916     0.0903     3.1618     0.2177      0.0    
   150       391.65     0.0748     0.0743     3.1132     0.1528      0.0    
   200       524.7      0.0682     0.0667     3.034      0.1196      0.0    
   250       657.43     0.0627     0.0622     2.3227     0.0575      0.0    
   300       789.88     0.0624     0.0609     1.9993     0.0309      0.0    
   350       922.31     0.0607      0.06      1.8798     0.0241      0.0    
   400      1054.59     0.061      0.0599     2.0112     0.0207      0.0    
   450      1186.77     0.0598     0.0588     1.8298     0.0224      0.0    
   500       1319.0     0.0594     0.0585     1.7212     0.0139      0.0    
   550      1452.29     0.0596     0.0584     1.5182     0.011       0.0    
   600      1585.04     0.0596     0.0588     1.286      0.0068      0.0    
   650      1718.09     0.0591     0.0581     1.6162     0.0101      0.0    
   700      1850.68     0.0597     0.0587     1.4044     0.009       0.0    
   750      1982.57     0.0591     0.0581     1.1001     0.0064      0.0    
   800      2115.47     0.0591     0.0581     0.982      0.0053      0.0    
   850      2248.04     0.0591     0.058       1.1       0.0056      0.0    
   900      2380.78     0.0588     0.0579     1.0063     0.0052      0.0    
   950      2513.83     0.0588     0.0578     0.9537     0.0045      0.0    
   1000     2646.23     0.0588     0.0577     0.873      0.0031      0.0    
   1050     2779.27     0.0589     0.0578     0.9077     0.003       0.0    
   1100     2911.11     0.0592     0.0582     0.992      0.0036      0.0    
   1150     3043.26     0.0585     0.0576     1.0341     0.0021      0.0    
   1200      3175.9     0.0589     0.0579     1.0557     0.0025      0.0    
   1250     3308.36     0.059      0.0579     1.1916     0.003       0.0    
   1300     3442.89     0.0586     0.0579     1.0879     0.0021      0.0    
   1350     3575.45     0.0592     0.058      0.8337     0.0023      0.0    
   1400     3707.94     0.0587     0.0576     0.9755     0.0023      0.0    
   1450     3840.84     0.0588     0.0579     1.0166     0.002       0.0    
   1500     3973.35     0.0588     0.0579     0.9238     0.002       0.0    
   1550     4106.01     0.0588     0.0578     1.0101     0.0017      0.0    
   1600     4238.61     0.0584     0.0576     0.8786     0.0015      0.0    
   1650     4370.93     0.0587     0.0577     0.927      0.0017      0.0    
   1700     4504.15     0.0589     0.058      1.3183     0.0017      0.0    
   1750     4636.93     0.0587     0.0576     1.3346     0.0015      0.0    
   1800      4769.1     0.0586     0.0574     0.8417     0.0014      0.0    
   1850     4901.18     0.0589     0.0577     0.8363     0.0012      0.0    
   1900     5034.12     0.0585     0.0575     0.8687     0.0013      0.0    
   1950     5167.28     0.0591     0.0579     0.8973     0.0011      0.0    
   2000     5300.17     0.0588     0.0577     1.025      0.0013      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       126.83     0.1218     0.1208     3.0516     4.4678      0.0    
   100       260.48     0.0755     0.0745     2.8579     0.3735      0.0    
   150       393.91     0.0668     0.0664     2.3646     0.1143      0.0    
   200       527.44     0.0629     0.062      1.7246     0.0604      0.0    
   250       660.51     0.0611     0.0606     1.7208     0.0381      0.0    
   300       794.22     0.0611     0.0603     1.8017     0.0335      0.0    
   350       927.11     0.061      0.0602     2.1788     0.0233      0.0    
   400      1059.65     0.0603     0.0597     2.087      0.0169      0.0    
   450      1193.22     0.0599     0.0591     1.5541     0.0133      0.0    
   500       1326.7     0.0599     0.0591     1.6487     0.0093      0.0    
   550      1460.08     0.0597     0.0591     1.5219     0.0091      0.0    
   600      1593.22     0.0597     0.0592     1.3527     0.0061      0.0    
   650      1727.11     0.0592     0.0586     1.646      0.0058      0.0    
   700      1859.83     0.0595     0.0587     1.336      0.0052      0.0    
   750      1993.51     0.0594     0.0585     1.1264     0.0051      0.0    
   800      2127.15     0.0591     0.0584     1.0998     0.0039      0.0    
   850      2260.05     0.0591     0.0585     0.8744     0.0036      0.0    
   900      2393.98     0.0594     0.0586     1.0198     0.0037      0.0    
   950      2526.94     0.0591     0.0587     1.3625     0.0032      0.0    
   1000      2660.5     0.0588     0.0582     0.9793     0.0025      0.0    
   1050     2793.64     0.0588     0.0584     1.0705     0.0023      0.0    
   1100     2927.21     0.0594     0.0585     0.998      0.0021      0.0    
   1150     3060.18     0.0592     0.0585     1.0026     0.0021      0.0    
   1200     3193.94     0.0589     0.0582     1.2404     0.002       0.0    
   1250     3327.43     0.0591     0.0584     1.1765     0.0018      0.0    
   1300      3460.2     0.0592     0.0585     1.1107     0.0016      0.0    
   1350      3594.3     0.0586     0.0582     1.3168     0.0014      0.0    
   1400     3727.64     0.0591     0.0584     1.2075     0.0014      0.0    
   1450     3860.62     0.0591     0.0584     1.3152     0.0012      0.0    
   1500     3993.69     0.0588     0.0583     1.2602     0.0013      0.0    
   1550     4126.91     0.0591     0.0584     1.1741     0.0011      0.0    
   1600     4255.04     0.0587     0.0581     0.8544     0.0011      0.0    
   1650     4381.39     0.0586     0.0579     0.9732     0.0009      0.0    
   1700     4508.04     0.0586     0.0579     0.8882     0.0008      0.0    
   1750     4633.99     0.0591     0.0582     1.1468     0.0009      0.0    
   1800     4760.01     0.0586     0.058      1.156      0.0008      0.0    
   1850      4886.0     0.0588     0.058      0.8617     0.0008      0.0    
   1900     5011.67     0.0588     0.0581     0.8007     0.0008      0.0    
   1950     5137.62     0.0585     0.0579     0.9885     0.0008      0.0    
   2000     5263.53     0.0588     0.0582     1.0854     0.0008      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       120.1      0.1152     0.1141     2.8708     4.4664      0.0    
   100       246.32     0.0748     0.0742     2.7371     0.3848      0.0    
   150       374.71     0.0675     0.0668     1.8247     0.1314      0.0    
   200       503.83     0.062      0.0615     1.6563     0.0725      0.0    
   250       637.38     0.0611     0.0606     1.8043     0.0427      0.0    
   300       770.0      0.0602     0.0599     1.4576     0.0282      0.0    
   350       899.05     0.0597     0.0593     1.2665     0.0233      0.0    
   400      1027.51     0.0597     0.0589      1.81      0.0197      0.0    
   450      1157.01     0.0589     0.0585     1.6451     0.0142      0.0    
   500      1288.14     0.0592     0.0585     1.4521     0.0127      0.0    
   550      1421.95     0.0589     0.0583     1.3535     0.0095      0.0    
   600       1555.4     0.0591     0.0585     1.0748     0.0094      0.0    
   650      1686.64     0.0593     0.0588     1.1479     0.0065      0.0    
   700      1819.28     0.059      0.0587     1.1956     0.0062      0.0    
   750      1950.32     0.0591     0.0587     1.1294     0.0052      0.0    
   800      2080.85     0.0591     0.0586     1.6932     0.0049      0.0    
   850      2213.36     0.0587     0.0582     1.1732     0.0037      0.0    
   900      2343.77     0.0591     0.0586     1.2954     0.0042      0.0    
   950      2475.81     0.0586     0.0582     0.8752     0.0029      0.0    
   1000     2607.53     0.0587     0.0581     0.7006     0.0033      0.0    
   1050     2739.52     0.0587     0.0583     0.8323     0.0028      0.0    
   1100     2871.43     0.0586     0.0582     0.8734     0.0025      0.0    
   1150     3001.01     0.0589     0.0583     1.1406     0.0025      0.0    
   1200     3133.33     0.0588     0.0582     1.0327     0.0022      0.0    
   1250     3264.69     0.0591     0.0585     0.876      0.0021      0.0    
   1300     3395.58     0.0584     0.058      1.0028     0.0022      0.0    
   1350     3528.58     0.0588     0.0582     0.7533     0.0017      0.0    
   1400     3658.76     0.0585     0.0581     0.7594     0.0017      0.0    
   1450     3791.26     0.0587     0.0583     0.8188     0.0014      0.0    
   1500     3921.53     0.0586     0.0581     1.0185     0.0015      0.0    
   1550     4052.85     0.0587     0.0583     0.9041     0.0013      0.0    
   1600     4185.14     0.0583     0.0579     1.1494     0.0014      0.0    
   1650      4314.9     0.0586     0.0581     0.9641     0.0012      0.0    
   1700     4447.06     0.0584     0.0581     0.9774     0.0012      0.0    
   1750     4578.19     0.0585     0.058      0.8816     0.0009      0.0    
   1800     4708.12     0.0585     0.058      1.0367     0.0011      0.0    
   1850     4840.54     0.0584     0.058      1.1282     0.0011      0.0    
   1900     4971.14     0.0584     0.058      0.8331     0.0009      0.0    
   1950     5102.76     0.0582     0.0577     1.0667     0.0009      0.0    
   2000     5234.44     0.0586     0.058      0.798      0.0008      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       123.38     0.116      0.1155     2.7756     4.4658      0.0    
   100       256.99     0.0766     0.0759     2.5017     0.4362      0.0    
   150       387.49     0.0678     0.0672     1.6136     0.1576      0.0    
   200       519.13     0.0628     0.0621     1.5239     0.0686      0.0    
   250       651.47     0.0612     0.0607     1.1021     0.0385      0.0    
   300       782.54     0.0603     0.0598     1.3522     0.0313      0.0    
   350       916.05     0.0597     0.0591     1.4486     0.0182      0.0    
   400      1047.08     0.059      0.0585     1.2584     0.0171      0.0    
   450      1177.69     0.0584     0.0581     1.2007     0.0124      0.0    
   500      1305.24     0.0587     0.0581     0.8393     0.0108      0.0    
   550      1432.28     0.0582     0.0579     0.9069     0.0087      0.0    
   600      1559.58     0.0585     0.058      1.0613     0.0082      0.0    
   650      1686.47     0.0585     0.0581     0.7985     0.006       0.0    
   700       1816.9     0.0587     0.0582      0.9       0.0054      0.0    
   750      1947.54     0.0589     0.0584     1.0516     0.005       0.0    
   800      2078.53     0.0587     0.0582     1.2197     0.0043      0.0    
   850      2208.67     0.0587     0.0582     0.8834     0.0043      0.0    
   900      2337.24     0.0587     0.0582     0.8296     0.0036      0.0    
   950      2469.77     0.0584     0.0579     0.8142     0.0029      0.0    
   1000     2602.05     0.0584     0.058      0.9106     0.003       0.0    
   1050     2735.17     0.0587     0.0582     1.1755     0.0031      0.0    
   1100     2867.14     0.0588     0.0581     0.8991     0.0023      0.0    
   1150     2999.76     0.0583     0.0579     0.9168     0.0023      0.0    
   1200     3132.33     0.0586     0.0582     0.9269     0.002       0.0    
   1250     3265.01     0.0584     0.058      0.8981     0.0019      0.0    
   1300      3397.3     0.0584     0.0578     0.8456     0.0016      0.0    
   1350      3525.7     0.0586     0.0581     0.848      0.0017      0.0    
   1400     3657.42     0.0585     0.0583     0.8923     0.0017      0.0    
   1450     3789.97     0.0583     0.0579     0.8684     0.0015      0.0    
   1500     3922.81     0.0584     0.0579     0.946      0.0013      0.0    
   1550     4055.41     0.0581     0.0577      0.73      0.0012      0.0    
   1600     4188.16     0.0585     0.058      0.636      0.0012      0.0    
   1650     4320.65     0.0585     0.0579     0.6737     0.0011      0.0    
   1700     4453.19     0.0582     0.0578     0.9169     0.0011      0.0    
   1750     4586.41     0.0585     0.0579     1.0055     0.0011      0.0    
   1800     4718.72     0.0581     0.0579     0.8521     0.0011      0.0    
   1850     4851.87     0.0588     0.0582     0.861      0.001       0.0    
   1900     4985.18     0.0584     0.0578     0.8695     0.0009      0.0    
   1950     5118.24     0.0584     0.058      0.9488     0.0009      0.0    
   2000     5250.94     0.0583     0.0577     1.0623     0.0008      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       124.96     0.116      0.1154     2.6634     4.4653      0.0    
   100       258.86     0.0779     0.0773     2.5088     0.5825      0.0    
   150       392.28     0.0692     0.0685     1.7217     0.1397      0.0    
   200       525.93     0.0636     0.0631     1.132      0.0912      0.0    
   250       660.16     0.0611     0.0606     0.7463     0.0449      0.0    
   300       793.9      0.0601     0.0596     0.8911     0.0336      0.0    
   350       928.0      0.0597     0.0593     1.0531     0.0244      0.0    
   400      1061.38     0.0588     0.0584     0.9705     0.0188      0.0    
   450      1195.03     0.0586     0.0582     0.8002     0.0142      0.0    
   500      1328.99     0.0584     0.058      0.8819     0.0114      0.0    
   550      1462.47     0.0588     0.0583     1.1709     0.009       0.0    
   600      1595.72     0.0588     0.0583     1.0776     0.0087      0.0    
   650      1728.76     0.0583     0.0579     0.8326     0.0061      0.0    
   700      1862.23     0.0583     0.0579     0.7348     0.0061      0.0    
   750      1995.71     0.0585     0.058      0.8588     0.0052      0.0    
   800      2129.97     0.0581     0.0577     0.8122     0.0045      0.0    
   850      2263.23     0.0585     0.058      0.9801     0.0044      0.0    
   900       2396.8     0.0587     0.0583     1.2586     0.0039      0.0    
   950      2531.06     0.0587     0.0583     1.0194     0.0035      0.0    
   1000     2664.67     0.0583     0.0579     1.0864     0.003       0.0    
   1050     2797.97     0.0583     0.0579     1.0428     0.0026      0.0    
   1100     2931.55     0.0582     0.0578     0.8322     0.0029      0.0    
   1150     3065.51     0.0581     0.0578     0.7596     0.0024      0.0    
   1200     3198.92     0.0582     0.0577     0.6641     0.0021      0.0    
   1250     3331.85     0.0583     0.0579     0.6926     0.002       0.0    
   1300     3465.47     0.0584     0.0581     0.9193     0.0018      0.0    
   1350     3599.01     0.0585     0.058      0.7855     0.0017      0.0    
   1400     3732.99     0.0583     0.0579     0.9068     0.0016      0.0    
   1450     3867.87     0.0585     0.058      0.8469     0.0016      0.0    
   1500     4001.86     0.0587     0.0581     0.8779     0.0015      0.0    
   1550      4135.8     0.0581     0.0577     0.729      0.0015      0.0    
   1600     4269.01     0.0582     0.0578     0.7466     0.0013      0.0    
   1650     4403.24     0.0581     0.0577     0.7639     0.0012      0.0    
   1700     4536.74     0.0579     0.0577     0.5452     0.0012      0.0    
   1750     4670.91     0.0581     0.0578     0.7958     0.0011      0.0    
   1800     4804.98     0.0583     0.0578     0.6336     0.001       0.0    
   1850      4938.2     0.0581     0.0577     0.7901     0.001       0.0    
   1900     5071.95     0.0584     0.0579     0.7386     0.001       0.0    
   1950     5204.76     0.0581     0.0577     0.6942     0.0009      0.0    
   2000      5339.0     0.0581     0.0579     0.7341     0.0009      0.0    
