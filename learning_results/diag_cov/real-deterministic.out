Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.549437761306763
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       30.63      0.1764     0.1758     3.1623     1.8524   
   100       62.06      0.1361     0.1354     3.1195     0.6733   
   150       93.48      0.111      0.1123     3.0947     0.3618   
   200       124.91     0.0892     0.0904     3.0341     0.234    
   250       156.33     0.0855     0.085      2.8505     0.1656   
   300       187.77     0.079      0.0791     2.6016     0.1278   
   350       219.2      0.0735     0.0738     2.6628     0.1002   
   400       250.64     0.0727     0.0724     2.2206     0.0794   
   450       282.07     0.0698     0.0699     1.8812     0.0662   
   500       313.52     0.0702     0.0707     1.7045     0.0564   
   550       344.95     0.0684     0.0687     1.7233     0.0491   
   600       376.38     0.0676     0.0678     1.7199     0.0413   
   650       407.8      0.0695     0.0697     1.6818     0.0363   
   700       439.22     0.0668     0.0673     1.5899     0.0327   
   750       470.64     0.0659     0.0666     1.4985     0.0292   
   800       502.05     0.0663     0.0666     1.6907     0.0267   
   850       533.49     0.0662     0.0665     1.4248     0.0237   
   900       564.92     0.0663     0.0666     1.6839     0.022    
   950       596.33     0.0656     0.0664     1.1991     0.0198   
   1000      627.77     0.066      0.0666     1.3555     0.0183   
   1050      659.22     0.066      0.0665     1.8196     0.0168   
   1100      690.67     0.0646     0.0655     1.1694     0.0158   
   1150      722.13     0.0642     0.0646     1.2208     0.0145   
   1200      753.58     0.0646     0.0653     0.9044     0.0139   
   1250      785.02     0.0646     0.0651     1.2489     0.0128   
   1300      816.44     0.0648     0.0653     0.9983     0.0119   
   1350      847.86     0.0642     0.0648     1.0432     0.0114   
   1400      879.27     0.064      0.0645     0.982      0.0107   
   1450      910.7      0.0639     0.0644     0.8871      0.01    
   1500      942.13     0.0631     0.0637     0.8391     0.0096   
   1550      973.55     0.0632     0.0638     1.3452     0.009    
   1600     1004.98     0.0632     0.0638     0.7644     0.0085   
   1650     1036.41     0.0631     0.0638      0.87      0.0082   
   1700     1067.84     0.0644     0.0645     0.8216     0.0078   
   1750     1099.29     0.0638     0.064      0.8892     0.0075   
   1800     1130.73     0.0637     0.0637     0.8847     0.0073   
   1850     1162.17     0.0643     0.0643     0.6555     0.0069   
   1900     1193.61     0.0637     0.0637     1.0608     0.0066   
   1950     1225.02     0.0638     0.0641     0.7432     0.0062   
   2000     1256.45     0.0637     0.0639     0.7676     0.006    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       30.73      0.1619     0.1614     3.139      1.2655   
   100       62.84      0.1373     0.1368     3.0441     0.634    
   150       94.92      0.1103     0.1106     2.8136     0.4283   
   200       127.01     0.0989     0.0987     2.6433     0.3095   
   250       159.1      0.089      0.0888     2.5398     0.2366   
   300       191.21     0.0851     0.0852     1.915      0.189    
   350       223.32     0.0788     0.0793     2.1319     0.1537   
   400       255.41     0.0778     0.0781     2.1893     0.1232   
   450       287.53     0.0767     0.0767     1.9661     0.104    
   500       319.62     0.072      0.0717     1.9838     0.0892   
   550       351.72     0.0722     0.0717     1.5052     0.077    
   600       383.81     0.0701     0.0705     1.5249      0.07    
   650       415.92     0.0695     0.0699     2.0122     0.0598   
   700       448.04     0.0696     0.0697     1.4339     0.0534   
   750       480.15     0.0685     0.0687     1.2129     0.0487   
   800       512.25     0.0679     0.0682     1.3846     0.0427   
   850       544.34     0.0669     0.0671     1.1073     0.0387   
   900       576.45     0.0672     0.0674     1.3099     0.0358   
   950       608.55     0.067      0.0672     1.2031     0.0319   
   1000      640.77     0.066      0.0663     1.1344     0.0303   
   1050      672.87     0.0652     0.0654     1.1248     0.0279   
   1100      704.97     0.0651     0.0655     0.9382     0.026    
   1150      737.09     0.0647     0.0652     0.9399     0.0236   
   1200      769.2      0.066      0.0661     1.1735     0.0224   
   1250      801.29     0.0649     0.0652     0.9502     0.021    
   1300      833.39     0.0648     0.0649     0.8336     0.0199   
   1350      865.49     0.064      0.0645     0.9495     0.0185   
   1400      897.58     0.0648     0.0654     0.9414     0.0178   
   1450      929.68     0.0641     0.0646     0.9009     0.0163   
   1500      961.77     0.0638     0.0643     0.9405     0.0153   
   1550      993.87     0.0641     0.0645     0.6949     0.0144   
   1600     1025.97     0.0643     0.0649     0.8047     0.014    
   1650     1058.08     0.0633     0.0638     0.9205     0.0129   
   1700     1090.19     0.0638     0.0643     0.9964     0.0125   
   1750      1122.3     0.0632     0.0635     0.7443     0.0117   
   1800     1154.39     0.0638     0.064      0.7536     0.0113   
   1850     1186.49     0.0641     0.0643     0.7712     0.0107   
   1900     1218.59     0.0638     0.0642     0.7569     0.0104   
   1950     1250.69     0.0632     0.0634     0.7633     0.0099   
   2000     1282.78     0.0638     0.0638     0.9183     0.0095   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       30.73      0.148      0.148      3.0358     1.0056   
   100       63.47      0.1279     0.1282     2.913      0.5639   
   150       96.21      0.1171     0.1172     2.6442     0.4025   
   200       128.96     0.1061     0.1057     2.3723     0.3019   
   250       161.72     0.096      0.0956     2.4938     0.246    
   300       194.49     0.0877     0.0881     2.1891     0.197    
   350       227.24     0.0877     0.0876     2.0307     0.1655   
   400       260.02     0.0814     0.0817     2.1082     0.1383   
   450       292.75     0.0789     0.0789     1.7475     0.1204   
   500       325.5      0.0759     0.0762     1.6322     0.1031   
   550       358.24     0.0761     0.0761     1.2837     0.0897   
   600       390.99     0.0744     0.0746     1.3956     0.0778   
   650       423.71     0.0725     0.0724     1.5955     0.0714   
   700       456.47      0.07      0.0704     1.5925     0.0644   
   750       489.23      0.07      0.0703     1.0611     0.0575   
   800       521.96     0.0684     0.0685     1.2714     0.0517   
   850       554.71     0.0678     0.0681     1.1618     0.0492   
   900       587.46     0.0663     0.0666     1.1153     0.0432   
   950       620.22     0.0671     0.0674     1.0986     0.0399   
   1000      652.97     0.0667     0.0669     1.0146     0.037    
   1050      685.72     0.0663     0.0663     1.0456     0.0333   
   1100      718.48     0.0661     0.0663     1.2269     0.0316   
   1150      751.22     0.0659     0.066      0.7825     0.0299   
   1200      783.97     0.0647     0.065      1.2301     0.0274   
   1250      816.72     0.0661     0.0663     0.7979     0.0255   
   1300      849.47     0.065      0.0653     0.7054     0.0238   
   1350      882.23     0.065      0.0653     0.8096     0.0226   
   1400      914.98     0.0652     0.0652     0.6975     0.0212   
   1450      947.74     0.0649     0.065      0.633      0.0201   
   1500      980.51     0.064      0.0643     0.784      0.0196   
   1550     1013.28     0.064      0.0644     0.7959     0.018    
   1600     1046.04     0.0635     0.0639     0.6362     0.0172   
   1650     1078.81     0.0644     0.0648     0.5987     0.0163   
   1700      1111.6     0.0642     0.0647     0.658      0.0158   
   1750     1144.37     0.0643     0.0646     0.7167     0.0146   
   1800     1177.19     0.0639     0.0641     0.6649     0.0141   
   1850     1209.97     0.0639     0.0642     0.8397     0.0135   
   1900     1242.74     0.0639     0.0641     0.7113     0.0129   
   1950     1275.52     0.0634     0.0637     0.638      0.0123   
   2000     1308.29     0.0635     0.0638     0.6978     0.0119   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       30.85      0.1599     0.1596     3.0195     0.8474   
   100       64.27      0.1384     0.1377     2.6081     0.5036   
   150       97.72      0.1183     0.1167     2.3358     0.3734   
   200       131.17     0.1041     0.1034     2.3569     0.3004   
   250       164.61     0.0952     0.0952     2.0083     0.2436   
   300       198.05     0.0894     0.0894     2.4716     0.2026   
   350       231.48     0.0866     0.0857     1.8083     0.1699   
   400       264.91     0.0795     0.0793     1.9143     0.1449   
   450       298.35     0.0758     0.0765     1.5481     0.1268   
   500       331.78     0.0766     0.0771     1.5915     0.1087   
   550       365.23     0.0743     0.0747     1.2114     0.0951   
   600       398.66     0.0718     0.0719     1.4667     0.0852   
   650       432.12     0.0715     0.0714     1.0513     0.0761   
   700       465.55     0.0699     0.0698     1.3448     0.0697   
   750       498.98     0.0688     0.0689     1.005      0.0628   
   800       532.44     0.0677     0.0678     1.207      0.0579   
   850       565.9      0.0679     0.0683     1.1978     0.053    
   900       599.34     0.0688     0.0694     0.6865     0.0495   
   950       632.8      0.0681     0.0682     0.8984     0.0445   
   1000      666.23     0.0675     0.0673     0.9552     0.0414   
   1050      699.68     0.0666     0.0664     1.0296     0.0388   
   1100      733.14     0.0657     0.0657     0.9097     0.036    
   1150      766.58     0.0652     0.0653     0.8881     0.0331   
   1200      800.0      0.0653     0.0654     0.9234     0.0312   
   1250      833.41     0.0643     0.0648     0.7447     0.0292   
   1300      866.83     0.0647     0.0652     1.0431     0.028    
   1350      900.28     0.0641     0.0645     0.7155     0.0262   
   1400      933.73     0.0646     0.0651     0.9428     0.025    
   1450      967.17     0.0643     0.0645     0.6917     0.0234   
   1500     1000.62     0.0639     0.064      0.9388     0.022    
   1550     1034.07     0.0638     0.064      0.805      0.021    
   1600     1067.51     0.0639     0.0642     0.9148     0.0196   
   1650     1100.93     0.0633     0.0635     0.5593     0.019    
   1700     1134.36     0.0639     0.0639     0.5222     0.018    
   1750     1167.78     0.0642     0.0644     0.5879     0.0172   
   1800     1201.22     0.0639     0.0641     0.6188     0.0161   
   1850     1234.66     0.0636     0.0637     0.6017     0.0154   
   1900     1268.08     0.0641     0.0642     0.5541     0.0149   
   1950     1301.53     0.064      0.064      0.486      0.0145   
   2000     1334.97     0.0635     0.0636     0.6452     0.0138   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | 
    50       30.81      0.1554     0.1543     2.8221     0.7553   
   100       64.78      0.1201     0.1194     2.2712     0.4545   
   150       98.76      0.1097     0.1097     2.2485     0.3566   
   200       132.72     0.0988     0.0986     2.0048     0.2844   
   250       166.7      0.0956     0.0962     1.6037     0.2332   
   300       200.67     0.089      0.0898     1.6863     0.1976   
   350       234.64     0.0838     0.0841     1.7476     0.1695   
   400       268.61     0.0812     0.0813     1.7683     0.145    
   450       302.59     0.077      0.0774     1.2943     0.1266   
   500       336.59     0.0754     0.0757     1.2837     0.113    
   550       370.56     0.0756     0.0759     1.5117     0.1004   
   600       404.55     0.0737     0.074      1.5126     0.0916   
   650       438.53     0.0721     0.0723     1.2595     0.0826   
   700       472.56     0.0707     0.0709     1.3134     0.073    
   750       506.56     0.0691     0.0693     1.2493     0.0673   
   800       540.55     0.0695     0.0697     1.0502     0.0633   
   850       574.55     0.0687     0.0691     0.9228     0.057    
   900       608.54     0.0673     0.0678     1.0077     0.0525   
   950       642.49     0.0668     0.0673     0.9357     0.0485   
   1000      676.48     0.0669     0.0674     0.9472     0.0442   
   1050      710.47     0.067      0.0675     0.8324     0.0414   
   1100      744.46     0.0683     0.0685     0.8473     0.0399   
   1150      778.43     0.0678     0.068      0.7714     0.0363   
   1200      812.45     0.0663     0.0665      0.78      0.0344   
   1250      846.44     0.0656     0.0658     0.6692     0.0319   
   1300      880.43     0.0657     0.0659     0.6409     0.0303   
   1350      914.42     0.0655     0.0655     0.7102     0.0281   
   1400      948.4      0.0654     0.0654     0.6536     0.0265   
   1450      982.37     0.0653     0.0653     0.9323     0.0254   
   1500     1016.35     0.0654     0.0653     0.7407     0.0246   
   1550     1050.34     0.0651     0.0651     0.5251     0.0228   
   1600     1084.31     0.0647     0.0648     0.7429     0.0218   
   1650     1118.29     0.0649     0.065      0.8154     0.0207   
   1700     1152.24     0.0642     0.0644     0.6067     0.0197   
   1750     1186.22     0.0645     0.0647     0.7744     0.019    
   1800     1220.18     0.0637     0.064      0.7962     0.0183   
   1850     1254.15     0.0634     0.0636      0.74      0.0171   
   1900     1288.12     0.0633     0.0634     0.5861     0.0162   
   1950     1322.09     0.0637     0.0638     0.4732     0.0159   
   2000     1356.05     0.0635     0.0637     0.5767     0.0152   
