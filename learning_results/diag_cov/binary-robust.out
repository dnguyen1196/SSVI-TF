Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  4.883338689804077
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       73.88      0.4995     0.4962     3.1623    30.5717     0.1736  
   100       203.54     0.5021     0.5026     3.1341   21089.4152   0.185   
   150       334.81     0.4967     0.5002     3.1094   20790.8053   0.0912  
   200       465.82      0.5       0.4984     2.9073   2447.6022    0.159   
   250       596.77     0.498      0.5022     3.0104   4212.6429    0.303   
   300       727.67     0.4969     0.4965     2.2955   4154.7387    0.1486  
   350       858.53     0.5028     0.4984     2.1559   8933.7915    0.1487  
   400       989.39     0.4978     0.4987     1.7331   8927.0767    0.0947  
   450      1120.15     0.5002     0.4976     1.6145   1238.8293    0.109   
   500      1250.83     0.4963     0.5022     3.1524    2556.268    0.0859  
   550      1381.45     0.5047     0.5026     1.4251   5371.3795    0.0798  
   600      1512.04     0.4953     0.4934     2.9372   5419.2312    0.0429  
   650      1642.54     0.4986     0.504      1.1307   2000.3244    0.0332  
   700      1772.93     0.5012     0.5005     2.6211   2347.1574    0.0445  
   750      1903.32     0.5011     0.5036     2.0096   2389.7819    0.0241  
   800      2033.46     0.4976     0.5032     3.1246   64581.4651   0.022   
   850      2163.84     0.5011     0.4979     1.228    64062.8386   0.0587  
   900      2294.25     0.5082     0.5034     2.4797   11355.7237   0.0102  
   950      2424.69     0.5026     0.5065     1.369    11263.091    0.0254  
   1000     2554.98     0.4994     0.5026     1.6818   4011.4584    0.0244  
   1050     2685.05     0.4932     0.4977     1.1112   6253.2544    0.0139  
   1100     2814.94     0.5002     0.4956     2.5361   2922.5088    0.0085  
   1150     2944.74     0.4987     0.4947     1.0468   3625.5453    0.0094  
   1200     3074.38     0.5013     0.4962     3.1464   3520.5904    0.0116  
   1250     3203.88     0.4993     0.4996     2.8812    698.7451    0.0083  
   1300     3333.24     0.498      0.497      0.7361    742.2123    0.0053  
   1350     3462.53     0.4979     0.4994     0.7929   4647.1325    0.0047  
   1400     3591.67     0.4961     0.5011     0.9343   4291.4243    0.0059  
   1450     3720.74     0.5006     0.4952     0.7826    2814.962    0.0045  
   1500      3849.7     0.4936     0.4902     3.1605    555.1641    0.003   
   1550     3978.55     0.5009     0.5042     2.978    1045.5339    0.003   
   1600     4107.39     0.5068     0.4962     3.1575   1156.6637    0.0033  
   1650     4236.31     0.496      0.4985     2.1253   1316.9797    0.0027  
   1700     4365.23     0.4957     0.5008     3.1418    310.5825    0.003   
   1750     4494.09     0.5036     0.5006     0.7559    1457.41     0.0016  
   1800      4622.0     0.4988     0.4954     3.0778   3297.9635    0.0029  
   1850     4749.16     0.4967     0.4912     1.0142   5344.2775    0.0017  
   1900     4876.22     0.5008     0.4911     2.871    1794.7969    0.0029  
   1950     5003.26     0.4986     0.4958     0.6961   1047.1843    0.0027  
   2000     5130.28     0.4968     0.4939     0.6844   1361.4242    0.001   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       73.19      0.4999     0.4997     0.2648     0.293      0.0001  
   100       222.7      0.4984     0.5038     0.2219     0.3828     0.0005  
   150       372.2      0.4993     0.5001     0.2398     0.5164     0.0003  
   200       521.73     0.4966     0.503      0.2447     0.7032     0.0012  
   250       671.27     0.4993      0.5       0.2553     1.5048     0.0012  
   300       821.0      0.5067     0.4998     0.3284     5.8676     0.0015  
   350       971.02     0.4967     0.4975     0.7737     248.86     0.0193  
   400      1122.48     0.497      0.4994     1.2131   1320.6341    0.0276  
   450      1275.28     0.497      0.5021     1.1059   1341.0744    0.014   
   500       1428.2     0.4988     0.503      1.0599   3136.7405    0.0791  
   550      1581.21     0.5023     0.4993     3.1387   2806.2224    0.0363  
   600      1734.26     0.4952     0.4996     1.1313    997.3786    0.0223  
   650      1887.42     0.4998     0.4988     1.0797   2403.6873    0.0035  
   700      2040.53     0.4967     0.5066     1.2403   2402.2316    0.0055  
   750      2193.73     0.5024      0.5       1.1113   5806.4782    0.3225  
   800      2346.91     0.5012     0.4972     0.9662   5804.2625    0.0321  
   850      2499.87     0.5009     0.4994     1.2793   7322.4037    0.0619  
   900      2652.92     0.5021     0.4972     1.4291   7073.3106    0.1076  
   950      2806.72     0.4969     0.5007     2.6083   10156.7474   0.0583  
   1000     2960.45     0.4927     0.5006     0.9052   10216.4986   0.1151  
   1050     3113.95     0.5026     0.4966     3.0307   1693.5133    0.014   
   1100     3267.44     0.5011     0.4966     0.9174   4859.4837    0.0449  
   1150     3421.02     0.503      0.4981     0.9839   4843.9454    0.0842  
   1200     3574.65     0.5039     0.4991     0.9207   1481.6684    0.0414  
   1250     3728.21     0.4981     0.498      1.1916   7822.8139    0.0586  
   1300     3881.95     0.5068     0.4974     1.1239   10464.674    0.1064  
   1350      4035.7     0.4972     0.5016     3.0641   11551.0331   0.0155  
   1400     4189.58     0.4996     0.499      1.3187   64573.8179   0.1637  
   1450     4341.15     0.4966     0.4986     1.1458   64700.5209   0.0476  
   1500     4490.25     0.5003     0.4987     1.0783   21237.9897   0.0412  
   1550     4640.45     0.4975     0.4997     1.1577   5065.0879    0.0472  
   1600     4793.84     0.5045     0.4962     2.2917   2110.7897    0.0322  
   1650     4944.95     0.4952     0.4997     3.0345    654.9971    0.1432  
   1700     5093.69     0.4985     0.5027     2.7551   4398.9986    0.1863  
   1750     5242.48     0.5025     0.5024     0.9922   5823.8256    0.0651  
   1800      5391.3     0.5015     0.4998     2.8632   5808.3592    0.0805  
   1850     5540.37     0.4989     0.4953     2.4835   1735.7215    0.0615  
   1900     5689.23     0.5012     0.4947     1.988    3757.4189    0.0979  
   1950     5838.42     0.4981     0.4986     1.1061   4182.3105    0.0733  
   2000     5987.58     0.4951     0.4993     1.2516   15691.4969   0.003   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       72.67      0.4992     0.4991     0.0932     0.0436     0.0126  
   100       241.98     0.4961     0.4977     0.0953     0.0786     0.0048  
   150       410.61     0.5008     0.4989     0.1359     0.1497     0.002   
   200       579.37     0.4977     0.5016     0.1905     0.221      0.001   
   250       747.88     0.5002     0.497      0.1436     0.3052     0.0003  
   300       916.36     0.4951     0.4972     0.1508     0.5303     0.0004  
   350      1084.91     0.503      0.5016     0.1953     0.6243     0.0011  
   400      1253.37     0.5036     0.501      0.1859     1.3848     0.0005  
   450      1421.79     0.4978     0.5014     0.1747     2.9663     0.0007  
   500      1590.34     0.4999     0.5027     0.2488    48.3816     0.0011  
   550      1759.33     0.4991     0.5021     0.6622    200.3359    0.003   
   600      1929.41     0.5032     0.5014     0.7183    193.1194    0.0068  
   650      2100.28     0.4992     0.5003     1.0367    848.719     0.1794  
   700      2271.62     0.4938     0.4984     0.8386    771.9545    0.0322  
   750      2443.39     0.4985     0.4995     0.8478    456.2107    0.0328  
   800      2614.24     0.5007     0.5015     0.6983    244.8457    0.0039  
   850      2784.68     0.5024     0.4973     0.817     772.6739    0.0607  
   900      2955.19     0.4996     0.5009     0.7708   1139.8742    0.026   
   950      3125.83     0.4999     0.4983     0.8267   1092.3432    0.0201  
   1000     3296.61     0.5035     0.4981     0.9762    1022.071    0.0112  
   1050     3467.53     0.4938     0.4972     3.0758    998.9679    0.0352  
   1100     3638.28     0.4926     0.4976     2.7586    680.2751    0.0161  
   1150     3809.13     0.4959      0.5       0.8824   1189.4547    0.0332  
   1200     3980.03     0.4998     0.4965     1.0358   1202.9328    0.0188  
   1250     4150.93     0.5022     0.5012     1.0212   7622.5521    0.0256  
   1300      4322.1     0.5016     0.4998     0.9969   7576.5986    0.286   
   1350     4493.02     0.4957     0.5014     0.8944   9919.1853    0.1029  
   1400     4664.11     0.5015     0.5006     2.2312   9457.8851    0.1206  
   1450     4835.18     0.5008     0.4987     0.8992    661.0862    0.1872  
   1500     5005.96     0.4966     0.4998     0.8662   1069.3175    0.0788  
   1550     5176.91     0.4953      0.5       0.9538    1570.683    0.0429  
   1600     5348.05     0.501      0.5029     0.9373    2665.391    0.0148  
   1650      5519.2     0.4968      0.5       2.4015   2384.8035    0.1119  
   1700     5690.14     0.4993     0.5008     1.0694   1407.5055    0.0784  
   1750     5861.07     0.5016     0.5025     2.8174   2815.9018    0.0256  
   1800     6032.06     0.4938     0.498      2.0952   3163.3397    0.0125  
   1850     6202.99     0.503      0.4977     1.507     698.1535    0.0283  
   1900     6373.98     0.5006     0.5019     1.0248    1404.855    0.0135  
   1950     6544.92     0.4982      0.5       2.6808   2525.6568    0.0065  
   2000     6715.83     0.498      0.4982      0.76    1097.0548    0.0114  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       72.01      0.4993     0.5008     0.0545     0.0405     0.011   
   100       263.4      0.4994     0.4975     0.0654     0.0478     0.005   
   150       454.25     0.4995     0.4995     0.0909     0.0798     0.0025  
   200       644.83     0.5025     0.4997     0.0985     0.115      0.0011  
   250       835.26     0.4984     0.5009     0.1009     0.1567     0.0004  
   300      1025.59     0.5044     0.5004     0.1112     0.1745     0.0002  
   350      1215.94     0.4994     0.4978     0.1063     0.2207     0.0005  
   400      1406.31     0.5069     0.4992     0.1155     0.3041     0.0003  
   450      1596.64     0.4979     0.5011     0.1246     0.3741     0.0002  
   500      1787.02     0.501      0.4994     0.1219     0.4962     0.0001  
   550      1977.42     0.5052     0.4989     0.1174     0.7282     0.0006  
   600      2167.83     0.4988     0.4978     0.1439     1.6479     0.0007  
   650      2358.27     0.5031     0.4981     0.1576     4.7875     0.0009  
   700      2548.78     0.5017     0.5008     0.187     43.2616     0.0028  
   750      2739.53     0.502      0.5029     0.647     89.6255     0.0124  
   800      2930.99     0.5026     0.4994     0.701     809.6214    0.082   
   850      3123.59     0.4936     0.5018     0.7483    787.1703    0.0066  
   900      3316.48     0.5028     0.5001     0.6455    183.5374    0.0053  
   950      3509.87     0.4992     0.4995     0.8809   13732.7361   0.002   
   1000     3703.87     0.497      0.5022     2.712    13845.6652   0.0353  
   1050     3897.97     0.4978     0.5025     0.7836   1552.3147    0.0203  
   1100     4092.08     0.5049     0.5019     0.8923    791.7128    0.037   
   1150     4286.04     0.4971     0.4998     0.8698    832.0935    0.0411  
   1200     4480.27     0.501      0.5035     1.1187   1375.7041    0.0627  
   1250     4674.71     0.5011     0.5006     0.9274    2312.559    0.2352  
   1300     4869.23     0.4978     0.4985     0.7396   2149.6319    0.1033  
   1350     5063.77     0.4977     0.498      0.9934   52767.0244    0.21   
   1400     5258.75     0.4994      0.5       0.9774   51984.5154   0.1314  
   1450     5453.33     0.5028     0.5017     0.8978   7235.1878    0.2118  
   1500     5648.06     0.502      0.5027     2.5931    9444.34     0.0718  
   1550     5842.57     0.4992     0.5001     0.7403   1904.0926    0.0259  
   1600     6036.92     0.4966     0.5004     0.6307   1952.4491    0.0852  
   1650     6231.25     0.5018     0.4986     1.1614   2087.9056    0.0333  
   1700     6425.63     0.4941     0.4989     0.7616    781.9626    0.0067  
   1750     6620.03     0.4986     0.5008     0.7123    599.2604    0.1154  
   1800     6814.43     0.497      0.5004     0.7347   1558.9493    0.0192  
   1850     7008.97     0.4962     0.5007     0.9505   4314.9595    0.0276  
   1900     7203.48     0.5004     0.4999     0.7676    4357.922    0.0251  
   1950     7397.92     0.5017      0.5       2.9082    915.9973    0.007   
   2000     7592.37     0.5008     0.4999     0.7168   1947.4777    0.0066  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       72.35      0.497      0.4976     0.0418     0.0224     0.0105  
   100       287.35     0.5055     0.4989     0.0535     0.0308     0.0048  
   150       501.97     0.4978     0.4991     0.0757     0.0455     0.0029  
   200       716.26     0.501      0.501      0.0781     0.0778     0.0023  
   250       930.32     0.4972     0.4997     0.0838     0.082      0.0009  
   300      1144.25     0.5031     0.5012     0.0749     0.0932     0.0006  
   350      1358.11     0.4947     0.5007     0.1034     0.1167     0.0002  
   400       1571.9     0.5015     0.499      0.0961     0.1319     0.0001  
   450      1785.63     0.4963     0.4993     0.0979     0.2035     0.0004  
   500      1999.34     0.5008     0.4985     0.1199     0.1871     0.0001  
   550      2213.12     0.5009     0.5001     0.1147     0.1966     0.0001  
   600      2427.06     0.5055     0.4976     0.1071     0.2535     0.0006  
   650      2640.81     0.5018     0.5023     0.1283     0.4267     0.0007  
   700      2854.62     0.4982     0.4998     0.1379     0.4496     0.0003  
   750      3068.42     0.4994     0.5012     0.1446     0.9122     0.0005  
   800      3282.26     0.493      0.5009     0.1134     0.9643     0.001   
   850      3496.12     0.5018     0.5025     0.1601     2.8964     0.001   
   900      3710.06     0.5052     0.5028     0.4761    83.3109     0.0026  
   950      3924.18     0.5038     0.5018     0.6716    171.7973     0.01   
   1000     4139.04     0.502      0.5005     0.6637    736.7231    0.0223  
   1050     4354.81     0.4983     0.5004     0.6948    721.3514    0.0006  
   1100     4571.33     0.4934     0.5012     3.1155   4944.2405    0.0328  
   1150     4788.67     0.5011     0.4992     0.9546   4929.2564    0.0399  
   1200     5006.08     0.4998     0.4994     0.8609   163310.0222   0.3218  
   1250     5223.83     0.4986     0.4984     0.7706   163244.5677   0.0298  
   1300     5441.14     0.501      0.4999     0.8278    466.3881    0.0181  
   1350     5658.53     0.5017     0.5005     0.8162   1601.1628    0.0186  
   1400     5876.02     0.4976     0.4986     0.9875   1468.8767    0.0357  
   1450     6093.78     0.4944     0.4997     0.8059    518.2524    0.024   
   1500     6311.49     0.5009     0.4989     1.0162   26447.7678   0.0136  
   1550     6529.91     0.4986     0.5005     1.128    23200.144    0.1269  
   1600     6748.56     0.4999     0.5023     2.7061   3300.1533     0.04   
   1650     6966.89     0.5021     0.4996     0.7275    1412.405    0.1971  
   1700     7185.21     0.4995     0.5013     0.8394    852.8428    0.0287  
   1750     7403.65     0.5055     0.5001     0.7082   1687.1678    0.0268  
   1800     7622.14     0.4987     0.5011     1.6958   1684.6282    0.0225  
   1850     7840.63     0.4969      0.5       1.0644   22069.0973   0.0105  
   1900     8059.56     0.4995     0.5006     0.7942   22321.6957   0.0444  
   1950     8279.22     0.5004     0.4993     0.7076   1947.6772    0.0275  
   2000     8498.26     0.5019     0.4985     0.7114   3803.7225    0.0174  
