Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.6944944858551025
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       100.84     0.1595     0.1586     3.1623     1.8524      0.0    
   100       201.77     0.0882     0.0877     3.1622     0.7658      0.0    
   150       302.59     0.0736     0.0729     2.9094     0.2863      0.0    
   200       403.46     0.0685     0.0684     2.7249     0.1673      0.0    
   250       503.29     0.0646     0.0644     2.5308     0.1454      0.0    
   300       603.05     0.0627     0.0624     2.195      0.1056      0.0    
   350       702.87     0.0613     0.0608     1.7096     0.0958      0.0    
   400       802.62     0.0605      0.06      1.3483     0.0638      0.0    
   450       902.49     0.0604     0.0599     1.7978     0.0519      0.0    
   500      1002.31     0.0601     0.0596     1.7518     0.0441      0.0    
   550      1102.17     0.0599     0.0594     1.4241     0.0479      0.0    
   600      1202.05     0.0602     0.0597     1.1346     0.0472      0.0    
   650       1301.9     0.0598     0.0594     1.0583     0.0367      0.0    
   700       1401.8     0.0599     0.0593     0.8537     0.0354      0.0    
   750      1501.64     0.0596     0.059      0.7076     0.0384      0.0    
   800      1601.55     0.0601     0.0596     1.0988     0.039       0.0    
   850      1701.43      0.06      0.0595     1.2729     0.0359      0.0    
   900       1801.3     0.0597     0.0593     1.0078     0.0294      0.0    
   950      1901.18     0.0598     0.0595     1.1953     0.0278      0.0    
   1000     2001.04     0.0599     0.0595     1.3446     0.0337      0.0    
   1050     2101.01     0.0594     0.0591     1.2758     0.0264      0.0    
   1100     2200.86     0.0595     0.059      1.345      0.0272      0.0    
   1150     2300.84     0.0597     0.0591     0.733      0.0259      0.0    
   1200     2400.74     0.0594     0.0593     1.0047     0.0306      0.0    
   1250     2500.68      0.06      0.0594     1.0043     0.0245      0.0    
   1300     2600.55     0.0597     0.0592     0.8171     0.0235      0.0    
   1350     2700.46     0.0597     0.0593     0.9437     0.0246      0.0    
   1400     2800.37     0.0596     0.0591     0.8654     0.0226      0.0    
   1450     2900.23     0.0595     0.0592     0.9457     0.0221      0.0    
   1500     3000.19     0.0596     0.0592     0.8538     0.0266      0.0    
   1550     3100.07     0.0594     0.0592     0.8397     0.0202      0.0    
   1600     3199.95     0.0597     0.0591     0.7954     0.0199      0.0    
   1650     3299.82     0.0595     0.0595     0.8106     0.0196      0.0    
   1700     3399.68     0.0598     0.0591     0.8257     0.0189      0.0    
   1750     3499.56     0.0593     0.059      0.8219     0.0198      0.0    
   1800     3599.43     0.0596     0.0592     0.9301     0.0218      0.0    
   1850     3699.31     0.0594     0.0591     0.8065     0.019       0.0    
   1900     3799.17     0.0595     0.0591     0.8763     0.0165      0.0    
   1950     3899.09     0.0595     0.059      0.8536     0.0178      0.0    
   2000     3998.96     0.0595     0.0592     0.8876     0.0152      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       99.16      0.1303      0.13      3.0445     1.8328      0.0    
   100       199.04     0.0797     0.0794     2.8332     1.0067      0.0    
   150       299.19     0.0704     0.0698     2.3252     0.2152      0.0    
   200       399.56     0.064      0.0636     2.4149     0.219       0.0    
   250       499.94     0.0625     0.0621     1.9634     0.1663      0.0    
   300       600.34     0.0619     0.0613     1.9274     0.1649      0.0    
   350       700.78     0.0612     0.0607     1.8425     0.1402      0.0    
   400       801.23     0.0607     0.0601     1.6242      0.12       0.0    
   450       901.62     0.0606     0.0601     1.8262     0.1064      0.0    
   500       1002.0     0.0604     0.0599     1.8956     0.0834      0.0    
   550      1102.48     0.0601     0.0596     1.6834     0.0833      0.0    
   600      1202.91     0.0603     0.0596     1.4209     0.0807      0.0    
   650      1303.34     0.0598     0.0593     1.1979     0.0825      0.0    
   700      1403.81     0.0598     0.0593     1.0361     0.0571      0.0    
   750      1504.29      0.06      0.0593     1.2801     0.0996      0.0    
   800      1604.78     0.0595     0.0591     1.1013     0.0602      0.0    
   850       1705.3      0.06      0.0591     1.1128     0.068       0.0    
   900      1805.84     0.0596     0.0592     1.1997     0.0534      0.0    
   950      1906.29     0.0604     0.0596     1.3067     0.0466      0.0    
   1000     2006.74     0.0597     0.0592     1.2531     0.0411      0.0    
   1050     2107.18     0.0598     0.0592     1.2304     0.0438      0.0    
   1100     2207.63      0.06      0.0595     1.2005     0.0431      0.0    
   1150     2308.17      0.06      0.0593     1.1336     0.0382      0.0    
   1200     2408.61     0.0594     0.0589     1.1164     0.0385      0.0    
   1250     2509.15     0.0597     0.0593     0.9881     0.0419      0.0    
   1300     2609.64     0.0595     0.059      0.9156     0.0397      0.0    
   1350     2710.15     0.0597     0.0591     0.9171     0.0406      0.0    
   1400     2810.74     0.0597     0.0589     1.1565     0.0315      0.0    
   1450     2911.26     0.0595     0.059      1.0939     0.0316      0.0    
   1500     3011.86     0.0595     0.0589     0.8583     0.0349      0.0    
   1550      3112.4     0.0597     0.0591     1.2217     0.0289      0.0    
   1600     3212.98     0.0597     0.0591     0.9456     0.0266      0.0    
   1650     3313.35     0.0597     0.0591     0.9464     0.0259      0.0    
   1700     3413.17     0.0598     0.0593     0.8614     0.0231      0.0    
   1750     3513.02     0.0594     0.0588     0.8322     0.0288      0.0    
   1800     3612.84     0.0598     0.0591     0.9665     0.0243      0.0    
   1850     3712.65     0.0596     0.059      0.9343     0.0239      0.0    
   1900     3812.48     0.0597     0.0591     0.8216     0.0254      0.0    
   1950      3912.3     0.0597     0.0591     0.979      0.0194      0.0    
   2000     4012.15     0.0594     0.0588     1.2389     0.0271      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       98.55      0.1278     0.1271     2.8543     1.8032      0.0    
   100       198.28     0.0799     0.0793     2.6323     0.8012      0.0    
   150       298.36     0.0697     0.0693     1.9506     0.2306      0.0    
   200       398.5      0.0643     0.0638     1.6293     0.1731      0.0    
   250       498.75     0.0622     0.0617     1.6318     0.2492      0.0    
   300       599.05     0.0611     0.0605     1.7836     0.1502      0.0    
   350       699.39     0.0608     0.0604     1.4789     0.1659      0.0    
   400       799.71     0.0607     0.0599     1.3048     0.1214      0.0    
   450       900.04     0.0596     0.0594     1.344      0.0955      0.0    
   500      1000.38     0.0601     0.0595     1.1828     0.0929      0.0    
   550      1100.77     0.0594     0.0591     1.0698     0.0702      0.0    
   600      1201.13     0.0598     0.0594     1.2848     0.0831      0.0    
   650      1301.51     0.0597     0.0594     1.2942     0.0792      0.0    
   700      1401.85     0.0602     0.0596     1.0516     0.0638      0.0    
   750      1502.16     0.0597     0.0592     1.4523     0.0546      0.0    
   800       1602.5     0.0602     0.0597     1.4456     0.0559      0.0    
   850      1702.91     0.0596     0.0591     1.2456     0.0708      0.0    
   900      1803.32     0.0597     0.0593     1.0275     0.0597      0.0    
   950      1903.72     0.0593     0.0589     1.037      0.0471      0.0    
   1000     2004.09     0.0595     0.059      1.2114     0.0408      0.0    
   1050     2104.46     0.0593     0.0589     0.7967     0.0448      0.0    
   1100     2204.87     0.0595     0.0591     0.9186     0.0369      0.0    
   1150     2305.26     0.0594     0.059      1.0081     0.0346      0.0    
   1200     2405.67     0.0594     0.059      1.2533     0.0409      0.0    
   1250      2506.1     0.0593     0.059      0.979      0.0374      0.0    
   1300     2606.46     0.0596     0.0592     1.071      0.0317      0.0    
   1350     2706.83     0.0595     0.0589     0.841      0.0298      0.0    
   1400     2807.21     0.0594     0.0589     0.7539     0.0314      0.0    
   1450     2907.61     0.0593     0.0589     0.7145     0.0256      0.0    
   1500      3008.0     0.0594     0.0589     1.2237     0.0284      0.0    
   1550     3108.47     0.0593     0.0588     1.2212      0.03       0.0    
   1600     3208.92     0.059      0.0586     0.6838     0.0222      0.0    
   1650     3309.34     0.0592     0.0588     0.8301     0.0293      0.0    
   1700     3409.75     0.0591     0.0587     0.7924     0.0241      0.0    
   1750     3510.13     0.0591     0.0587     0.8238     0.0245      0.0    
   1800     3610.56     0.0594     0.059      0.9716     0.0277      0.0    
   1850     3711.01     0.0591     0.0588     0.8931     0.0247      0.0    
   1900     3811.45     0.0592     0.0586     0.7385     0.0181      0.0    
   1950     3911.85     0.0593     0.0588     0.8763     0.0196      0.0    
   2000     4012.24     0.0593     0.0588     0.8927     0.0197      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       98.93      0.1278     0.1271     2.7171     1.8033      0.0    
   100       199.4      0.0824     0.0819     2.6141     0.9152      0.0    
   150       300.21     0.0708     0.0701     1.3558     0.2143      0.0    
   200       401.14     0.0648     0.0644     1.1076     0.1765      0.0    
   250       502.12     0.0623     0.0619     1.1561     0.1699      0.0    
   300       603.14     0.0614     0.061      1.1167     0.1403      0.0    
   350       704.16     0.0604      0.06      1.3357     0.1146      0.0    
   400       805.23     0.0603     0.0598     1.2723     0.1111      0.0    
   450       906.32      0.06      0.0595     1.2647     0.1047      0.0    
   500      1007.38     0.0597     0.0592     1.3149     0.0892      0.0    
   550      1108.48     0.0599     0.0593     1.4446     0.085       0.0    
   600      1209.53     0.0595     0.059      1.2425     0.0946      0.0    
   650       1310.6     0.0594     0.0589     0.9213     0.0654      0.0    
   700      1412.24     0.0596     0.0591     0.9857     0.0594      0.0    
   750      1513.47     0.0594     0.059      1.2298     0.0544      0.0    
   800      1614.65     0.0597     0.059      1.3118     0.0601      0.0    
   850      1715.83     0.0595     0.059      1.3454     0.0627      0.0    
   900      1816.96     0.059      0.0587     1.1741     0.0593      0.0    
   950      1917.96     0.0591     0.0587      0.78      0.0458      0.0    
   1000     2019.01     0.0591     0.0586     0.8192     0.0454      0.0    
   1050     2120.03     0.0592     0.0588     0.8413     0.0382      0.0    
   1100     2221.07     0.0593     0.0589     0.8541     0.0431      0.0    
   1150      2322.1     0.0592     0.0588     0.7976     0.0371      0.0    
   1200     2423.11     0.0592     0.0588     0.7185     0.0336      0.0    
   1250     2524.16     0.0593     0.0589     0.9427     0.0359      0.0    
   1300      2625.2     0.0591     0.0587     0.7803     0.0351      0.0    
   1350     2726.23     0.0591     0.0588     0.6755     0.0369      0.0    
   1400     2827.26     0.0593     0.0589     0.8547     0.0278      0.0    
   1450     2928.28     0.0592     0.0588      1.02      0.0299      0.0    
   1500     3029.32     0.0594     0.0589     0.7942     0.0258      0.0    
   1550     3130.35     0.0593     0.059      0.8797     0.0305      0.0    
   1600     3231.41     0.0594     0.0589     0.9938     0.0295      0.0    
   1650     3332.45     0.0593     0.059      0.885      0.031       0.0    
   1700     3433.53     0.0592     0.0587     0.8972     0.0284      0.0    
   1750     3534.57     0.0592     0.0587     0.6695     0.0268      0.0    
   1800     3635.65     0.0592     0.0588     0.6488     0.0201      0.0    
   1850     3736.82     0.0591     0.0589     0.6874     0.0219      0.0    
   1900     3838.18     0.0595     0.0589     0.7913     0.0266      0.0    
   1950     3939.58     0.059      0.0587     0.665      0.0201      0.0    
   2000     4041.08     0.0592     0.0587     0.6984     0.0193      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       99.76      0.1263     0.1255     2.7412     1.8003      0.0    
   100       201.73     0.0854     0.0844     2.2443     0.8008      0.0    
   150       304.01     0.0734     0.0727     1.4757     0.2449      0.0    
   200       406.39     0.067      0.0665     1.2089     0.2054      0.0    
   250       508.87     0.0636     0.0632     1.3574     0.1617      0.0    
   300       611.37     0.0617     0.0614     1.1247     0.1238      0.0    
   350       713.83     0.0607     0.0602     0.8254     0.129       0.0    
   400       816.41     0.0603     0.0598     0.8116     0.1047      0.0    
   450       919.0      0.0599     0.0596     1.1714     0.1434      0.0    
   500       1021.5     0.0597     0.0593     1.0944     0.1076      0.0    
   550      1124.04     0.0593     0.059       0.74      0.0864      0.0    
   600      1226.59     0.0593     0.059      0.8795     0.0683      0.0    
   650      1329.16     0.0595     0.059      0.8014     0.0769      0.0    
   700      1431.71     0.0594     0.059      0.8327     0.0751      0.0    
   750      1534.23     0.0592     0.0589     0.9002     0.0539      0.0    
   800      1636.75     0.0594     0.059      1.0551     0.0559      0.0    
   850       1739.3     0.0594     0.0591     1.1755     0.0695      0.0    
   900       1841.8     0.0594     0.0591     1.014      0.0541      0.0    
   950      1944.27     0.0592     0.0588     1.0126     0.0498      0.0    
   1000     2046.79     0.0591     0.0588     0.6823     0.0441      0.0    
   1050     2149.29     0.0591     0.0588     0.8313     0.0484      0.0    
   1100     2251.85     0.0589     0.0587     0.5555     0.0347      0.0    
   1150     2353.95     0.0592     0.0588     0.8497     0.038       0.0    
   1200     2456.07     0.0594     0.0591     0.8243     0.0343      0.0    
   1250     2558.34     0.0592     0.0589     1.0539     0.0431      0.0    
   1300     2660.15     0.0593     0.059      0.9456     0.0356      0.0    
   1350     2761.98     0.0591     0.0588     0.7978     0.0428      0.0    
   1400     2863.81     0.0592     0.059      0.7799     0.0299      0.0    
   1450     2965.64     0.0593     0.0589     1.4569     0.0385      0.0    
   1500     3067.48     0.0592     0.0589     1.0556     0.0319      0.0    
   1550     3169.34     0.0593     0.0589     0.9717     0.0288      0.0    
   1600     3271.18     0.0591     0.0588     0.9106     0.0248      0.0    
   1650      3373.1     0.0591     0.0589     0.7921     0.0268      0.0    
   1700     3475.12     0.059      0.0588     0.8716     0.0272      0.0    
   1750     3577.18     0.059      0.0587     0.7983     0.0249      0.0    
   1800     3679.21     0.0591     0.0588     0.7196     0.025       0.0    
   1850     3781.25     0.0592     0.0588     0.8943     0.0277      0.0    
   1900     3883.39     0.059      0.0587     0.9873     0.0214      0.0    
   1950     3985.45     0.0591     0.0588     0.8082     0.0215      0.0    
   2000     4087.72     0.0592     0.0588     0.8162     0.0225      0.0    
