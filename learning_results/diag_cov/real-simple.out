Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.618595361709595
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       31.82      0.1844     0.1861     3.1623     1.8524      0.0    
   100       64.22      0.1424     0.1434     3.1544     0.6727      0.0    
   150        96.6      0.1074     0.1083     2.9981     0.3636      0.0    
   200       128.99     0.088      0.0879     2.9779     0.2365      0.0    
   250       161.38     0.0782     0.0784     2.786      0.1679      0.0    
   300       193.98     0.0746     0.0744     2.431      0.1255      0.0    
   350       226.36     0.0725     0.0727     2.5888     0.097       0.0    
   400       258.75     0.069      0.0694     2.3209     0.0806      0.0    
   450       291.14     0.0725     0.0724     2.3531     0.0661      0.0    
   500       323.53     0.0675     0.0677     1.7631     0.0557      0.0    
   550       355.91     0.0677     0.0676     2.5877     0.0475      0.0    
   600       388.27     0.0666     0.0665     1.5622     0.0419      0.0    
   650       420.64     0.0671     0.067      1.766      0.0366      0.0    
   700       453.0      0.0652     0.0654     1.1554     0.0325      0.0    
   750       485.39     0.0648     0.0648     1.4098     0.0288      0.0    
   800       517.78     0.0638     0.0637     1.7626     0.0261      0.0    
   850       550.16     0.065      0.0647     1.2635     0.0238      0.0    
   900       582.52     0.0641     0.0638     1.5518     0.0217      0.0    
   950       614.88     0.0647     0.0643     1.1956     0.0199      0.0    
   1000      647.32     0.0641     0.0639     1.3476     0.0181      0.0    
   1050      679.69     0.063      0.0631     1.2873     0.0169      0.0    
   1100      712.06     0.0639     0.0635     1.122      0.0156      0.0    
   1150      744.45     0.0627     0.0622     1.1811     0.0145      0.0    
   1200      776.82     0.0633     0.0632     1.3272     0.0137      0.0    
   1250      809.19     0.0633     0.0631     1.0532     0.0128      0.0    
   1300      841.55     0.0633     0.0625     1.0436     0.012       0.0    
   1350      873.91     0.0627     0.0627     1.3858     0.0114      0.0    
   1400      906.25     0.0625     0.0626     0.8899     0.0107      0.0    
   1450      938.62     0.0625     0.0622     1.1834     0.0101      0.0    
   1500      970.97     0.0631     0.0626     1.239      0.0096      0.0    
   1550     1003.31     0.0628     0.0625     1.1246     0.0095      0.0    
   1600     1035.65     0.0621     0.0619     0.794      0.0086      0.0    
   1650     1068.02     0.062      0.0619     0.8158     0.0081      0.0    
   1700      1100.4     0.062      0.062       1.03      0.0078      0.0    
   1750     1132.79     0.0616     0.0617     0.799      0.0074      0.0    
   1800     1165.19     0.0617     0.0616     1.0457     0.0072      0.0    
   1850     1197.57     0.0615     0.0614     0.7581     0.0068      0.0    
   1900     1229.96     0.0613     0.0612     0.6869     0.0067      0.0    
   1950     1262.31     0.0614     0.0615     0.6559     0.0062      0.0    
   2000     1294.68     0.0616     0.0615     0.6206     0.0061      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       31.72      0.1593     0.1586     3.123      1.278       0.0    
   100       64.78      0.1363     0.1359     3.0159     0.6459      0.0    
   150       97.83      0.109      0.109      2.8134     0.4251      0.0    
   200       130.89     0.1015     0.1007     2.3787     0.3107      0.0    
   250       163.94     0.0915     0.0913     2.2792     0.2391      0.0    
   300       196.99     0.085      0.0857     2.5757     0.1882      0.0    
   350       230.03     0.0797     0.0797     2.1265     0.1514      0.0    
   400       263.11     0.0787     0.0783     2.0468     0.129       0.0    
   450       296.17     0.0732     0.0729     1.8893     0.1069      0.0    
   500       329.23     0.0744     0.0743     1.7318     0.0891      0.0    
   550       362.31     0.0736     0.0731     1.783      0.0786      0.0    
   600       395.37     0.0692     0.0689     1.705      0.0697      0.0    
   650       428.42     0.0677     0.0679     1.7837     0.0603      0.0    
   700       461.48     0.0674     0.0676     1.1804     0.055       0.0    
   750       494.54     0.0665     0.0666     1.4907     0.0485      0.0    
   800       527.62     0.0661     0.0662     1.4085     0.0447      0.0    
   850       560.68     0.0662     0.0661     1.2146     0.0411      0.0    
   900       593.74     0.0656     0.0654     1.5668     0.0369      0.0    
   950       626.79     0.0658     0.0655     1.225      0.0334      0.0    
   1000      659.86     0.065      0.0647     1.3328     0.0316      0.0    
   1050      692.92     0.0642     0.0641     1.3734     0.0285      0.0    
   1100      726.0      0.0641     0.0637     1.1183     0.0263      0.0    
   1150      759.09     0.063      0.063      1.1801     0.0245      0.0    
   1200      792.17     0.064      0.064      1.0866     0.023       0.0    
   1250      825.48     0.0634     0.0637     1.0755     0.0215      0.0    
   1300      859.35     0.0633     0.0634     1.0371     0.0208      0.0    
   1350      892.39     0.063      0.063      0.9157     0.0193      0.0    
   1400      925.44     0.0629     0.0629     0.9141     0.0178      0.0    
   1450      958.49     0.063      0.0629     0.9714     0.017       0.0    
   1500      991.52     0.0627     0.0628     1.2538     0.0161      0.0    
   1550     1024.58     0.0627     0.0628     1.0668     0.015       0.0    
   1600     1057.64     0.0624     0.0623     0.8974     0.0141      0.0    
   1650     1090.69     0.0623     0.0625     0.6159     0.0134      0.0    
   1700     1123.75     0.0621     0.0621     0.7699     0.0129      0.0    
   1750     1156.84     0.0625     0.0624     0.6905     0.012       0.0    
   1800     1189.95     0.0618     0.0619     0.7149     0.0117      0.0    
   1850     1223.02     0.0622     0.0622     0.851      0.0113      0.0    
   1900     1256.08     0.0621     0.0621     0.7248     0.0109      0.0    
   1950     1289.17     0.062      0.062      0.8246     0.0102      0.0    
   2000     1322.23     0.0619     0.0619     0.6678      0.01       0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       31.72      0.1461     0.1471     3.0872     0.9794      0.0    
   100       65.32      0.1282     0.1295     2.8606     0.5643      0.0    
   150       98.93      0.1118     0.1125     2.6698     0.4038      0.0    
   200       132.54     0.1046     0.1053     2.4105     0.3101      0.0    
   250       166.13     0.0921     0.0924     2.4369     0.2457      0.0    
   300       199.75     0.0871     0.0872     2.0587     0.199       0.0    
   350       233.35     0.0824     0.0823     1.9217     0.1628      0.0    
   400       266.97     0.0765     0.0767     1.9562     0.1373      0.0    
   450       300.61     0.0772     0.0773     1.6854     0.1179      0.0    
   500       334.23     0.0741     0.074      1.5343     0.1007      0.0    
   550       367.9      0.0721     0.0719     1.4641     0.0914      0.0    
   600       401.55     0.0718     0.0717     1.2986     0.0793      0.0    
   650       435.2      0.0736     0.0734     1.5397     0.0706      0.0    
   700       468.85     0.0691     0.0695     1.3419     0.0628      0.0    
   750       502.51     0.0678     0.0681     1.2813     0.0569      0.0    
   800       536.16     0.0671     0.0672     1.2208     0.0516      0.0    
   850       569.8      0.0675     0.0676     0.9868     0.0484      0.0    
   900       603.46     0.0657     0.0659     1.2917     0.0437      0.0    
   950       637.09     0.0658     0.0658     1.0381     0.0398      0.0    
   1000      670.74     0.0661     0.0661     1.0432     0.0368      0.0    
   1050      704.39     0.0651     0.065      0.9758     0.0341      0.0    
   1100      738.06     0.0653     0.0649     0.9146     0.0316      0.0    
   1150      771.71     0.0639     0.064      1.058      0.0294      0.0    
   1200      805.36     0.0645     0.0643     1.0187     0.0275      0.0    
   1250      838.99     0.0645     0.0642     0.9034     0.0257      0.0    
   1300      872.66     0.065      0.0648     0.8113     0.0242      0.0    
   1350      906.3      0.0637     0.0635     0.9697     0.0224      0.0    
   1400      939.93     0.0639     0.064      1.1348     0.0217      0.0    
   1450      973.6      0.0632     0.0631     0.608      0.0199      0.0    
   1500     1007.27     0.0629     0.0629     0.633      0.0192      0.0    
   1550      1041.0     0.0626     0.0628     0.6614     0.0183      0.0    
   1600     1074.65     0.0627     0.0629     0.7307     0.0176      0.0    
   1650     1108.28     0.0626     0.0627     0.6597     0.0165      0.0    
   1700     1141.92     0.0627     0.0628     0.5864     0.0156      0.0    
   1750     1175.56     0.0625     0.0625     0.7447     0.0148      0.0    
   1800      1209.2     0.0625     0.0626     0.7072     0.0141      0.0    
   1850     1242.86     0.0625     0.0625     0.567      0.0134      0.0    
   1900     1276.52     0.0621     0.0621     0.6802     0.0129      0.0    
   1950     1311.15     0.0618     0.0619     0.7423     0.0123      0.0    
   2000     1344.85     0.0619     0.0619     0.7314     0.0117      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       31.77      0.1645     0.1637     2.9077     0.8317      0.0    
   100       65.97      0.129      0.1292     2.5391     0.5009      0.0    
   150       100.2      0.1152     0.1152     2.4247     0.3736      0.0    
   200       134.4      0.1101     0.1106     2.3217     0.2945      0.0    
   250       168.61     0.1011     0.1017     2.4395     0.2357      0.0    
   300       202.81     0.0898     0.0905     2.3489     0.1964      0.0    
   350       237.05     0.081      0.0814      1.69      0.1664      0.0    
   400       271.29     0.0807     0.081      1.8843     0.1456      0.0    
   450       305.53     0.0772     0.0773     1.7573     0.1249      0.0    
   500       339.81     0.0766     0.0768     1.5524     0.1105      0.0    
   550       374.08     0.0755     0.0751     1.3936     0.0965      0.0    
   600       408.35     0.0719     0.0718     1.2604     0.0859      0.0    
   650       442.61     0.0703     0.0703     1.382      0.0774      0.0    
   700       476.86     0.0706     0.0703     1.1501     0.0696      0.0    
   750       511.08     0.0689     0.0684     1.212      0.0641      0.0    
   800       545.32     0.0688     0.0684     1.2167     0.0571      0.0    
   850       579.56     0.0674     0.0674     1.2429     0.0531      0.0    
   900       613.8      0.0669     0.067      1.2487     0.0496      0.0    
   950       648.04     0.0662     0.0665     1.0661     0.0449      0.0    
   1000      682.27     0.0654     0.0656     1.0853     0.0409      0.0    
   1050      716.5      0.0659     0.0661     0.9942     0.0385      0.0    
   1100      750.72     0.0654     0.0655     0.9792     0.036       0.0    
   1150      784.96     0.0651     0.0653     1.0099     0.0335      0.0    
   1200      819.19     0.0649     0.0651     0.7762     0.0312      0.0    
   1250      853.45     0.064      0.0643     0.867      0.0293      0.0    
   1300      887.69     0.0639     0.0641     0.8549     0.0276      0.0    
   1350      921.93     0.0635     0.0635     0.8653     0.0266      0.0    
   1400      956.19     0.0635     0.0639     0.8532     0.0246      0.0    
   1450      990.44     0.0634     0.0636     1.1632     0.0234      0.0    
   1500     1024.66     0.0631     0.063      0.8095     0.0225      0.0    
   1550      1058.9     0.0637     0.0636     0.7356     0.021       0.0    
   1600     1093.14     0.0631     0.0632     0.7228     0.0203      0.0    
   1650     1127.37     0.0625     0.0627     0.7556     0.0191      0.0    
   1700     1161.59     0.0627     0.063      0.5844     0.0183      0.0    
   1750     1195.82     0.0621     0.0624     0.7374     0.0173      0.0    
   1800     1230.05     0.062      0.0621     0.6474     0.0163      0.0    
   1850      1264.3     0.0623     0.0624     0.5655     0.0155      0.0    
   1900     1298.55     0.0623     0.0623     0.6114     0.015       0.0    
   1950     1332.78     0.0624     0.0622     0.6027     0.0143      0.0    
   2000     1367.04     0.0623     0.0623     0.7765     0.014       0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  |    dw   
    50       31.84      0.152      0.1519     2.9458     0.7345      0.0    
   100       66.73      0.1259     0.1272     2.5966     0.455       0.0    
   150       101.64     0.1099     0.1115     2.2157     0.3509      0.0    
   200       136.55     0.1004     0.1012     2.3077     0.2842      0.0    
   250       171.47     0.0969     0.0978     2.1362     0.2434      0.0    
   300       206.37     0.089      0.0895     1.8181     0.2053      0.0    
   350       241.27     0.0886     0.089      2.3054     0.1735      0.0    
   400       276.2      0.0837     0.084      1.6646     0.1471      0.0    
   450       311.12     0.0796      0.08      1.3984     0.1297      0.0    
   500       346.05     0.0778     0.0781     1.2523     0.1154      0.0    
   550       381.13     0.0745     0.075      1.2296     0.102       0.0    
   600       416.8      0.0743     0.0747     1.2712     0.0911      0.0    
   650       451.72     0.0738     0.0741     1.306      0.0809      0.0    
   700       486.66     0.072      0.0718     1.0975     0.0732      0.0    
   750       521.57     0.0709     0.0706     1.2297     0.0675      0.0    
   800       556.49     0.0694     0.0694     1.0987     0.0611      0.0    
   850       591.41     0.0669     0.0666     1.3499     0.0564      0.0    
   900       626.32     0.0666     0.0663     0.8689     0.0526      0.0    
   950       661.23     0.0677     0.0678     1.2461     0.0482      0.0    
   1000      696.18     0.0655     0.0655     1.0301     0.0445      0.0    
   1050      731.09     0.0655     0.0655     0.9205     0.0409      0.0    
   1100      766.02     0.0651     0.0651     0.9006     0.0383      0.0    
   1150      800.96     0.066      0.0661     0.8876     0.0354      0.0    
   1200      835.88     0.0652     0.0652     0.9094     0.0336      0.0    
   1250      870.81     0.0654     0.0654     0.8227     0.0314      0.0    
   1300      905.76     0.0645     0.0647     0.9053     0.0294      0.0    
   1350      940.68     0.0655     0.0655     0.7282     0.0283      0.0    
   1400      975.6      0.0647     0.0647     0.6853     0.0264      0.0    
   1450     1010.51     0.0634     0.0637     0.8283     0.0252      0.0    
   1500     1045.42     0.0633     0.0635     0.616      0.0239      0.0    
   1550     1080.33     0.064      0.064      0.5896     0.0227      0.0    
   1600     1115.25     0.0631     0.0631     0.6466     0.0214      0.0    
   1650     1150.18     0.0627     0.0628     0.5497     0.0205      0.0    
   1700     1185.11     0.0622     0.0625     0.7487     0.0196      0.0    
   1750     1220.05     0.0625     0.0628     0.6186     0.0183      0.0    
   1800     1255.01     0.062      0.0622     0.5995     0.0176      0.0    
   1850     1289.97     0.0624     0.0627     0.6556     0.017       0.0    
   1900     1324.92     0.0621     0.0623     0.5795     0.0162      0.0    
   1950     1359.86     0.0617     0.062      0.5498     0.0157      0.0    
   2000     1394.86     0.0618     0.062      0.5711     0.0151      0.0    
