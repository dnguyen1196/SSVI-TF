Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  8.10182237625122
max_count =  17  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       215.43     2.4635     2.4349     0.3162     0.3579    52942.47   42418.4      0.0    
   100       550.53     2.4686     2.4361     0.2995     0.3321    52117.11   41741.51     0.0    
   150       885.61     2.4709     2.4405     0.2534     0.3125    51560.3    41285.98     0.0    
   200       1220.7     2.4824     2.4489     0.2286     0.3117    51146.16   40946.14     0.0    
   250      1555.93     2.4913     2.4578     0.2074     0.3024    50818.85   40680.46     0.0    
   300       1891.0     2.4991     2.4659     0.1921     0.3055    50530.97   40445.8      0.0    
   350      2226.12     2.4981     2.4795     0.1766     0.301     50280.57   40243.25     0.0    
   400      2561.26     2.5105     2.4805     0.165      0.576     50053.66   40058.24     0.0    
   450      2896.52     2.5164     2.4867     0.1614     0.3189    49858.77   39899.65     0.0    
   500      3231.76     2.5241     2.4932      0.15      0.3321    49688.36   39761.41     0.0    
   550      3567.06     2.5345     2.5037     0.1554     0.3383    49534.02   39635.24     0.0    
   600      3902.37     2.5383     2.5137     0.1383     0.3484    49391.47   39519.44     0.0    
   650      4237.61     2.5484     2.5198     0.1275     0.5991    49260.83   39412.62     0.0    
   700      4572.97     2.5561     2.5204     0.1292     0.3844    49144.6    39317.78     0.0    
   750       4908.4     2.5674     2.5358     0.1397     0.3837    49033.8    39227.83     0.0    
   800      5243.89     2.574      2.5445     0.1324     0.3989    48923.04   39137.22     0.0    
   850      5579.34     2.5833     2.5524     0.1193     0.4185    48832.13   39063.16     0.0    
   900      5914.86     2.5945     2.5643     0.1125     0.4445    48730.4    38980.18     0.0    
   950      6250.41     2.601      2.5752     0.1134     0.4594    48648.58   38913.2      0.0    
   1000     6585.88     2.6139     2.5803     0.1115     0.4887    48573.08   38851.3      0.0    
   1050     6921.24     2.6247     2.5908     0.1036     0.5032    48496.72   38788.3      0.0    
   1100     7256.65     2.6376     2.6063     0.1134     0.5519    48429.06   38732.89     0.0    
   1150     7592.06     2.645      2.6153     0.1025     0.5823    48361.19   38677.37     0.0    
   1200     7927.41     2.6543     2.6303     0.1053     0.6063    48297.66   38624.77     0.0    
   1250     8262.75     2.6709     2.6422     0.0974     0.6414    48240.15   38576.73     0.0    
   1300     8598.31     2.6799     2.655      0.0917     0.693     48183.67   38530.2      0.0    
   1350     8933.83     2.6927     2.6716     0.1003     0.7104    48130.22   38486.3      0.0    
   1400     9269.37     2.7082     2.679      0.1005     0.7674    48079.51   38444.31     0.0    
   1450      9604.7     2.7268     2.6973     0.1009     0.8201    48034.09   38406.32     0.0    
   1500     9940.05     2.7377     2.7114     0.0949     0.877     47987.43   38367.76     0.0    
   1550     10275.33    2.7582     2.7292     0.0956     0.9824    47942.77   38330.55     0.0    
   1600     10610.86    2.7773     2.7457     0.0982     1.0004    47903.48   38297.77     0.0    
   1650     10946.13    2.7911     2.7593     0.0932     1.0833    47864.14   38264.86     0.0    
   1700     11281.28    2.809      2.7799     0.0886     1.1786    47829.64   38236.05     0.0    
   1750     11616.58    2.8342     2.8008     0.0944     1.2607    47791.51   38204.07     0.0    
   1800     11951.72    2.8563     2.8308     0.0861     1.4084    47755.7    38173.81     0.0    
   1850     12286.91    2.8726     2.853      0.0861     1.4976    47718.8    38142.93     0.0    
   1900     12621.96    2.9085     2.8715     0.0859     1.6449    47687.64   38116.32     0.0    
   1950     12957.15    2.9235     2.8964     0.0829     1.7708    47657.61   38090.58     0.0    
   2000     13292.36    2.9484     2.9284     0.0834     2.009     47626.42   38063.6      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       215.45     2.4606     2.4431     0.1922     0.2443    53460.74   85607.2      0.0    
   100       605.4      2.4623     2.4445     0.185      0.2354    52823.07   84567.03     0.0    
   150       994.87     2.473      2.4519     0.1698     0.2167    52323.11   83752.41     0.0    
   200      1384.36     2.471      2.4514     0.1621     0.2712    51908.04   83074.75     0.0    
   250      1773.74     2.4817     2.4604     0.1575      0.21     51581.56   82545.9      0.0    
   300      2163.31     2.4891     2.4649     0.1521     0.2046    51286.66   82069.36     0.0    
   350      2553.11     2.4861     2.4675     0.1381     0.2086    51013.7    81627.62     0.0    
   400      2942.76     2.4901     2.4736     0.1423     0.2087    50794.38   81271.4      0.0    
   450      3332.48     2.4957     2.4765     0.143      0.2114    50590.41   80940.4      0.0    
   500      3722.25     2.5015     2.4842     0.1216     0.2134    50408.81   80645.54     0.0    
   550      4111.97     2.5093     2.4894     0.1274     0.2136    50243.14   80377.23     0.0    
   600      4502.01     2.5137     2.4929     0.1259     0.2244    50104.02   80152.87     0.0    
   650       4892.5     2.5195     2.4954     0.1223     0.2259    49970.6    79938.98     0.0    
   700      5283.43     2.5274     2.5032     0.1077     0.2272    49845.75   79737.48     0.0    
   750      5674.57     2.528      2.5087     0.112      0.2358    49725.75   79544.04     0.0    
   800      6065.53     2.5338     2.5104     0.1193     0.242     49621.28   79375.84     0.0    
   850      6456.67     2.5339     2.5194     0.1061     0.2465    49514.85   79204.12     0.0    
   900      6847.76     2.5479     2.5257     0.102      0.2563    49418.12   79047.86     0.0    
   950       7238.7     2.5524     2.5327     0.0972     0.2676    49332.67   78909.97     0.0    
   1000     7629.15     2.5575     2.5383     0.1142     0.2652    49242.49   78764.1      0.0    
   1050      8019.6     2.5634     2.5474     0.0962     0.2754    49162.84   78635.7      0.0    
   1100     8409.86     2.5709     2.5502     0.0929     0.2893    49080.8    78503.06     0.0    
   1150     8800.31     2.578      2.5567     0.1011     0.2926    49003.43   78378.17     0.0    
   1200     9190.71     2.5812     2.5656     0.0909     0.3085    48930.23   78260.04     0.0    
   1250     9581.23     2.5938     2.574      0.0902     0.316     48863.5    78152.36     0.0    
   1300     9971.65      2.6       2.5799     0.0865     0.3262    48799.58   78049.13     0.0    
   1350     10362.25    2.6042     2.584      0.0857     0.3456    48737.7    77949.45     0.0    
   1400     10753.5     2.6139     2.595      0.0865     0.3525    48677.51   77852.07     0.0    
   1450     11143.81    2.6202     2.6015     0.0773     0.3625    48621.53   77761.82     0.0    
   1500     11534.2     2.6316     2.6058     0.0827     0.3905    48567.72   77674.98     0.0    
   1550     11924.65    2.6383     2.6175     0.0849     0.4074    48513.95   77588.21     0.0    
   1600     12315.22    2.6458     2.6264     0.0795     0.4152    48463.45   77506.61     0.0    
   1650     12705.63    2.6531     2.636      0.0799     0.4325    48416.7    77430.67     0.0    
   1700     13095.97    2.6613     2.6434     0.0736     0.4528    48370.86   77356.18     0.0    
   1750     13486.39    2.6705     2.6502     0.0758     0.475     48328.43   77287.3      0.0    
   1800     13876.82    2.6818     2.6631     0.082      0.5074    48284.02   77215.46     0.0    
   1850     14266.67    2.6945     2.6702     0.0781     0.528     48241.16   77146.18     0.0    
   1900     14656.71    2.7038     2.6861     0.0777     0.5446    48203.04   77084.25     0.0    
   1950     15046.25    2.7117     2.694      0.0712     0.5695    48165.96   77023.93     0.0    
   2000     15435.91    2.7207     2.7058     0.074      0.614     48127.26   76961.52     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       214.45     2.4576     2.4403     0.1527     0.208     53688.66  129077.06     0.0    
   100       658.57     2.4632     2.4489     0.1434     0.1845    53160.11  127783.95     0.0    
   150      1102.72     2.4669     2.4484     0.1338     0.1583    52737.08  126749.24     0.0    
   200      1546.77     2.4694     2.4495     0.1286     0.2022    52366.68  125844.24     0.0    
   250      1991.49     2.4728     2.4531     0.138      0.2044    52062.17  125098.98     0.0    
   300      2486.34     2.4791     2.4556     0.1301     0.1503    51778.69  124406.43     0.0    
   350      2987.28     2.4759     2.4574     0.1227      0.15     51542.79  123827.75     0.0    
   400      3497.26     2.4878     2.4625     0.1117     0.1503    51331.76  123312.65     0.0    
   450      3998.38     2.4847     2.4658     0.1102     0.1556    51139.0   122840.01     0.0    
   500      4499.08     2.4872     2.4704     0.1152     0.1527    50955.09  122390.86     0.0    
   550      5008.33     2.4913     2.4741     0.1063     0.1548    50792.94  121993.85     0.0    
   600      5513.52     2.4923     2.4744     0.1144     0.1599    50637.2   121614.34     0.0    
   650      6016.07     2.4991     2.4767     0.1033     0.1554    50499.98  121280.56     0.0    
   700      6518.26     2.4996     2.485      0.1003     0.1603    50365.34  120952.92     0.0    
   750      7029.09     2.5078     2.487      0.0953     0.1634    50243.0   120655.24     0.0    
   800      7531.43     2.5111     2.4946     0.101      0.1634    50124.54  120365.51     0.0    
   850       8033.7     2.5152     2.4922     0.0929     0.1692    50008.46   120083.6     0.0    
   900       8543.9     2.5179     2.4983     0.091      0.1702    49904.06  119828.99     0.0    
   950      9046.35     2.5177     2.5028     0.1126     0.1763    49809.88  119601.31     0.0    
   1000     9549.14     2.5233     2.5048     0.091      0.1788    49710.87  119358.82     0.0    
   1050     10051.53    2.5278     2.5135     0.0913     0.1849    49627.01  119155.67     0.0    
   1100     10562.91    2.5334     2.5139     0.0812     0.1894    49540.06  118942.73     0.0    
   1150     11065.17    2.5364     2.5193     0.0862     0.1891    49463.18  118755.49     0.0    
   1200     11566.97    2.5438     2.5256     0.0848     0.1986    49388.15  118573.17     0.0    
   1250     12068.45    2.5477     2.5303     0.0864     0.2007    49314.29  118393.02     0.0    
   1300     12578.8     2.5533     2.538      0.0771     0.2032    49247.4    118229.4     0.0    
   1350     13080.89    2.5595     2.5417     0.0832     0.2064    49180.8   118067.26     0.0    
   1400     13582.87    2.5645     2.5429     0.0784     0.215     49109.74  117893.67     0.0    
   1450     14092.55    2.5695     2.5464     0.0782     0.2183    49047.37  117740.21     0.0    
   1500     14582.18    2.5707     2.554      0.0766     0.2284    48989.04  117597.95     0.0    
   1550     15035.43    2.578      2.5585     0.0719     0.2316    48927.91  117448.43     0.0    
   1600     15488.83    2.5868     2.5636     0.0824     0.2399    48877.08  117323.96     0.0    
   1650     15942.2     2.5909     2.5739     0.0747     0.2495    48824.99   117196.8     0.0    
   1700     16395.44    2.5946     2.5749     0.0701     0.2551    48773.4   117070.41     0.0    
   1750     16848.69    2.6041     2.5802     0.0797     0.2627    48725.15   116952.9     0.0    
   1800     17302.09    2.6046     2.5867     0.0794     0.2719    48678.02  116837.66     0.0    
   1850     17755.47    2.6101     2.5945     0.0758     0.2782    48629.25  116718.77     0.0    
   1900     18208.61    2.6104      2.6       0.0724     0.2848    48583.69  116606.87     0.0    
   1950     18661.84    2.6254     2.6061     0.0712     0.2947    48542.67  116506.06     0.0    
   2000     19114.83    2.6278     2.6105     0.0671     0.3076    48503.26  116409.73     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       219.4      2.4617     2.448      0.1302     0.1787    53835.86  172677.85     0.0    
   100       727.73     2.4624     2.4499     0.1194     0.1655    53351.63  171106.18     0.0    
   150      1235.89     2.4631     2.4511     0.1187     0.1816    52968.5   169859.82     0.0    
   200      1746.74     2.4638     2.451      0.1117     0.1598    52630.71  168761.62     0.0    
   250      2254.86     2.4673     2.4508     0.1084     0.1219    52329.27  167787.43     0.0    
   300      2763.22     2.4686     2.4577     0.104      0.1237    52068.9   166940.41     0.0    
   350      3271.52     2.4693     2.4597     0.1064     0.119     51844.92  166215.88     0.0    
   400      3782.43     2.4715     2.4601     0.1102     0.1176    51637.58  165543.94     0.0    
   450      4290.77     2.4754     2.4659     0.0928     0.1251    51443.3   164914.01     0.0    
   500      4799.18     2.4797     2.4645     0.0917     0.1246    51269.36  164353.64     0.0    
   550      5307.76     2.4826     2.4708     0.094      0.1207    51107.21   163831.6     0.0    
   600      5819.12     2.4872     2.4715     0.0935     0.1231    50962.87  163368.28     0.0    
   650      6327.95     2.4851     2.4733     0.0885     0.1575    50823.69  162918.03     0.0    
   700       6837.0     2.4858     2.4777     0.0849     0.1234    50687.94  162481.34     0.0    
   750      7346.15     2.4897     2.4806     0.0855     0.1271    50567.64  162096.52     0.0    
   800      7855.29     2.4964     2.481      0.0874     0.1273    50447.63  161708.13     0.0    
   850      8364.19     2.4997     2.4853     0.083      0.1308    50335.93  161348.57     0.0    
   900      8873.24     2.5031     2.4896     0.0894     0.1322    50231.29  161011.11     0.0    
   950      9382.57     2.5022     2.4936     0.0823     0.1314    50136.46  160705.37     0.0    
   1000     9891.73     2.5068     2.494      0.0778     0.1406    50047.88  160418.22     0.0    
   1050     10401.06    2.5126     2.4979     0.0784     0.136     49957.91  160127.31     0.0    
   1100     10914.28    2.5138     2.5011     0.085      0.139     49880.04   159876.1     0.0    
   1150     11423.54    2.5222     2.5073     0.0722     0.1399    49796.09  159602.63     0.0    
   1200     11932.96    2.5204     2.5102     0.0759     0.1465    49722.98  159365.57     0.0    
   1250     12442.68    2.5239     2.5118     0.0814     0.1447    49652.59  159138.59     0.0    
   1300     12954.96    2.529      2.515      0.0744     0.1479    49584.77  158919.12     0.0    
   1350     13464.48    2.5351     2.519      0.0784     0.1528    49515.66  158694.95     0.0    
   1400     13973.94    2.5335     2.5216     0.0836     0.1525    49452.16  158488.75     0.0    
   1450     14484.78    2.5379     2.5279     0.0689     0.1563    49390.69  158289.33     0.0    
   1500     14995.03    2.5419     2.5324     0.067      0.1617    49329.81  158092.94     0.0    
   1550     15504.66    2.5467     2.5357     0.0821     0.1674    49273.01  157909.24     0.0    
   1600     16014.24    2.5503     2.5377     0.0689     0.1696    49217.03  157728.37     0.0    
   1650     16526.66    2.5522     2.5406     0.0675     0.172     49163.06  157554.25     0.0    
   1700     17036.99    2.5577     2.5466     0.0643     0.1776    49110.02  157381.46     0.0    
   1750     17547.89    2.563      2.5488     0.0641     0.1822    49056.39  157207.35     0.0    
   1800     18057.93    2.5638     2.5552     0.0654     0.1819    49008.17  157051.45     0.0    
   1850     18568.27    2.5726     2.5576     0.0652     0.1885    48956.78  156885.17     0.0    
   1900     19078.52    2.5744     2.5611     0.0647     0.1911    48912.37  156741.54     0.0    
   1950     19589.6     2.5761     2.5671     0.0719     0.1962    48866.11  156591.63     0.0    
   2000     20100.82    2.5875     2.5698     0.0642     0.1988    48819.77   156440.4     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   | test_err | train_err|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       220.75     2.4558     2.4489     0.1071     0.183     53924.36  216173.54     0.0    
   100       787.12     2.4613     2.4448     0.1005     0.173     53516.56  214526.07     0.0    
   150      1354.34     2.459      2.4504     0.1189     0.128     53153.49  213052.79     0.0    
   200      1921.04     2.4589      2.45      0.0968     0.1243    52852.28   211826.0     0.0    
   250      2488.08     2.4663     2.4542     0.0959     0.1152    52586.34  210746.78     0.0    
   300      3055.83     2.4697     2.4539     0.1007     0.1006    52352.19  209798.19     0.0    
   350      3622.39     2.4654     2.4558     0.0926     0.1114    52123.23  208872.55     0.0    
   400      4188.86     2.4655     2.4569     0.0979     0.0995    51930.07  208088.93     0.0    
   450      4755.52     2.4699     2.4614     0.0947     0.1022    51750.44  207362.35     0.0    
   500      5322.23     2.4769     2.462      0.0908     0.1002    51584.48  206691.15     0.0    
   550      5889.08     2.4802     2.4657     0.0902     0.1008    51429.6   206065.32     0.0    
   600       6456.1     2.4801     2.4662     0.0807     0.1489    51285.13  205483.06     0.0    
   650      7022.93     2.4774     2.4713     0.0869     0.1037    51148.33  204929.57     0.0    
   700      7589.18     2.4867     2.4716     0.0758     0.1028    51025.52  204433.96     0.0    
   750      8155.49     2.484      2.473      0.0776     0.1043    50905.84  203951.25     0.0    
   800      8721.97     2.4921     2.4771     0.0795     0.1045    50797.9   203515.86     0.0    
   850      9288.06     2.4968     2.4786     0.0792     0.1056    50690.44  203082.78     0.0    
   900      9854.45     2.4965     2.4817     0.0833     0.1077    50586.63  202663.07     0.0    
   950      10420.69    2.4953     2.4843     0.0745     0.1084    50491.13  202277.47     0.0    
   1000     10987.0     2.499      2.4858     0.0693     0.1085    50397.17  201897.98     0.0    
   1050     11553.56    2.5013     2.4889     0.0742     0.1107    50309.18  201541.49     0.0    
   1100     12121.03    2.5011     2.4923     0.0676     0.137     50211.94  201149.88     0.0    
   1150     12688.4     2.509      2.494      0.0738     0.1171    50130.82  200822.17     0.0    
   1200     13255.25    2.5107     2.4971     0.0724     0.1451    50048.15  200486.68     0.0    
   1250     13821.52    2.5141     2.4986     0.0713     0.1169    49975.6   200194.66     0.0    
   1300     14388.48    2.5154     2.5024     0.0652     0.119     49904.78   199907.3     0.0    
   1350     14955.5     2.5168     2.5043     0.0672     0.1201    49838.42  199639.16     0.0    
   1400     15522.59    2.5236     2.5086     0.0682     0.1208    49771.29  199367.17     0.0    
   1450     16091.72    2.5274     2.5105     0.0738     0.124     49708.08  199112.17     0.0    
   1500     16661.26    2.5264     2.5117     0.0677     0.1278    49644.61  198855.47     0.0    
   1550     17227.61    2.5293     2.5176     0.0662     0.1283    49584.26  198612.18     0.0    
   1600     17795.99    2.5343     2.5183     0.0657     0.1305    49527.21  198382.61     0.0    
   1650     18363.3     2.5346     2.5229     0.0675     0.1352    49471.21  198157.76     0.0    
   1700     18931.9     2.5376     2.5263      0.06      0.1338    49417.3   197939.84     0.0    
   1750     19499.23    2.5388     2.5288     0.0581     0.1412    49362.99  197719.69     0.0    
   1800     20068.41    2.5423     2.5322     0.0602     0.1414    49312.84  197517.39     0.0    
   1850     20636.42    2.5458     2.534      0.0713     0.1467    49264.06  197320.07     0.0    
   1900     21204.23    2.5513     2.5397     0.0588     0.1429    49216.18  197126.28     0.0    
   1950     21771.77    2.5556     2.5415     0.0587     0.1508    49169.48  196936.36     0.0    
   2000     22339.53     2.56      2.5452     0.0616     0.1539    49124.27  196752.82     0.0    
