Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  10.357068538665771
max_count =  19  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       105.56     3.9019     3.9311     0.2853     0.2295    57838.85   46685.87  
   200       335.71     3.9464     3.9761     0.2203     0.1547    56303.17   45448.6   
   300       565.58     3.9769     4.0102     0.1897     0.117     55266.09   44610.36  
   400       795.14     4.0029     4.0376     0.1748     0.0939    54493.01   43983.89  
   500      1024.93     4.028      4.0614     0.1684     0.0788    53897.39   43500.18  
   600      1254.71     4.0458     4.0833     0.1477     0.0689    53421.44   43113.62  
   700      1484.26     4.061      4.0948     0.1257     0.0583    53029.68   42795.53  
   800      1714.04     4.0735     4.1111     0.1236     0.0525    52703.35   42530.67  
   900      1943.74     4.088      4.1255     0.1187     0.0465    52430.82   42309.77  
   1000     2173.33     4.0962     4.1308     0.1166     0.0411    52196.06   42119.79  
   1100     2403.28     4.1069     4.1419     0.1015     0.0372    52000.42   41961.73  
   1200     2633.12     4.1109     4.1481     0.0983     0.0349    51834.79   41828.13  
   1300     2862.86     4.1188     4.1572     0.1035     0.0319    51694.6    41715.09  
   1400     3093.25     4.1258     4.1639      0.1       0.0286    51575.69   41619.48  
   1500     3323.17     4.1305     4.1659     0.0954     0.0286    51473.72   41537.93  
   1600     3552.92     4.1297     4.168      0.0987     0.0242    51379.09   41462.13  
   1700     3783.04     4.1182     4.1582     0.1143     0.024     51262.19   41368.76  
   1800     4013.26     4.094      4.1304     0.1215     0.0225    51076.07   41220.07  
   1900     4242.99     4.0424     4.0756     0.1284     0.0212    50777.24   40981.15  
   2000     4471.72     3.9577     3.9949     0.1362     0.0199    50371.45   40656.34  
   2100     4699.89     3.8283     3.8659     0.1516     0.0198    49882.27   40265.06  
   2200     4927.97     3.6312     3.6673     0.1641     0.0191    49381.86   39863.88  
   2300     5156.13     3.353      3.3855     0.1754     0.0197    48978.73   39538.26  
   2400     5384.44     3.0038     3.0391     0.1769     0.0171    48690.35   39302.79  
   2500     5612.08     2.6368     2.655      0.1592     0.0162    48520.25   39173.19  
   2600     5839.87     2.309      2.3243     0.1585     0.0154    48605.62   39259.21  
   2700     6067.54     2.1433     2.1556     0.1478     0.0144    49059.79   39634.22  
   2800     6295.17     2.2271     2.2278     0.1392     0.0135    49826.16   40251.13  
   2900     6522.89     2.5436     2.5433     0.1386     0.0128    50704.64   40959.8   
   3000     6750.88     3.0501     3.0511     0.1307     0.0121    51734.12   41799.03  
   3100     6978.51      3.65      3.6653     0.1549     0.0116    52818.57   42686.2   
   3200     7206.54     4.3202     4.3418     0.148      0.0119    53978.09   43643.98  
   3300     7434.68     4.9997     5.0337     0.2258     0.0114    55098.41   44570.02  
   3400     7662.75     5.6695     5.6993     0.152      0.0104    55839.81   45168.09  
   3500     7891.44     6.336      6.3975     0.1847     0.011     56548.66   45718.48  
   3600     8119.85     7.0402     7.0793     0.1352     0.0099    56672.94   45813.16  
   3700     8348.16     7.6717     7.7169     0.1352     0.0096    56478.72   45644.68  
   3800     8576.84     8.2872     8.3466     0.1255     0.0093    56092.39   45331.53  
   3900     8805.35     8.8627     8.9251     0.1458     0.0091    55915.18   45179.65  
   4000     9033.68     9.4696     9.5138     0.1348     0.009     55687.4    44977.99  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       105.07     3.8483     3.8664     0.2775     0.0823    59802.02   96696.86  
   200       389.19     3.8645     3.8845     0.2263     0.0856    59111.25   95576.58  
   300       672.83     3.8849     3.8974     0.1972     0.0742    58479.13   94551.69  
   400       956.13     3.8948     3.9183     0.1844     0.0677    57904.26   93619.09  
   500      1239.24     3.9126     3.928      0.1618     0.0616    57381.05   92769.34  
   600      1522.26     3.9255     3.948      0.1428     0.0593    56903.19   91991.86  
   700      1805.25     3.9405     3.9606     0.1397     0.0571    56465.81   91279.59  
   800      2088.31     3.9551     3.9743     0.1369     0.051     56061.02   90619.77  
   900       2371.3     3.9657     3.9859     0.126      0.0501    55687.21   90009.79  
   1000     2654.41     3.9762     3.9976     0.1125     0.0445    55341.48   89444.98  
   1100     2937.81     3.9909     4.0091     0.1145     0.0439    55024.27   88926.56  
   1200     3220.84     4.0008     4.0196     0.1096     0.0402    54729.77   88444.86  
   1300     3503.85     4.0071     4.0282     0.1159     0.0384    54457.79   88000.14  
   1400     3787.11     4.0148     4.0366     0.1062     0.0363    54206.76   87589.2   
   1500     4070.18     4.028      4.0486     0.0996     0.0341    53970.37   87202.01  
   1600     4353.35     4.0352     4.0544     0.0944     0.0347    53749.57   86840.7   
   1700     4630.79     4.0441     4.0615     0.0881     0.0325    53543.58   86503.84  
   1800     4900.78     4.0492     4.0693     0.0942     0.0304    53350.75   86188.69  
   1900      5168.1     4.0541     4.0753     0.0986     0.029     53165.34   85885.88  
   2000     5434.95     4.0531     4.0759     0.0965     0.0276    52980.79   85585.28  
   2100     5700.19     4.0421     4.0637     0.0955     0.0266    52781.23   85261.98  
   2200     5967.58     4.0246     4.0432     0.0954     0.0255    52551.6    84892.07  
   2300     6233.83     3.9904     4.0101     0.0908     0.0247    52286.4    84466.42  
   2400     6500.22     3.9447     3.9644     0.0869     0.0251    51981.25   83978.74  
   2500     6765.19     3.8845     3.9079     0.0894     0.023     51638.04   83431.15  
   2600     7030.68     3.8135     3.8327     0.085      0.0227    51249.12   82811.14  
   2700     7285.01     3.7334     3.7544     0.0835     0.0216    50825.7    82135.47  
   2800     7539.61     3.6377     3.6634     0.0725     0.0197    50361.39   81393.96  
   2900     7795.68     3.5377     3.5592     0.0723     0.0177    49864.54   80598.24  
   3000     8050.29     3.4336     3.449      0.0721     0.0176    49333.64   79746.06  
   3100      8304.8     3.3177     3.3369     0.0659     0.0164    48779.14   78853.85  
   3200     8558.22     3.1994     3.2149     0.0727     0.0158    48211.77   77939.67  
   3300      8811.5     3.0706     3.0895     0.0541     0.0149    47631.39   77005.99  
   3400      9064.8     2.9427     2.9639     0.0574     0.0135    47068.58   76103.43  
   3500     9314.92     2.8275     2.8374     0.0604     0.0138    46538.5    75255.28  
   3600     9564.57     2.7082     2.7167     0.0807     0.0125    46046.08   74471.35  
   3700     9815.88     2.5929     2.6093     0.0469     0.0114    45596.41   73759.01  
   3800     10072.97    2.492      2.5079     0.0448     0.0111    45178.3    73098.31  
   3900     10324.88    2.3936     2.4174     0.0522      0.01     44804.19   72507.06  
   4000     10576.64    2.3225     2.3329     0.0523     0.0094    44461.42   71964.57  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       93.25      3.846      3.8571     0.291      0.0624    60002.61  144949.28  
   200       396.45     3.8573     3.8669     0.2352     0.0582    59478.47   143679.4  
   300       698.73     3.8693     3.8849     0.1906     0.0544    58982.02  142477.79  
   400      1000.84      3.88      3.897      0.1815     0.0605    58510.02  141336.03  
   500       1296.4     3.8897     3.9066     0.1684     0.052     58069.91  140271.51  
   600      1597.75     3.9041     3.9148     0.1543     0.0482    57660.94  139281.73  
   700      1900.01     3.9176     3.9303     0.1477     0.049     57271.74  138341.18  
   800      2202.43     3.9242     3.9409     0.1386     0.0517    56908.44  137461.58  
   900      2504.06     3.9362     3.9507     0.1252     0.0428    56568.45  136637.39  
   1000      2804.0     3.9509     3.9611     0.1236     0.043     56250.84  135867.46  
   1100     3097.94     3.9558     3.9711     0.1151     0.0383    55951.44  135141.05  
   1200     3393.72     3.9628     3.9787     0.1085     0.0382    55668.27  134453.78  
   1300      3691.4     3.9752     3.9869     0.1058     0.0373    55405.54  133815.98  
   1400     3987.05     3.9832     3.9968     0.1026     0.0362    55155.24  133207.25  
   1500     4280.84     3.9921     4.006      0.0998     0.0323    54917.54  132629.59  
   1600     4574.58     4.0008     4.0167     0.0967     0.0317    54694.65  132087.54  
   1700     4870.67     4.0062     4.0221     0.0987     0.0333    54482.15  131570.69  
   1800     5166.47     4.013      4.0267     0.0945     0.029     54281.35  131082.21  
   1900     5460.29     4.0188     4.0345     0.0877     0.0282    54092.21  130621.86  
   2000     5753.95     4.028      4.0413     0.0877     0.0287    53911.42  130181.78  
   2100     6054.93     4.0341     4.048      0.085      0.0267    53740.05  129764.96  
   2200     6356.72     4.0398     4.055      0.0774     0.0262    53576.09  129365.92  
   2300     6653.39     4.0456     4.0609     0.0765     0.0244    53423.51  128994.72  
   2400     6948.85     4.0506     4.0672     0.0772     0.0234    53277.16  128638.69  
   2500     7249.76     4.0566     4.0736     0.0764     0.0238    53138.96  128302.58  
   2600     7552.51     4.059      4.0788     0.0863     0.0224    53007.62  127983.18  
   2700     7846.69     4.067      4.0833     0.0709     0.0224    52881.31  127675.96  
   2800     8142.91     4.0732     4.0893     0.0682     0.0219    52762.28  127386.51  
   2900     8444.35     4.0777     4.0932     0.0736     0.0216    52649.19  127111.55  
   3000     8746.49     4.0806     4.097      0.0766     0.0196    52540.99  126848.67  
   3100     9049.17     4.0871     4.1013     0.0741     0.0182    52437.68  126597.76  
   3200     9351.67     4.0899     4.1085     0.0699     0.0183    52340.25  126361.16  
   3300     9654.04     4.0937     4.1099     0.0649     0.0169    52247.71  126136.45  
   3400     9947.92     4.0963     4.1151     0.0669     0.0169    52160.32  125924.25  
   3500     10251.63    4.1025     4.1181     0.0652     0.017     52077.63  125723.67  
   3600     10552.63    4.107      4.1221     0.0651     0.016     51998.97   125532.8  
   3700     10853.75    4.1075     4.124      0.062      0.0146    51924.25  125351.77  
   3800     11155.01    4.1129     4.1281     0.056      0.0157    51854.04  125181.57  
   3900     11457.24    4.1173     4.1321     0.0604     0.0147    51787.4   125020.05  
   4000     11759.74    4.1165     4.135      0.0635     0.0146    51724.97   124868.7  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       92.76      3.8415     3.8526     0.2852     0.0496    60100.8   193739.17  
   200       441.0      3.8523     3.8624     0.219      0.0476    59665.8   192335.48  
   300       783.78     3.857      3.871      0.1904     0.0465    59247.91  190986.54  
   400      1128.56     3.8658     3.8851     0.1892     0.045     58847.83  189697.01  
   500      1479.85     3.884      3.8908     0.1629     0.0435    58465.05  188462.93  
   600      1831.41     3.8886     3.9026     0.1669     0.0439    58099.96  187285.12  
   700      2175.69     3.9013     3.9132     0.1417     0.0414    57753.2   186166.59  
   800      2518.89     3.9105     3.9218     0.129      0.0408    57423.28  185102.19  
   900       2870.4     3.9159     3.9328     0.1224     0.0372    57107.25  184081.92  
   1000     3221.15     3.9322      3.94      0.1265     0.0412    56807.19  183112.54  
   1100     3568.29     3.9382     3.9505     0.1162     0.035     56522.71  182192.55  
   1200     3909.39     3.9438     3.9567     0.123      0.0351    56249.98  181310.57  
   1300      4251.8     3.9532     3.9655     0.1143     0.0337    55992.8   180478.32  
   1400     4596.01     3.9636     3.9746     0.1093     0.0309    55746.79  179681.56  
   1500     4940.32     3.9689     3.9812     0.1004     0.0304    55512.39  178922.06  
   1600     5283.72     3.9782     3.9909     0.0956     0.0306    55288.57  178196.54  
   1700     5624.22     3.984      3.9978     0.0995     0.0294    55074.2   177500.67  
   1800      5965.6     3.9916     4.0049     0.0978     0.0304    54870.31  176839.19  
   1900      6307.3     4.0006     4.0105     0.0868     0.0265    54676.68  176210.55  
   2000     6648.39     4.0073     4.0173     0.0911     0.0275    54490.33  175605.77  
   2100     6989.85     4.0102     4.0246     0.0831     0.028     54312.64  175028.83  
   2200     7336.57     4.017      4.0312     0.0867     0.0254    54143.55  174479.55  
   2300     7682.83     4.022      4.037      0.0837     0.0249    53980.17  173949.09  
   2400     8029.41     4.0313     4.0427     0.0829     0.0237    53824.45  173443.35  
   2500     8372.52     4.0348     4.0486     0.0792     0.0221    53675.05  172957.74  
   2600     8713.59     4.0409     4.0554     0.0727     0.0231    53533.17  172496.89  
   2700     9055.41     4.0466      4.06      0.0757     0.0224    53395.36   172049.1  
   2800     9396.85     4.052      4.0655     0.0711     0.0221    53264.04  171622.68  
   2900     9737.88     4.0559     4.0688     0.0662     0.0199    53138.36  171214.68  
   3000     10079.58    4.063      4.0727     0.0641     0.0194    53017.99  170823.41  
   3100     10422.03    4.0677     4.0781     0.0707      0.02     52901.46  170445.08  
   3200     10768.18    4.0695     4.0835     0.0688     0.0194    52790.58  170085.05  
   3300     11113.85    4.0733     4.087      0.0651     0.0185    52685.09  169742.71  
   3400     11459.97    4.0767     4.091      0.0634     0.0176    52583.77  169413.77  
   3500     11804.55    4.0826     4.0963     0.0681     0.0178    52486.65  169098.83  
   3600     12151.01    4.0867     4.0997     0.0635     0.0164    52394.81  168800.87  
   3700     12497.57    4.0926     4.1044     0.0587     0.0167    52306.61   168514.5  
   3800     12843.61    4.0934     4.1076     0.0605     0.0159    52222.44  168241.16  
   3900     13190.0     4.0991     4.1115     0.0592     0.0158    52142.08  167980.42  
   4000     13536.25    4.0995     4.1146     0.0599     0.0153    52065.18  167730.84  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       92.31      3.8362     3.8589     0.2841     0.0441    60165.48  242579.11  
   200       484.48     3.8472     3.8643     0.2364     0.0432    59783.05   241036.5  
   300       877.13     3.8526     3.8756     0.2026     0.0403    59409.52  239529.05  
   400      1266.89     3.8639     3.8839     0.1773     0.0414    59051.17  238080.91  
   500      1657.03     3.8789     3.8948     0.1677     0.0385    58707.46   236693.6  
   600      2048.59     3.8837     3.9062     0.1502     0.0384    58372.26  235341.34  
   700      2444.67      3.89      3.9103     0.1462     0.0379    58050.69  234044.16  
   800      2840.63     3.9023     3.9219     0.1302     0.0353    57743.73  232804.33  
   900      3236.19     3.9084     3.9274     0.1291     0.0349    57451.57  231624.38  
   1000     3631.43     3.9207     3.9349     0.1357     0.0359    57169.78  230485.93  
   1100     4027.15     3.9272     3.9446     0.1147     0.0341    56898.8    229389.8  
   1200     4422.99     3.9344     3.951      0.118      0.0336    56639.73  228342.94  
   1300     4818.91     3.9409     3.9591     0.1148      0.03     56394.15  227348.65  
   1400     5214.64     3.9461     3.9678     0.0998     0.0293    56155.62  226382.38  
   1500     5610.14     3.9565     3.9767     0.1179     0.0309    55928.84  225463.12  
   1600     6001.41     3.9635     3.9851     0.1087     0.0294    55710.03  224576.43  
   1700     6390.95     3.9709     3.9885     0.1036     0.0277    55499.64  223723.28  
   1800     6780.68     3.9771     3.9963     0.0875     0.0265    55298.34  222906.89  
   1900     7170.39     3.9859     4.0031     0.0931     0.0263    55106.01  222126.44  
   2000     7559.78     3.9893     4.0107     0.0924     0.0283    54919.36  221368.73  
   2100     7954.03     3.9928     4.0166     0.095      0.0274    54739.46  220638.21  
   2200     8349.77     4.0044     4.0218     0.0908     0.0248    54566.68  219936.02  
   2300      8745.4     4.007      4.0285     0.0823     0.0244    54401.58  219265.53  
   2400      9138.9     4.0125     4.0352     0.0924     0.0251    54242.33  218618.21  
   2500     9534.19     4.0187     4.0396     0.082      0.0226    54090.11  217999.77  
   2600     9926.63     4.0279     4.0446     0.0752     0.0217    53943.28  217402.76  
   2700     10321.39    4.0284     4.0493     0.0773     0.0218    53801.27  216825.48  
   2800     10716.97    4.0314     4.0569     0.0825     0.0207    53664.51  216269.98  
   2900     11112.38    4.0378     4.0625     0.0717     0.0201    53533.13  215736.18  
   3000     11508.03    4.045      4.0665     0.0747     0.0205    53406.03  215219.48  
   3100     11903.28    4.0524     4.0703     0.0724     0.0194    53284.59  214726.05  
   3200     12297.64    4.0552     4.0765     0.0661     0.0194    53168.06  214252.48  
   3300     12691.32    4.0601     4.0804     0.0654     0.0191    53055.42  213794.61  
   3400     13085.32    4.0642     4.0858     0.0612     0.0182    52946.51  213352.27  
   3500     13475.11    4.0675     4.0897     0.0761     0.0173    52842.42  212929.56  
   3600     13864.51    4.0709     4.0928     0.0617     0.0174    52742.78  212525.13  
   3700     14254.2     4.0758     4.0969     0.0607     0.0166    52645.98   212132.3  
   3800     14643.62    4.0794     4.1001     0.0628     0.0164    52553.82  211758.25  
   3900     15033.48    4.0851     4.1046     0.0619     0.0158    52465.11  211398.35  
   4000     15423.5     4.0861     4.1081     0.0564     0.0157    52379.83  211052.26  
