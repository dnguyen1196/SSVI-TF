Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.172726392745972
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100        90.6      0.7779     0.738      3.1274     0.7064   
   200       182.59     0.7144     0.7014     2.9752     0.2353   
   300       275.18     0.7171     0.6964     3.0349     0.1498   
   400       368.36     0.7141     0.6948     3.0569     0.098    
   500       460.33     0.7227     0.7038     2.882      0.0788   
   600       551.83     0.7188     0.6984     2.9229     0.0652   
   700       643.07     0.7165     0.7054     2.6638     0.072    
   800       734.13     0.7194     0.7047     2.9592     0.0547   
   900       825.1      0.7161     0.697      2.766      0.068    
   1000      916.06     0.7209     0.7087     2.5475     0.0508   
   1100     1006.92     0.7136     0.6933     2.6816     0.0551   
   1200     1097.96     0.7153     0.6974     2.8198     0.0431   
   1300     1188.85     0.7243     0.7017     2.6186     0.0358   
   1400     1279.63     0.7145     0.6997     2.5787     0.0536   
   1500     1370.47     0.7193     0.7021     2.2466     0.0765   
   1600     1461.24     0.714      0.6924     2.2685     0.0509   
   1700      1552.0     0.7161     0.7053     2.1736     0.0339   
   1800     1642.62     0.7172     0.707      1.9602     0.0417   
   1900     1733.34     0.7171     0.7067     1.8806     0.0607   
   2000     1823.98     0.7161     0.7054     1.9366     0.0391   
   2100     1914.62     0.7172     0.707      2.3673     0.0254   
   2200     2005.28     0.7176     0.7067     2.3344     0.0303   
   2300     2095.95     0.718      0.7055     1.6596     0.0282   
   2400     2186.64     0.7183     0.7055     1.806      0.0373   
   2500     2277.39     0.7178     0.707      2.1138     0.031    
   2600      2368.1     0.7189     0.7046     2.0828     0.0327   
   2700      2458.8     0.7186     0.6976     2.1247     0.0603   
   2800      2549.5     0.7203     0.696      1.6098     0.0155   
   2900     2640.22     0.7259     0.7054     2.1505     0.0291   
   3000     2730.93     0.7262     0.7055     1.8193     0.049    
   3100     2821.68     0.7252     0.7013     1.7571     0.0236   
   3200     2912.65     0.7242     0.7006     1.5901     0.0234   
   3300     3003.44     0.7237     0.7046     1.5244     0.0139   
   3400     3094.14     0.7174     0.7011     2.3045     0.0106   
   3500     3184.92     0.7172     0.6967     2.1694     0.0245   
   3600     3275.68     0.7182     0.7007     2.1366     0.0299   
   3700     3366.41     0.7174     0.7027     1.582      0.0138   
   3800     3457.18     0.7209     0.7048      1.4       0.0184   
   3900      3548.0     0.7204     0.7051     1.7801     0.011    
   4000     3638.94     0.7177     0.7044     1.4168     0.0191   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.42      0.7526     0.7345     3.1283     0.1561   
   200       184.93     0.7231     0.7077     3.0132     0.1176   
   300       280.09     0.7172     0.7084     2.9685     0.112    
   400       374.01      0.71      0.702      3.0746     0.1069   
   500       467.22     0.7172     0.7084     2.8393     0.0973   
   600       560.19     0.7172     0.7084     2.8851     0.1073   
   700       653.11     0.7172     0.7084     2.537      0.1091   
   800       745.94     0.7172     0.7084     2.8845     0.1356   
   900       838.74     0.7172     0.7084     2.254      0.1201   
   1000      931.41     0.7172     0.7084     2.6051     0.1255   
   1100      1024.1     0.7172     0.7084     2.4573     0.1139   
   1200     1116.81     0.7172     0.7084     2.2021     0.1318   
   1300     1209.45     0.7172     0.7084     2.4432     0.1463   
   1400     1302.13     0.7172     0.7084     2.2573     0.1541   
   1500     1394.68     0.7172     0.7084     2.3265     0.1293   
   1600     1487.53     0.7172     0.7084     2.0185     0.1373   
   1700     1580.35     0.7172     0.7084     2.1711     0.1763   
   1800      1673.1     0.7172     0.7084     2.3263     0.1286   
   1900     1765.92     0.7172     0.7084     1.894      0.1882   
   2000     1858.57     0.7172     0.7084     1.9582     0.1626   
   2100     1951.21     0.7172     0.7084     1.7978     0.1408   
   2200     2043.85     0.7172     0.7084     1.503      0.1517   
   2300      2136.5     0.7172     0.7084     1.439      0.1392   
   2400     2229.26     0.7172     0.7084     1.4493     0.1905   
   2500     2322.01     0.7172     0.7084     2.2303     0.1229   
   2600     2414.68     0.7172     0.7084     2.074      0.1333   
   2700     2507.52     0.7172     0.7084     1.4844     0.1367   
   2800     2600.18     0.7172     0.7084     2.2106     0.1445   
   2900     2692.99     0.7172     0.7084      1.38      0.1541   
   3000     2785.81     0.7172     0.7084     1.5841     0.1265   
   3100     2878.62     0.7172     0.7084     1.5096     0.1448   
   3200     2971.27     0.7172     0.7084     1.5125     0.1297   
   3300     3064.15     0.7172     0.7084     1.1724     0.1352   
   3400     3156.78     0.7172     0.7084     1.3576     0.1053   
   3500     3249.49     0.7172     0.7084     1.5261     0.1677   
   3600     3342.23     0.7172     0.7084     1.6513     0.095    
   3700     3434.88     0.7172     0.7084     1.2413     0.1309   
   3800     3527.48     0.7172     0.7084     1.3109     0.1311   
   3900     3620.02     0.7172     0.7084     1.7017     0.1263   
   4000     3712.61     0.7172     0.7084     1.8938     0.1174   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       90.74      0.8552     0.8342     3.0982     0.1371   
   200       184.74     0.726      0.7108     2.922      0.1516   
   300       280.53     0.7172     0.706      2.9426     0.1333   
   400       375.51     0.7172     0.706      2.9636     0.1181   
   500       469.66     0.7172     0.706      3.0145     0.1106   
   600       563.41     0.7172     0.7058     2.8314     0.1115   
   700       657.05     0.7172     0.706      2.8111     0.1052   
   800       750.64     0.7172     0.706      2.4759     0.1132   
   900       844.12     0.7172     0.706      2.6703     0.1064   
   1000      937.92     0.7172     0.706      2.7313     0.1138   
   1100     1031.39     0.7172     0.706      2.4321     0.1352   
   1200     1124.92     0.7172     0.706      2.4915     0.1415   
   1300     1218.32     0.7172     0.706      2.3785     0.1419   
   1400     1311.75     0.7172     0.706      2.4992     0.1438   
   1500      1405.2     0.7172     0.706      2.3081     0.1485   
   1600     1498.64     0.7172     0.706      2.413      0.1283   
   1700     1592.07     0.7172     0.706      1.9197     0.1344   
   1800     1685.52     0.7172     0.706      1.8871     0.1268   
   1900     1778.96     0.7172     0.706      2.0739     0.1835   
   2000     1872.42     0.7172     0.706      2.0474     0.1655   
   2100      1965.8     0.7172     0.706      1.9772     0.1356   
   2200     2059.14     0.7172     0.706      2.0184     0.1259   
   2300     2152.51     0.7172     0.706      1.6677     0.1443   
   2400     2245.82     0.7172     0.706      1.5601     0.1388   
   2500     2339.09     0.7172     0.706      1.8136     0.1099   
   2600     2432.31     0.7172     0.706      1.5417     0.111    
   2700     2525.33     0.7172     0.706      2.2008     0.1787   
   2800     2618.35     0.7172     0.706      1.4186     0.1431   
   2900     2711.44     0.7172     0.706      1.4651     0.1428   
   3000     2804.54     0.7172     0.706      1.5393     0.1191   
   3100     2897.63     0.7172     0.706      1.549      0.1236   
   3200     2990.71     0.7172     0.706      1.2605     0.1253   
   3300      3083.8     0.7172     0.706      1.7514     0.1175   
   3400     3176.87     0.7172     0.706      1.8665     0.1411   
   3500     3269.96     0.7172     0.706      1.6055     0.1344   
   3600     3363.02     0.7172     0.706      1.5796      0.11    
   3700     3456.07     0.7172     0.706      1.6373     0.1306   
   3800     3549.09     0.7172     0.706      1.0517     0.097    
   3900     3642.15     0.7172     0.706      1.3843     0.1148   
   4000     3735.15     0.7172     0.706      1.0941     0.093    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100        90.7      0.7906     0.7727     3.1213     0.1335   
   200       185.31     0.7171     0.7031     3.015      0.1425   
   300       281.66     0.7172     0.7035     2.987      0.1259   
   400       376.88     0.7172     0.7035     2.9177     0.1142   
   500       471.4      0.7172     0.7035     2.9319     0.1125   
   600       565.63     0.7172     0.7035     2.8958     0.109    
   700       659.75     0.7172     0.7035     2.6773     0.1296   
   800       753.87     0.7172     0.7035     2.8695     0.1324   
   900       848.08     0.7172     0.7035     2.8725     0.1384   
   1000      942.15     0.7172     0.7035     2.7972     0.1285   
   1100     1036.21     0.7172     0.7035     2.8745     0.1309   
   1200     1130.33     0.7172     0.7035     2.1902     0.1282   
   1300      1224.4     0.7172     0.7035     2.3795     0.1595   
   1400     1318.46     0.7172     0.7035     2.227      0.1365   
   1500     1412.53     0.7172     0.7035     1.9519     0.1314   
   1600     1506.61     0.7172     0.7035     2.2373     0.1581   
   1700     1600.65     0.7172     0.7035     1.7943     0.1406   
   1800     1694.74     0.7172     0.7035     2.3163     0.164    
   1900     1788.91     0.7172     0.7035     1.795      0.1386   
   2000     1883.03     0.7172     0.7035     2.0354     0.1618   
   2100     1977.07     0.7172     0.7035     1.7092     0.1664   
   2200     2071.13     0.7172     0.7035     2.0368     0.159    
   2300     2165.24     0.7172     0.7035     1.9897     0.1695   
   2400     2259.29     0.7172     0.7035     1.7336     0.1617   
   2500     2353.35     0.7172     0.7035     1.5164     0.1316   
   2600     2447.44     0.7172     0.7035     1.9481     0.1239   
   2700     2540.29     0.7172     0.7035     1.4089     0.1223   
   2800     2631.57     0.7172     0.7035     1.8081     0.1623   
   2900      2722.7     0.7172     0.7035     1.3809     0.1828   
   3000     2813.91     0.7172     0.7035     1.6029     0.1284   
   3100     2902.52     0.7172     0.7035     1.237      0.1874   
   3200     2990.76     0.7172     0.7035     1.8861     0.0904   
   3300     3079.04     0.7172     0.7035     1.6033     0.1268   
   3400     3167.54     0.7172     0.7035     2.0123     0.1015   
   3500     3256.64     0.7172     0.7035     1.1742     0.1471   
   3600     3345.14     0.7172     0.7035     1.2233     0.0814   
   3700     3433.67     0.7172     0.7035     1.5092     0.0829   
   3800     3521.41     0.7172     0.7035     1.2159     0.1672   
   3900     3609.12     0.7172     0.7035     1.5278     0.088    
   4000     3696.89     0.7172     0.7035     1.8133     0.125    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100        85.4      0.9504     0.9358     3.0834     0.1153   
   200       174.69     0.7157     0.7029     2.9357     0.1306   
   300       266.36     0.7172     0.7044     3.0426     0.1165   
   400       357.3      0.7172     0.7044     2.9774     0.1043   
   500       446.23     0.7174     0.7051     2.9504     0.1029   
   600       535.15     0.7172     0.7044     3.0541     0.1009   
   700       624.72     0.7172     0.7044     3.0182     0.0976   
   800       714.4      0.715      0.7013     2.5327     0.106    
   900       803.57     0.7172     0.7044     3.0097     0.1173   
   1000      892.47     0.7172     0.7044     2.3532     0.1227   
   1100      981.12     0.7172     0.7044     2.1936     0.1389   
   1200     1069.27     0.7172     0.7044     2.5935     0.1257   
   1300     1158.31     0.7172     0.7044     2.1864     0.1242   
   1400      1247.5     0.7172     0.7044     2.7417     0.1298   
   1500     1336.72     0.7172     0.7044     2.1975     0.1488   
   1600     1423.44     0.7172     0.7044     2.1915     0.125    
   1700     1508.75     0.7172     0.7044     2.0704     0.1545   
   1800     1598.67     0.7172     0.7044     2.011      0.149    
   1900     1688.42     0.7172     0.7044     2.3244     0.1316   
   2000     1778.39     0.7172     0.7044     2.2394     0.1548   
   2100     1868.53     0.7172     0.7044     2.4694     0.1793   
   2200      1958.8     0.7172     0.7044     1.7356     0.1527   
   2300     2048.56     0.7172     0.7044     1.6375     0.158    
   2400     2140.17     0.7172     0.7044     1.8645     0.1464   
   2500     2230.91     0.7172     0.7044     1.5329     0.1258   
   2600     2321.13     0.7172     0.7044     1.9087     0.1182   
   2700     2410.96     0.7172     0.7044     1.6617     0.1375   
   2800     2501.24     0.7172     0.7044     1.7639     0.1463   
   2900      2591.1     0.7172     0.7044     2.0648     0.1335   
   3000     2680.78     0.7172     0.7044     1.3832     0.1218   
   3100     2770.39     0.7172     0.7044     2.2583     0.1635   
   3200     2859.99     0.7172     0.7044     1.3498     0.1532   
   3300     2949.57     0.7172     0.7044     1.7212     0.1176   
   3400     3039.13     0.7172     0.7044     1.9623     0.1376   
   3500     3128.73     0.7172     0.7044     1.3243     0.1448   
   3600     3218.31     0.7172     0.7044     1.3494     0.0992   
   3700     3307.92     0.7172     0.7044     1.5323     0.0992   
   3800     3397.45     0.7172     0.7044     1.168      0.1265   
   3900     3486.98     0.7172     0.7044     1.2365     0.1138   
   4000     3576.48     0.7172     0.7044     1.3242     0.124    
