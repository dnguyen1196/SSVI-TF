Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  8.26749587059021
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       68.78     274.4975   275.285     3.1603     0.6948      0.0    
   200       138.6     242.2761   240.9655    2.7687     0.2337      0.0    
   300       208.44    235.4249   235.1853    2.0985     0.1273      0.0    
   400       278.55    232.854    232.7149    1.9559     0.0792      0.0    
   500       348.15    231.6646   232.2086    1.754      0.0565      0.0    
   600       417.84    231.5005   231.3307    1.3553     0.0416      0.0    
   700       487.41    233.4689   232.8653    2.4715     0.0325      0.0    
   800       557.04    231.2366   230.5363    1.6487     0.0258      0.0    
   900       626.64    232.2167   231.9889    1.443      0.0214      0.0    
   1000      696.24    230.258    230.1994    1.2013     0.0182      0.0    
   1100      765.82    230.1077   229.8646    1.1525     0.0157      0.0    
   1200      835.45    229.7854   229.8654    1.2406     0.0137      0.0    
   1300      905.08    229.914    229.7196    1.294      0.012       0.0    
   1400      974.67    229.4649   229.7049    1.3624     0.0108      0.0    
   1500     1044.22    229.5956   229.9475    1.1667     0.0094      0.0    
   1600     1113.91    229.383    229.6969    0.9549     0.0085      0.0    
   1700     1183.64    229.1749   229.2378    1.0861     0.0077      0.0    
   1800     1253.37    228.8608   228.8119    1.3285     0.0072      0.0    
   1900     1325.69    230.7215   230.3447    1.1116     0.0065      0.0    
   2000     1396.51    228.8022   228.7441    0.8584     0.0061      0.0    
   2100     1466.19    229.8472   230.5417    1.3058     0.0055      0.0    
   2200     1535.99    228.8443   228.7295    0.8392     0.0053      0.0    
   2300     1605.69    229.3425   229.3947    1.0054     0.0048      0.0    
   2400     1676.72    228.9381   229.0324    0.8953     0.0045      0.0    
   2500     1748.46    228.7082   228.8726    1.2209     0.0042      0.0    
   2600     1818.14    228.2678   229.6741    0.8283     0.004       0.0    
   2700     1887.93    228.8912   229.708     1.3198     0.0037      0.0    
   2800     1957.74    228.8227   229.7718    1.0765     0.0035      0.0    
   2900     2027.56    229.7279   229.4734    1.201      0.0034      0.0    
   3000     2097.33    228.7119   229.1618    0.877      0.0032      0.0    
   3100     2167.34    228.8695   228.8868    0.9585     0.003       0.0    
   3200     2239.82    228.8557   229.0127    0.9289     0.0028      0.0    
   3300     2310.62    228.3166   229.1221    0.8841     0.0027      0.0    
   3400     2380.56    229.1105   228.975     0.8575     0.0026      0.0    
   3500      2450.2    227.6096   228.1933    0.9755     0.0024      0.0    
   3600     2519.78    228.8613    228.65     0.8268     0.0024      0.0    
   3700     2589.46    228.9513   228.9822    1.0075     0.0023      0.0    
   3800     2658.93    228.7494   228.505     0.7409     0.0022      0.0    
   3900     2728.53    229.0623   228.688     0.9332     0.0021      0.0    
   4000     2798.29    229.1463    229.16     0.8635     0.002       0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       69.14     259.7936   261.3252    3.1609     0.6508      0.0    
   200       140.22    238.3797   239.1099    2.7854     0.3203      0.0    
   300       211.04    234.2999   235.3847    2.2206     0.201       0.0    
   400       281.89    233.5522   233.8703    2.1812     0.1393      0.0    
   500       352.79    232.9714   233.6821    1.9833     0.1022      0.0    
   600       423.58    230.1301   230.9795    1.299      0.0797      0.0    
   700       494.35    230.4314   231.6715    1.6062     0.0627      0.0    
   800       565.26    229.087    230.1903    1.4986     0.0517      0.0    
   900       636.26    229.191    229.4033    1.2729     0.0429      0.0    
   1000      707.39    230.079    230.2262    1.172      0.0363      0.0    
   1100      778.45    232.0571   232.1805    1.6669     0.0313      0.0    
   1200      849.51    230.1744   230.7235    1.384      0.027       0.0    
   1300      920.6     230.5117   231.0982    1.0475     0.0239      0.0    
   1400      991.75    230.0048   230.5275    1.2371     0.0217      0.0    
   1500     1062.73    229.2195   229.7586    1.0989     0.0186      0.0    
   1600      1133.6    229.7015   230.0961    1.1485     0.0166      0.0    
   1700     1204.46    230.3384   230.9137    0.9125     0.0151      0.0    
   1800     1275.37    229.4401   229.6498    1.2645     0.0139      0.0    
   1900     1346.35    229.7927   229.9066    1.043      0.0124      0.0    
   2000     1417.34    229.4749   230.0698    1.3367     0.0114      0.0    
   2100     1488.59    229.1915   229.5472    0.8252     0.0106      0.0    
   2200     1559.45    228.8162   229.4088    0.9044     0.0099      0.0    
   2300     1630.27    229.424    229.6214    1.1473     0.0092      0.0    
   2400     1700.96    228.8539    229.51     1.0016     0.0083      0.0    
   2500     1771.74    228.8103   228.9904    0.9348     0.0078      0.0    
   2600     1842.46    228.9358   229.1555    1.1166     0.0073      0.0    
   2700     1913.33    229.7227   229.6696    0.996      0.007       0.0    
   2800     1983.87    229.2156   229.3312    0.9446     0.0065      0.0    
   2900     2054.37    229.2988   229.885     1.0762     0.0061      0.0    
   3000     2123.87    228.9824   229.1859    0.7637     0.0058      0.0    
   3100     2193.55    228.403    229.1281    1.0854     0.0057      0.0    
   3200     2263.46    228.3629   228.8692    0.9711     0.0052      0.0    
   3300     2333.32    229.6341   229.7586    1.0419     0.0048      0.0    
   3400     2403.04    228.4269   228.9499    0.8895     0.0047      0.0    
   3500     2472.83    228.3231   228.8678    0.8943     0.0046      0.0    
   3600     2542.72    228.2777   228.6056    0.7456     0.0042      0.0    
   3700      2612.5    228.9974   229.216     0.6999     0.0041      0.0    
   3800      2682.2    228.7209   228.7554    0.797      0.0038      0.0    
   3900     2751.87    228.8398   229.0785     0.68      0.0038      0.0    
   4000      2821.7    229.0369   229.2071    0.7134     0.0035      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       68.35     265.4749   265.2614    3.1621     0.5179     0.0001  
   200       139.16    240.4984   240.1678    2.7744     0.3074      0.0    
   300       209.85     236.94    236.4738    2.4872     0.222       0.0    
   400       280.59    232.2024   233.0881    1.8898     0.1613      0.0    
   500       351.42    231.7983   232.3924    1.7175     0.1129      0.0    
   600       422.06    232.7674   232.8312    1.6522     0.0918      0.0    
   700       492.76    230.2548   230.771     1.574      0.0732      0.0    
   800       563.56    229.6621   230.1515    1.3514     0.0618      0.0    
   900       634.35    230.3857   231.3601    1.8138     0.0507      0.0    
   1000      705.04    230.7937   230.7175    1.2534     0.0439      0.0    
   1100      775.8     231.1181   231.3518    1.3688     0.0385      0.0    
   1200      846.85    231.4361   231.6474    1.3182     0.0329      0.0    
   1300      917.99    229.7748   229.8191    1.2534     0.0288      0.0    
   1400      988.55    229.7929   229.913     1.5363     0.0259      0.0    
   1500     1059.43    228.9064   229.3925    1.2686     0.0235      0.0    
   1600     1130.12    229.3716   230.2899    1.3871     0.021       0.0    
   1700     1200.82    229.9269   229.8642    1.0978     0.0194      0.0    
   1800     1271.38    229.8984   229.6933    1.2497     0.0174      0.0    
   1900     1342.13    229.2184   229.3748    1.6075     0.0159      0.0    
   2000     1412.73    230.4879   230.5337    1.0508     0.0147      0.0    
   2100     1483.33    229.2072   230.3464    1.4118     0.0132      0.0    
   2200     1553.93    229.5564   229.6955    1.0203     0.0124      0.0    
   2300      1624.5    228.9454   229.358     0.9712     0.0114      0.0    
   2400     1695.17    229.1254   229.4363    1.0565     0.0106      0.0    
   2500     1765.63    229.1945   229.5048    1.049      0.0099      0.0    
   2600     1836.11    229.8411   229.7063    1.1234     0.0093      0.0    
   2700     1906.65    229.3098   229.6816    0.8666     0.0087      0.0    
   2800      1977.2    228.2968   228.3536    0.977      0.0082      0.0    
   2900     2047.78    228.6855   228.9257    0.8441     0.0078      0.0    
   3000     2118.49    228.3008   229.1064    1.1982     0.0073      0.0    
   3100     2189.16    228.4493   228.9868    0.7353     0.007       0.0    
   3200     2259.79    229.5925   229.5663    0.9805     0.0065      0.0    
   3300     2330.34    228.1495   228.9028    0.9081     0.0062      0.0    
   3400     2400.93    229.0742   229.9339    0.8897     0.006       0.0    
   3500     2471.47     228.32    228.9238    0.8487     0.0055      0.0    
   3600     2542.07    228.0604   228.9066    0.6957     0.0054      0.0    
   3700     2612.63    227.7158   228.5866    0.7609     0.0051      0.0    
   3800     2683.27    228.4385   228.7118    1.0347     0.0049      0.0    
   3900     2753.89    228.6015   228.8987    1.0567     0.0047      0.0    
   4000     2824.46    228.6587   229.4447    0.9534     0.0044      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       68.21     257.9125   258.9007    3.162      0.4436     0.0001  
   200       139.54    240.978    241.7195    2.7536     0.2932      0.0    
   300       211.0     235.5121   236.2349    2.2782     0.2051      0.0    
   400       282.45    233.317    234.1171    2.4744     0.1557      0.0    
   500       353.88    233.4535   234.381     1.9915     0.1201      0.0    
   600       425.31    230.4277   231.2948    1.787      0.0963      0.0    
   700       496.69    232.1331   233.1165    1.7602     0.079       0.0    
   800       568.09    230.6016   231.7813    1.401      0.0671      0.0    
   900       639.6     229.7034   230.3629    1.1298     0.0589      0.0    
   1000      710.82    230.9917   231.5001    1.416      0.0499      0.0    
   1100      781.99     231.01    232.0435    1.1689     0.0428      0.0    
   1200      853.19    229.7614   231.2035    1.5232     0.0382      0.0    
   1300      924.36    229.3101   230.1345    1.323      0.0333      0.0    
   1400      995.58    230.0818   230.7861    1.5162     0.0307      0.0    
   1500     1066.73    230.9067   231.3258    1.4354     0.027       0.0    
   1600     1137.92    229.5913   230.6296    1.0526     0.0244      0.0    
   1700     1209.05    229.5286   230.2867    0.9979     0.0215      0.0    
   1800     1280.16    228.6804   229.8272    1.1283     0.0199      0.0    
   1900     1351.26    229.1204   230.212     1.1299     0.0184      0.0    
   2000     1422.37    228.9435   229.7697    0.9353     0.0167      0.0    
   2100     1493.47    229.2184   230.5302    1.0776     0.0158      0.0    
   2200     1564.61    229.8015   230.4109    1.3563     0.0141      0.0    
   2300     1635.73    228.2808   229.3576    1.2522     0.0134      0.0    
   2400     1706.81    230.0543   230.9517    1.1843     0.0123      0.0    
   2500     1777.92    229.0027   229.5701    0.9808     0.0115      0.0    
   2600     1848.93    229.4147   230.7932     1.1       0.0108      0.0    
   2700     1918.94    228.6664   229.8508    1.0503     0.0102      0.0    
   2800     1988.79    228.7217   229.962     1.3578     0.0095      0.0    
   2900     2058.45    231.1786   231.0947    0.9563     0.0089      0.0    
   3000     2127.61    228.271    229.2455    0.7341     0.0086      0.0    
   3100     2197.24    228.5887   229.5063    0.8818     0.008       0.0    
   3200     2266.78    229.1607   229.7665    0.9342     0.0079      0.0    
   3300      2336.4    228.4578   229.5792    0.7672     0.0073      0.0    
   3400     2406.24    228.5856   230.1121    0.8578     0.0069      0.0    
   3500     2476.22    227.7071   228.9264    0.9796     0.0066      0.0    
   3600      2545.7    228.0248   229.3042    0.7083     0.0064      0.0    
   3700     2615.34    227.6964   229.1416    0.8685     0.006       0.0    
   3800     2686.03    228.2604   229.0946    1.0307     0.0056      0.0    
   3900     2756.94    228.0595   229.0612    0.8716     0.0056      0.0    
   4000     2827.89    228.6946   229.5504    0.7966     0.0053      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       67.95     260.5973   261.559     3.162      0.3923     0.0001  
   200       139.59    237.9285   238.0296    2.6542     0.2692      0.0    
   300       211.31    238.3475   238.7594    2.7035     0.1981      0.0    
   400       283.17    236.6165   236.2589    2.2248     0.159       0.0    
   500       354.99    232.1412   232.845     1.8154     0.1201      0.0    
   600       426.74    232.8578   233.8507    1.9411     0.1026      0.0    
   700       498.48    231.2599   232.2137    1.7335     0.0828      0.0    
   800       570.2     229.8072   230.7713    1.1839     0.0684      0.0    
   900       642.03    230.8284   232.1514    1.4775     0.0631      0.0    
   1000      713.73    229.7425   231.0756    1.5766     0.0531      0.0    
   1100      785.42    229.3949   230.6696    1.2729     0.0468      0.0    
   1200      857.16    229.0074   230.3338    1.4491     0.0404      0.0    
   1300      928.9     230.5022    231.21     1.1375     0.0359      0.0    
   1400     1000.57    230.2566   230.3709    1.4134     0.0323      0.0    
   1500      1072.3    229.4247   230.9609    1.2416     0.0301      0.0    
   1600     1144.11    228.9481   229.3436    1.0536     0.0274      0.0    
   1700     1215.94    229.5727   230.3363    1.4155     0.0254      0.0    
   1800     1287.71    229.0854   229.8385    1.1259     0.0232      0.0    
   1900      1359.5    228.5445   229.7335    1.4827     0.0208      0.0    
   2000     1431.26    229.8423   230.5744    1.0562     0.019       0.0    
   2100     1503.04    229.577     230.03     1.2268     0.0172      0.0    
   2200     1574.89    230.9826   231.1984    1.1965     0.0163      0.0    
   2300     1646.77    229.3551   229.797     1.0565     0.0151      0.0    
   2400     1718.55    229.6087   230.3272    1.0758     0.0142      0.0    
   2500     1790.35    228.5944   229.4238    0.9833     0.013       0.0    
   2600     1862.09    229.022    229.9957    0.8187     0.0128      0.0    
   2700     1933.72    229.1621   229.658     0.9922     0.0117      0.0    
   2800      2005.5    228.4926   229.3455    0.9764     0.0107      0.0    
   2900     2077.31    228.8526   229.8506    1.0588     0.0102      0.0    
   3000     2149.03    229.1424   229.7688    0.9587     0.0096      0.0    
   3100     2220.92    228.7873   229.4605    1.0665     0.0092      0.0    
   3200     2292.54    228.5208   229.0276    0.7777     0.0086      0.0    
   3300     2364.14    228.5703   228.9154    0.906      0.0083      0.0    
   3400     2435.87    228.0476   228.9083    1.0427     0.0081      0.0    
   3500     2507.46    228.5049   229.5304    1.2819     0.0076      0.0    
   3600     2579.14    228.6481   229.2733    0.9391     0.0072      0.0    
   3700     2650.82    227.8927   229.0007    1.0172     0.0068      0.0    
   3800     2722.51    228.6619   229.5523    0.9252     0.0065      0.0    
   3900     2794.15    228.1671   228.7076    0.7902     0.0062      0.0    
   4000     2865.76    228.1476   228.924     0.8912     0.0061      0.0    
