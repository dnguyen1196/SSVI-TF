Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  8.128386974334717
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       211.06    258.7474   259.7354    3.1622     0.7633      0.0    
   200       422.04    237.4756   236.8397    2.6595     0.2923      0.0    
   300       633.08    232.5376   232.7835    2.2713     0.1346      0.0    
   400       844.25    230.3471   230.2099     1.63      0.1158      0.0    
   500      1055.34    230.3773   230.2756    1.5102     0.1114      0.0    
   600      1266.99    231.1414   230.8103    1.8323     0.0828      0.0    
   700       1478.4    229.5725   229.5025    1.4541     0.0623      0.0    
   800      1689.52    230.6896   230.4004    1.4787     0.0532      0.0    
   900      1900.54    229.9279   229.6316    1.5477     0.0526      0.0    
   1000     2111.53    229.927    229.2756    1.1588     0.0442      0.0    
   1100     2322.63    230.3308   230.1722    1.1795     0.0452      0.0    
   1200     2533.43    229.8842   229.8587    1.2999     0.0422      0.0    
   1300     2744.21    229.2773   229.5057    1.0907     0.0404      0.0    
   1400     2955.44    228.9036   229.2273    1.0185     0.0373      0.0    
   1500     3166.41    229.2925   229.4552    1.3037     0.0341      0.0    
   1600     3377.38    229.3959   229.5486    1.1709     0.0321      0.0    
   1700     3588.48    229.1823   229.2134    0.9059     0.0285      0.0    
   1800     3799.78    228.4815   228.6751    1.0748     0.0227      0.0    
   1900     4010.91    229.2395   229.0963    1.0873     0.0292      0.0    
   2000     4222.03    228.6944   228.7862    0.8539     0.0279      0.0    
   2100     4433.14    228.882    229.0392    1.2117     0.0247      0.0    
   2200     4644.11    228.215    228.9116    1.1399     0.0261      0.0    
   2300     4854.96    228.2488   228.8648    0.9667     0.0206      0.0    
   2400     5065.84    228.7181   228.5999    0.8665     0.0201      0.0    
   2500     5277.25    229.2494   229.3906    1.2299     0.016       0.0    
   2600     5488.53    228.9451   228.687     0.837      0.0229      0.0    
   2700     5699.92    228.2596   228.5824    1.0197     0.0162      0.0    
   2800     5911.42    228.8111   228.4891    0.7191     0.0166      0.0    
   2900     6122.92    229.1829   229.2252    0.7856     0.0224      0.0    
   3000     6334.25    227.9989   228.6002    0.8098     0.0148      0.0    
   3100     6545.67    228.3897   228.3266    0.9469     0.0186      0.0    
   3200     6755.64    228.1654   228.486     0.8581     0.0157      0.0    
   3300     6964.87    228.1315   228.2386    0.8891     0.0128      0.0    
   3400     7174.19    228.2898   228.3178    0.7768     0.016       0.0    
   3500     7383.52    228.2481   228.475     0.7832     0.0125      0.0    
   3600     7592.87    228.426    228.7618    0.934      0.0163      0.0    
   3700     7802.13    228.1121   228.4091    0.837      0.0132      0.0    
   3800     8011.52    228.3689   228.0339    1.0791     0.0107      0.0    
   3900     8220.92    228.3639   228.5026    0.7096     0.0126      0.0    
   4000     8430.37    227.7559   228.3043    0.948      0.0098      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       210.21    267.2401   268.388     3.1623     1.0877      0.0    
   200       420.85    240.0167   239.583     2.6743     0.2983      0.0    
   300       631.49    233.6557   233.8555    2.3301     0.183       0.0    
   400       841.85     231.91    232.3329    1.685      0.1409      0.0    
   500      1052.07    231.002    231.7514    1.4245     0.1348      0.0    
   600      1262.25    231.0193   231.4752    2.206      0.0897      0.0    
   700      1472.37    229.9564   229.9746    1.1357     0.0842      0.0    
   800      1682.52     230.81    230.6744    1.6226     0.0615      0.0    
   900      1892.56    229.4911   229.7675    1.3354     0.0507      0.0    
   1000     2098.42    229.2232   229.6373    1.3369     0.0569      0.0    
   1100     2304.05    229.2664   229.7186    1.1797     0.0526      0.0    
   1200     2510.55    229.0691   229.0976    1.4366     0.0562      0.0    
   1300     2717.49    230.1493   230.0228    1.2194     0.0405      0.0    
   1400     2927.53    228.8047   229.6906    0.9359     0.045       0.0    
   1500     3137.55    229.0297   229.2271    1.056      0.0385      0.0    
   1600     3347.53    228.5834   229.4589    1.1523     0.0383      0.0    
   1700     3557.77    228.8885   228.7863    0.9157     0.0325      0.0    
   1800     3767.87    229.194    228.8325    0.9606     0.0318      0.0    
   1900     3978.07    229.173    229.5353    1.3562     0.0313      0.0    
   2000      4188.4    228.6275   229.3425    0.9288     0.0293      0.0    
   2100     4398.75    228.9491   229.1989    1.1311     0.0232      0.0    
   2200     4609.06    228.4511   229.0469    1.3865     0.0288      0.0    
   2300     4819.29    229.1628   229.1912    0.7963     0.0213      0.0    
   2400     5029.51    228.4841   228.5154    1.0062     0.0214      0.0    
   2500      5239.6    228.6244   229.5029    1.2121     0.0272      0.0    
   2600     5449.59    228.3402   228.9258    0.8148     0.019       0.0    
   2700     5659.58    228.3399   228.5706    0.8335     0.0251      0.0    
   2800     5869.63    228.5876   228.718     0.718      0.0199      0.0    
   2900     6079.55    228.8852   228.8279    0.714      0.0244      0.0    
   3000     6287.64    228.1169   228.7874    1.0238     0.0177      0.0    
   3100     6492.92    228.3349   228.4922    0.8687     0.0156      0.0    
   3200     6690.49    228.3686   229.1659    0.7935     0.0222      0.0    
   3300     6889.92    228.3672   228.3872    0.9394     0.0129      0.0    
   3400     7088.32    228.8061   228.6714    0.8192     0.0192      0.0    
   3500     7287.52    227.7281   228.5739    0.8758     0.0195      0.0    
   3600     7486.92    229.0354   229.1004    0.8171     0.015       0.0    
   3700     7684.66    228.3563   228.7099    1.3463     0.0149      0.0    
   3800     7881.39    227.942    228.3049    0.8154     0.0129      0.0    
   3900      8077.2    228.3144   228.5323    0.6661     0.0121      0.0    
   4000     8273.43    227.6248   228.1646    1.2616     0.011       0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       197.05    265.5496   265.1692    3.1618      1.29       0.0    
   200       391.86    239.6801   238.9952    2.9694     0.4146      0.0    
   300       587.52    235.9932   235.2475    1.9887     0.1634      0.0    
   400       786.91    232.4302   232.8492    1.775      0.1571      0.0    
   500       987.64    231.2469   231.5127    2.1895     0.156       0.0    
   600      1188.17    230.1321   230.1084    1.2562     0.1018      0.0    
   700      1388.64    229.842    230.1154    1.3677     0.0644      0.0    
   800      1589.47    229.829    230.1151    1.5055     0.1061      0.0    
   900      1789.43    229.3217   229.5794    1.2205     0.0657      0.0    
   1000     1985.53    229.2248   229.5843    1.2631     0.0558      0.0    
   1100     2180.48    229.9219   230.4735    1.3879     0.0496      0.0    
   1200     2375.69    228.4703   229.3041    1.074      0.0431      0.0    
   1300      2570.6    229.724    230.2019    1.4087     0.0541      0.0    
   1400      2765.5    228.9355   229.6734    1.1491     0.0424      0.0    
   1500     2960.59    229.2545   229.5079    1.1241     0.036       0.0    
   1600     3155.83    228.7306   229.2595    1.222      0.0468      0.0    
   1700     3350.96    228.9399   229.1369    1.1476     0.0306      0.0    
   1800     3546.03    229.2918   229.4627    1.1893     0.0304      0.0    
   1900     3741.14    228.8553   229.4572    1.1851     0.0328      0.0    
   2000     3936.18    228.8347   229.3479    1.014      0.0308      0.0    
   2100     4131.24    228.528    228.835     0.986      0.0363      0.0    
   2200     4326.77    228.5226   229.199     1.2262     0.0222      0.0    
   2300     4524.08    228.375    228.4494    1.105      0.0237      0.0    
   2400     4719.21    228.1788   228.6629    1.1639     0.0268      0.0    
   2500     4914.32    228.2808   228.8926    0.9689     0.0185      0.0    
   2600     5109.47    228.2677   228.468     0.8201     0.0196      0.0    
   2700     5304.53    227.7951   228.7339    1.043      0.0236      0.0    
   2800     5499.61    228.4888   228.8537    1.2653      0.02       0.0    
   2900     5694.69    228.0435   228.6289    0.737      0.0171      0.0    
   3000     5889.64    228.1019   229.0928    0.7589     0.0254      0.0    
   3100     6086.67    229.1144   229.335     0.9276     0.0194      0.0    
   3200      6286.3    228.4062   229.0577    0.9053     0.0147      0.0    
   3300      6485.6    228.2466   228.6716    0.818      0.0221      0.0    
   3400     6680.65    227.9822   228.5718    0.9726     0.0157      0.0    
   3500     6875.64    228.6534   229.2796    0.9718     0.0173      0.0    
   3600     7070.63    228.5915   228.7762    0.8986     0.015       0.0    
   3700     7268.33    228.3234   228.4828    0.8369     0.0187      0.0    
   3800     7468.29    228.3562   228.7791    0.9493     0.0132      0.0    
   3900     7665.09     228.27    228.4801    0.9987     0.0144      0.0    
   4000      7861.7    228.1106   228.0951    0.7974     0.0138      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       193.71    263.7403   265.3273    3.1622     1.1413      0.0    
   200       389.44    238.2658   238.7452    2.8829     0.2591      0.0    
   300       586.52    233.5314   233.532     2.1074     0.1746      0.0    
   400       786.45    233.0681   233.8509    1.7187     0.1892      0.0    
   500       986.29    231.4414   232.4784    1.6473     0.1096      0.0    
   600      1185.13    230.5128   230.8463    1.4891     0.1109      0.0    
   700      1381.03    229.6853   230.7871    1.5057     0.0736      0.0    
   800      1576.91    229.8329   230.7579    1.6383     0.0877      0.0    
   900      1773.43    229.169    230.1484    1.3866     0.0629      0.0    
   1000      1970.3    230.3048   231.3201    1.6639     0.0516      0.0    
   1100     2168.09    229.5835   230.3212    1.4546     0.0531      0.0    
   1200     2367.03    229.5706   230.429     1.4564     0.0469      0.0    
   1300     2562.84    228.9309   229.763     1.3297     0.0451      0.0    
   1400     2758.76    229.4018   229.8267    1.324      0.057       0.0    
   1500     2955.21    228.7894   229.4939    1.126      0.0457      0.0    
   1600     3154.79    228.4436   228.8794    1.1037     0.0363      0.0    
   1700     3353.69    229.4825   230.4529    1.1144     0.0391      0.0    
   1800     3553.14    228.6039   229.2314    0.9967     0.0422      0.0    
   1900     3748.97    229.0945   229.2801    1.1805     0.0339      0.0    
   2000     3944.91    229.3891   230.0382    1.1821     0.0254      0.0    
   2100     4140.86    228.7522   229.7952    0.963      0.0288      0.0    
   2200     4336.79    228.6006   229.7547    1.1272     0.0354      0.0    
   2300     4532.74    229.3355   230.2011    1.2301     0.0329      0.0    
   2400     4728.64    228.1436   229.1074    0.9107     0.028       0.0    
   2500     4924.57    228.2242   229.0604    1.3752     0.0269      0.0    
   2600     5122.91    228.0999   228.8375    0.8117     0.0311      0.0    
   2700      5320.3    228.5131   229.2041    0.8074     0.0246      0.0    
   2800     5516.23    228.8379   230.007     0.8487      0.02       0.0    
   2900     5712.13    228.683    229.6296    1.0412     0.0213      0.0    
   3000     5908.05    228.6335   229.1844    0.7956     0.025       0.0    
   3100     6103.93    227.6556   228.6156    1.0454     0.0205      0.0    
   3200     6299.88    227.4828   228.7568    0.7537     0.027       0.0    
   3300     6495.78    228.0958   229.1135    0.8832      0.02       0.0    
   3400     6691.63    228.2011   229.0064    0.7613     0.0145      0.0    
   3500     6887.64    228.8083   229.6006    1.1879     0.0155      0.0    
   3600      7083.5    227.8618   228.9703    0.8964     0.0151      0.0    
   3700     7279.67    228.6604   229.1905    0.952      0.0184      0.0    
   3800     7475.57    227.9897   228.7198    0.749      0.0161      0.0    
   3900     7671.49    228.1391   228.8359    0.8067     0.013       0.0    
   4000     7867.62    228.0534   229.0905    0.7247     0.0152      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       198.43    266.3602   266.0971    3.1617     1.2589      0.0    
   200       399.69    241.4425   241.2133    2.9022     0.3623      0.0    
   300       597.73    231.6414   232.2568    1.8138     0.1703      0.0    
   400       794.91    230.8111   232.4809    2.0842     0.1159      0.0    
   500       991.95    231.6507   231.7826    1.7585     0.1062      0.0    
   600      1190.88    231.6061   231.796     1.9053     0.1324      0.0    
   700      1387.97    230.3151   230.472     1.4728     0.0887      0.0    
   800      1585.74    230.0069   230.3442    1.3086     0.0696      0.0    
   900      1788.31    231.0481   231.5116    1.7575     0.0768      0.0    
   1000      1987.3    229.4257   230.3121    1.4776     0.0546      0.0    
   1100     2184.36    228.9354   230.2234    1.5748     0.0522      0.0    
   1200     2381.49    229.934    230.3557    1.7137     0.0587      0.0    
   1300     2579.28    228.9137   229.7118    1.0433     0.0439      0.0    
   1400     2776.34    229.2529   229.8491    1.329      0.0518      0.0    
   1500     2976.88    229.6143   230.3704    1.2893     0.0419      0.0    
   1600     3178.39    228.1312   228.9692    0.9669     0.039       0.0    
   1700     3375.51    229.1349   229.7574    1.2256     0.0557      0.0    
   1800     3573.21    229.1202   229.3797    0.9986     0.0299      0.0    
   1900     3770.44    229.062    229.5289    1.0152     0.0551      0.0    
   2000      3967.8    229.1962   229.5252    1.0382     0.0316      0.0    
   2100     4164.95    229.0212   229.5785    1.2829     0.041       0.0    
   2200     4362.35    228.9641   229.5802    0.9134     0.0316      0.0    
   2300     4559.45    228.8155   229.2496    1.1145     0.0329      0.0    
   2400      4756.9    228.2404   229.2339    1.0584     0.0312      0.0    
   2500     4954.01    228.2016   228.8532    0.8705     0.022       0.0    
   2600      5151.8    228.1124   229.2445    1.4151     0.0222      0.0    
   2700     5350.07    228.6587   229.321     1.3359     0.025       0.0    
   2800      5549.6    228.1846   228.8489    0.7604     0.0221      0.0    
   2900     5748.98    227.8009   228.3917    0.7777     0.0249      0.0    
   3000     5949.22    228.6528   229.3103    0.9901     0.0206      0.0    
   3100     6149.34    228.4414   229.0253    0.9677     0.0252      0.0    
   3200     6349.07    228.9628   229.1153    0.9157     0.0208      0.0    
   3300     6547.32    228.0599   228.9982    0.8011     0.0191      0.0    
   3400     6744.06    228.5794   229.0221    0.9961     0.0164      0.0    
   3500     6940.85    227.953    228.7807    1.1454     0.0242      0.0    
   3600     7137.58    228.2285   228.8157    0.8192     0.0258      0.0    
   3700      7334.3    227.8885   228.8281    0.6502     0.021       0.0    
   3800     7531.12    228.0453   228.8648    0.8664      0.02       0.0    
   3900     7730.11    228.624    229.0375    1.2021     0.0235      0.0    
   4000     7929.78    228.1467   228.7264    0.7074     0.0155      0.0    
