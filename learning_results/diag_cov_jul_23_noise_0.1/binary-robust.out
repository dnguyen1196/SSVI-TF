Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.344115734100342
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       172.85     1.4094     1.4156     3.1431   39168.1636   0.1377  
   200       413.51      1.41      1.4146     3.0108   173159.9202   0.4007  
   300       653.14     1.4158     1.4135     1.894    20055.1915   0.8983  
   400       892.85     1.4112     1.4061     3.0938   7542.3477    0.0186  
   500      1132.13     1.4227     1.422      1.6298    5800.312    0.0867  
   600      1371.36     1.4093     1.419      3.1598   5176.3467    0.0366  
   700      1611.55     1.4157     1.4147     3.1588   70985.0837   0.0453  
   800      1849.21     1.4196     1.4072     2.7006   16037.6764   0.0277  
   900      2084.92     1.4102     1.4184     1.0795   3479.3847    0.0187  
   1000     2320.13     1.417      1.4052     1.8349   6015.1542    0.0292  
   1100     2554.16     1.4144     1.4115     0.931     3093.99     0.0141  
   1200     2787.78     1.4259     1.4151     2.8791   3387.8217    0.0113  
   1300     3021.84     1.4172     1.4071     3.0616   3196.5255    0.0135  
   1400     3255.91     1.4104     1.4079     0.967     1199.197    0.013   
   1500     3489.45     1.406      1.4071     2.8301    373.711     0.0078  
   1600     3722.68     1.3998     1.4085     2.3127   2202.1756    0.0063  
   1700     3955.88     1.4221     1.4159     3.0898   5365.8018    0.0124  
   1800     4189.34     1.4096     1.4119     1.1957    834.0867    0.0091  
   1900     4422.63     1.4117     1.4117     0.9407   4934.3352     0.01   
   2000     4655.75     1.4222      1.41      3.1276   13712.2719   0.0067  
   2100     4889.01     1.4139     1.416      3.1405   59634.3008   0.0017  
   2200     5122.37     1.4167     1.4224     2.7227   8388.1986    0.0055  
   2300     5355.36      1.42      1.4099     2.8703    868.4393    0.0063  
   2400     5588.03     1.4152     1.4155     0.6966   1457.9224    0.0051  
   2500     5820.42     1.4152     1.407      2.2902    930.756     0.0031  
   2600      6052.4     1.4146     1.4125     2.852     383.4329    0.0032  
   2700      6284.0     1.4135     1.4149     2.1749   1259.9193    0.0028  
   2800     6515.88     1.416       1.41      3.1621   3459.4029    0.0024  
   2900     6748.22     1.4162      1.41      2.9019   1274.3519    0.0056  
   3000     6980.81     1.4178     1.4075     1.3828   55587.0291   0.0003  
   3100     7214.21     1.4139     1.4062     0.8999   143723.6246   0.0066  
   3200     7447.74     1.4139     1.4155     0.6474   14352.6732   0.0049  
   3300     7680.46     1.4056     1.4121     3.0938   2276.5495    0.0032  
   3400     7912.95     1.4131     1.4184     1.3102   1789.0042    0.0036  
   3500     8144.93     1.4227     1.4032     3.0872    803.9398    0.0029  
   3600     8375.98     1.4134     1.4134     1.3077   1182.2274    0.0011  
   3700     8606.93     1.4117     1.4127     0.5643   6338.8337    0.002   
   3800     8837.72     1.4181     1.4105     0.7976   2855.2704    0.0013  
   3900     9068.29     1.4135     1.4076     0.615    2520.1372    0.002   
   4000     9296.88     1.4181     1.4014     0.9793    692.9439    0.0018  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       161.96     1.3444     1.3497     3.1572   4067.1902    0.0008  
   200       405.53     1.2041     1.1987     2.5159   9751.2981    0.0003  
   300       649.86     0.9648     0.956      3.1623   13747.3482   0.0002  
   400       892.77     0.7833     0.7779     1.9455   44631.5451   0.0001  
   500      1136.11     0.7567     0.7504     1.4006   2995.5893    0.0001  
   600      1379.72     0.7492     0.732      1.3928   7323.9716     0.0    
   700      1622.37     0.7363     0.7257     1.2104   361529.4268    0.0    
   800      1864.46     0.7378     0.7251     1.0574   1105.8123     0.0    
   900      2108.78     0.7599     0.7489     3.1405   29056.973     0.0    
   1000     2359.34     0.7399     0.7189     1.0767   35858.0291    0.0    
   1100     2618.01     0.731      0.7232     0.9965   11937.4983    0.0    
   1200     2876.76     0.7253     0.717      0.9363   79216.6951    0.0    
   1300     3135.37     0.7377     0.7298     0.808    2417.8004     0.0    
   1400      3394.0     0.7689     0.7566     1.0829   4385.5015     0.0    
   1500     3652.62     0.7478     0.7239     0.7771     4390.8      0.0    
   1600     3911.12     0.7543     0.7338     0.7609   5191.7062     0.0    
   1700      4169.6     0.7331     0.7248     0.9487   51189.1731    0.0    
   1800     4428.09     0.7488     0.7275     2.3519   3695.0601     0.0    
   1900     4682.04     0.7463     0.7218     3.0505    519.8527     0.0    
   2000     4929.15     0.739      0.7302     2.8007    792.2113     0.0    
   2100     5176.93     0.7439     0.7331     2.9004    8961.409     0.0    
   2200     5422.07     0.7402     0.7268     0.6578   6985.5901     0.0    
   2300     5656.71     0.7307     0.7198     0.6518   3126.3786     0.0    
   2400      5892.4     0.7625     0.7508     3.1378    1244.988     0.0    
   2500     6127.15     0.7343     0.723      0.6905   13699.6563    0.0    
   2600     6361.18     0.7295     0.7193     0.8366   33278.0815    0.0    
   2700     6589.44     0.734      0.7235     0.6966   5412.1624     0.0    
   2800      6816.3     0.7358     0.7192     0.7591   6163.2957     0.0    
   2900     7046.58     0.7402     0.7253     3.1598   2868.7848     0.0    
   3000     7279.35     0.7359     0.7244     0.5042   2951.4874     0.0    
   3100     7507.86     0.7345     0.7172     2.5151    2370.816     0.0    
   3200      7735.3     0.7357     0.7192     0.6384   18634.6467    0.0    
   3300      7961.7     0.732      0.7193     0.4883   1192.2569     0.0    
   3400     8187.82     0.7362     0.7185     0.6475    775.1777     0.0    
   3500     8414.68     0.733      0.7199     0.6947   22510.9469    0.0    
   3600     8641.43     0.7238     0.7181     0.8149   6739.8881     0.0    
   3700     8866.97     0.7274     0.7218     0.8404   1468.9198     0.0    
   3800     9092.71     0.738      0.729      0.7203   24830.8678    0.0    
   3900     9319.45     0.7239     0.7204     0.6865   6043.4346     0.0    
   4000     9546.21     0.734      0.7224     0.689    4612.9598     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       146.39     0.9287     0.9142     3.1569     0.8244     0.0001  
   200       395.22     0.7641     0.7549     1.4362    195.8037    0.0001  
   300       642.68     0.8073     0.7997     1.2137   2254.2578    0.0001  
   400       892.86     1.1726     1.1712     1.6535   6293.0771    0.0001  
   500      1147.66     1.3462     1.3473     2.5443   7201.7437    0.0001  
   600      1405.69     1.3924     1.3885     1.6028   17203.7809    0.0    
   700      1659.68     1.4007     1.4047     1.6673   6037.1154     0.0    
   800      1912.37     1.4021     1.4044     3.1341   83745.118     0.0    
   900      2164.06     1.4116     1.4125     1.7829   24596.8531    0.0    
   1000      2416.8     1.4215     1.4134     1.6631   6821.8735     0.0    
   1100     2669.58     1.4149     1.4127     1.6073   5905.6902     0.0    
   1200     2921.99     1.4037     1.4118     1.4932   11306.2496    0.0    
   1300     3173.44     1.4148     1.4128     1.3179   43223.4518    0.0    
   1400     3424.86     1.4128     1.4138     1.2037   22405.0266    0.0    
   1500     3675.99     1.4127     1.4088     1.1636   399183.0754    0.0    
   1600     3927.82     1.4125     1.4132     1.204    18579.124     0.0    
   1700     4179.79     1.4134     1.4159     0.9577   9958.8823     0.0    
   1800     4431.22     1.4147     1.4154     1.0553   19481.8636    0.0    
   1900     4682.73     1.4168     1.4097     0.9588   67820.0907    0.0    
   2000     4934.32     1.4112     1.4131     1.1701   85514.4933    0.0    
   2100      5185.8     1.4155     1.4203     1.1168   216869.5213    0.0    
   2200     5437.32     1.4158     1.414      0.7946   22951.0515    0.0    
   2300     5688.86     1.4156     1.4179     0.8281   78468.8625    0.0    
   2400      5940.6     1.4099     1.4119     0.8547   10199.0211    0.0    
   2500     6192.38     1.4142     1.4093     0.9246   49760.9167    0.0    
   2600     6441.83     1.4181     1.4136     0.825    79300.6364    0.0    
   2700     6691.36     1.403      1.4174     0.841    31533.227     0.0    
   2800     6942.92     1.4163     1.4093      0.93    144580.1016    0.0    
   2900      7196.2     1.4143     1.4149     0.7353    3440.961     0.0    
   3000     7447.74     1.4187     1.4127     0.723    33651.4231    0.0    
   3100     7701.23     1.421      1.4121     0.8538   155506.8967    0.0    
   3200     7953.98     1.4154     1.412      0.8424   163062.8812    0.0    
   3300     8206.27     1.4112     1.414       0.84    229978.8285    0.0    
   3400     8458.79     1.4198     1.4139     0.6788   3624.7882     0.0    
   3500     8712.47     1.4117     1.4163     0.7041   62231.4962    0.0    
   3600     8964.67     1.4123     1.4189     0.8281   249935.3311    0.0    
   3700     9217.31     1.4112     1.4154     0.5995    1199.967     0.0    
   3800     9470.15     1.4019     1.4139     0.6834    1198.052     0.0    
   3900     9719.81     1.4063     1.4121     1.9181   2007.8948     0.0    
   4000     9970.42     1.4151     1.4115     0.6075   9744.5883     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       146.97     1.2145     1.2041     3.156      0.236      0.0001  
   200       421.9      0.7767     0.763      1.2803     0.6089      0.0    
   300       694.49     0.7511     0.7427     1.001     15.7392      0.0    
   400       968.43     1.0171     1.0073     1.0813    176.0983     0.0    
   500      1242.24     1.4128     1.4076     1.2885    689.3406     0.0    
   600      1518.71     1.4166     1.4107     1.2382    657.5462     0.0    
   700      1791.19     1.4112     1.413      1.3085   3196.4888     0.0    
   800      2063.46     1.409      1.418      1.5249    19608.64     0.0    
   900      2336.08     1.4085     1.4125     3.1006   12968.8757    0.0    
   1000     2608.81     1.4182     1.4183     1.4453   5997.2986     0.0    
   1100     2881.58     1.413      1.4131     3.1623   10103.5885    0.0    
   1200     3154.59     1.4093     1.416      1.3685    3614.456     0.0    
   1300     3427.42     1.4215     1.4116     2.7635   5152.5133     0.0    
   1400     3700.31     1.4147     1.4125     1.4978   15262.623     0.0    
   1500     3973.23     1.4099     1.4157     1.5602   21267.9623    0.0    
   1600     4246.26     1.4241     1.4156     1.3643   621299.8452    0.0    
   1700     4519.22     1.4153     1.414      1.2248   32096.2439    0.0    
   1800     4792.36     1.4066     1.413      1.3581   15543.6949    0.0    
   1900     5065.25     1.4202     1.4161     1.1438   7700.1597     0.0    
   2000     5338.18     1.4195     1.4132     1.8446   88240.9698    0.0    
   2100     5611.12     1.4105     1.4148     1.4787   144416.0502    0.0    
   2200      5884.2     1.4203     1.4169     1.6433   29451.4734    0.0    
   2300     6157.24     1.4118     1.4153     1.4999   3918.8663     0.0    
   2400      6430.4     1.419      1.4151     1.6115   31104.8403    0.0    
   2500     6703.62     1.4131     1.4084     1.226    41428.9172    0.0    
   2600     6976.63     1.4236     1.4167     1.2781   21824.5074    0.0    
   2700      7249.6     1.4138     1.4142     1.1435   90867.1254    0.0    
   2800     7522.63     1.4135     1.4134     1.2876   1365087.974    0.0    
   2900     7795.57     1.4096     1.4146     1.1853   3045.7385     0.0    
   3000     8068.63     1.4153     1.4191     1.5008   44040.1876    0.0    
   3100      8341.6     1.4071     1.4144     1.053    14873.9936    0.0    
   3200     8614.77     1.4089     1.4143     1.1253   150801.9432    0.0    
   3300     8887.71     1.4111     1.4132     1.4759   28144.805     0.0    
   3400     9160.68     1.4131     1.4191     1.319    316622.8163    0.0    
   3500     9433.68     1.4142     1.4125     1.2521   189954.9474    0.0    
   3600     9706.62     1.417      1.4149     1.1478   95105.2083    0.0    
   3700      9979.6     1.4062     1.4143     1.2286   58021.0342    0.0    
   3800     10252.74    1.4166     1.4164     0.9588   91024.5449    0.0    
   3900     10525.53    1.4106     1.4126     0.9999   39453.0066    0.0    
   4000     10798.23    1.4177     1.4172     0.9365   23509.1331    0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       144.37     0.9701     0.9661     3.158      0.1004     0.0001  
   200       437.52     0.7541     0.7452     1.2872     0.1311      0.0    
   300       730.89     0.7271     0.7247     1.2552     0.1485      0.0    
   400      1024.25     0.7409     0.7343     1.188      0.1937      0.0    
   500      1317.97     0.7353     0.7261     1.0035     0.3296      0.0    
   600      1611.52     0.7418     0.7311     1.007      0.5165      0.0    
   700      1905.14     0.7383     0.7246     0.9223     1.1226      0.0    
   800      2198.74     0.7354     0.7178     0.9724     3.297       0.0    
   900      2492.39     0.7352     0.7223     0.8901    18.5801      0.0    
   1000     2785.92     0.7572     0.7423     0.9854    494.2917     0.0    
   1100     3079.67     0.7559     0.7408     0.8619    72.1659      0.0    
   1200     3373.22     0.8179     0.7951     1.0027    515.912      0.0    
   1300     3666.77     1.0156     1.0137     1.1781    1719.55      0.0    
   1400     3960.37     1.3638     1.3643     1.3633   10279.0416    0.0    
   1500     4254.31     1.4116     1.4104     1.577    3714.8874     0.0    
   1600     4550.04     1.4208     1.4178     1.3955   9301.3589     0.0    
   1700     4846.57     1.4136     1.4129     1.0776   13278.2189    0.0    
   1800     5143.19     1.4148     1.4145     1.1951   11939.7162    0.0    
   1900     5439.95     1.4164     1.4143     3.1481   8813.9832     0.0    
   2000     5736.87     1.4091     1.4148     1.2635   6346.9327     0.0    
   2100     6034.86     1.4188     1.4103     1.0836   8056.2595     0.0    
   2200     6331.79     1.411      1.4154     1.3099   19042.2348    0.0    
   2300     6628.76     1.4149     1.4147     0.9907   14707.2728    0.0    
   2400     6925.61     1.4207     1.4185     1.0906   64410.9246    0.0    
   2500     7222.35     1.4092     1.4136     1.1058   11217.4978    0.0    
   2600     7519.07     1.4178     1.4124     1.1476   2119.1949     0.0    
   2700     7815.82     1.4114     1.4118     1.0751   1876.9096     0.0    
   2800      8112.7     1.415      1.4111     1.1402   3342.9321     0.0    
   2900     8409.73     1.4161     1.4162     2.9152   1805.6967     0.0    
   3000     8706.57     1.4122     1.4185     1.2769   3411.0527     0.0    
   3100     9003.58     1.4179     1.4153     2.1303   11994.4331    0.0    
   3200      9300.6     1.4165     1.4193     1.4252   217355.7309    0.0    
   3300      9597.6     1.4111     1.4141     1.5641   14778.2073    0.0    
   3400     9894.85     1.4117     1.4152     1.5332   152754.7693    0.0    
   3500     10192.12    1.4196     1.4113     1.4991   291384.603    0.0    
   3600     10489.25    1.4194     1.4151     1.2148   788183.2535    0.0    
   3700     10786.34    1.4152     1.415      1.123    20194.1624    0.0    
   3800     11083.34    1.4089     1.4158     1.3028   23242.7476    0.0    
   3900     11380.44    1.4156     1.4176     1.0202   37209.2727    0.0    
   4000     11677.49    1.4038     1.4141     1.0547   185313.6015    0.0    
