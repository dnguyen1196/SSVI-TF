Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.4546217918396
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       93.86      0.7491     0.7279     3.1254     0.7039      0.0    
   200       251.8      0.7172     0.6928     2.913      0.2231      0.0    
   300       410.19     0.7096     0.6893     2.8772     0.1516      0.0    
   400       569.47     0.7171     0.7067     3.0002     0.1079      0.0    
   500       727.56     0.7227     0.6991     2.9294     0.0867      0.0    
   600       884.93     0.7104     0.685      2.7163     0.0847      0.0    
   700      1041.88     0.7061     0.6859     2.6029     0.0701      0.0    
   800      1199.11     0.7097     0.693      2.5345     0.0696      0.0    
   900      1355.89     0.7165     0.7041     2.6589     0.0655      0.0    
   1000      1512.6     0.7176     0.705      2.6492     0.0845      0.0    
   1100     1669.23     0.7157     0.6917     2.6024     0.0583      0.0    
   1200     1825.75     0.7177     0.7043     2.3408     0.0729      0.0    
   1300     1982.22     0.7182     0.6927     2.5704     0.0932      0.0    
   1400     2138.69     0.7222     0.6958     2.5109     0.0867      0.0    
   1500     2295.11     0.7214     0.6973     2.1036     0.0743      0.0    
   1600     2451.34     0.7198     0.7004     2.2784     0.0444      0.0    
   1700      2607.6     0.718      0.7047     1.9262     0.0621      0.0    
   1800      2763.8     0.7174     0.7046     1.9685     0.064       0.0    
   1900     2920.23     0.7174     0.7067     2.332      0.0526      0.0    
   2000     3076.65     0.7171     0.7011     1.9128     0.0417      0.0    
   2100      3232.9     0.7163     0.7031     1.8955     0.0699      0.0    
   2200     3389.26     0.7155     0.7016     1.8869     0.0543      0.0    
   2300     3545.59     0.7177     0.7061     2.1406     0.0481      0.0    
   2400     3701.95     0.7164     0.7051     1.8068     0.0361      0.0    
   2500     3858.41     0.7162     0.7009     2.199      0.0284      0.0    
   2600     4014.87     0.7165     0.6977     1.6528     0.0386      0.0    
   2700     4171.25     0.7178     0.7011     1.532      0.0349      0.0    
   2800     4327.71     0.7171     0.7009     1.6911     0.0348      0.0    
   2900     4484.18     0.718      0.7011     1.6697     0.0227      0.0    
   3000     4640.54     0.7165     0.7044     1.8088     0.0404      0.0    
   3100     4796.88     0.7171     0.7023     1.7112     0.018       0.0    
   3200     4953.15     0.717      0.7006     1.5348     0.0194      0.0    
   3300     5109.59     0.7169     0.7029     1.5139     0.0321      0.0    
   3400     5266.24     0.7164     0.6966     1.3529     0.0239      0.0    
   3500     5422.83     0.718      0.7004     1.8411     0.0434      0.0    
   3600      5578.9     0.7163     0.6996     1.6195     0.0212      0.0    
   3700     5733.52     0.7155     0.699      1.4606     0.019       0.0    
   3800     5888.16     0.7165     0.6996     1.4211     0.0302      0.0    
   3900     6042.78     0.7171     0.701      1.5103     0.0212      0.0    
   4000     6197.42     0.7176     0.6994     1.6462     0.0194      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       94.02      0.7691     0.7528     3.1063     0.1989      0.0    
   200       279.78      0.72      0.7099     2.9878     0.1218      0.0    
   300       467.04     0.7163     0.7075     2.9503     0.1122      0.0    
   400       653.26     0.7172     0.7084     2.981      0.0981      0.0    
   500       837.94     0.7172     0.7084     2.8216     0.1146      0.0    
   600      1022.55     0.7172     0.7084     2.8892     0.1215      0.0    
   700      1207.06     0.7172     0.7084     3.093      0.1335      0.0    
   800      1391.55     0.7172     0.7084     2.5269     0.1176      0.0    
   900      1575.94     0.7172     0.7084     2.3806     0.1253      0.0    
   1000     1760.45     0.7172     0.7084     2.4716     0.1333      0.0    
   1100     1944.99     0.7172     0.7084     2.4243     0.164       0.0    
   1200     2129.65     0.7172     0.7084     2.3508     0.1662      0.0    
   1300     2314.15     0.7172     0.7084     2.555      0.1408      0.0    
   1400     2498.62     0.7172     0.7084     2.3826     0.1518      0.0    
   1500     2683.13     0.7172     0.7084     2.057      0.1495      0.0    
   1600     2867.39     0.7172     0.7084     2.0772     0.1443      0.0    
   1700     3051.45     0.7172     0.7084     1.9025     0.1406      0.0    
   1800     3235.53     0.7172     0.7084     2.3401     0.157       0.0    
   1900     3419.58     0.7172     0.7084     1.6896     0.1479      0.0    
   2000     3603.68     0.7172     0.7084     1.722      0.1494      0.0    
   2100     3787.69     0.7172     0.7084     1.6848     0.1479      0.0    
   2200     3971.79     0.7172     0.7084     2.1284     0.1846      0.0    
   2300     4154.04     0.7172     0.7084     2.0095     0.1763      0.0    
   2400     4333.88     0.7172     0.7084     1.7191     0.1513      0.0    
   2500     4514.43     0.7172     0.7084     1.6416     0.1239      0.0    
   2600     4695.48     0.7172     0.7084     2.3816     0.1477      0.0    
   2700     4876.75     0.7172     0.7084     1.7926     0.1891      0.0    
   2800     5060.65     0.7172     0.7084     1.6688     0.1609      0.0    
   2900     5244.53     0.7172     0.7084     1.5662     0.2034      0.0    
   3000     5428.61     0.7172     0.7084     1.9467     0.1264      0.0    
   3100      5613.2     0.7172     0.7084     1.3802     0.1439      0.0    
   3200     5797.37     0.7172     0.7084     2.0026     0.1189      0.0    
   3300     5981.47     0.7172     0.7084     1.3204     0.1448      0.0    
   3400     6165.56     0.7172     0.7084     1.4178     0.1614      0.0    
   3500     6349.53     0.7172     0.7084     1.3815     0.1389      0.0    
   3600     6533.53     0.7172     0.7084     1.4683     0.1221      0.0    
   3700     6717.46     0.7172     0.7084     1.562      0.0991      0.0    
   3800     6901.37     0.7172     0.7084     1.2717     0.1127      0.0    
   3900     7085.39     0.7172     0.7084     1.2272     0.0954      0.0    
   4000     7269.41     0.7172     0.7084     1.1554     0.1042      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       93.97      0.8212     0.8025     3.0752     0.1313      0.0    
   200       306.85     0.7154     0.704      3.028      0.1426      0.0    
   300       521.43     0.7206     0.7084     3.0019     0.1253      0.0    
   400       735.17     0.7172     0.706      2.958      0.1118      0.0    
   500       947.6      0.7172     0.706      2.7658     0.1144      0.0    
   600      1156.66     0.7172     0.706      2.7788     0.1157      0.0    
   700       1361.4     0.7172     0.706      2.4988     0.1212      0.0    
   800       1561.2     0.7172     0.706      2.5912     0.1074      0.0    
   900      1761.44     0.7172     0.706      2.6189     0.118       0.0    
   1000     1962.52     0.7172     0.706      2.6308     0.1211      0.0    
   1100      2163.6     0.7172     0.706      2.532      0.1188      0.0    
   1200     2363.84     0.7172     0.706      2.7703     0.1199      0.0    
   1300     2563.39     0.7172     0.706      1.9541     0.1398      0.0    
   1400     2760.72     0.7172     0.706      2.476      0.1534      0.0    
   1500     2958.95     0.7172     0.706      2.7093     0.1949      0.0    
   1600     3158.35     0.7172     0.706      2.0494     0.115       0.0    
   1700     3356.78     0.7172     0.706      1.9392     0.1522      0.0    
   1800     3553.53     0.7172     0.706      2.0664     0.1493      0.0    
   1900      3752.0     0.7172     0.706      2.2439     0.1358      0.0    
   2000     3954.27     0.7172     0.706      2.0597     0.1773      0.0    
   2100     4156.59     0.7172     0.706      2.7933     0.1588      0.0    
   2200      4359.0     0.7172     0.706      2.3385     0.1509      0.0    
   2300     4561.53     0.7172     0.706      1.4664     0.1446      0.0    
   2400     4764.41     0.7172     0.706      2.006      0.1225      0.0    
   2500     4964.48     0.7172     0.706      1.7493     0.1349      0.0    
   2600     5161.14     0.7172     0.706      1.427      0.1512      0.0    
   2700     5357.89     0.7172     0.706      1.6468     0.1415      0.0    
   2800     5554.87     0.7172     0.706      1.3635     0.1818      0.0    
   2900     5751.56     0.7172     0.706      1.8778     0.1251      0.0    
   3000     5948.31     0.7172     0.706      1.4882     0.1269      0.0    
   3100     6145.09     0.7172     0.706      1.8166     0.1838      0.0    
   3200     6341.77     0.7172     0.706      1.4715     0.1044      0.0    
   3300     6538.45     0.7172     0.706      2.0648     0.0952      0.0    
   3400     6735.21     0.7172     0.706      1.6056     0.1191      0.0    
   3500     6931.84     0.7172     0.706      1.2714     0.1338      0.0    
   3600     7128.51     0.7172     0.706      1.5519     0.1053      0.0    
   3700     7325.59     0.7172     0.706      1.2909     0.0904      0.0    
   3800     7524.01     0.7172     0.706      2.3077     0.1149      0.0    
   3900     7721.08     0.7172     0.706      1.3818     0.1024      0.0    
   4000      7917.7     0.7172     0.706      1.3076     0.1015      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       87.38      0.7969     0.7852     3.0865     0.1193     0.0001  
   200       311.69     0.7208     0.7083     2.9079     0.1358      0.0    
   300       537.47     0.7159     0.7007     2.9152     0.1217      0.0    
   400       762.51     0.7127     0.6972     2.8203     0.1151      0.0    
   500       986.84     0.7172     0.7033     2.8507     0.1114      0.0    
   600      1212.76     0.7109     0.6964     2.6979     0.1261      0.0    
   700      1441.29     0.7172     0.7035     2.815      0.1122      0.0    
   800      1666.32     0.7172     0.7035     2.7096     0.1234      0.0    
   900       1889.2     0.7172     0.7035     2.7138     0.1084      0.0    
   1000     2112.19     0.7172     0.7035     2.6163     0.1509      0.0    
   1100     2337.91     0.7172     0.7035     2.4619     0.1214      0.0    
   1200     2565.44     0.7172     0.7035     2.7767     0.1534      0.0    
   1300     2788.25     0.7172     0.7035     2.2546     0.1388      0.0    
   1400     3011.68     0.7172     0.7035     2.4024     0.1418      0.0    
   1500     3234.63     0.7172     0.7035     2.4956     0.1524      0.0    
   1600     3457.65     0.7172     0.7035     2.2028     0.1653      0.0    
   1700     3685.07     0.7172     0.7035     2.1813     0.1548      0.0    
   1800     3912.58     0.7172     0.7035     1.877      0.1708      0.0    
   1900     4138.45     0.7172     0.7035     1.7704     0.1399      0.0    
   2000      4361.4     0.7172     0.7035     2.0273     0.2254      0.0    
   2100     4584.27     0.7172     0.7035     1.7556     0.123       0.0    
   2200     4807.79     0.7172     0.7035     2.4438     0.1534      0.0    
   2300     5032.76     0.7172     0.7035     2.1661     0.1956      0.0    
   2400     5259.07     0.7172     0.7035     1.4762     0.1446      0.0    
   2500     5481.86     0.7172     0.7035     1.7686     0.1541      0.0    
   2600     5704.66     0.7172     0.7035     1.7679     0.1332      0.0    
   2700     5928.84     0.7172     0.7035     2.017      0.1471      0.0    
   2800      6157.0     0.7172     0.7035     1.5357     0.1339      0.0    
   2900     6384.41     0.7172     0.7035      1.55      0.1204      0.0    
   3000     6607.85     0.7172     0.7035     1.9968     0.1245      0.0    
   3100     6830.69     0.7172     0.7035     2.0921     0.1226      0.0    
   3200     7053.51     0.7172     0.7035     1.6938     0.1302      0.0    
   3300     7276.28     0.7172     0.7035     1.3103     0.109       0.0    
   3400     7499.08     0.7172     0.7035     1.3922     0.0888      0.0    
   3500     7721.86     0.7172     0.7035     1.5171     0.1405      0.0    
   3600     7946.08     0.7172     0.7035     1.5392     0.0947      0.0    
   3700     8168.82     0.7172     0.7035     1.8268     0.1402      0.0    
   3800     8391.82     0.7172     0.7035     1.1606     0.1815      0.0    
   3900      8614.5     0.7172     0.7035     1.6849     0.1266      0.0    
   4000     8837.26     0.7172     0.7035     1.2445     0.0767      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100        91.7      0.8885     0.8757     3.1195     0.1224     0.0001  
   200       368.81     0.7218     0.7129     2.935      0.1423      0.0    
   300       648.48     0.7179     0.7023     2.9487     0.1236      0.0    
   400       928.03     0.7172     0.7044     2.8887     0.115       0.0    
   500      1177.36     0.7172     0.7044     2.8809     0.1149      0.0    
   600      1426.04     0.7209     0.7068     3.0231     0.114       0.0    
   700      1674.62     0.7172     0.7044     2.7907     0.1093      0.0    
   800      1925.63     0.7172     0.7044     2.5312     0.1101      0.0    
   900      2179.25     0.7172     0.7044      2.36      0.1126      0.0    
   1000     2429.78     0.7172     0.7044     2.5312     0.1357      0.0    
   1100     2678.32     0.7172     0.7044     2.2106     0.1215      0.0    
   1200     2927.26     0.7172     0.7044     2.2109     0.1393      0.0    
   1300     3177.08     0.7172     0.7044     2.2252     0.1292      0.0    
   1400     3426.59     0.7172     0.7044     2.1262     0.1257      0.0    
   1500     3681.92     0.7172     0.7044     2.5998     0.158       0.0    
   1600     3930.43     0.7172     0.7044     2.0594     0.1325      0.0    
   1700     4178.84     0.7172     0.7044     1.9119     0.1603      0.0    
   1800     4427.94     0.7172     0.7044     1.7685     0.1364      0.0    
   1900     4677.13     0.7172     0.7044     2.304      0.1953      0.0    
   2000     4931.79     0.7172     0.7044     1.8808     0.1497      0.0    
   2100     5181.04     0.7172     0.7044     2.234      0.1612      0.0    
   2200      5429.4     0.7172     0.7044     2.089      0.1661      0.0    
   2300     5678.07     0.7172     0.7044     2.2208     0.1723      0.0    
   2400     5926.48     0.7172     0.7044     2.1025     0.1696      0.0    
   2500     6175.02     0.7172     0.7044     1.6815     0.1293      0.0    
   2600     6423.69     0.7172     0.7044     2.1656     0.153       0.0    
   2700     6672.38     0.7172     0.7044     1.8611     0.1586      0.0    
   2800     6920.93     0.7172     0.7044     1.8147     0.1154      0.0    
   2900     7170.14     0.7172     0.7044     1.5843     0.1337      0.0    
   3000      7420.8     0.7172     0.7044     1.5376     0.1481      0.0    
   3100     7671.97     0.7172     0.7044     1.7515     0.1288      0.0    
   3200     7923.47     0.7172     0.7044     1.4736     0.1074      0.0    
   3300     8174.77     0.7172     0.7044     1.4399     0.1715      0.0    
   3400     8423.79     0.7172     0.7044     1.3839     0.0984      0.0    
   3500     8671.89     0.7172     0.7044     1.4852     0.0979      0.0    
   3600     8919.95     0.7172     0.7044     1.441      0.1139      0.0    
   3700     9167.96     0.7172     0.7044     1.8571     0.1229      0.0    
   3800     9416.28     0.7172     0.7044     1.6335     0.099       0.0    
   3900     9668.17     0.7172     0.7044     1.3796     0.0901      0.0    
   4000      9913.3     0.7172     0.7044     1.6262     0.084       0.0    
