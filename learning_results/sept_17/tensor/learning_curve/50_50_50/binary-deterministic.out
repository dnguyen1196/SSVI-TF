Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  18.15574550628662
Evaluation for true params: 
 test_rsme | train_rsme | rel-te-err | rel-tr-err |  test_nll  |  train_nll |
0.7319016327 0.7379701891   0.13392      0.13615      24593.08     97734.26  
Using  0.01  of training data 
Using  0.01 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         0.27      1.4129     0.499      1.3372     0.447      3.1623     1.2047     0.373      0.1214    18611.94    714.75    -4714.0   -4700.152    14.23    
    10       40.38      1.4091     0.4964     1.2083     0.365      3.1623     1.8177     3.4868     0.9432    18456.45    656.55    -4300.0   -4286.011    14.255   
    25       91.27      1.4108     0.4976     0.9798      0.24      3.1623     2.417      7.9052     2.1274    18401.8     574.63    -3685.0   -3670.835    14.195   
   500       349.75     1.3494     0.4552      0.0        0.0       1.4812     0.7396     5.3812     1.9905    24641.9     46.17      -940.0    -921.002    18.559   
   1000      642.47     1.2832     0.4117      0.0        0.0       1.2495     0.5391     5.3198     1.6548    26978.74    27.15      -724.0    -706.195    18.152   
   1500      932.6      1.2276     0.3767      0.0        0.0       1.392      0.6157     6.2857     1.5999    28472.97    22.34      -686.0    -668.901    17.535   
   2000     1221.31     1.1973     0.3584      0.0        0.0       1.1409     0.5911     6.5638     1.5803    31396.89    19.93      -648.0    -631.235    16.638   
   2500     1507.28     1.171      0.3428      0.0        0.0       1.1143     0.8787     7.6648     1.7418    34341.79    18.97      -594.0    -577.433    16.439   
   3000     1792.81     1.1577     0.3351      0.0        0.0       1.2211     1.2641     8.0484     1.8441    36789.38    17.74      -622.0    -604.973    16.653   
   3500      2079.7     1.1409     0.3254      0.0        0.0       1.2893     0.9522    10.3756     1.8527    39661.34    16.91      -578.0    -560.751    16.79    
   4000     2368.89     1.135      0.3221      0.0        0.0       1.3423     1.7775    11.1719     2.0171    42145.73    15.52      -539.0    -521.324    17.18    
   4500      2653.3     1.1289     0.3186      0.0        0.0       1.055      1.3866    11.8522     1.8456    44978.58    16.04      -563.0    -546.124    17.193   
   5000     2932.66     1.1223     0.3149      0.0        0.0       1.2927     2.3696    13.6358     1.6259    47541.13    16.01      -560.0    -542.995    17.322   
   5500     3212.91     1.1211     0.3142      0.0        0.0       1.2631     2.3173     14.296     1.9849    49374.91    15.87      -598.0    -580.113    17.858   
   6000     3493.74     1.1161     0.3114      0.0        0.0       1.1852     5.0917    15.6339     2.2146    52227.75    15.95      -600.0    -581.822    17.977   
   6500     3774.15     1.1138     0.3102      0.0        0.0       1.2599     2.3723    18.1121     2.0433    54470.48    16.52      -638.0    -620.444    17.848   
   7000     4057.17     1.1084     0.3072      0.0        0.0       1.9001     3.595     19.3514     1.8305    54132.55    17.87      -577.0    -559.074    17.97    
   7500      4340.9     1.1074     0.3066      0.0        0.0       1.2046     4.8765    20.9646     2.7134    56928.28    18.55      -614.0    -595.935    18.098   
   8000     4621.05     1.1041     0.3048      0.0        0.0       1.1679     4.1144    22.5745     1.4768    58805.08    17.36      -614.0    -595.86     18.251   
   8500      4901.5     1.1027     0.304       0.0        0.0       1.276      3.1506    23.4528     1.9428    59125.08    18.35      -636.0    -617.301    18.205   
   9000     5180.74     1.0995     0.3022      0.0        0.0       1.2267     2.7352    26.1382     2.8385    60736.19     18.1      -642.0    -623.367    18.473   
   9500     5460.07     1.1007     0.3029      0.0        0.0       1.1026     6.7536    27.5661     1.6133    63580.91     18.6      -653.0    -634.324    18.507   
  10000     5739.52     1.0973     0.301       0.0        0.0       1.0694    11.9951    30.6893     2.4045    63852.47    18.76      -668.0     -649.3     18.51    
Using  0.02  of training data 
Using  0.02 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         0.57      1.4161     0.5014     1.4121     0.4985     3.1623     2.1154    31.2129     2.5637    18665.19   1469.74    -9727.0   -9709.001    17.555   
    10       45.37      1.4136     0.4996     1.2853     0.413      3.1623     2.1154    29.5291     3.1956    18601.52   1364.61    -8469.0   -8451.472    18.027   
    25       104.32     1.4139     0.4998     1.1349     0.322      3.1623     2.1154    27.4877     4.3683    18604.94   1230.07    -6618.0   -6599.105    18.901   
   500       546.4      1.0443     0.2726      0.0        0.0       1.2312     0.1927     4.8832     8.5469    21096.17    73.55      -701.0    -683.167    17.934   
   1000     1043.19     1.0021     0.251       0.0        0.0       1.3052     0.1212     4.1293     9.0128    26079.37    47.45      -586.0    -567.773    18.379   
   1500     1535.64     0.9911     0.2456      0.0        0.0       1.1589     0.1027     4.3965     9.3219    32098.97    43.38      -629.0    -608.785    20.408   
   2000     2027.22     0.9799      0.24       0.0        0.0       1.3017     0.0915     5.1614     9.4806    38252.76    37.28      -695.0    -672.33     22.412   
   2500     2517.88     0.977      0.2386      0.0        0.0       1.0095     0.0985     5.8256     9.7461    42901.91    35.02      -745.0     -720.7     24.532   
   3000      3002.6     0.9766     0.2384      0.0        0.0       1.2335     0.0971     6.653      10.073    49923.15    33.83      -823.0    -795.835    27.138   
   3500     3486.21     0.9769     0.2386      0.0        0.0       1.1972     0.0877     7.6207    10.2355    56483.18    33.74      -773.0    -743.226    29.926   
   4000     3970.88     0.9769     0.2386      0.0        0.0       1.1041     0.0996     8.4728    10.5726    62834.42    31.52      -849.0    -815.477    33.243   
   4500     4452.84     0.9795     0.2398      0.0        0.0       1.1286     0.1225    10.0338     11.289    65967.88    32.27      -868.0    -832.513    35.39    
   5000     4934.05     0.9761     0.2382      0.0        0.0       0.9728     0.1387     9.8796    11.7299    72483.75    30.17      -859.0    -822.18     37.256   
   5500      5416.6     0.9769     0.2386      0.0        0.0       1.0885     0.1318    10.8525     11.713    76606.68    29.41      -832.0    -792.458    39.099   
   6000     5900.72     0.9792     0.2397      0.0        0.0       1.1188     0.2039    11.3669    12.3304    81375.89    28.67      -888.0     -848.1     40.294   
   6500      6384.3     0.9809     0.2405      0.0        0.0       1.197      0.2206    12.7345    12.8899    84470.62    28.31      -928.0    -886.562    41.69    
   7000     6866.13     0.9801     0.2402      0.0        0.0       1.086      0.1958    15.2795    12.7042    86712.83    28.35      -895.0    -852.093    42.826   
   7500     7347.16     0.9822     0.2412      0.0        0.0       1.1687     0.3337    15.4143    13.3116    89682.95    25.82      -969.0    -924.487    44.155   
   8000     7827.88     0.9833     0.2417      0.0        0.0       0.9091     0.1356     15.208    13.6326    91811.93    25.55      -900.0    -855.196    45.068   
   8500     8311.43     0.9861     0.2431      0.0        0.0       0.9709     0.3396     17.275    14.7684    94504.12    26.29      -915.0    -869.371    46.068   
   9000     8790.75     0.9878     0.2439      0.0        0.0       1.1047     0.4275    16.7245     15.617    96788.23    24.95      -859.0    -811.823    46.914   
   9500     9270.78     0.9872     0.2436      0.0        0.0       0.9098     0.4304    16.7361    15.3271   100197.88     24.0      -812.0    -764.559    47.648   
  10000     9746.77     0.9893     0.2447      0.0        0.0       0.9484     0.5718    19.8758     15.886    100863.6    24.64      -828.0    -779.551    48.615   
Using  0.05  of training data 
Using  0.05 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         1.45      1.4184     0.503      1.4168     0.5018     3.1623     3.0029    20.4006    15.7809    18580.27   3705.69    -23179.0  -23169.362   9.887    
    10       62.55      1.4176     0.5024     1.3511     0.4564     3.1623     3.0029     27.151    15.1893    18648.24   3540.08    -18702.0  -18692.027   10.297   
    25       144.82     1.4121     0.4985     1.2696     0.403      3.1623     3.0029    32.6549    14.3067    18817.13   3377.13    -13089.0  -13077.682   11.442   
   500      1097.38     0.9197     0.2114     0.4176     0.0436     1.5043     0.041      7.8829    37.1982    12365.46    794.1     -1501.0   -1487.836    13.221   
   1000     2193.14     0.9489     0.2251     0.049      0.0006     1.1316     0.0256     6.975     52.2948    24886.39    215.58     -697.0    -676.321    20.243   
   1500     3300.01     0.959      0.2299      0.0        0.0       0.7771     0.0133     4.9248    63.4087    35413.46    114.17     -445.0    -422.536    22.401   
   2000      4403.3     0.9665     0.2335      0.0        0.0       0.8106     0.0092     4.4575     70.321    41159.49    95.01      -419.0    -396.282    22.487   
   2500     5506.42     0.9645     0.2326      0.0        0.0       0.9295     0.0068     5.2156    75.1021    43799.94    88.96      -429.0    -406.178    22.499   
   3000     6606.24     0.9617     0.2312      0.0        0.0       0.7214     0.0052     4.6319    75.3404    47463.05    80.52      -407.0    -385.072    22.287   
   3500     7704.24     0.9632     0.2319      0.0        0.0       0.7417     0.0048     4.7467    76.3703    49970.13    78.15      -391.0    -368.367    22.203   
   4000      8798.1     0.9587     0.2298      0.0        0.0       0.856      0.0045     5.0416    78.0596    52822.66    79.09      -456.0    -433.899    22.116   
   4500     9890.74     0.9592      0.23       0.0        0.0       0.7033     0.0044     5.5788    78.2423    55852.13    77.47      -491.0    -469.223    22.018   
   5000     10984.18    0.9636     0.2321      0.0        0.0       0.8284     0.0043     5.7546    79.9138    59250.39    81.79      -527.0    -505.337    21.692   
   5500     12078.14    0.9642     0.2324      0.0        0.0       0.8096     0.0046     5.9424    79.4478    62138.69     80.4      -577.0    -555.264    21.55    
   6000     13171.17    0.9685     0.2345      0.0        0.0       1.0237     0.0043     7.5963    79.9109    66140.63    82.56      -681.0    -660.031    21.352   
   6500     14263.93    0.9671     0.2338      0.0        0.0       1.0026     0.0045     8.2651    81.2658    69403.92    86.38      -782.0    -761.371    20.875   
   7000     15357.17    0.9689     0.2347      0.0        0.0       0.821      0.0042     7.8227    82.8186    71772.65    88.75      -869.0    -847.652    21.013   
   7500     16449.71    0.9679     0.2342      0.0        0.0       1.1699     0.0043    10.1073    82.8585    75721.18     95.9     -1113.0   -1092.918     20.4    
   8000     17540.78    0.9686     0.2346     0.0566     0.0008     2.3912     0.0041    16.5642    84.9392    79956.87    112.26    -1356.0   -1334.873    20.767   
   8500     18632.68    0.9737     0.237       0.0        0.0       0.9309     0.0038     9.4138    87.1928    82601.32    92.04     -1382.0   -1361.937    20.357   
   9000     19725.55    0.9723     0.2363      0.0        0.0       0.9278     0.0037    11.0239    88.8413    82021.27    94.42     -1433.0   -1413.239    19.932   
   9500     20825.17    0.9746     0.2374     0.049      0.0006     2.1446     0.0038    14.4847    90.2966    86620.29    99.21     -1519.0   -1498.748    20.128   
  10000     21920.25    0.9727     0.2366      0.0        0.0       1.0663     0.0038    11.3782    93.0045    86348.04    98.81     -1627.0   -1607.423    19.704   
Using  0.1  of training data 
Using  0.1 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         1.83      1.4072     0.4951     1.4064     0.4945     3.1623     3.6108    13.1713    91.7826    18616.13   7443.68    -48206.0  -48196.042   10.07    
    10       79.19      1.4099     0.497      1.3771     0.4741     3.1623     3.6108    29.6212     76.677    18878.43   7325.41    -37944.0  -37933.894   10.575   
    25       182.61     1.4133     0.4994     1.3415     0.4499     3.1623     3.6108    50.2921     55.443    19319.18   7246.66    -25519.0  -25506.459   12.135   
   500      1366.82     0.8684     0.1885     0.7526     0.1416     1.398      0.0543    13.0714    54.0736    10341.31   3059.41    -3787.0   -3770.602    16.045   
   1000     2709.39     0.8859     0.1962     0.6027     0.0908     1.3535     0.0221    15.7366    71.4865    11724.41   2253.85    -3286.0   -3265.277    21.014   
   1500      4078.6     0.919      0.2112     0.5028     0.0632     1.3138     0.0193    18.1828    87.3838    14626.02   1723.72    -2900.0    -2876.13    23.569   
   2000     5461.35     0.937      0.2195     0.4285     0.0459     1.1329     0.0155    20.6564    99.1153    19170.55   1351.45    -2547.0   -2521.203    26.135   
   2500     6852.12     0.9608     0.2308     0.3527     0.0311     1.1023     0.0145    22.2355    117.0699   25235.24    1080.8    -2321.0   -2291.407    29.141   
   3000     8245.72     0.9597     0.2302     0.3053     0.0233     1.0558     0.0139    23.4564    126.6897   33179.28    875.84    -1996.0   -1964.231    31.539   
   3500     9642.36     0.9676     0.234      0.2877     0.0207     1.1934     0.0117    27.5572    141.1218   41266.82    771.53    -1934.0   -1901.732    32.119   
   4000     11039.03    0.9699     0.2352     0.2349     0.0138     0.9829      0.01     25.4338    153.1103   50145.24    615.87    -1582.0   -1548.811    33.662   
   4500     12435.37    0.9755     0.2379     0.2098     0.011      0.9049     0.0071    24.5502    167.8234   58324.22    524.05    -1424.0    -1388.92    34.853   
   5000     13834.04    0.9801     0.2402     0.1939     0.0094     0.8962     0.0065     25.098    182.3387   65006.95    443.55    -1268.0   -1231.619    36.372   
   5500     15233.96    0.9803     0.2402     0.1673     0.007      0.8841     0.0055    24.0754    193.0051   71419.52    383.7     -1115.0    -1078.47    36.995   
   6000     16630.99    0.9841     0.2421     0.1414     0.005      0.674      0.0046    21.7215    198.8342   77401.51    313.29     -989.0    -950.824    38.038   
   6500     18026.23    0.9886     0.2443     0.1296     0.0042     0.6155     0.0041    21.4747    208.2463   82575.05    279.71     -899.0    -860.269    38.948   
   7000     19421.15    0.9887     0.2444     0.098      0.0024     0.723      0.0039    19.6447    212.4893   87901.28    242.03     -806.0    -765.974    40.104   
   7500     20814.93    0.9913     0.2457     0.0721     0.0013     0.6059     0.0031    17.7643    225.2973   91302.15    207.45     -739.0    -698.365    40.531   
   8000     22207.33    0.9894     0.2447     0.0566     0.0008     0.555      0.0028     17.228    234.0837   95190.01    185.06     -666.0    -625.145    41.068   
   8500     23600.03    0.9915     0.2458     0.0447     0.0005     0.5487     0.0025     17.262    239.1656   98326.18    167.16     -641.0    -599.325    41.663   
   9000     24992.66    0.9882     0.2441      0.04      0.0004     0.585      0.0023    16.2133    244.2141   100850.9    152.04     -591.0    -549.453    41.867   
   9500     26383.28    0.9925     0.2462      0.0        0.0       0.5254     0.0019    15.5896    249.361   102837.07    136.64     -534.0    -491.661    42.196   
  10000     27774.59    0.9896     0.2448      0.0        0.0       0.5146     0.0017    14.1895    252.8282   104314.0    130.12     -505.0    -462.667    42.482   
Using  0.2  of training data 
Using  0.2 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         1.85      1.406      0.4942     1.4125     0.4988     3.1623     3.9783    17.6095    244.7242   18652.05   14915.52   -96559.0  -96548.978   9.915    
    10       103.66     1.4085     0.496      1.4005     0.4904     3.1623     3.9783    55.1783    198.4196   19172.56   15056.63   -74022.0  -74011.078    10.5    
    25       238.11     1.4155     0.5009     1.3793     0.4756     3.1623     3.9783    98.6438    137.2396   20209.97   15417.49   -48351.0  -48338.79    12.407   
   500      1451.75     0.8627     0.1861     0.8037     0.1615     1.5619     0.0449    24.2793    79.3768    9902.17     7020.0    -7760.0    -7742.53    17.367   
   1000     2827.46     0.846      0.1789     0.7632     0.1456     1.4319     0.0268    27.5338    100.8449    9985.4    6391.36    -7415.0   -7397.789    17.624   
   1500     4217.46     0.8524     0.1816     0.7291     0.1329     1.4783     0.0169    31.2051    114.0762   10352.31   6003.34    -7203.0   -7183.255    19.365   
   2000     5610.35     0.8548     0.1827     0.6883     0.1184     1.1483     0.017     30.0163    125.7948   10470.55   5550.57    -6785.0   -6762.745    22.235   
   2500     7007.23     0.859      0.1845     0.6759     0.1142     1.0209     0.0099    32.7866    130.1752   10609.69   5353.83    -6636.0   -6613.037    22.535   
   3000     8408.79     0.8568     0.1835     0.6744     0.1137     1.0081     0.0097    34.4776    145.0393   10999.6    5208.23    -6515.0   -6491.428    23.258   
   3500     9813.31     0.8646     0.1869     0.6611     0.1092     0.8172     0.0095    33.5498    155.279    11225.01   5103.43    -6436.0   -6413.039    22.923   
   4000     11217.71    0.8743     0.1911     0.6464     0.1044     0.8274     0.0129    34.4377    162.7871   11500.75   4986.84    -6375.0   -6350.607    24.312   
   4500     12624.99    0.8742     0.191      0.6518     0.1062     0.9021     0.0109     37.039    167.3982   11618.19   4946.19    -6318.0   -6295.118    23.181   
   5000     14030.21    0.8689     0.1888     0.6417     0.103      0.7176     0.0095    34.7381    171.6071   11822.78   4820.47    -6193.0   -6168.105    24.629   
   5500     15436.42    0.8743     0.1911     0.6261     0.098      0.8007     0.005     36.2634    179.5521   11907.99   4784.97    -6118.0   -6093.566    24.509   
   6000     16841.76    0.8825     0.1947     0.6294     0.099      0.6736     0.0072    35.6674    190.2915   12202.34    4710.6    -6077.0   -6051.937    25.425   
   6500     18249.16    0.8809     0.194      0.631      0.0996     0.6765     0.005     38.2062    198.7279   12390.25    4699.1    -6104.0   -6077.772    26.217   
   7000     19655.68    0.8825     0.1947     0.625      0.0976     0.7368     0.0045    37.9098    191.9224   12427.53   4646.27    -6015.0   -5988.559    26.715   
   7500     21059.25    0.8779     0.1927     0.6161     0.0949     0.7556     0.0044    38.0364    207.4063   12666.15   4571.87    -5935.0   -5907.302    27.465   
   8000     22461.68    0.8855     0.196      0.6156     0.0948     0.6713     0.0042    38.0034    208.2722   12846.74   4539.41    -5885.0   -5857.145    27.964   
   8500     23865.18    0.8833     0.195      0.6102     0.0931     0.5894     0.0042    37.0561    216.5983   12951.48   4483.16    -5801.0   -5772.459    28.531   
   9000     25270.88    0.8822     0.1946     0.6125     0.0938     0.5451     0.004     37.5153    216.7643   13259.45   4474.04    -5819.0   -5790.216    29.007   
   9500     26672.95    0.8979     0.2016     0.6106     0.0932     0.7205     0.0038    39.4752    223.1996   13389.55   4462.51    -5809.0   -5779.237    29.412   
  10000     28074.78    0.8882     0.1972     0.6109     0.0933     0.6615     0.0036    40.3113    231.0646   13413.09   4438.25    -5820.0   -5790.041    29.802   
Using  0.5  of training data 
Using  0.5 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         1.84      1.4153     0.5008     1.4087     0.4961     3.1623     4.2592    50.7868    227.581    18746.75   37385.5   -246208.0  -246196.929   10.674   
    10       176.51     1.4164     0.5015     1.4084     0.4959     3.1623     4.2592    142.3782   193.7894   19525.41   38610.19  -184790.0  -184778.355   11.346   
    25       405.34     1.416      0.5012     1.4021     0.4914     3.1623     4.2592    249.0478   149.0465   20839.07   40874.28  -118467.0  -118453.496   13.602   
   500       1713.1     0.8642     0.1867     0.8337     0.1738     1.0761     0.036      55.333    189.3035   9771.77    18753.8    -19568.0  -19540.74    27.555   
   1000     3196.16     0.8504     0.1808     0.8216     0.1688     1.2037     0.0224    61.5311    209.5972   9605.14    17907.16   -19002.0  -18977.32    24.836   
   1500      4685.1     0.8482     0.1798     0.797      0.1588     1.3858     0.0126     64.945    223.4544   9493.69    17232.29   -18485.0  -18460.471   24.849   
   2000     6176.61     0.8347     0.1742     0.776      0.1505     1.0335     0.0111    66.5886    240.5761    9384.3    16738.1    -18103.0  -18078.602   24.85    
   2500     7669.57     0.8351     0.1744     0.7751     0.1502     1.2147     0.0077    65.0953    244.6226    9359.1    16574.51   -17937.0  -17913.161   23.993   
   3000     9165.85     0.826      0.1706     0.7681     0.1475     0.8022     0.0106    63.4352    263.8937   9348.96    16378.21   -17765.0  -17741.796   23.142   
   3500     10666.0     0.8268     0.1709     0.7681     0.1475     1.1385     0.0089     71.883    278.0622   9369.87    16352.05   -17788.0  -17763.818   24.674   
   4000     12160.17    0.8271     0.171      0.7664     0.1468     0.756      0.0085    71.7556    284.184    9432.13    16245.5    -17737.0  -17711.267   26.183   
   4500     13657.41    0.8308     0.1726     0.7611     0.1448     0.9134     0.0047    72.4447     282.56    9461.39    16153.46   -17635.0  -17609.882   25.062   
   5000     15155.83    0.8254     0.1703     0.7575     0.1435     0.7126     0.0042    66.9355    303.7779   9371.17    15947.7    -17398.0  -17374.377   23.864   
   5500     16655.92    0.824      0.1698     0.7606     0.1446     0.7687     0.0033    71.7768     312.84    9395.25    15979.67   -17450.0  -17423.764   25.794   
   6000     18153.91    0.8236     0.1696     0.7531     0.1418     0.8004     0.0055    73.2098    327.7406   9366.94    15914.96   -17372.0  -17347.287   24.333   
   6500     19652.82     0.83      0.1722     0.7487     0.1401     0.6001     0.0034    72.8609    333.3164   9353.48    15782.66   -17233.0  -17207.876   25.41    
   7000     21153.19    0.8244     0.1699     0.7553     0.1426     0.6569     0.0029    79.8906    349.859    9391.79    15805.99   -17262.0  -17237.391   24.421   
   7500     22658.0     0.8257     0.1704     0.7521     0.1414     0.755      0.0036    72.2994    362.8659    9358.6    15760.12   -17268.0  -17244.211   23.752   
   8000     24164.39    0.8272     0.1711     0.7447     0.1387     0.6387     0.0031    74.5744    376.7637   9380.05    15746.01   -17271.0  -17244.106   26.811   
   8500     25668.4     0.8262     0.1706     0.7478     0.1398     0.5978     0.0032    78.3138    388.8066   9407.78    15691.81   -17263.0  -17239.052   24.385   
   9000     27169.84    0.8252     0.1702     0.7443     0.1385     0.5259     0.0037     73.141    402.307    9329.07    15595.42   -17126.0  -17100.985   24.908   
   9500     28670.28    0.8198     0.168      0.7477     0.1398     0.5238     0.0029    71.2808    372.6237   9315.66    15628.69   -17132.0  -17106.464   26.018   
  10000     30173.73    0.8241     0.1698     0.7465     0.1393     0.5929     0.0025    69.1286    397.3892   9336.47    15572.18   -17117.0  -17091.799   25.405   
Using  1  of training data 
Using  1 of training data
Factorizing with max_iteration = 10000  fixing covariance?:  False
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
Using diagonal covariance?:  False
Random start? True
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |rel-te-err|train_rsme|rel-tr-err|   d_mean  |   d_cov  |avg_grad_m|avg_grad_c| test_nll | train_nll |    VLB   |  E-term  |    KL     
    0         1.86      1.4095     0.4966     1.4111     0.4978     3.1623     4.3594    89.3298    390.8307   18572.05   74486.57  -479702.0  -479690.155   11.662   
    10       300.91     1.4058     0.494      1.4097     0.4968     3.1623     4.3596    264.8806   337.4836   19339.24   77324.35  -359508.0  -359494.831   12.738   
    25       688.13     1.4054     0.4938     1.406      0.4942     3.1623     4.3596    482.4809   270.1466   20943.18   82843.12  -227077.0  -227060.911   15.914   
   500       2224.4     0.856      0.1832     0.8475     0.1796     1.1964     0.0229    113.0248   364.2483   9802.97    38750.61   -39627.0  -39584.814   42.56    
   1000     4036.48     0.8393     0.1761     0.8235     0.1695     1.2183     0.0161    107.6815   362.5753   9525.07    37102.86   -38284.0  -38248.55    35.24    
   1500     5849.24     0.831      0.1726     0.8093     0.1638     1.2144     0.0122    121.4844   378.0771    9366.5    36038.25   -37547.0  -37515.771   30.93    
   2000     7674.85     0.8216     0.1688     0.7991     0.1596     0.901      0.0095    109.4153   394.4421   9151.46    35226.17   -36783.0  -36752.54    30.548   
   2500     9500.68     0.8147     0.1659     0.7931     0.1573     1.1859     0.0088    121.649    402.8945   9079.64    34958.78   -36661.0  -36630.308   30.416   
   3000     11329.04    0.8158     0.1664     0.7905     0.1562     0.8316     0.0079    119.6307   394.3986   9094.21    34500.95   -36223.0  -36193.534   29.184   
   3500     13164.52    0.8121     0.1649     0.7847     0.154      0.7269     0.0068    117.3327   400.3633   9074.81    34226.43   -36001.0  -35969.758   31.306   
   4000     14999.23    0.8128     0.1652     0.7826     0.1531     1.1145     0.005     120.9088   412.8441   9026.18    33999.46   -35726.0  -35696.938   28.764   
   4500     16838.33    0.8143     0.1658     0.7851     0.1541     0.8468     0.0051    128.1588   421.9461   9178.32    34015.84   -35814.0  -35784.33    30.114   
   5000     18676.99    0.8148     0.166      0.784      0.1537     0.7423     0.0046    129.6374   440.7952   9101.38    33867.66   -35633.0  -35601.211   31.594   
   5500     20520.27    0.8037     0.1615     0.7753     0.1503     0.8002     0.0046    127.1399   432.2514   9035.21    33511.31   -35263.0  -35232.434   30.088   
   6000     22359.43    0.8093     0.1638     0.7762     0.1506     0.6436     0.0051    123.1789   433.3186   8985.17    33424.69   -35209.0  -35178.544   30.121   
   6500     24191.84    0.8027     0.1611     0.7726     0.1492     0.5963     0.0046    126.3021   450.8546   8947.84    33159.71   -34940.0  -34910.881   29.611   
   7000     26022.66    0.8086     0.1634     0.7709     0.1486     0.6183     0.0038    128.5341   452.5787   8983.46    33205.88   -34948.0  -34917.335   30.818   
   7500     27861.58    0.8127     0.1651     0.7762     0.1506     0.8454     0.0036    128.8261   461.3576   9042.76    33250.36   -35015.0  -34983.708   31.651   
   8000     29693.84    0.8046     0.1618     0.7703     0.1484     0.6531     0.0025    127.9484   460.8431   8969.37    33032.2    -34781.0  -34748.225   33.092   
   8500     31521.46    0.8005     0.1602     0.7691     0.1479     0.5226     0.0034    126.6136   465.0666   8922.67    32945.17   -34747.0  -34715.093   31.527   
   9000     33346.55    0.8111     0.1645     0.7743     0.1499     0.6618     0.0039    134.4626   484.4734   8988.62    33120.0    -34863.0  -34833.085   30.333   
   9500     35173.35    0.8019     0.1608     0.7697     0.1481     0.6845     0.0026    134.8363   499.7997   8926.96    32846.01   -34636.0  -34605.152   31.341   
  10000     36994.13    0.8132     0.1653     0.7705     0.1484     0.6978     0.0019     136.11    503.726    8967.18    32890.16   -34643.0  -34612.889   30.24    
