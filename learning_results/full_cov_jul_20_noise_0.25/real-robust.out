Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  8.399418592453003
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       426.67    531.4359   533.4483    3.1623     0.1368      0.0    
   200       861.59    523.3169   523.8315    2.9838      0.0        0.0    
   300      1295.72    519.5909   520.2346    2.3326      0.0        0.0    
   400      1728.35    519.038    519.3444    2.0896      0.0        0.0    
   500      2159.38    518.6264   519.2031    2.1224      0.0        0.0    
   600      2591.51    518.9666   519.2073    2.2466      0.0        0.0    
   700      3022.96    517.2627   518.1024    1.744       0.0        0.0    
   800      3453.79    518.7182   518.9471    1.7819      0.0        0.0    
   900      3885.74    517.6692   517.9963    1.6406      0.0        0.0    
   1000     4317.52    518.4429   517.7973    1.6445      0.0        0.0    
   1100     4750.01    518.2886   518.9844    1.4503      0.0        0.0    
   1200     5180.63    518.3559   518.5691    1.4263      0.0        0.0    
   1300     5611.83    517.2388   518.071     1.2964      0.0        0.0    
   1400     6041.57    517.7739   518.2213    1.5594      0.0        0.0    
   1500      6472.1    516.9372   517.5848    1.8061      0.0        0.0    
   1600     6902.43    518.1925   518.4842    1.2092      0.0        0.0    
   1700     7334.84    517.2274   517.8048    1.3312      0.0        0.0    
   1800     7766.08    517.4224   517.5035    1.5126      0.0        0.0    
   1900     8196.51    517.0623   517.7192    1.3027      0.0        0.0    
   2000     8628.05    515.9494   517.3298    1.2761      0.0        0.0    
   2100     9059.82    516.4934   517.287     1.4751      0.0        0.0    
   2200     9492.07    515.6693   517.5189    1.1131      0.0        0.0    
   2300     9922.71    515.5328   517.2351     1.17       0.0        0.0    
   2400     10354.44   516.3675   516.9887    1.0438      0.0        0.0    
   2500     10786.11   516.4867   517.5422    1.6547      0.0        0.0    
   2600     11216.51   516.9774   516.9963    1.2608      0.0        0.0    
   2700     11644.52   515.7199   517.0572    1.2411      0.0        0.0    
   2800     12073.93   517.5185   517.2694    1.2042      0.0        0.0    
   2900     12502.91   516.9536   517.3725    1.1834      0.0        0.0    
   3000     12933.92   515.6365   516.9601    0.9688      0.0        0.0    
   3100     13363.25    516.41    516.8916    1.0561      0.0        0.0    
   3200     13794.81   516.1004   517.0833    1.2896      0.0        0.0    
   3300     14226.51   515.7087   516.484     1.068       0.0        0.0    
   3400     14657.03   516.2149   516.5578    1.1186      0.0        0.0    
   3500     15095.15   515.9521   516.7892    0.9981      0.0        0.0    
   3600     15525.59   515.8942   516.9423    1.1153      0.0        0.0    
   3700     15955.65   515.6162   516.5847    1.0179      0.0        0.0    
   3800     16383.77   516.7016   516.2829    1.2824      0.0        0.0    
   3900     16812.96   516.3264   516.7834    0.8945      0.0        0.0    
   4000     17242.22   515.6237   516.7481    1.2861      0.0        0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       422.32    523.0104   525.5971    2.8764      0.0        0.0    
   200       850.6     520.4638   520.5626    1.8676      0.0        0.0    
   300      1280.83    517.9861   519.5894    2.0189      0.0        0.0    
   400      1711.85    517.1411   519.7944    1.4835      0.0        0.0    
   500      2143.23    516.5833   519.2103    2.0036      0.0        0.0    
   600      2571.65    517.3984   519.5145    1.7452      0.0        0.0    
   700      2998.89    517.0678   518.7832    1.4773      0.0        0.0    
   800      3428.02    517.0922   518.6874    1.7686      0.0        0.0    
   900      3859.31    515.5733   517.8191    1.6371      0.0        0.0    
   1000     4289.39    515.6429   518.0408    1.4924      0.0        0.0    
   1100      4719.2    515.8204   517.9913    1.3317      0.0        0.0    
   1200     5150.49    516.3874   517.7345    1.5518      0.0        0.0    
   1300     5580.93    517.6728   518.9088    1.1829      0.0        0.0    
   1400     6009.92    515.6736   518.6726    1.1989      0.0        0.0    
   1500     6440.43    516.4047   517.8744    1.0446      0.0        0.0    
   1600      6870.8    514.5325   517.4462    1.6508      0.0        0.0    
   1700     7298.11    516.1995   517.2798    1.3671      0.0        0.0    
   1800     7724.74    517.2913   517.9866    1.2021      0.0        0.0    
   1900     8153.12    516.0035   517.9464    1.3105      0.0        0.0    
   2000     8581.59    515.3031   518.0592     1.2        0.0        0.0    
   2100     9009.72    515.4673   517.1526    1.3937      0.0        0.0    
   2200     9438.26    515.2923   517.6407    1.2668      0.0        0.0    
   2300     9867.32    516.3187   517.9809    1.0985      0.0        0.0    
   2400     10296.88   515.6771   517.2788    1.0147      0.0        0.0    
   2500     10725.25   515.2904   518.0479    1.0353      0.0        0.0    
   2600     11153.92   515.2699   517.4411    0.9044      0.0        0.0    
   2700     11581.17   515.5589   517.2905    0.9615      0.0        0.0    
   2800     12009.5     515.72    517.3501    1.1594      0.0        0.0    
   2900     12434.25   515.8523   517.3807    1.0375      0.0        0.0    
   3000     12860.74   514.9786   517.385     1.2039      0.0        0.0    
   3100     13289.41   516.662    517.7516    1.1387      0.0        0.0    
   3200     13714.23   515.2933   517.7963    1.0801      0.0        0.0    
   3300     14133.25   515.4086   516.6909    0.9526      0.0        0.0    
   3400     14551.4    515.6825   517.1463    1.0278      0.0        0.0    
   3500     14970.37   514.5716   517.2273    0.9737      0.0        0.0    
   3600     15387.39   515.6789   517.4666    1.0767      0.0        0.0    
   3700     15805.16   515.4565   517.4297    1.3742      0.0        0.0    
   3800     16224.8    514.8655   516.9992    0.9239      0.0        0.0    
   3900     16639.11   515.2907   517.0402    0.732       0.0        0.0    
   4000     17046.51   514.157    516.5694    1.1524      0.0        0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       403.87    526.6234   528.3271    2.5029      0.0        0.0    
   200       825.16    518.0193   519.6115    1.3375      0.0        0.0    
   300      1237.08    517.8146   518.7121    1.5653      0.0        0.0    
   400      1649.79    515.4623   517.9959    1.5491      0.0        0.0    
   500       2061.5    514.8057   517.7557    1.511       0.0        0.0    
   600      2472.57    516.1154   517.6976    1.1381      0.0        0.0    
   700      2882.37    515.3553   517.2949    1.1408      0.0        0.0    
   800      3294.01    515.549    517.1856    1.1797      0.0        0.0    
   900      3698.75    515.0317   516.8105    1.0578      0.0        0.0    
   1000     4100.22    514.5798   517.4719    1.2529      0.0        0.0    
   1100     4502.48    515.1397   517.6126    1.3448      0.0        0.0    
   1200     4907.32    514.3208   517.1779    1.267       0.0        0.0    
   1300     5320.87    516.1221   518.1266    1.2375      0.0        0.0    
   1400      5733.8    515.2472   517.4604    1.3733      0.0        0.0    
   1500     6148.86    515.8231   517.3171    1.1525      0.0        0.0    
   1600     6562.44    514.7272   517.125     1.026       0.0        0.0    
   1700      6970.2    515.5358   517.2502    1.1813      0.0        0.0    
   1800     7371.27    515.4712   517.5075    0.9117      0.0        0.0    
   1900     7774.55    514.7555   517.0715    1.0929      0.0        0.0    
   2000     8175.89    514.5501   516.943     1.1184      0.0        0.0    
   2100     8576.96    514.8822   517.309     1.0729      0.0        0.0    
   2200     8976.35    514.6716   517.0053    1.3004      0.0        0.0    
   2300     9374.95    514.9372   516.7349    0.8415      0.0        0.0    
   2400     9776.04    514.7682   516.7865    1.1396      0.0        0.0    
   2500     10179.63   514.4621   516.9975    0.8797      0.0        0.0    
   2600     10591.6    514.6628   517.0424    0.7049      0.0        0.0    
   2700     10996.05   513.687    516.8904    0.8693      0.0        0.0    
   2800     11399.56   515.0491   517.0951    0.9956      0.0        0.0    
   2900     11799.99   514.2171   516.7355    0.7145      0.0        0.0    
   3000     12201.32   514.1531   517.1601    0.8849      0.0        0.0    
   3100     12601.13   515.7758   517.9097    0.9496      0.0        0.0    
   3200     13001.36   515.2533   517.6132    0.9491      0.0        0.0    
   3300     13410.84   514.3875   516.5688    1.0925      0.0        0.0    
   3400     13822.1    514.3428   516.4353    0.9015      0.0        0.0    
   3500     14231.62   514.8668   517.1707    1.0567      0.0        0.0    
   3600     14641.77   515.2001   516.7894    0.8649      0.0        0.0    
   3700     15041.82   514.6737   516.5002    0.7092      0.0        0.0    
   3800     15443.0    514.6232   516.8724    1.0129      0.0        0.0    
   3900     15843.38   514.8736   516.7251    0.8084      0.0        0.0    
   4000     16242.82   515.1557   516.6395    0.7646      0.0        0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       397.36    529.473    531.5729    2.3987      0.0        0.0    
   200       800.02    517.364    519.6894    1.104       0.0        0.0    
   300      1201.53    515.9443   517.8293    0.9349      0.0        0.0    
   400      1603.98    515.8877   518.113     1.1435      0.0        0.0    
   500      2005.54    515.3754   518.1976    1.1446      0.0        0.0    
   600       2407.6    514.4298    517.2      0.9626      0.0        0.0    
   700       2822.4    514.4557   517.2682    1.3232      0.0        0.0    
   800      3237.97    514.5892   517.2417    1.0417      0.0        0.0    
   900      3652.03    513.8838   516.9999    1.0305      0.0        0.0    
   1000     4063.56    514.6644   517.5911    1.2371      0.0        0.0    
   1100     4478.44    514.6183   517.3129    0.916       0.0        0.0    
   1200     4891.27    514.5908   517.563     1.0139      0.0        0.0    
   1300     5304.07    513.7793   516.9073    0.8496      0.0        0.0    
   1400     5717.95    514.3102   516.9118    0.9597      0.0        0.0    
   1500     6130.51    515.026    516.7647    1.1854      0.0        0.0    
   1600     6540.01    514.7942   516.8049    0.7608      0.0        0.0    
   1700     6941.09    514.7151   517.2304    1.1287      0.0        0.0    
   1800     7344.12    514.5357   516.9493    0.8656      0.0        0.0    
   1900      7746.9    514.7693   516.7745    1.1242      0.0        0.0    
   2000     8151.36    515.2486   517.2479    1.0917      0.0        0.0    
   2100     8560.11    514.7024   517.2376    0.8042      0.0        0.0    
   2200     8973.91    514.5073   517.3407    0.851       0.0        0.0    
   2300     9388.11    514.471    517.4012    1.2748      0.0        0.0    
   2400     9800.97    514.518    516.8157    0.8197      0.0        0.0    
   2500     10214.18   514.8183   517.1063    0.888       0.0        0.0    
   2600     10627.73   515.0239   517.1858    0.754       0.0        0.0    
   2700     11042.55   514.9089   517.2095    1.0547      0.0        0.0    
   2800     11455.39   515.0858   517.4516    0.9984      0.0        0.0    
   2900     11867.02   514.4498   517.2297    0.7992      0.0        0.0    
   3000     12281.03   514.4797   516.8979    0.8068      0.0        0.0    
   3100     12695.78   513.8771   516.4109    0.8122      0.0        0.0    
   3200     13107.91   513.3167   516.8048    0.7799      0.0        0.0    
   3300     13523.41   514.1832   517.0178    0.7024      0.0        0.0    
   3400     13930.88   514.0766   516.9778    0.8185      0.0        0.0    
   3500     14334.05   515.0496   517.3388    1.1562      0.0        0.0    
   3600     14736.54   514.553    517.1062    0.9101      0.0        0.0    
   3700     15140.66   514.6168   516.995     1.0062      0.0        0.0    
   3800     15553.97   513.8847   516.7755    0.7882      0.0        0.0    
   3900     15960.77   514.1863   516.9314    0.8901      0.0        0.0    
   4000     16369.7    514.2143   517.1998    0.7553      0.0        0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       393.65    533.9006   535.2831    1.6512      0.0        0.0    
   200       801.92    517.5907   519.7934    1.0831      0.0        0.0    
   300      1213.46    515.1422   517.6008    1.0508      0.0        0.0    
   400      1625.14    513.899    516.8807    1.3162      0.0        0.0    
   500      2037.79    514.8725   517.0089    1.0645      0.0        0.0    
   600      2444.95    514.8938   517.0307    1.0532      0.0        0.0    
   700      2849.92    515.0789   516.8291    0.8282      0.0        0.0    
   800      3250.91    515.015    516.8046    0.8071      0.0        0.0    
   900      3652.94    515.3751   517.1522    1.1321      0.0        0.0    
   1000     4059.66    514.6672   516.8632    1.0251      0.0        0.0    
   1100      4472.2    513.9793   516.7926    0.7634      0.0        0.0    
   1200     4873.04    514.3442   516.7704    1.2449      0.0        0.0    
   1300     5275.88    514.4918   516.8011    0.8454      0.0        0.0    
   1400     5675.77    514.4229   516.6702    0.875       0.0        0.0    
   1500     6080.11    514.6996   517.0001    0.839       0.0        0.0    
   1600     6483.73    513.8634   516.573     1.0033      0.0        0.0    
   1700     6885.47    514.363    516.8973    0.8938      0.0        0.0    
   1800     7289.39    515.0147   516.7664    0.8134      0.0        0.0    
   1900     7694.64    514.5066   516.5997    0.9745      0.0        0.0    
   2000      8099.3    514.7959   516.689     0.7757      0.0        0.0    
   2100     8503.98    514.3743   516.7002    0.9574      0.0        0.0    
   2200     8906.16    514.1277   516.5825    0.7951      0.0        0.0    
   2300     9308.88    514.7074   516.6685    0.8788      0.0        0.0    
   2400     9712.12    513.5896   516.6448    0.9967      0.0        0.0    
   2500     10115.31   513.8639   516.4102    0.5939      0.0        0.0    
   2600     10519.47    514.06    516.6917    0.7404      0.0        0.0    
   2700     10923.3    514.1351   516.7503    0.7793      0.0        0.0    
   2800     11327.31   514.0011   516.5018    0.6694      0.0        0.0    
   2900     11730.8    514.1956   516.4337    0.767       0.0        0.0    
   3000     12134.07   513.896    516.4247    0.8455      0.0        0.0    
   3100     12536.51   514.207    516.7264    0.9015      0.0        0.0    
   3200     12939.44   515.0948   516.6293    0.982       0.0        0.0    
   3300     13342.64   513.7544   516.4041    0.8426      0.0        0.0    
   3400     13742.03   514.343    516.4678    0.8445      0.0        0.0    
   3500     14138.78   513.9467   516.4009    0.7014      0.0        0.0    
   3600     14537.74   514.2246   516.3011    0.6704      0.0        0.0    
   3700     14936.01   514.0188   516.4673    0.8713      0.0        0.0    
   3800     15333.79   514.404    516.6529    0.8694      0.0        0.0    
   3900     15732.13   514.2867   516.6296    0.7495      0.0        0.0    
   4000     16131.04   514.2324   516.2943    0.7261      0.0        0.0    
