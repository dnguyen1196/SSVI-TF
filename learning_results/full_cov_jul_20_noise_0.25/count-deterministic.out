Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  4.943557024002075
max_count =  27  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       155.69     4.3933     4.4302     0.313      0.2667   130091.05  105455.69  
   200       442.84     4.342      4.3783     0.2947     0.0233   121081.07   98117.62  
   300       727.84     4.1557     4.1908     0.2819     0.0218    99187.94   80347.88  
   400      1012.09     3.7684     3.8011     0.2597     0.0163    72986.82   59107.11  
   500      1297.06     3.2819     3.3121     0.2249     0.0104    53674.61   43489.31  
   600       1582.8     2.8416     2.8678     0.1898     0.0112    42667.08   34578.73  
   700      1867.71     2.5006     2.5233     0.2075     0.0119    36903.38   29910.51  
   800      2152.84     2.2425     2.2645     0.1668     0.0112    33678.66   27293.95  
   900      2438.76     2.0547     2.0743     0.2122     0.0111    31814.01   25778.95  
   1000     2723.84     1.9109     1.929      0.0829     0.0103    30610.18   24809.11  
   1100     3009.44     1.8066     1.8237     0.1492     0.0097    29884.53   24213.85  
   1200     3294.41     1.7324     1.7462     0.1193     0.0095    29370.37   23804.36  
   1300     3580.52     1.6746     1.6882      0.17      0.009     29043.12   23535.46  
   1400     3866.06     1.6319     1.6456     0.142      0.0082    28814.17   23345.32  
   1500     4151.12     1.601      1.6117     0.1076     0.0075    28651.99   23202.82  
   1600     4436.05     1.5776     1.5875     0.1873     0.007     28540.06   23107.98  
   1700     4721.96     1.5606     1.5682     0.0752     0.0065    28453.73   23032.53  
   1800     5006.74     1.5462     1.5536     0.1325     0.006     28387.11   22983.87  
   1900     5291.48     1.5345     1.5427     0.0991     0.0057    28316.83   22934.81  
   2000     5577.73     1.5249     1.5343     0.0979     0.0054    28275.02   22903.97  
   2100     5863.01     1.5166     1.5245     0.1174     0.0055    28244.97   22875.21  
   2200     6147.69     1.5116     1.5199     0.1319     0.0049    28220.86   22847.93  
   2300     6433.95     1.5076     1.5152     0.0845     0.0043    28205.22   22837.36  
   2400     6719.84     1.5031     1.5106     0.1572     0.0043    28183.76   22817.43  
   2500     7004.69     1.5015     1.5069     0.0709     0.0043    28167.43   22799.58  
   2600      7291.2     1.4974     1.5012     0.122      0.004     28173.11   22798.08  
   2700      7577.6     1.4937     1.4996     0.1311     0.0039    28133.46   22778.5   
   2800     7862.89     1.4933     1.4979     0.0656     0.0041    28128.54   22769.53  
   2900     8147.54     1.4913     1.4945     0.0813     0.0034    28127.6    22764.67  
   3000     8434.03     1.4894     1.4937     0.1138     0.0035    28108.91   22754.37  
   3100     8719.81     1.4882     1.4931     0.0953     0.0032    28112.53   22749.61  
   3200     9006.27     1.4873     1.4947     0.0755     0.0037    28101.97   22754.2   
   3300     9292.26     1.4894     1.4895     0.1413     0.003     28124.44   22759.74  
   3400     9577.84     1.487      1.4906     0.1198     0.0029    28101.4    22747.53  
   3500     9864.44     1.4861     1.4882     0.1031     0.0031    28109.99   22748.64  
   3600     10151.51    1.4852     1.4872     0.0744     0.0029    28108.27   22743.83  
   3700     10438.62    1.4867     1.4892     0.1149     0.0025    28105.22   22742.28  
   3800     10724.19    1.4844     1.4858     0.0627     0.0028    28092.08   22728.84  
   3900     11011.18    1.4846     1.4891     0.0852     0.0023    28101.38   22736.32  
   4000     11298.62    1.4838     1.4862     0.0641     0.0024    28096.66   22739.5   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       153.22     4.3676     4.3859     0.0419     0.9592    94664.63  152890.05  
   200       496.59     4.389      4.4087     0.0262     0.2916   113637.36  183497.55  
   300       841.0      4.3932     4.412      0.0233     0.1659   120054.28  193819.16  
   400      1184.44     4.3942     4.4137     0.0248     0.1102   123082.65  198686.21  
   500      1526.92     4.3952     4.4144     0.0236     0.0783   124813.19  201466.12  
   600      1869.91     4.3954     4.4145     0.025      0.0608    125914.2   203234.4  
   700      2212.53     4.3958     4.415      0.0242     0.0492   126674.58  204455.16  
   800      2555.87     4.3958     4.4152     0.0252     0.0401   127227.77   205343.2  
   900      2899.09     4.396      4.4153     0.0256     0.0338   127639.94  206004.79  
   1000     3242.71     4.3961     4.4153     0.0266     0.0295   127952.68  206506.66  
   1100     3585.32     4.3959     4.4154     0.0267     0.0257   128187.32  206883.02  
   1200     3927.93     4.3959     4.4152     0.029      0.0227   128355.98  207153.26  
   1300     4272.32     4.3957     4.415      0.0303     0.0202   128459.71  207318.96  
   1400     4613.88     4.3951     4.4146     0.0346     0.0183   128496.01  207376.07  
   1500     4956.97     4.3944     4.4139     0.0386     0.0166   128439.21  207283.64  
   1600     5298.42     4.3931     4.4127     0.0434     0.0152   128249.87  206976.78  
   1700     5640.92     4.391      4.4102     0.0528     0.0142   127839.49  206313.21  
   1800     5983.94     4.3865     4.4059     0.0665     0.0134   127011.36   204976.6  
   1900     6324.43     4.3777     4.397      0.0943     0.0131   125347.97  202287.85  
   2000     6665.98     4.3587     4.3779     0.1204     0.0129    121953.4   196804.8  
   2100     7006.78     4.3152     4.3341     0.1538     0.0143   115025.34  185616.71  
   2200     7347.67     4.218      4.236      0.185      0.0184   102709.28  165722.01  
   2300     7688.82     4.0313     4.0492     0.2001     0.0227    85958.68  138685.13  
   2400     8025.01     3.7516     3.7671     0.2041     0.0231    69217.89  111702.85  
   2500     8362.45     3.4196     3.4345     0.1632     0.0194    55996.51   90444.36  
   2600     8700.16     3.0936     3.1077     0.1358     0.0159    46921.13   75843.63  
   2700     9038.45     2.8097     2.8229     0.138      0.0145    41099.72   66461.37  
   2800     9376.43     2.5701     2.5804      0.1       0.0124    37288.98   60340.22  
   2900     9712.99     2.3744     2.3841     0.1019     0.0124    34767.35   56281.52  
   3000     10050.14    2.2118     2.2228     0.096      0.0117    33017.67   53476.52  
   3100     10384.65    2.0819     2.0916     0.0868     0.0109    31797.36   51520.09  
   3200     10719.0     1.9768     1.986      0.1021     0.0102    30927.93   50126.01  
   3300     11053.67    1.8897     1.8999     0.0927     0.0092    30276.91   49077.18  
   3400     11387.82    1.8228     1.8307     0.1008     0.0085    29794.5    48299.96  
   3500     11721.51    1.7655     1.774      0.0713     0.0086    29456.06   47747.39  
   3600     12055.28    1.7194     1.7268     0.2162     0.0088    29195.81   47327.95  
   3700     12388.2     1.6809     1.6891     0.0886     0.0068    28934.51   46938.43  
   3800     12720.47    1.6484     1.6574     0.0729     0.0065    28750.41   46650.19  
   3900     13052.07     1.62      1.6307     0.1123     0.0064    28623.9    46427.63  
   4000     13383.79    1.6027     1.6102     0.0978     0.0065    28520.35   46257.4   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       155.22     4.3492     4.3623     0.0327     0.9805    86194.15   208185.9  
   200       558.15     4.3836     4.3963     0.0207     0.3095   107894.27  260545.12  
   300       960.43     4.3909     4.4028     0.0153     0.1767   116321.05  280810.42  
   400      1364.34     4.393      4.4053     0.0159     0.1163   120456.57  290745.09  
   500       1765.8     4.394      4.4065     0.0135     0.0869   122839.63  296466.37  
   600       2168.6     4.3948     4.4071     0.0134     0.0662   124362.47  300121.95  
   700      2571.71     4.3952     4.4076     0.0136     0.0525   125424.07  302669.03  
   800      2974.09     4.3958     4.4078     0.0131     0.0439   126201.09  304533.78  
   900      3376.13     4.3956     4.4082     0.0135     0.0363   126791.65  305951.06  
   1000     3778.75     4.3958     4.4081     0.0137     0.0312   127253.78  307060.14  
   1100     4180.44     4.396      4.4084     0.0139     0.0276   127626.26   307953.9  
   1200     4579.51     4.396      4.4085     0.0145     0.0246   127932.89   308689.3  
   1300     4962.32     4.3962     4.4085     0.0149     0.0218   128185.62  309295.72  
   1400     5340.13     4.3961     4.4086     0.0138     0.0197   128400.06  309810.13  
   1500     5712.69     4.3964     4.4086     0.0136     0.0181   128581.66  310245.72  
   1600     6084.81     4.3965     4.4086     0.0149     0.0165   128736.02  310615.92  
   1700     6455.83     4.3964     4.4086     0.0144     0.0152   128867.44  310930.88  
   1800     6827.42     4.3962     4.4086     0.016      0.014    128978.69  311197.57  
   1900     7198.84     4.3965     4.4087     0.0164     0.0129   129070.59  311417.72  
   2000      7571.1     4.3962     4.4085     0.0176     0.012     129144.0  311593.29  
   2100     7943.68     4.3962     4.4085     0.0188     0.0114    129198.1  311722.59  
   2200     8314.88     4.396      4.4083     0.0209     0.0105   129228.56  311795.05  
   2300      8687.1     4.3957     4.408      0.0223     0.0099   129230.88  311799.66  
   2400     9059.13     4.3953     4.4075     0.0257     0.0095   129190.04  311700.69  
   2500     9431.25     4.3945     4.4068     0.0323     0.009    129080.09  311435.41  
   2600     9803.12     4.3929     4.4054     0.0375     0.0087   128843.58  310864.96  
   2700     10176.03    4.3904     4.4027     0.0454     0.0082   128341.06  309654.93  
   2800     10548.82    4.3846     4.3969     0.0606     0.0081   127259.87  307050.37  
   2900     10920.19    4.3723     4.3846     0.0895     0.0085   124974.38  301548.18  
   3000     11291.21    4.348      4.3603     0.0864     0.0097   120722.72  291308.31  
   3100     11670.46    4.3051     4.317      0.0907     0.0119   113983.82  275073.79  
   3200     12076.62    4.2365     4.2487     0.0985     0.0146    104994.1   253419.0  
   3300     12482.75    4.137      4.149      0.1012     0.0157    94556.77  228275.29  
   3400     12889.0     4.0046     4.0167     0.1051     0.0153    83796.98  202370.07  
   3500     13294.47    3.8425     3.8546     0.1015     0.014     73661.54   177941.0  
   3600     13699.9     3.6583     3.6701     0.092      0.0113    64658.87  156253.66  
   3700     14104.14    3.4677     3.4789     0.0897     0.009     57236.8   138362.95  
   3800     14507.9     3.2777     3.2892     0.086      0.0081    51308.59  124084.76  
   3900     14910.84    3.0959     3.1072     0.1044     0.0073    46634.09  112842.76  
   4000     15313.33    2.9289     2.9408     0.085      0.0067    43063.83  104237.91  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       155.82     4.3417     4.3514     0.0284     0.9904    82870.24  266721.32  
   200       616.59     4.3826     4.3931     0.0199     0.3247   105235.38   338582.8  
   300      1077.86     4.3895     4.401      0.0167     0.1773   114495.73  368296.58  
   400       1536.7     4.3921     4.4038     0.0146     0.1206   119129.01  383141.03  
   500      1995.95     4.3936     4.405      0.0138     0.0878   121829.45  391791.66  
   600      2454.99     4.3947     4.4059     0.0115     0.0673   123565.22  397351.52  
   700      2914.73     4.395      4.4063     0.0133     0.0543   124771.85  401216.94  
   800      3376.27     4.3954     4.4066     0.0117     0.0454   125657.66   404052.9  
   900      3836.57     4.3956     4.407      0.0109     0.0383   126332.13  406212.68  
   1000     4297.64     4.3958     4.4073     0.0122     0.0325   126862.94  407912.46  
   1100     4757.52     4.3961     4.4072     0.0113     0.0286   127289.01  409276.92  
   1200     5216.93     4.3961     4.4075     0.012      0.0252   127639.52   410399.0  
   1300     5676.63     4.3963     4.4075     0.0121     0.0224   127932.04  411335.43  
   1400     6136.03     4.3963     4.4076     0.0112     0.0202    128179.6  412127.83  
   1500     6596.37     4.3966     4.4077     0.0123     0.0185   128391.17  412804.96  
   1600     7055.65     4.3964     4.4077     0.0112     0.0167   128574.33  413391.35  
   1700      7516.3     4.3962     4.4079     0.0123     0.0155   128733.02  413899.22  
   1800     7976.27     4.3964     4.4079     0.0138     0.0142   128872.27  414344.73  
   1900     8434.34     4.3965     4.4079     0.012      0.0133   128994.43  414735.57  
   2000     8892.82     4.3968     4.4079     0.0116     0.0124   129101.63  415078.54  
   2100     9351.24     4.3966     4.4079     0.0135     0.0114   129195.12  415377.66  
   2200     9807.84     4.3965     4.4079     0.0135     0.0108   129276.92   415639.3  
   2300     10265.39    4.3965     4.4078     0.0141     0.0102   129347.67  415865.64  
   2400     10723.23    4.3964     4.4078     0.0156     0.0095   129406.67  416054.11  
   2500     11182.32    4.3963     4.4078     0.0157     0.0091   129455.19  416209.05  
   2600     11640.18    4.3964     4.4076     0.0164     0.0087   129490.76  416322.48  
   2700     12099.7     4.3962     4.4075     0.0181     0.0083   129511.85  416389.52  
   2800     12558.94    4.396      4.4074     0.0197     0.0078   129516.54  416403.67  
   2900     13018.14    4.3957     4.407      0.0215     0.0075   129498.22  416343.97  
   3000     13476.99    4.3952     4.4066     0.0222     0.0073   129448.41  416183.01  
   3100     13929.67    4.3947     4.406      0.0265     0.0069    129352.1  415872.68  
   3200     14382.38    4.3937     4.405      0.0323     0.0067   129179.17  415315.84  
   3300     14836.17    4.3919     4.4034     0.037      0.0065   128878.85  414350.19  
   3400     15291.94    4.3892     4.4005     0.0457     0.0064   128335.92  412604.98  
   3500     15745.85    4.3836     4.395      0.0624     0.0064   127311.11  409313.26  
   3600     16181.82    4.3726     4.384      0.0765     0.0067   125320.23  402917.52  
   3700     16612.82    4.3498     4.3613     0.0951     0.0077    121392.3  390300.76  
   3800     17040.18    4.3049     4.3164     0.1152     0.0098   114418.39  367911.09  
   3900     17464.54    4.225      4.2359     0.1221     0.0134   104014.77  334510.87  
   4000     17886.05    4.0969     4.1082     0.1319     0.0159    91177.29  293300.09  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       143.13     4.3377     4.3564     0.0318     1.0049    81165.3   327222.29  
   200       622.51     4.3803     4.4016     0.024      0.3224   103603.79  417736.15  
   300      1102.44     4.3889     4.4103     0.0195     0.1827   113441.61  457392.21  
   400      1581.31     4.3923     4.4132     0.0172     0.1225   118378.94  477282.85  
   500      2061.23     4.3937     4.4147     0.0136     0.0895   121265.85  488902.06  
   600      2541.38     4.3945     4.4159     0.0123     0.0705    123133.8  496422.13  
   700      3022.83     4.395      4.4164     0.011      0.0567    124424.5  501618.54  
   800      3501.26     4.3951     4.4166     0.0135     0.0455   125369.09  505421.75  
   900      3981.85     4.3956     4.417      0.0114     0.0388   126090.04  508324.28  
   1000     4461.86     4.3958     4.417      0.0131     0.0331   126655.95  510602.85  
   1100     4942.06     4.396      4.4172     0.0107     0.0291    127112.2  512440.27  
   1200     5421.83     4.396      4.4175     0.0123     0.0256   127486.12  513945.31  
   1300     5924.55     4.3962     4.4176     0.0104     0.0231   127797.99  515200.79  
   1400     6442.82     4.3963     4.4177     0.0108     0.0207   128063.39  516268.91  
   1500     6962.91     4.3962     4.4177     0.0101     0.0188   128291.01  517185.11  
   1600     7479.62     4.3965     4.4178     0.0112     0.017    128487.96  517977.86  
   1700     7968.05     4.3963     4.4179     0.0113     0.0157   128660.19  518671.02  
   1800      8453.6     4.3965     4.4179     0.0104     0.0145   128811.78  519281.17  
   1900     8938.45     4.3966     4.4179     0.0104     0.0135   128945.08  519817.74  
   2000     9426.19     4.3967     4.418       0.01      0.0126   129064.51  520298.43  
   2100      9911.9     4.3969     4.418      0.0109     0.0116   129170.79  520726.17  
   2200     10400.2     4.3964     4.418      0.011      0.011    129265.91  521108.98  
   2300     10887.6     4.3966     4.418      0.0105     0.0104   129351.23  521452.17  
   2400     11374.81    4.3967     4.4181     0.0112     0.0096   129427.14  521757.71  
   2500     11860.24    4.3967     4.418      0.012      0.0092   129494.69  522029.74  
   2600     12373.6     4.3966     4.4179     0.0124     0.0088   129554.28  522269.55  
   2700     12888.84    4.3967     4.418      0.0129     0.0083   129605.83  522476.89  
   2800     13405.49    4.3966     4.4179     0.0136     0.0079   129649.08  522650.85  
   2900     13922.65    4.3965     4.4178     0.0134     0.0076    129683.6  522789.53  
   3000     14439.5     4.3965     4.4178     0.0138     0.0071   129708.76  522890.48  
   3100     14954.83    4.3963     4.4176     0.0149     0.0068   129722.64  522946.14  
   3200     15470.39    4.3962     4.4175     0.0171     0.0067   129722.31  522944.46  
   3300     15984.86    4.396      4.4172     0.0194     0.0064   129701.83  522861.95  
   3400     16500.39    4.3955     4.4168     0.0207     0.0061   129651.53  522659.21  
   3500     17016.19    4.3949     4.4163     0.0246     0.006    129559.25  522287.13  
   3600     17532.03    4.394      4.4153     0.027      0.0058    129392.4  521615.41  
   3700     18049.6     4.3922     4.4136     0.038      0.0056   129085.52  520378.55  
   3800     18568.88    4.3892     4.4106     0.0457     0.0057   128516.09  518086.13  
   3900     19089.28    4.3834     4.4045     0.058      0.0057   127393.44  513568.41  
   4000     19608.25    4.3707     4.3921     0.0759     0.0062   125121.88  504420.82  
