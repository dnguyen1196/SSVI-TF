Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  3.484346389770508
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       126.83    537.124    537.5815    3.1622     0.3108      0.0    
   200       259.4     528.2947   526.5579    2.9844     0.1462      0.0    
   300       391.54    522.8274   523.6952    2.7841     0.0479      0.0    
   400       523.28    521.4077   522.6391    2.039      0.0197      0.0    
   500       654.93    520.3979   521.4569    1.9827     0.0187      0.0    
   600       784.46    520.8109   521.137     2.2161     0.0093      0.0    
   700       914.9     520.4184   520.0289    2.0949     0.0079      0.0    
   800      1046.27    520.6229   519.9093    1.7029     0.0065      0.0    
   900      1178.29    520.1309   520.9469    2.0294     0.0044      0.0    
   1000     1309.65    519.0699   518.9408    1.3287     0.004       0.0    
   1100     1440.15    518.2855   518.5793    1.8586     0.0038      0.0    
   1200     1571.74    517.2653   518.5408    1.5607     0.0033      0.0    
   1300     1701.87    519.4315   518.7979    1.511      0.0026      0.0    
   1400     1833.58    517.3919   518.3622    1.5673     0.0021      0.0    
   1500     1963.54    517.1832   518.7556    1.5087     0.002       0.0    
   1600     2094.33    516.3934   518.2015    1.401      0.0016      0.0    
   1700     2225.83    516.8567   517.7294    1.4295     0.0015      0.0    
   1800     2356.49    516.2319   516.9719    1.3893     0.0015      0.0    
   1900     2486.27    518.9709   518.7261    1.5105     0.0014      0.0    
   2000     2616.99    516.8094   517.0891    1.1011     0.0011      0.0    
   2100     2747.25    517.6934   519.3596    1.6665     0.001       0.0    
   2200     2877.17    517.0677   517.4235    1.0865     0.0008      0.0    
   2300     3008.76    517.4552   518.4113     1.31      0.0008      0.0    
   2400     3138.67    516.6455   517.3757    1.0804     0.0008      0.0    
   2500      3269.3    516.2759   516.9593    1.3262     0.0007      0.0    
   2600     3398.44    515.8533   518.3494    1.2649     0.0006      0.0    
   2700     3529.67    516.3358   517.5812    1.6314     0.0007      0.0    
   2800      3659.1    516.5248   517.8875    1.3772     0.0006      0.0    
   2900      3789.4    517.8688   517.629     1.2951     0.0006      0.0    
   3000     3920.98    516.8816   517.5454    1.2597     0.0005      0.0    
   3100     4052.12    517.3677   517.2684    0.9626     0.0006      0.0    
   3200     4182.26    517.337    517.0085    1.2755     0.0005      0.0    
   3300     4313.13    516.1361   517.3603    1.1385     0.0004      0.0    
   3400     4443.04    517.0229   517.0355    1.0763     0.0005      0.0    
   3500     4574.14    515.1501   516.561     1.0844     0.0004      0.0    
   3600     4704.67    516.7783   516.4746    1.1314     0.0004      0.0    
   3700     4835.59    517.3894   516.9139    1.1082     0.0003      0.0    
   3800     4967.13    517.1272   516.3684    1.0063     0.0003      0.0    
   3900     5096.81    517.2753   516.3614    1.0836     0.0003      0.0    
   4000     5227.96    516.8229   516.9204    1.2286     0.0003      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       125.28    524.4294   526.2679    2.5868     0.3911      0.0    
   200       257.74    518.6413   521.3338    2.0633     0.0763      0.0    
   300       388.98    518.6136   521.0231    1.6838     0.034       0.0    
   400       522.64    517.4187   519.3594    1.8921     0.0196      0.0    
   500       655.54    517.5408   519.7827    1.5971     0.0125      0.0    
   600       788.61    517.831    519.4643    1.6753     0.0085      0.0    
   700       921.06    516.9225   519.9282    1.312      0.006       0.0    
   800       1053.4    516.0105   518.9501    1.6197     0.0045      0.0    
   900       1185.4    517.0562   518.9719    1.3547     0.0037      0.0    
   1000     1317.68    517.1233   518.9037    1.4657     0.0032      0.0    
   1100     1450.52    517.6094   519.618     1.4883     0.0027      0.0    
   1200     1583.37    517.2029   519.0161    1.7352     0.0022      0.0    
   1300     1716.33    517.1948   519.9557    1.3551     0.002       0.0    
   1400     1848.98    517.085    519.5714    1.5543     0.0018      0.0    
   1500     1979.71    515.4682   518.1465    1.4908     0.0015      0.0    
   1600     2112.16    516.8095   518.8345    1.2502     0.0013      0.0    
   1700     2245.17    517.9831   520.1983    1.2651     0.0011      0.0    
   1800     2377.44    516.0661   518.3017    1.5134     0.001       0.0    
   1900     2510.24    517.7209   519.0011    1.213      0.001       0.0    
   2000     2642.92    516.8347   518.7453    1.6811     0.0008      0.0    
   2100     2774.23    515.8925   518.426     1.0606     0.0008      0.0    
   2200     2905.42    515.5366   518.1774    1.2567     0.0008      0.0    
   2300     3038.27    515.6826   517.9559    1.2837     0.0007      0.0    
   2400     3170.27    515.0304   517.5244    1.3469     0.0006      0.0    
   2500     3302.05    515.5227   517.3671    1.0271     0.0006      0.0    
   2600     3433.65    515.5288   517.2374    1.0866     0.0005      0.0    
   2700     3564.91    516.5928   518.1688    1.0292     0.0005      0.0    
   2800     3697.16    516.7634   518.0203    1.135      0.0005      0.0    
   2900     3829.28    516.5456   518.2682    1.201      0.0004      0.0    
   3000     3961.61    515.6678   517.6654    1.0171     0.0004      0.0    
   3100     4092.99    515.2406   517.5593    1.1781     0.0004      0.0    
   3200     4224.84    515.314    517.5912    1.0052     0.0004      0.0    
   3300     4356.35    516.8558   518.3324    1.1326     0.0004      0.0    
   3400     4488.72    515.6678   517.7059    1.0228     0.0003      0.0    
   3500     4621.27    515.3836   517.3838    1.0319     0.0003      0.0    
   3600     4753.68    515.195    517.4716    0.8728     0.0003      0.0    
   3700      4885.3    515.9185   518.0378    0.9238     0.0003      0.0    
   3800     5017.38    515.2661   517.1037    1.1034     0.0003      0.0    
   3900     5149.55    516.3336   517.7174    0.9578     0.0002      0.0    
   4000     5282.61    516.6616   518.0755    0.9666     0.0002      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       125.96    529.2832   530.2182    2.2379     0.297       0.0    
   200       257.93    517.4492   519.5008    1.5675     0.0659      0.0    
   300       390.48    517.2831   518.8815    1.4839     0.0312      0.0    
   400       522.62    516.3805   518.6977    1.3007     0.0174      0.0    
   500       654.03    515.4937   518.2156    1.0903     0.0111      0.0    
   600       786.19    516.6283   518.4874    1.7531     0.0085      0.0    
   700       919.05    515.5701   518.1118    1.0407     0.0061      0.0    
   800      1051.83    515.4577   517.851     1.3358     0.005       0.0    
   900      1184.52    515.5798   518.4535    1.3213     0.0043      0.0    
   1000     1317.28    515.636    517.5403    1.1731     0.0036      0.0    
   1100     1449.87    516.3355   518.2598    1.2185     0.0028      0.0    
   1200     1581.44    516.2065   518.4091    1.4236     0.0022      0.0    
   1300     1714.42    515.4902   517.1578    1.1462     0.002       0.0    
   1400     1847.29    515.0371   517.5695    1.3914     0.0017      0.0    
   1500     1980.24    515.5782   517.604     1.1013     0.0016      0.0    
   1600     2111.22    515.0274   517.7826    1.2406     0.0014      0.0    
   1700     2242.14    516.1819   517.8243    1.3729     0.0012      0.0    
   1800     2372.91    515.6109   517.8323    1.0791     0.0011      0.0    
   1900     2505.02    515.251    516.989     0.953      0.001       0.0    
   2000     2634.99    516.208    518.1665    1.1898     0.0009      0.0    
   2100     2765.58    515.0494   518.0446    1.1131     0.0008      0.0    
   2200     2896.69    515.3276   517.4469    1.1152     0.0008      0.0    
   2300     3026.48    514.7033   517.3467    1.0751     0.0007      0.0    
   2400     3156.89    515.4207   517.3937    0.9227     0.0007      0.0    
   2500     3287.51    514.8243   516.9081    1.0209     0.0006      0.0    
   2600     3417.35    515.7534   517.6398    1.1219     0.0005      0.0    
   2700     3548.47    515.478    517.473     0.8933     0.0005      0.0    
   2800     3679.69    515.6343   517.1212    0.8549     0.0005      0.0    
   2900     3809.98    515.5729   517.2785    1.1883     0.0005      0.0    
   3000     3940.73    514.5844   517.2587    1.2927     0.0005      0.0    
   3100     4071.75    515.0833   517.5169    0.8422     0.0004      0.0    
   3200     4202.78    515.9956   517.7343    1.0381     0.0004      0.0    
   3300     4332.63    514.2734   517.3332    1.2524     0.0004      0.0    
   3400     4463.68    515.2959   518.1915    0.9039     0.0004      0.0    
   3500      4593.8    514.6826   517.2626    1.0253     0.0003      0.0    
   3600     4724.07    514.4904   517.379     0.8796     0.0003      0.0    
   3700     4854.46    514.6206   517.2506    1.107      0.0003      0.0    
   3800     4984.62    514.9329   516.7993    0.9168     0.0003      0.0    
   3900     5114.92    514.7265   516.8533    0.955      0.0002      0.0    
   4000     5245.41    514.6511   517.3582    0.9519     0.0002      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       124.68    532.0291   533.7929    1.9449     0.4216      0.0    
   200       255.92    517.3694   520.1644    1.2331     0.0788      0.0    
   300       387.23    516.2185   518.3965    1.2537     0.0323      0.0    
   400       517.74    514.9365   517.4728    0.9054     0.0195      0.0    
   500       648.2     514.1237   517.2826    1.0184     0.0116      0.0    
   600       779.2     514.843    518.1062    0.903      0.0089      0.0    
   700       909.86    514.2711   517.4398    1.0957     0.0073      0.0    
   800      1040.42    514.5373   517.8758    1.1563     0.0053      0.0    
   900      1171.17    514.9437   517.6868    1.0141     0.0038      0.0    
   1000     1301.64    515.7807   518.039     0.9665     0.0034      0.0    
   1100     1431.92    515.2286   517.9511    0.964      0.003       0.0    
   1200     1561.94    514.3942   517.3981    1.0508     0.0024      0.0    
   1300     1693.39    513.9289   517.0986    1.1908     0.002       0.0    
   1400      1823.7    515.7988   518.0455    1.1609     0.0018      0.0    
   1500     1953.64    515.5416   517.3746    1.2049     0.0017      0.0    
   1600     2083.46    515.1968   517.9209    1.095      0.0014      0.0    
   1700     2214.48    515.427    518.0744    0.8958     0.0013      0.0    
   1800     2345.59    514.1786   517.1316    0.8393     0.0012      0.0    
   1900     2476.42    514.876    517.5069    0.8842     0.001       0.0    
   2000     2607.26    513.9873   516.9986    0.7621     0.001       0.0    
   2100     2738.27    514.1659   517.7181    1.1207     0.0009      0.0    
   2200     2870.01    514.8824   517.7043    1.3296     0.0008      0.0    
   2300     3001.03    514.5306   517.3382    1.2176     0.0007      0.0    
   2400     3131.63    515.5279   517.9742    1.1253     0.0007      0.0    
   2500     3263.14    515.0728   517.4168    0.9013     0.0007      0.0    
   2600     3393.61    515.3609   518.4432    1.1056     0.0006      0.0    
   2700     3524.32    514.5379   517.5907    0.9719     0.0006      0.0    
   2800     3655.33    514.436    517.7016    1.2506     0.0005      0.0    
   2900     3786.32    516.132    518.056     0.9681     0.0005      0.0    
   3000     3918.45    514.6208   517.3197    0.7217     0.0005      0.0    
   3100     4048.68    514.5491   517.228     0.8728     0.0004      0.0    
   3200     4179.75    514.9577   517.4501    0.9255     0.0004      0.0    
   3300     4311.77    514.2076   517.0426    0.9331     0.0004      0.0    
   3400     4444.38    513.7944   517.4733    1.0019     0.0004      0.0    
   3500     4576.43    513.5898   517.0405    0.9488     0.0004      0.0    
   3600     4709.85    514.4802   517.3979    0.8752     0.0003      0.0    
   3700     4841.87    513.1416   516.9946     0.81      0.0003      0.0    
   3800     4975.46    514.0068   517.1806    0.9799     0.0003      0.0    
   3900     5107.45    514.0986    517.38     0.8158     0.0003      0.0    
   4000     5239.58    513.9979   517.1521    0.8421     0.0003      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       126.79    534.9728   536.3454    1.7359     0.479       0.0    
   200       259.96    518.1146   520.1452    0.9334     0.0824      0.0    
   300       393.2     515.868    518.0203    0.8714     0.0363      0.0    
   400       526.49    515.8943   517.6081    0.9362     0.0207      0.0    
   500       659.3     515.1279   517.4423    1.1008     0.0127      0.0    
   600       793.69    514.4755   517.3066    1.0404     0.0091      0.0    
   700       927.45    514.3108   517.0692    0.8014     0.0069      0.0    
   800      1061.79    513.8544   516.8824    0.8719     0.0058      0.0    
   900      1195.32    514.292    517.0546    0.9895     0.0044      0.0    
   1000     1328.24    513.6915   516.8853    1.4242     0.0035      0.0    
   1100     1461.26    514.1825   517.2609    0.7635     0.0029      0.0    
   1200     1594.53    513.9826   517.2757    0.7609     0.0026      0.0    
   1300     1727.94    514.7989   517.1887    0.8054     0.0022      0.0    
   1400     1860.56    515.0482   516.8124    0.9571     0.0019      0.0    
   1500     1993.78    513.8547   517.1683    0.8812     0.0017      0.0    
   1600     2126.31    514.754    516.9033    0.9191     0.0015      0.0    
   1700     2259.91    514.591    517.1359    1.1592     0.0015      0.0    
   1800     2393.23    514.4368   516.6858    0.9032     0.0012      0.0    
   1900     2527.27    513.5602   516.5546    0.7695     0.001       0.0    
   2000     2660.73    514.2562   516.769     0.8808     0.0009      0.0    
   2100     2794.36    514.3805   516.7566    1.1162     0.0009      0.0    
   2200     2927.97    515.8898   517.7213    0.8354     0.0008      0.0    
   2300     3060.86    515.0232   516.8625    0.9855     0.0008      0.0    
   2400     3194.07    515.121    517.2313    0.834      0.0008      0.0    
   2500     3328.29    514.2261   516.6462    0.7426     0.0007      0.0    
   2600     3460.89    514.8366   517.2721    0.6623     0.0006      0.0    
   2700     3594.35    514.5151   517.0011    1.0447     0.0006      0.0    
   2800     3726.91    514.9716   517.1753    0.9843     0.0006      0.0    
   2900     3859.26    514.0905   516.8234    0.8558     0.0005      0.0    
   3000     3991.66    514.2021   516.648     0.6741     0.0005      0.0    
   3100     4124.96    515.3373   517.1623    0.806      0.0004      0.0    
   3200     4259.85    514.4975   516.6098    0.8049     0.0004      0.0    
   3300     4393.61    515.7632   517.1628    0.8358     0.0004      0.0    
   3400     4528.59    514.6605   516.7285    0.8396     0.0004      0.0    
   3500     4663.76    513.8547   516.695     1.1234     0.0003      0.0    
   3600     4799.23    514.7716   516.878     0.8177     0.0003      0.0    
   3700     4933.31    514.0723   516.9446    0.6501     0.0003      0.0    
   3800     5068.85    514.289    517.2407    0.8453     0.0003      0.0    
   3900     5202.82    514.4172   516.653     0.7418     0.0003      0.0    
   4000     5337.46    513.9673   516.3408    0.7812     0.0003      0.0    
