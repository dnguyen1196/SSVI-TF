Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  3.6233091354370117
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       143.26     0.7855     0.7467     3.1298     0.2283   
   200       290.99     0.7073     0.6716     2.4467     0.1144   
   300       438.61     0.6813     0.6219     2.4936     0.0871   
   400       586.15     0.6559     0.6008     2.2341     0.076    
   500       734.88     0.6409     0.5598     2.038      0.058    
   600       884.09     0.6389     0.5404     2.3989     0.0462   
   700      1031.59     0.6286     0.5142     1.6735     0.047    
   800      1179.85     0.6141     0.4848     2.0011     0.0371   
   900      1328.94     0.5996     0.4771     1.605      0.0363   
   1000     1477.08     0.6003     0.4758     1.4873     0.0417   
   1100     1625.05     0.5999     0.4703     1.6318     0.0352   
   1200     1772.65     0.5949     0.4576     1.4908     0.0347   
   1300     1920.83     0.5877     0.4477     1.6605     0.0342   
   1400     2068.81     0.5819     0.4429     1.5136     0.0297   
   1500      2215.4     0.5993     0.4552     1.414      0.0293   
   1600     2362.33     0.5891     0.4338     1.6458     0.0306   
   1700     2509.31     0.5733     0.4096     1.6165     0.0314   
   1800     2656.99     0.5938     0.4363     2.0956     0.0294   
   1900     2804.57     0.6202     0.4769     1.8026     0.029    
   2000     2951.72     0.6142     0.4718     2.3782     0.031    
   2100     3098.59     0.6328     0.5187     2.4634     0.0322   
   2200     3245.78     0.6307     0.5255     2.7174     0.0394   
   2300     3393.36     0.6439     0.5466     2.7894      0.05    
   2400      3541.4     0.6168     0.5132     2.7766     0.0432   
   2500     3688.26     0.6317     0.5448     2.528      0.0433   
   2600     3835.94      0.61      0.5075     2.4903     0.0477   
   2700     3982.93     0.5993     0.4927     2.6529     0.0576   
   2800     4128.24     0.6009     0.4942     2.6506     0.0492   
   2900     4273.29     0.5956     0.4966     2.3245     0.0584   
   3000      4419.1     0.5832     0.4693     2.3843     0.0641   
   3100     4565.24     0.5868     0.487      2.5181     0.0597   
   3200      4710.3     0.5777     0.4808     2.1716     0.0753   
   3300     4854.62     0.581      0.4779     1.9655     0.0623   
   3400     4999.71     0.5672     0.4652     2.0469     0.0601   
   3500     5144.54     0.5624     0.4539     2.1015     0.0668   
   3600     5288.54     0.564      0.458      2.2709     0.0619   
   3700     5432.35     0.5623     0.4552     2.2336     0.0639   
   3800     5577.39     0.5604     0.4414     2.315      0.0724   
   3900     5721.12     0.562      0.4425     2.0963     0.0754   
   4000     5865.43     0.556      0.4386     1.9371     0.0548   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       143.01     1.4205     1.4229     0.0178     0.6154   
   200       291.33     1.4674     1.4636     0.0138     0.381    
   300       438.99     1.4946     1.4904     0.0121     0.2203   
   400       586.79     1.4954     1.4931      0.01      0.1306   
   500       734.82     1.4956     1.4933     0.0103     0.0844   
   600       883.97     1.4956     1.4933     0.0126     0.0591   
   700      1032.05     1.4956     1.4933     0.0129     0.0432   
   800      1179.16     1.4956     1.4933     0.0119     0.034    
   900      1327.53     1.4956     1.4933     0.0129     0.0271   
   1000     1474.92     1.4956     1.4933     0.0132     0.0227   
   1100      1622.5     1.4956     1.4933     0.0124     0.0191   
   1200     1770.22     1.4956     1.4933     0.0131     0.0164   
   1300     1918.38     1.4956     1.4933     0.0136     0.0142   
   1400     2065.03     1.4956     1.4933     0.0137     0.0126   
   1500     2212.68     1.4956     1.4933     0.0134     0.0113   
   1600     2360.83     1.4956     1.4933     0.0143     0.0101   
   1700     2508.71     1.4956     1.4933     0.0136     0.0091   
   1800     2657.14     1.4956     1.4933     0.0138     0.0084   
   1900     2805.54     1.4956     1.4933     0.0148     0.0077   
   2000     2953.38     1.4956     1.4933     0.0148     0.0071   
   2100     3100.78     1.4956     1.4933     0.0155     0.0066   
   2200     3248.93     1.4956     1.4933     0.0173     0.0062   
   2300     3397.09     1.4956     1.4933     0.0174     0.0059   
   2400     3545.53     1.4956     1.4933     0.0177     0.0055   
   2500     3693.27     1.4956     1.4933     0.0187     0.0051   
   2600     3842.26     1.4956     1.4933     0.0198     0.0049   
   2700     3990.07     1.4956     1.4933     0.0211     0.0046   
   2800     4138.56     1.4956     1.4933     0.0211     0.0044   
   2900     4286.97     1.4956     1.4933     0.0248     0.0042   
   3000     4434.55     1.4956     1.4933     0.024      0.004    
   3100     4581.88     1.4956     1.4933     0.0269     0.0039   
   3200     4729.09     1.4956     1.4933     0.0288     0.0037   
   3300     4877.22     1.4956     1.4933     0.0318     0.0036   
   3400     5024.87     1.4949     1.4931     0.0333     0.0035   
   3500     5172.89     1.455      1.4527     0.0355     0.0034   
   3600     5321.27     1.2525     1.248      0.0432     0.0034   
   3700     5468.34     1.0415     1.0415     0.0477     0.0034   
   3800     5616.38     0.9346     0.9302     0.0532     0.0036   
   3900      5764.3     0.8531     0.8454     0.0755     0.0039   
   4000     5911.84     0.7797     0.7703     0.0822     0.0046   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       143.63     1.4191     1.426      0.0257     0.5062   
   200       292.01     1.4286     1.4363     0.0174     0.3593   
   300       441.45     1.4471     1.4502     0.015      0.2401   
   400       589.72     1.4856     1.487      0.0163     0.1561   
   500       738.22     1.495      1.499      0.0134     0.1046   
   600       886.22     1.4958     1.4995     0.0127     0.0737   
   700      1035.36     1.4956     1.499      0.013      0.0544   
   800      1184.19     1.4956     1.499      0.0129     0.0419   
   900      1333.45     1.4956     1.499      0.0138     0.0334   
   1000     1482.08     1.4956     1.499      0.0125     0.0276   
   1100     1630.59     1.4956     1.499      0.0133     0.0235   
   1200     1779.62     1.4956     1.499      0.0137     0.0199   
   1300     1928.07     1.4956     1.499      0.0126     0.0173   
   1400     2076.96     1.4956     1.499      0.0136     0.0151   
   1500     2226.46     1.4956     1.499      0.0133     0.0135   
   1600     2374.89     1.4956     1.499      0.0133     0.0121   
   1700     2523.16     1.4956     1.499      0.0134     0.011    
   1800      2670.7     1.4956     1.499      0.0148     0.0099   
   1900     2820.25     1.4956     1.499      0.0139     0.0092   
   2000     2968.24     1.4956     1.499      0.0155     0.0084   
   2100      3116.5     1.4956     1.499      0.0159     0.0078   
   2200     3264.29     1.4956     1.499      0.0172     0.0073   
   2300     3413.56     1.4956     1.499      0.0169     0.0068   
   2400     3561.92     1.4956     1.499      0.019      0.0065   
   2500     3711.54     1.4956     1.499      0.0186     0.0061   
   2600     3859.74     1.4956     1.499      0.0231     0.0057   
   2700      4007.9     1.4956     1.499      0.0205     0.0054   
   2800     4157.07     1.4956     1.499      0.0232     0.0052   
   2900      4304.9     1.4956     1.499      0.0283     0.0049   
   3000     4453.53     1.4956     1.499      0.0296     0.0047   
   3100     4601.22     1.4927     1.4964     0.0349     0.0046   
   3200     4749.81     1.4007     1.4021     0.0339     0.0044   
   3300     4897.67     1.1141     1.1146     0.0489     0.0043   
   3400     5045.58     0.9442     0.9478     0.0597     0.0043   
   3500     5192.27     0.8764     0.8721     0.0674     0.0044   
   3600     5339.62     0.778      0.7731     0.1028     0.005    
   3700     5487.86     0.741      0.736      0.1582     0.0069   
   3800     5635.86     0.735      0.7283     0.1881     0.0116   
   3900     5783.88     0.7254     0.715      0.1699     0.0156   
   4000     5932.75     0.721      0.7115     0.1298     0.0126   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       142.21     1.4086     1.4136     0.035      0.4636   
   200       291.07     1.4195     1.4244     0.023      0.3426   
   300       438.82     1.4408     1.4398     0.0202     0.2419   
   400       587.9      1.4663     1.4622     0.0196     0.1645   
   500       737.77     1.4908     1.4895     0.0148     0.1131   
   600       886.39     1.4949     1.4969     0.0152     0.0809   
   700      1035.16     1.4958     1.4975     0.0153     0.0597   
   800       1183.9     1.4956     1.4975     0.017      0.0462   
   900      1333.19     1.4956     1.4974     0.0142     0.0367   
   1000     1481.77     1.4956     1.4974     0.0148     0.0301   
   1100     1630.62     1.4956     1.4974     0.0138     0.0251   
   1200      1779.5     1.4956     1.4974     0.0138     0.0214   
   1300     1929.51     1.4956     1.4974     0.0138     0.0186   
   1400     2078.43     1.4956     1.4974     0.0149     0.0163   
   1500     2228.25     1.4956     1.4974     0.014      0.0145   
   1600     2377.54     1.4956     1.4974     0.0149     0.0129   
   1700     2527.12     1.4956     1.4974     0.0153     0.0116   
   1800     2676.04     1.4956     1.4974     0.0148     0.0106   
   1900     2825.41     1.4956     1.4974     0.0157     0.0098   
   2000      2975.9     1.4956     1.4974     0.0172     0.009    
   2100     3124.93     1.4956     1.4974     0.016      0.0083   
   2200     3273.27     1.4956     1.4974     0.0168     0.0078   
   2300     3423.07     1.4956     1.4974     0.0186     0.0072   
   2400     3572.68     1.4956     1.4974     0.0184     0.0068   
   2500      3722.8     1.4956     1.4974     0.0191     0.0064   
   2600     3871.72     1.4956     1.4974     0.0208     0.006    
   2700     4020.87     1.4956     1.4974     0.0223     0.0057   
   2800     4169.87     1.4956     1.4974     0.0227     0.0055   
   2900     4318.82     1.4956     1.4974     0.0297     0.0052   
   3000     4467.26     1.4953     1.497      0.0301     0.005    
   3100     4615.13     1.4147     1.4155     0.0382     0.0048   
   3200      4764.7     1.0892     1.0898     0.0437     0.0046   
   3300     4913.88     0.9428     0.9453     0.0607     0.0046   
   3400     5061.59     0.8422     0.8388     0.0794     0.0046   
   3500     5210.47     0.777      0.7724     0.121      0.0053   
   3600     5359.58     0.7411     0.7342     0.1854     0.008    
   3700     5508.34     0.7254     0.7119     0.2315     0.0157   
   3800     5657.49     0.7173     0.7035     0.1912     0.0188   
   3900     5806.97     0.7173     0.7035     0.1226     0.0138   
   4000     5955.66     0.7173     0.7035     0.1938     0.008    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       143.2      1.4217     1.4176     0.0415     0.4399   
   200       292.51     1.424      1.4218     0.0404     0.3328   
   300       442.14     1.4278     1.4301     0.0299     0.2421   
   400       590.82     1.4504     1.4467     0.0218     0.1697   
   500       740.13     1.4802     1.4748     0.0179     0.1178   
   600       890.17     1.4833     1.4847     0.0171     0.0841   
   700      1039.63     1.4896     1.4901     0.018      0.0624   
   800      1189.48     1.4938     1.4957     0.0163     0.0481   
   900       1339.7     1.4956     1.4976     0.0149     0.0384   
   1000     1489.17     1.4956     1.4976     0.0148     0.0315   
   1100     1638.44     1.4956     1.4976     0.0147     0.0263   
   1200     1788.43     1.4956     1.4976     0.0164     0.0224   
   1300      1938.5     1.4956     1.4976     0.0142     0.0194   
   1400     2088.29     1.4956     1.4976     0.0165     0.0171   
   1500     2238.04     1.4956     1.4976     0.014      0.0151   
   1600     2388.68     1.4956     1.4976     0.0156     0.0136   
   1700     2537.61     1.4956     1.4976     0.015      0.0121   
   1800     2689.09     1.4956     1.4976     0.0144     0.011    
   1900     2838.92     1.4956     1.4976     0.0163     0.0101   
   2000     2987.96     1.4956     1.4976     0.0164     0.0093   
   2100     3137.35     1.4956     1.4976     0.0156     0.0086   
   2200     3287.38     1.4956     1.4976     0.0161     0.008    
   2300     3437.71     1.4956     1.4976     0.0172     0.0075   
   2400      3587.8     1.4956     1.4976     0.0174     0.007    
   2500      3738.4     1.4956     1.4976     0.0208     0.0066   
   2600     3888.65     1.4956     1.4976     0.022      0.0062   
   2700     4038.04     1.4956     1.4976     0.023      0.0059   
   2800     4188.73     1.4956     1.4976     0.0247     0.0056   
   2900     4339.17     1.4956     1.4976     0.0259     0.0054   
   3000     4489.21     1.4875     1.4903     0.0334     0.0052   
   3100     4640.32     1.2267     1.2252     0.0397     0.005    
   3200     4789.75     1.0023     1.0038     0.0455     0.0048   
   3300     4939.34     0.8885     0.8883     0.0635     0.0048   
   3400     5089.39     0.8069     0.8041     0.0847     0.0048   
   3500     5238.02     0.7539     0.7493     0.1336     0.0055   
   3600     5387.34     0.7305     0.7225     0.2169     0.0094   
   3700     5537.45     0.7173     0.7045     0.264      0.0208   
   3800     5686.42     0.7254     0.7132     0.1851     0.0196   
   3900     5834.04     0.7173     0.7045     0.2401     0.0118   
   4000     5983.82     0.7173     0.7045     0.3383     0.0073   
