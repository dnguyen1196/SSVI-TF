Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  6.06672739982605
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       87.41      1.4098     1.4033     3.1623    30.8778     0.2474  
   100       239.3      1.4137     1.4178     3.1316   5627.5422    0.0541  
   150       388.7      1.4105     1.4137     2.6291   5585.4012    0.2121  
   200       536.68     1.4133     1.4172     2.6786    4154.894    0.0476  
   250       687.09     1.4141     1.4279     3.0073   4080.7416    0.1627  
   300       841.17     1.4166     1.4117     2.9002   4262.0084    0.2316  
   350       995.29     1.4152     1.4116     2.8319   4297.5553    0.0375  
   400      1149.06     1.422      1.4156     3.0108   17123.7529   0.0232  
   450      1302.83     1.4099     1.4146     3.1603   17094.3809   0.2419  
   500      1456.23     1.4131     1.4169     1.6416   9199.0868    0.038   
   550      1609.51     1.4093     1.4206     1.4666   4886.9961    0.0189  
   600      1762.84     1.4148     1.4185      1.99    24572.6337   0.0239  
   650      1916.46     1.4156     1.418      1.3697    45335.27    0.0068  
   700      2069.74     1.4126     1.408      1.5103   49333.4226   0.0513  
   750      2223.25     1.4058     1.411      2.0728   3823.4653     0.01   
   800      2376.52     1.4153     1.4206     2.9803   1667.7969    0.0748  
   850      2529.65     1.4113     1.4146     2.8634   1230.9939    0.0191  
   900      2682.63     1.4152     1.4049     2.7886   1030.0877    0.0119  
   950      2835.24     1.4095     1.4181     3.0359   1868.9172    0.0149  
   1000     2988.04     1.4181     1.4139     2.421    1816.0177    0.0189  
   1050     3140.94     1.4029     1.4209     1.0952   2991.3979    0.0216  
   1100     3293.58     1.4082     1.4115     0.9481   5321.2312    0.0162  
   1150     3445.94     1.4108     1.4071     2.9186   6414.5462    0.0106  
   1200     3598.02     1.4189     1.4108     2.6022   10145.1715   0.0027  
   1250     3750.47     1.4148     1.4032     2.3071   11091.0979   0.0129  
   1300     3902.77     1.4127     1.4164     0.9958   82872.7349   0.0186  
   1350     4054.96     1.4173     1.4192     0.7909   50591.4431   0.0168  
   1400     4207.07     1.419      1.4099     0.9837   9272.9962    0.0147  
   1450     4358.98     1.4161     1.4059     0.8729   6852.2953    0.0094  
   1500     4510.65     1.4175     1.4146     0.7281   1171.6445    0.0083  
   1550     4662.11     1.4078     1.417      0.7085    1070.207    0.0056  
   1600     4813.47     1.4054     1.4042     0.9933    957.6381    0.0052  
   1650     4964.79     1.4213     1.4212     2.9249   4609.2296    0.0042  
   1700      5116.2     1.4149     1.4011     0.9033   1509.5709    0.0016  
   1750     5267.69     1.4001     1.4002     0.7987   2890.1721    0.0061  
   1800     5419.25     1.3904     1.4028     0.9377   6419.4402    0.0023  
   1850     5570.93     1.3836     1.3826     0.7293   9446.2956    0.0038  
   1900     5722.11     1.3804     1.3891     3.1433    2070.882    0.0023  
   1950     5873.34     1.3436     1.3358     0.9827    737.6611    0.0009  
   2000     6024.44     1.2486     1.2595     1.9082    709.8397    0.0033  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       87.42      1.4142     1.4151     0.2478     0.3534     0.0005  
   100       264.84     1.4161     1.4153     0.2384     0.5453      0.0    
   150       442.33     1.4206     1.415      0.2596     1.1997     0.0005  
   200       619.98     1.4164     1.4142     0.3175    15.5435     0.0011  
   250       798.37     1.4149     1.4143     0.8629    568.4079    0.0192  
   300       979.81     1.4068     1.4187     1.0145   1294.2414    0.0121  
   350      1162.63     1.4168     1.4172     0.9896   1821.8242    0.0698  
   400      1344.53     1.4144     1.4212     0.9721   1810.9459    0.0191  
   450      1526.98     1.4164     1.4137     1.2235   7286.1294    0.0383  
   500      1709.41     1.4114     1.4173     0.9616   7136.9992    0.0101  
   550       1891.7     1.4188     1.4101     1.0941   1554.7131    0.1348  
   600      2074.04     1.4148     1.4135     0.9898   1578.2453    0.1123  
   650      2256.47     1.4175     1.4126     0.9033   1319.8617    0.0763  
   700      2438.72     1.4133     1.4202     0.9407   2403.9865    0.0233  
   750      2621.04     1.4167     1.416      0.9561   2340.3039    0.0356  
   800      2803.47     1.4143     1.4091     1.143    65357.0814   0.0968  
   850       2985.2      1.41      1.4051     0.9845   65470.1186   0.0495  
   900      3165.68     1.4177     1.4156     0.9377   2861.8377    0.0685  
   950      3346.28     1.4087     1.4169     1.0543   2216.8771    0.0487  
   1000     3526.83     1.4112     1.419      0.9843   2305.6889    0.0136  
   1050      3707.3     1.4133     1.4148     0.9632   8394.0493    0.0443  
   1100     3887.72     1.4121     1.4176     3.0135    8336.128    0.0196  
   1150     4068.03     1.4146     1.4082     2.5551    800.1365    0.0098  
   1200     4248.49     1.4161     1.4111     1.1505   1351.5961    0.0192  
   1250      4428.7     1.4112     1.4075     0.8728   153571.8572   0.0582  
   1300     4609.22     1.4108     1.4137     0.9852   154186.3483   0.1354  
   1350     4790.72     1.4038     1.4083     0.699    1764.1233    0.0165  
   1400     4970.81     1.3919     1.3942     0.9217   7224.0201    0.0094  
   1450     5151.12     1.3834     1.3891     1.0038   7982.5589    0.0265  
   1500     5331.38     1.3876     1.3852     0.8684   3313.3325    0.0725  
   1550     5511.57     1.3742     1.3746     1.3941   3267.4345    0.2526  
   1600     5691.91     1.3563     1.3655     2.4625    586.5817    0.0411  
   1650     5871.95     1.3472     1.3454     2.0152   7756.3535    0.0203  
   1700     6052.32     1.3349     1.3317     1.0155   11544.4663   0.0301  
   1750     6232.77     1.3216     1.3184     0.8566   11407.6982   0.0228  
   1800     6413.12     1.2657     1.269      0.7311    875.7693    0.0003  
   1850     6593.54     1.2238     1.222      0.7668   1019.9924    0.0157  
   1900     6773.71      1.2       1.1949     1.0489   1935.5099    0.0414  
   1950     6954.27     1.1733     1.1702     2.5716   2479.9932    0.0214  
   2000     7134.65     1.2177     1.2059     0.9588   2569.2966    0.0335  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       88.42      1.4188     1.4179     0.1072     0.0641     0.0102  
   100       293.05     1.4034     1.4159     0.1357     0.1233     0.0041  
   150       497.23     1.4227     1.4155     0.1689     0.188      0.0019  
   200       701.08     1.4153     1.4123     0.1856     0.2864     0.0007  
   250       904.86     1.4122     1.4142     0.1854     0.507      0.0003  
   300      1109.34     1.4156     1.4131     0.2424     0.8943      0.0    
   350       1313.2     1.4177     1.4154     0.2643     2.6579     0.0007  
   400      1517.07     1.4159     1.4182     0.2916    16.4836     0.0017  
   450      1721.48     1.4166     1.4143     0.9435    401.7219    0.0093  
   500      1928.43     1.4137     1.4123     1.0816   1129.8647    0.0117  
   550      2136.77     1.4172     1.4092     2.3556   1068.8669    0.0394  
   600      2345.45     1.4077     1.413      1.5637   5744.4771    0.0103  
   650      2554.22     1.4084     1.4138     3.0551   5736.2711    0.1324  
   700      2763.51     1.4203     1.4161     1.1043   1968.8091    0.0547  
   750      2973.13     1.4121     1.4156     2.6176   9959.2581    0.0482  
   800      3182.65     1.4169     1.4166     2.3982   10128.3746   0.0588  
   850       3392.2     1.4105     1.4168     0.859     541.9537    0.0203  
   900      3601.66     1.4108     1.4176     1.1807    2783.376    0.0362  
   950      3811.38     1.4187     1.4177     2.4522   4643.4941    0.0692  
   1000     4021.26     1.4086     1.4153     1.0667   5267.6784    0.0557  
   1050     4230.87     1.4199     1.4129     1.1873   8930.8735    0.0559  
   1100     4440.68     1.4122     1.4184     3.044    17841.1686   0.0226  
   1150     4650.99     1.4214     1.4116     1.1832   37411.0287   0.1118  
   1200     4861.76     1.403      1.4141     1.3451   20840.7622   0.045   
   1250     5072.03     1.4136     1.4206     0.9613   19672.748    0.1435  
   1300     5281.78     1.4259     1.4124     1.166    28460.0411   0.0215  
   1350     5491.49     1.4174     1.4155     0.9432   28123.221    4.1122  
   1400     5701.42     1.4142     1.4183     1.3417   1108.2927    0.1599  
   1450     5910.79     1.4063     1.4132     0.8406   3891.6537    0.3872  
   1500     6120.39     1.4133     1.4124     0.8355   3204.3327    0.0484  
   1550     6329.47     1.4153     1.4144     0.9419   2543.4674    0.2557  
   1600     6540.02     1.4113     1.4164     1.2075   6732.7299    0.1129  
   1650      6749.8     1.4106     1.4152     0.9114   25428.5112   0.0051  
   1700     6959.93     1.4086     1.4148     0.9773   26235.0501   0.0519  
   1750     7169.52     1.4204     1.4176     3.1359   11030.9024   0.0344  
   1800     7379.08     1.4137     1.4176     3.0483   11341.8009   0.0216  
   1850     7588.77     1.4168     1.4155     0.7988   1785.1656    0.0012  
   1900     7798.21     1.4168     1.413      0.8952   217973.1226   0.012   
   1950     8008.06     1.4114     1.4125     0.9408   220221.6489   0.1071  
   2000     8217.48     1.4139     1.4175     2.9054   3798.5687    0.022   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       89.43      1.4192     1.4167     0.0605     0.0199     0.0127  
   100       323.28     1.4117     1.4094     0.085      0.0584     0.0057  
   150       556.35     1.4136     1.4153     0.1131     0.0613     0.0036  
   200       789.47     1.4213     1.4152     0.1074     0.0862     0.0022  
   250       1022.5     1.414      1.4167     0.1309     0.1309     0.0009  
   300      1255.38     1.4133     1.4169     0.1524     0.1575     0.0007  
   350      1487.92     1.4113     1.4137     0.133      0.2128     0.0003  
   400      1720.46     1.4145     1.4165     0.1472     0.413      0.0007  
   450      1953.35     1.4146     1.4136     0.1899     0.8607     0.0002  
   500      2186.27     1.4134     1.4144     0.1917     1.5382      0.0    
   550      2419.12     1.4074     1.4132     0.2127     14.239     0.0001  
   600      2652.06     1.4155     1.4115     0.2786    28.5739     0.0003  
   650      2885.29     1.4114     1.4124     0.7655    7147.31     0.0038  
   700      3120.81     1.4081     1.4164     0.9355   7080.4625    0.0023  
   750      3358.24     1.4162     1.4141     0.9234   1957.0202    0.0437  
   800      3596.33     1.4166     1.4157     0.898    1346.4695    0.0084  
   850      3833.91     1.4175     1.4206     1.3384    802.4538    0.0104  
   900      4072.01     1.4083     1.4186     0.9721   3245.4066    0.2183  
   950      4310.18     1.4117     1.4161     0.9776    3142.291    0.0234  
   1000     4548.82     1.4094     1.4128     1.7128   1405.0532    0.011   
   1050     4787.15     1.4195     1.4104     0.6883   1269.9573    0.0012  
   1100     5025.07     1.4076     1.416      0.8189   5393.6574     0.0    
   1150     5263.32     1.4132     1.4169     0.9375   5352.0097    0.0152  
   1200     5502.22     1.4152     1.4136     0.7521    927.225     0.0027  
   1250     5740.22     1.4174     1.4116     3.158     878.5865    0.001   
   1300     5978.58     1.4051     1.4146     0.4492    450.0456    0.0181  
   1350     6216.68     1.4106     1.4112     0.6955    3428.513    0.0098  
   1400     6454.75     1.4123     1.4188     0.8816   14492.9197   0.0023  
   1450     6693.74     1.4191     1.4138     0.937    14352.9556   0.0595  
   1500     6932.98     1.4114     1.4145     0.769    2466.5223    0.0313  
   1550     7171.83     1.4226     1.4145     0.9259    3154.495    0.026   
   1600      7410.7     1.4135     1.4154     2.9082   1317.8336    0.0086  
   1650     7649.33     1.4164     1.417      0.922    1393.6178    0.0124  
   1700     7888.04     1.4157     1.4106     0.9164    1124.328    0.0158  
   1750      8126.7     1.4163     1.4125     0.9173   212618.8723   0.0757  
   1800     8366.31     1.4152     1.4164     0.7882   212625.6013   0.0736  
   1850     8605.37     1.4094     1.4203     2.6934    747.198     0.0224  
   1900     8844.23     1.412      1.412      0.795    1401.1721    0.0218  
   1950     9083.74     1.413      1.4154     0.8701   1411.4613    0.0195  
   2000     9322.68     1.4141     1.4158     0.7371   8751.6444    0.005   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       89.56      1.4139     1.411      0.0404     0.0113     0.0097  
   100       352.44     1.4133     1.4169     0.0489     0.0249     0.0052  
   150       615.02     1.4155     1.4172     0.0697     0.034      0.0038  
   200       877.13     1.4064     1.4147     0.0589     0.039      0.0024  
   250      1139.26     1.4075     1.4146     0.0879     0.0585     0.0017  
   300      1401.44     1.4175     1.4179     0.087      0.0787     0.0014  
   350      1662.98     1.4157     1.4143     0.0711     0.0805     0.001   
   400      1924.47     1.4208     1.4135     0.0951     0.1071     0.001   
   450      2185.75     1.4179     1.4161     0.0861     0.1421     0.0006  
   500      2446.82     1.4206     1.4133     0.0907     0.2352     0.0002  
   550      2708.13     1.4025     1.4179     0.1035     0.1866     0.0004  
   600      2969.22     1.4117     1.4148     0.1085     0.3941     0.0003  
   650      3231.06     1.4113     1.4171     0.0986     0.7368     0.0001  
   700      3492.13     1.4054     1.4145     0.1179     1.279      0.0002  
   750      3753.25     1.4177     1.4151     0.1312     3.059       0.0    
   800      4014.19     1.4176     1.4182     0.166     25.3078     0.0004  
   850      4275.54     1.4134     1.4106     0.2311    40.8773     0.0001  
   900      4536.61     1.416      1.4148     0.2141    18.0106     0.0001  
   950      4797.86     1.4124     1.4158     0.5544    117.9791    0.0032  
   1000     5060.27      1.41      1.4158     0.6946    244.0517    0.0135  
   1050     5324.42     1.4143     1.416      0.6945    911.9757    0.0001  
   1100      5589.6     1.4131     1.414      0.696    3260.7081    0.0106  
   1150     5855.03     1.4175     1.412      0.7905   3010.3697    0.0098  
   1200      6112.0     1.4112     1.4137     0.8623    255.727     0.0065  
   1250     6367.44     1.4181     1.4104     0.6673    191.5222    0.0065  
   1300     6623.05     1.4165     1.4142     0.7622   1560.4605    0.0074  
   1350     6884.31     1.4195     1.4123     0.7364   1659.6417    0.0112  
   1400      7145.8     1.4149     1.4127     1.0246   1872.3927    0.0122  
   1450     7402.27     1.4023     1.4128     2.1894   5667.7142    0.0211  
   1500      7658.7     1.4098     1.4187     0.8926   5931.5822    0.0176  
   1550     7915.43     1.4093     1.4134     2.0759    756.9423    0.0173  
   1600     8172.47     1.4197     1.4158     0.6866    856.9397    0.0126  
   1650     8427.99     1.4142     1.4134     0.7637   176419.9543   0.0203  
   1700     8689.18     1.4187     1.4106     0.7583   176331.8856   0.0187  
   1750     8948.53     1.4126     1.4142     0.7587   1523.8637    0.0107  
   1800     9207.23     1.4064     1.415      0.6507    422.7822    0.0065  
   1850     9464.42     1.4136     1.4149     0.7414    962.8435    0.007   
   1900     9720.53     1.4156     1.4112     0.6798    853.3053    0.0071  
   1950     9978.43     1.4129     1.414      0.6947   4412.9516    0.0089  
   2000     10234.78    1.4108     1.4158     0.7071   4367.3047    0.0094  
