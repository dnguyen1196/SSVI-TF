Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  6.658584117889404
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       31.11     493.8202   493.1862    3.1623     1.8524     0.0001  
   100       66.46     329.0299   325.4348    3.1422     0.6693      0.0    
   150       101.73    253.6286   254.0512    3.0326     0.3617      0.0    
   200       137.09    217.2589   214.9591    3.1197     0.2368      0.0    
   250       172.49    201.033    199.7329    2.8766     0.167       0.0    
   300       207.77    186.3609   185.4102    2.6018     0.1281      0.0    
   350       243.06    182.594    181.6553    2.664      0.1002      0.0    
   400       278.33    180.6092    179.91     2.3505     0.0788      0.0    
   450       312.58    164.0332   163.8389    2.4627     0.0656      0.0    
   500       346.8     165.6651   165.2455    2.0252     0.0562      0.0    
   550       380.79    165.6071   163.9837    1.7352     0.0483      0.0    
   600       414.79    161.0486   160.5736    1.2099     0.0427      0.0    
   650       448.77    159.0715   160.3273    1.807      0.0381      0.0    
   700       482.78    154.1757   154.0887    1.4467     0.0335      0.0    
   750       516.79    154.1016   154.4623    1.0727     0.0302      0.0    
   800       550.79    155.8917   155.2354    1.074      0.0262      0.0    
   850       584.78    155.3548   154.2668    1.4855     0.0239      0.0    
   900       618.78    155.298    154.5389    1.4479     0.0217      0.0    
   950       652.89    154.3412   153.9668    1.1846     0.0198      0.0    
   1000      688.28     152.7     152.4039    1.3279     0.0187      0.0    
   1050      723.75    151.946    151.1401    1.0425     0.0168      0.0    
   1100      759.19    151.4657   150.7513    1.1895     0.0159      0.0    
   1150      794.64    149.4412   148.6035    0.9706     0.0146      0.0    
   1200      830.04    152.6734   151.8508    0.8959     0.0139      0.0    
   1250      865.42    151.9227   151.0897    0.9231     0.0129      0.0    
   1300      900.82    149.9163   149.7314     0.87      0.0119      0.0    
   1350      936.2     151.5184   150.7989    0.7935     0.0113      0.0    
   1400      971.59    151.2436   150.664     1.0827     0.0107      0.0    
   1450     1006.99    151.2796   150.6038    1.225      0.0103      0.0    
   1500     1042.38    151.4719   151.0172    0.9667     0.0096      0.0    
   1550     1077.83    149.6092   149.1046    0.7684     0.0091      0.0    
   1600     1113.31    149.937    148.504     0.7783     0.0086      0.0    
   1650     1148.74    150.912    149.7162    0.7048     0.0084      0.0    
   1700     1184.13    150.0132   148.4175    0.8534     0.008       0.0    
   1750     1219.53    148.4849   147.7393    0.9171     0.0076      0.0    
   1800      1255.1    149.3727   148.2392    0.7655     0.0074      0.0    
   1850     1290.51    148.3268   147.2613    0.7946     0.007       0.0    
   1900     1325.92    149.7058   148.8862    0.8427     0.0066      0.0    
   1950     1361.35    148.9208   148.0057    0.6566     0.0064      0.0    
   2000     1396.75    148.4764   147.9033    0.9302     0.0063      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       34.69     405.2613   405.1592    3.1205     1.2755      0.0    
   100       70.81     351.2751   349.3697    3.0728     0.6415      0.0    
   150       106.94    310.5185   310.465     2.9593     0.4228      0.0    
   200       143.08    248.6203   247.5833    2.8738     0.3106      0.0    
   250       179.27    221.0107   219.7971    2.4017     0.2285      0.0    
   300       215.38    199.8546   200.0442    2.3426     0.1818      0.0    
   350       251.5     200.0858   198.589     2.1427     0.1515      0.0    
   400       287.64    188.4211   188.1513    2.0701     0.1279      0.0    
   450       323.77    182.878    183.4796    2.0608     0.1061      0.0    
   500       359.88    170.1264   169.5145    2.1848     0.0907      0.0    
   550       396.0     169.8194   168.5458    1.8741     0.0772      0.0    
   600       432.12    168.1175   167.3658    1.779      0.0689      0.0    
   650       470.38    165.5735   164.4723    1.5516     0.0594      0.0    
   700       506.54    168.7338   167.059     1.9726     0.0543      0.0    
   750       542.67    161.8433   160.3281    1.7539     0.0492      0.0    
   800       578.84    163.0901   162.8073    1.2422     0.0432      0.0    
   850       614.97    160.5693   160.3558    1.3149     0.0399      0.0    
   900       651.1     156.8726   156.5211    1.238      0.0358      0.0    
   950       687.26    158.0663   157.9747    1.0432     0.0324      0.0    
   1000      723.4     156.7771   156.7548    1.3534     0.0308      0.0    
   1050      759.55    155.1355   154.5688    1.1372     0.0282      0.0    
   1100      795.63    153.6627   153.2932    0.9144     0.0255      0.0    
   1150      831.71    153.1444   152.8911    1.2453     0.0247      0.0    
   1200      867.8     152.4079   152.1309    0.8507     0.0224      0.0    
   1250      903.92    152.8902   152.7256    1.3279     0.0212      0.0    
   1300      940.06    151.4622   151.3029    0.9196     0.0194      0.0    
   1350      976.16    150.782    150.5031    0.9016     0.0182      0.0    
   1400      1012.2    150.8119   150.8426    0.5604     0.0171      0.0    
   1450     1048.37    151.4521   151.4834    0.8768     0.0163      0.0    
   1500     1084.55    150.5131   150.5884    1.1122     0.0153      0.0    
   1550     1120.71    149.9675   150.2421    0.8627     0.0149      0.0    
   1600     1156.86    149.1703   149.7784    0.9875     0.014       0.0    
   1650     1193.01    149.0839   149.5844    0.9715     0.0131      0.0    
   1700     1229.13    150.0216   150.241     0.875      0.0126      0.0    
   1750     1265.15    149.3655   149.2438    0.6336     0.0118      0.0    
   1800     1301.24    150.3661   149.8892    0.5423     0.0113      0.0    
   1850     1337.38    150.4466   149.6209    0.9581     0.0111      0.0    
   1900     1373.54    149.7324   149.1039    0.6841     0.0102      0.0    
   1950     1409.69    147.6775   147.2727    0.9205      0.01       0.0    
   2000     1445.86    148.0769   147.9771    0.7984     0.0095      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       34.75     371.7985   369.2331    3.094      0.9936      0.0    
   100       71.61     319.5028   314.8641    2.9398     0.5547      0.0    
   150       108.43    283.0052   278.6845    2.586      0.4099      0.0    
   200       145.3     252.1586   249.1458    2.3981     0.3116      0.0    
   250       182.14    230.0348   229.0269    2.1705     0.2518      0.0    
   300       219.16    218.2172   217.1884    2.1484     0.1949      0.0    
   350       256.07    194.1508   194.6135    2.6608     0.1667      0.0    
   400       293.05    193.2304   193.311     1.9294     0.1437      0.0    
   450       329.98    187.1101   185.7779    2.0825     0.1225      0.0    
   500       366.89    186.9228   185.1497    1.9489     0.1018      0.0    
   550       403.72    182.6316   181.5711    1.5016     0.0909      0.0    
   600       440.61    172.1109   171.2053    1.4972     0.0801      0.0    
   650       477.45    172.5137   171.2593    1.4171     0.071       0.0    
   700       514.39    171.2033   170.0608    1.1945     0.0633      0.0    
   750       551.38    165.1772   164.1342    1.3507     0.0578      0.0    
   800       588.33    165.4723   164.7575    1.1219     0.0523      0.0    
   850       625.24    167.6156   165.7865    1.2214     0.0465      0.0    
   900       662.27    161.9777   161.3574    1.2178     0.0445      0.0    
   950       699.22    160.6532   160.5138    0.9742     0.0398      0.0    
   1000      736.16    158.1454   157.7372    1.2097     0.0372      0.0    
   1050      773.05    155.3988   155.2827    1.0643     0.0338      0.0    
   1100      810.39    156.9202   156.7021    1.0322     0.0317      0.0    
   1150      847.29    157.0977   156.6162    1.1499     0.0287      0.0    
   1200      884.22    157.5837   156.6295    0.8143     0.0274      0.0    
   1250      921.06    155.7976   154.5396    0.769      0.0252      0.0    
   1300      958.05    152.6718   152.1538    1.0286     0.0239      0.0    
   1350      994.98    151.5375   151.1668    0.6571     0.0223      0.0    
   1400     1031.91    153.8059   152.7156    0.9846     0.021       0.0    
   1450     1068.81    153.246    152.6116    0.9723     0.0201      0.0    
   1500      1105.8    152.558    151.3923    0.7195     0.0189      0.0    
   1550     1142.77    151.086    150.5864    0.549      0.0179      0.0    
   1600     1179.72    150.5777    150.2      0.6684     0.0168      0.0    
   1650     1216.66    150.3791   150.0589    0.6408     0.0166      0.0    
   1700     1253.55    150.2329   149.9511    0.5422     0.0153      0.0    
   1750     1290.45    149.8092   149.5964    0.6986     0.0144      0.0    
   1800     1327.28    149.8331   149.4375    0.5987     0.014       0.0    
   1850     1364.19    150.6801   150.0752    0.601      0.0134      0.0    
   1900     1401.21    149.6507   149.056     0.8244     0.0129      0.0    
   1950     1438.19    149.4992   148.981     0.7596     0.0123      0.0    
   2000     1475.14    148.1996   147.924     0.6368     0.0117      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       34.75     349.3515   346.2859    2.8236     0.828       0.0    
   100       72.36     291.2135   289.7857    2.582      0.4955      0.0    
   150       110.0     276.1465   276.1684    2.3859     0.3777      0.0    
   200       147.52    258.5255   257.7246    1.9977     0.3018      0.0    
   250       185.02    231.1323   232.4776    2.2049     0.2423      0.0    
   300       222.67    214.4305   215.6409    2.1097     0.1988      0.0    
   350       260.28    202.2779   202.9837    2.5452     0.1739      0.0    
   400       297.9     193.8195   193.481     1.743      0.1481      0.0    
   450       335.49    189.3843   189.4905    1.4997     0.1291      0.0    
   500       373.09    181.1843   181.9745    1.3422     0.1111      0.0    
   550       410.69    181.064    181.4804    1.6383     0.0977      0.0    
   600       448.22    176.1823   176.9621    1.3662     0.0902      0.0    
   650       485.86    173.0153   173.3313    1.4175     0.078       0.0    
   700       523.57    170.1807   170.3192    1.2286     0.0717      0.0    
   750       561.16    167.5786   167.6832    1.1752     0.0621      0.0    
   800       598.84    167.1076   166.8957    1.2461     0.0573      0.0    
   850       636.37    166.2188   164.9704    1.2118     0.0521      0.0    
   900       673.96    165.3473   163.944     0.9782     0.0481      0.0    
   950       711.59    163.9052   163.2771    1.0633     0.0465      0.0    
   1000      749.23    161.1987   161.0944    1.2902     0.0424      0.0    
   1050      786.84    163.8338   164.0848    1.0126     0.039       0.0    
   1100      824.57    157.5393   157.7102    1.1946     0.0353      0.0    
   1150      862.27    154.1569    154.32     0.8729     0.0336      0.0    
   1200      899.94    155.0899   155.3138    0.7885     0.0313      0.0    
   1250      937.56    153.9485   154.4993    0.7509     0.0298      0.0    
   1300      975.09    155.4945   155.3432    0.9644     0.0275      0.0    
   1350     1012.63    154.3353   154.1907    0.8967     0.0257      0.0    
   1400     1050.17    154.1984   154.1837    0.7734     0.0249      0.0    
   1450     1087.69    152.9358   152.8586    0.9608     0.024       0.0    
   1500     1125.37    152.0698   152.0002    0.7033     0.022       0.0    
   1550      1163.3    150.8845   150.8758    0.5667     0.0207      0.0    
   1600     1200.95    150.0131   149.8105    0.6939     0.0208      0.0    
   1650     1238.52    148.7729   148.584     0.5696     0.0187      0.0    
   1700     1276.22    150.1796   150.2012    0.6449     0.0186      0.0    
   1750     1313.85    147.7766   147.7734    0.6615     0.0173      0.0    
   1800     1351.47    149.3091   148.9674    0.5896     0.0168      0.0    
   1850     1389.05    149.045    149.1425    0.499      0.0158      0.0    
   1900     1426.67    150.4647    150.69     0.5465     0.0145      0.0    
   1950     1464.28    150.6651   150.9961    0.4747     0.0145      0.0    
   2000     1501.91    148.9573   149.4058    0.577      0.0135      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50        34.8     384.5965   381.6971    2.8125     0.7275      0.0    
   100       73.14     307.6218   305.8459    2.3985     0.4565      0.0    
   150       111.39    266.2674   266.1768    2.1565     0.3483      0.0    
   200       149.65    240.3988   240.452     1.8021     0.288       0.0    
   250       187.87    208.8399   209.3301    2.1829     0.2408      0.0    
   300       226.29    203.9754   204.3832    1.6508     0.2056      0.0    
   350       264.62    200.3871   200.7751    1.6138     0.1729      0.0    
   400       302.9     195.818    195.6052    1.5633     0.1483      0.0    
   450       341.16    192.358    191.3364    1.6719     0.131       0.0    
   500       379.42    183.6534   182.8866    1.5381     0.1133      0.0    
   550       417.66    183.7894   183.039     1.6204     0.1022      0.0    
   600       455.97    180.6733   180.6509    1.6426     0.092       0.0    
   650       494.28    174.4467   174.4975    1.7773     0.0829      0.0    
   700       532.6     169.5809   169.6803    1.1177     0.0746      0.0    
   750       570.85    168.4068   168.9919    1.4427     0.0681      0.0    
   800       609.15    165.1299   165.5052    1.0959     0.0609      0.0    
   850       647.43    164.0964   164.5269    1.2155     0.0565      0.0    
   900       685.75    163.8989   164.2142    1.1832     0.053       0.0    
   950       724.09    162.7212   162.6241    1.0143     0.0482      0.0    
   1000      762.44    160.0458   159.8287    0.866      0.0439      0.0    
   1050      800.84    159.9351   160.0229    0.9764     0.0403      0.0    
   1100      839.25    158.4262   158.6712    1.1258     0.0386      0.0    
   1150      877.63    156.8539   156.5951    0.8037     0.0363      0.0    
   1200      916.04    157.9898   157.3379    0.7308     0.0334      0.0    
   1250      954.35    158.0619   157.2291    0.9503     0.0335      0.0    
   1300      992.66    153.5549   153.2169    0.7918     0.0306      0.0    
   1350     1030.93    152.9944   152.8984    1.0933     0.0278      0.0    
   1400     1069.33    153.0146   152.9757    0.6628     0.0265      0.0    
   1450     1107.71    153.1267   153.1276    0.8311     0.0262      0.0    
   1500     1146.12    152.808    152.7212    0.6748     0.0241      0.0    
   1550     1184.47    151.694    151.9341     1.13      0.0226      0.0    
   1600     1222.55    152.0026   152.6519    0.644      0.0219      0.0    
   1650     1260.51    151.7078   151.763     0.5726     0.0207      0.0    
   1700     1298.48    150.9462   150.9888    0.645      0.0199      0.0    
   1750     1336.47    150.7441   151.0357    0.5316     0.0186      0.0    
   1800     1374.38    150.4597   150.681     0.5569     0.0176      0.0    
   1850     1412.34    149.9975   150.0379    0.7974     0.0171      0.0    
   1900     1450.66    150.449    150.7534    0.6424     0.0163      0.0    
   1950     1488.52    149.561    149.9679    0.6331     0.0157      0.0    
   2000     1526.42    149.7519   149.8092    0.5066     0.0147      0.0    
