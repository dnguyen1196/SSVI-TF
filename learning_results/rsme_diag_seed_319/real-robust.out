Generating synthetic  real valued data ... 
Generating synthetic  real valued data took:  7.3107054233551025
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       107.94    361.7854   361.8257    3.1623     1.8524      0.0    
   100       215.42    191.6896   192.3627    3.162      0.7617      0.0    
   150       320.85    169.1302   168.7699    2.8839     0.3676      0.0    
   200       424.77    160.1656   159.8007     2.54      0.2458      0.0    
   250       528.76    156.3146   155.4488    2.6867     0.1433      0.0    
   300       633.24    151.6116   151.4862    2.0067     0.097       0.0    
   350       741.48    150.5734   150.3238    1.5584     0.0791      0.0    
   400       849.75    149.0919   148.5169    1.6409     0.0681      0.0    
   450       958.14    147.6368   147.8431     1.99      0.0915      0.0    
   500       1066.5    147.8405   147.2968    1.7172     0.0827      0.0    
   550      1174.84    146.9794   146.8545    1.4305     0.0589      0.0    
   600      1283.34    147.5312   146.8468    1.254      0.0476      0.0    
   650      1391.71    146.381    146.579     0.999       0.04       0.0    
   700      1500.02    146.5421   145.885     1.0348     0.0463      0.0    
   750      1608.42    146.6516   146.5661    0.962      0.0444      0.0    
   800      1716.78    147.3717   146.5616    0.9915     0.0407      0.0    
   850      1825.39    146.6564   146.5722    0.954      0.0343      0.0    
   900      1933.74    146.5949   146.2218    1.1635     0.0416      0.0    
   950      2041.93    146.4646   146.2955    1.1169     0.0362      0.0    
   1000     2150.14    146.5619   145.8605    0.9732     0.0356      0.0    
   1050     2258.32    145.5903   145.7006    0.9492     0.0291      0.0    
   1100     2366.59    147.4664   146.6268    0.9931     0.0277      0.0    
   1150     2474.88    146.3645   146.2676    1.2163     0.0321      0.0    
   1200      2583.1    146.5694   146.0631    1.0777     0.0304      0.0    
   1250     2691.38    146.0086   145.5801    1.1132     0.0268      0.0    
   1300      2799.7    146.0766   145.866     1.0131     0.0284      0.0    
   1350     2907.95    146.8502   146.1466    1.0844     0.0257      0.0    
   1400     3016.26    145.808    145.5992    0.877      0.0271      0.0    
   1450     3124.63    145.8045   145.3909    0.6955     0.0243      0.0    
   1500     3232.91    146.2804   145.9083    0.8548     0.0213      0.0    
   1550     3341.14    146.5324   145.8538    0.906      0.0212      0.0    
   1600     3449.36    146.0449   145.7992    1.0088     0.0212      0.0    
   1650     3557.56    145.7745   145.7584    1.1641     0.0221      0.0    
   1700      3666.3    145.8683   145.5879    1.0288     0.0193      0.0    
   1750     3774.59    146.5531   145.9623    0.8414     0.0208      0.0    
   1800     3882.85    145.4659   145.4016    0.6821     0.0181      0.0    
   1850      3991.1    145.9371   145.4018    0.6611     0.0171      0.0    
   1900     4099.33    146.1909   145.7472    0.788      0.0172      0.0    
   1950     4207.56    145.905    145.8172    0.9026     0.0174      0.0    
   2000     4315.93    146.0313   145.5564     0.74      0.0162      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       108.71    304.5752   304.0802    3.0301     1.8419      0.0    
   100       217.82    177.542    177.2763    3.089      0.7918      0.0    
   150       327.15    161.1321   160.1427    2.199      0.2792      0.0    
   200       436.48    154.483    154.3861    2.3232     0.2879      0.0    
   250       545.77    152.4013   152.0305    2.1013     0.1522      0.0    
   300       655.21    150.4617   150.0207    2.0626     0.1286      0.0    
   350       764.58    150.2192   149.6422    1.7531     0.1131      0.0    
   400       873.88    148.0784   148.0429    1.4926     0.1136      0.0    
   450       983.33    147.0258   146.7587    1.3839     0.0843      0.0    
   500      1092.86    148.1744   147.6066    1.2737     0.0826      0.0    
   550      1202.39    147.5191   146.9693    1.1921     0.0672      0.0    
   600      1311.71    147.4471   147.3639    1.1845     0.0792      0.0    
   650      1421.13    147.1138   146.7707    1.3646     0.0601      0.0    
   700      1530.52    146.9257   146.6141    1.6851     0.0579      0.0    
   750      1639.93    146.9043   146.822     1.5745     0.0586      0.0    
   800      1749.34    146.3703   146.0344    1.0266      0.05       0.0    
   850      1858.81    147.2852   146.8323    1.1669     0.0487      0.0    
   900      1968.24    146.5942   146.5415    1.1946     0.0495      0.0    
   950      2077.75    146.5945   146.2849    1.149      0.0454      0.0    
   1000     2187.12    146.624    146.1942    1.3691     0.0402      0.0    
   1050     2296.49    146.5813   146.297     1.5583     0.0393      0.0    
   1100     2405.82    147.0239   146.6061    0.9704     0.0421      0.0    
   1150     2515.17    146.2433   145.6575    0.8975     0.0345      0.0    
   1200     2624.52    145.9764   145.7623    1.0034     0.0378      0.0    
   1250     2733.84    145.6974   145.607     0.979      0.0396      0.0    
   1300     2843.18    145.8243   145.4343    0.9328      0.03       0.0    
   1350     2952.75    146.2304   145.6966    0.7532     0.0361      0.0    
   1400     3062.21    145.8812   145.4736    0.8212     0.029       0.0    
   1450     3171.71    146.703    146.1301    1.0039     0.0259      0.0    
   1500     3281.02    146.3419   146.3366    1.2946     0.0249      0.0    
   1550     3390.95    146.4064   146.0778    1.1008     0.0291      0.0    
   1600     3500.39    145.8968   145.6504    1.0389     0.0241      0.0    
   1650     3609.77    146.3871   146.0416    1.1918     0.025       0.0    
   1700     3719.16    146.7284   146.2142    0.9141     0.0251      0.0    
   1750     3828.02    145.2345   144.9479    0.9979     0.0259      0.0    
   1800     3936.29    145.4392   145.3149     0.95      0.0223      0.0    
   1850     4044.63    145.8415   145.6733    0.6846     0.0223      0.0    
   1900     4152.99    145.9998   145.8959    1.0583     0.0215      0.0    
   1950     4261.36    145.9934   145.7492    0.9369     0.0188      0.0    
   2000      4369.8    145.7929   145.7484    1.0038     0.0233      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       107.86    291.3667   290.0878    2.8807     1.8077      0.0    
   100       216.68    175.6104   174.5432    2.6954     0.974       0.0    
   150       326.23    158.3027   157.4181    1.6819     0.2865      0.0    
   200       435.91    151.4555   150.8096    1.4972     0.1602      0.0    
   250       545.03    147.6564   147.4209    1.4155     0.1526      0.0    
   300       654.17    147.4786   146.742     1.1194     0.1224      0.0    
   350       763.3     146.7063   146.1952    1.1109     0.1379      0.0    
   400       872.42    147.1491   146.5838    1.5983     0.1126      0.0    
   450       981.62    147.1035   146.5589    1.4875     0.0912      0.0    
   500      1090.81    146.8319   146.3488    1.2484     0.0865      0.0    
   550      1199.99    146.6862   146.3961    1.3462     0.0779      0.0    
   600      1309.17    146.6284   146.3967    1.4747     0.079       0.0    
   650      1418.31    146.4212   146.2516    1.5193     0.0751      0.0    
   700       1527.5    146.579    146.1585    1.0846     0.067       0.0    
   750      1637.22    146.1563   145.5445    1.2091     0.0727      0.0    
   800      1746.47    145.6169   145.4335    1.1111     0.0665      0.0    
   850      1855.84    146.3014   145.7789    0.8949     0.0486      0.0    
   900      1965.13    145.9893   145.3909    1.0044     0.0438      0.0    
   950      2074.38    145.5396   145.4621    0.7916     0.0563      0.0    
   1000     2185.09    145.9565   145.501     0.8567     0.0437      0.0    
   1050      2294.4    145.6398   145.3902    0.9019     0.0395      0.0    
   1100      2403.7    145.9846   145.5119    0.9257      0.04       0.0    
   1150     2513.03    146.3988   146.1391    0.8067     0.0422      0.0    
   1200     2622.37    145.619    145.217     0.9798     0.0368      0.0    
   1250      2731.7    145.8818   145.4781    0.7855     0.0453      0.0    
   1300     2841.09    145.595    145.3972    0.9508     0.0334      0.0    
   1350      2950.4    145.7112   145.1805    0.9672     0.0299      0.0    
   1400     3060.14    145.5229   145.3407    0.8799     0.0299      0.0    
   1450     3169.47    146.2743   145.6869    1.0599     0.0297      0.0    
   1500     3278.83    145.5265   145.3027    0.6602     0.0278      0.0    
   1550     3388.14    145.8512   145.2426    0.8877     0.0267      0.0    
   1600     3497.41    146.3904   145.8664    0.8685     0.0271      0.0    
   1650     3606.73    145.2701   145.2308    0.7859     0.0265      0.0    
   1700     3716.08    145.904    145.6181    0.816      0.0261      0.0    
   1750     3825.43    145.9115   145.4923    0.8561     0.0263      0.0    
   1800     3935.36    145.5197   145.0925    0.9016     0.0275      0.0    
   1850     4044.75    145.5761   145.3809    0.8982     0.0194      0.0    
   1900     4154.06    145.3438   144.9105    0.6497     0.0211      0.0    
   1950     4263.36    145.2376   144.8279    0.6231     0.0212      0.0    
   2000     4373.16    145.1372   145.0501    0.9729     0.0203      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       107.58    285.689    284.6068    2.7513     1.763       0.0    
   100       216.87    179.4435   178.3812    2.625      0.7759      0.0    
   150       326.33    159.2547   158.7484    2.0271     0.3269      0.0    
   200       435.92    152.211    152.2089    1.2974     0.1736      0.0    
   250       545.46    148.7822   148.5351    1.1054     0.1464      0.0    
   300       655.11    146.6522   146.8506    0.963      0.1234      0.0    
   350       764.76    147.0479   147.1072    0.9211     0.1099      0.0    
   400       874.5     146.0026   146.2061    0.8249     0.104       0.0    
   450       984.56    145.6897   145.7286    1.0941     0.1055      0.0    
   500      1094.22    146.3953   146.4128    0.9221     0.1002      0.0    
   550      1203.83    146.3488   146.5577    1.1935     0.0751      0.0    
   600      1313.92    146.2647   146.0236    0.9905     0.0783      0.0    
   650      1423.77    146.0186   145.9183    0.8186     0.0637      0.0    
   700      1533.35    145.5884   145.6859    1.2234     0.0722      0.0    
   750      1642.92    146.021    146.0007    1.2932     0.0622      0.0    
   800      1752.61    145.3479   145.4535    0.8082     0.0556      0.0    
   850      1862.29    146.5345   146.1154    0.9673     0.0582      0.0    
   900       1971.9    145.9063   145.9531    1.0328     0.0535      0.0    
   950      2081.47    145.7241   145.643     0.9836     0.0518      0.0    
   1000     2191.34    145.7935   145.691     1.2022     0.0456      0.0    
   1050     2301.23    145.3981   145.4918    0.9063     0.0486      0.0    
   1100     2411.27    145.8088   146.0881    0.9688     0.0371      0.0    
   1150     2521.31    145.2703   145.6305    0.8633     0.0379      0.0    
   1200     2631.25    144.9158   145.2072    0.7907     0.0368      0.0    
   1250      2741.4    145.2571   145.1784    0.6123     0.0331      0.0    
   1300      2851.3    146.058    146.117     0.7598     0.036       0.0    
   1350     2961.26    145.7687   145.981     1.2647     0.0356      0.0    
   1400     3071.24    146.0208   146.0746    1.0745     0.0296      0.0    
   1450     3182.23    145.2619   145.364     0.7775     0.0289      0.0    
   1500     3292.26    145.5979   145.646     0.7946     0.0278      0.0    
   1550     3402.53    145.7139   145.9497    0.8489     0.0254      0.0    
   1600     3512.53    145.6167   145.5064    0.7139     0.0308      0.0    
   1650     3622.65    145.1087   145.2752    0.5991     0.0253      0.0    
   1700     3732.68    145.9769   145.6507    0.9122     0.0285      0.0    
   1750     3842.74    145.0344   145.3032    0.7542     0.0235      0.0    
   1800     3953.21    145.626    145.534     0.7115     0.0258      0.0    
   1850     4062.99    144.9993   145.2907    0.8751     0.0222      0.0    
   1900     4173.08     145.46    145.5165    0.9522     0.0239      0.0    
   1950     4283.03    145.3429   145.6253    0.8901     0.0197      0.0    
   2000     4392.88    144.9531   145.2643    0.9562     0.0192      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       107.96    286.232    285.1615    2.6386     1.7552      0.0    
   100       218.24    186.3089   184.8052    2.4155     0.7919      0.0    
   150       328.81    162.472    161.859     1.7003     0.2718      0.0    
   200       439.51    153.0484   152.4727    1.2569     0.217       0.0    
   250       550.18    149.7423   149.2798    1.0027     0.2128      0.0    
   300       661.37    147.3158   147.3427    1.2616     0.1375      0.0    
   350       772.24    146.4011   146.3364    1.0663     0.1307      0.0    
   400       883.37    146.6027   146.6578    1.1482     0.1148      0.0    
   450       994.1     145.4983   145.5503    1.1885     0.1104      0.0    
   500      1105.09    145.5093   145.6055    0.7364     0.1005      0.0    
   550      1215.94    145.787    145.5316    0.8622     0.087       0.0    
   600      1326.72    144.9747   145.1603    0.9644     0.0777      0.0    
   650      1437.46    145.6211   145.5234    0.8843     0.0758      0.0    
   700       1548.2    145.3154   145.4163    0.7096     0.0564      0.0    
   750      1659.09    145.6123   145.6858    0.852      0.0558      0.0    
   800      1769.92    145.446    145.3757    1.078      0.0611      0.0    
   850      1880.67    145.7399   145.6249    1.0444     0.052       0.0    
   900      1991.52    145.4674   145.5792    1.0064     0.0516      0.0    
   950      2102.17    145.3487   145.3648    0.7974     0.0474      0.0    
   1000     2212.85    145.8482   145.584     0.8604     0.0453      0.0    
   1050      2324.1    145.1389   145.4141    0.8901     0.0442      0.0    
   1100      2435.6    145.0486   144.9608    0.7956     0.0383      0.0    
   1150     2546.38    145.9232   146.0145    0.9822     0.049       0.0    
   1200     2657.08    144.8287   144.9708    0.8936     0.0375      0.0    
   1250     2767.87    145.0621   145.0521    0.5811     0.0374      0.0    
   1300      2878.8    145.5984   145.5178    0.5011     0.0324      0.0    
   1350     2989.59    145.1214   145.0061    0.7396     0.033       0.0    
   1400     3100.46    144.8493   145.067     0.6516     0.0369      0.0    
   1450      3211.2    145.1756   145.1775    0.7572     0.0368      0.0    
   1500     3322.06    145.317    145.1882    0.8497     0.0367      0.0    
   1550     3432.82    145.4325   145.2312    0.9434     0.0267      0.0    
   1600     3543.51    145.1604   145.2929    0.8967     0.0274      0.0    
   1650     3654.09    145.5665   145.5129    0.8145     0.0292      0.0    
   1700     3764.73    145.3425   145.3665    0.7973     0.0265      0.0    
   1750     3875.44    145.148    145.3119    0.769      0.0236      0.0    
   1800     3986.12    145.2387   145.0641    0.6877     0.0257      0.0    
   1850     4096.87    144.8435   144.885     0.6294     0.0251      0.0    
   1900     4207.88    145.109    145.0692    0.6659     0.0205      0.0    
   1950     4318.76    144.781    144.7915    0.6633      0.02       0.0    
   2000     4429.41    144.7636    144.91     0.6696     0.0197      0.0    
