Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  7.772694826126099
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       48.22      3.8516     3.869      0.3162     0.2923    58955.93   47617.91  
   100       209.76     3.8712     3.9028     0.2804     0.2278    57820.9    46696.97  
   150       371.42     3.8941     3.9281     0.2475     0.1816    56960.03   45996.5   
   200       532.97     3.9198     3.9419     0.2192     0.1599    56272.57   45435.52  
   250       694.27     3.9358     3.9687     0.2017     0.1358    55709.13   44975.78  
   300       855.98     3.9535     3.9886     0.2045     0.1187    55238.85   44591.59  
   350      1018.53     3.9656     3.999      0.1788     0.105     54832.4    44259.47  
   400       1180.3     3.9807     4.0099     0.1644     0.0941    54478.65   43970.5   
   450      1341.93     3.9937     4.0228     0.1544     0.0873    54164.3    43713.77  
   500       1503.5     4.0037     4.0338     0.1584     0.0802    53884.22   43485.42  
   550      1665.16     4.015      4.0448     0.1513     0.0735    53634.74   43282.23  
   600      1826.66     4.0228     4.0518     0.1491     0.0673    53410.69   43099.87  
   650      1987.78     4.0335     4.0598     0.1357     0.0639    53207.59   42934.52  
   700      2149.05     4.0354     4.0689     0.133      0.0582    53022.13   42783.93  
   750      2310.46     4.0408     4.0741     0.1279     0.0561    52852.41   42646.01  
   800      2471.93     4.0507     4.0832     0.1272     0.0535    52697.37   42519.94  
   850      2633.44     4.0586     4.0888     0.1239     0.049     52555.89   42405.18  
   900      2795.41     4.0627     4.0935     0.1247     0.0466    52424.58   42298.92  
   950      2957.37     4.067      4.098      0.1184     0.0435    52304.39   42201.56  
   1000     3119.27     4.0711     4.1041     0.1295     0.0408    52195.14   42113.2   
   1050     3281.26     4.0767     4.1071     0.1153     0.0404    52095.49   42032.51  
   1100     3442.91     4.0803     4.1115     0.1126     0.0374    52001.74   41956.76  
   1150     3604.22     4.0831     4.1168     0.1109     0.0352    51914.44   41886.29  
   1200     3765.65     4.0845     4.1168     0.1093     0.0356    51834.15   41821.47  
   1250     3927.18     4.0844     4.1159     0.1115     0.0315    51757.24   41759.45  
   1300     4088.85     4.0832     4.1161     0.1189     0.0306    51677.66   41695.35  
   1350     4250.65     4.0768     4.1076     0.127      0.0294    51594.48   41628.49  
   1400      4412.8     4.0676     4.0976     0.1321     0.0304    51497.79   41550.97  
   1450     4577.66     4.0503     4.0809     0.1444     0.0283    51385.6    41461.24  
   1500     4737.92     4.0228     4.0579     0.1408     0.0268    51244.24   41348.0   
   1550     4898.07     3.9882     4.0179     0.1377     0.0269    51072.09   41210.8   
   1600     5058.24     3.9381     3.9705     0.148      0.0265    50862.24   41043.53  
   1650     5218.99     3.8783     3.908      0.1452     0.0248    50625.67   40855.08  
   1700     5379.59     3.806      3.8355     0.1432     0.0249    50361.14   40644.61  
   1750     5540.28     3.717      3.7458     0.149      0.0249    50091.55   40430.0   
   1800     5701.16     3.618      3.646      0.1465     0.024     49817.77   40212.01  
   1850     5862.42     3.498      3.5255     0.1416     0.0233    49564.54   40009.78  
   1900     6022.72     3.3732     3.393      0.1462     0.0223    49332.06   39823.0   
   1950     6184.41     3.2291     3.2545     0.1542     0.0218    49120.44   39652.53  
   2000     6347.15     3.0723     3.0966     0.1592     0.0209    48955.38   39518.68  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       47.15      3.8229     3.8403     0.1632     0.1193    60022.42   97126.04  
   100       255.56     3.8313     3.8489     0.1472     0.1079    59512.82   96293.25  
   150       467.83     3.8456     3.8648     0.1562     0.1061    59037.56   95517.83  
   200       683.33     3.854      3.8735     0.1359     0.1024    58587.87   94783.02  
   250       895.75     3.8652     3.8897     0.1338     0.0945    58170.43   94100.38  
   300      1107.83     3.8816     3.8979     0.1266     0.0886    57779.2    93460.13  
   350      1318.48     3.8862     3.9074     0.1262     0.0861    57414.18   92862.53  
   400      1529.03     3.8986     3.9174     0.1197     0.0816    57067.45   92293.82  
   450      1739.79     3.9088     3.9261     0.1436     0.0749    56746.43   91767.33  
   500      1949.85     3.9188     3.9364     0.1217     0.0744    56445.68   91273.92  
   550      2159.67     3.9245     3.9435     0.1165     0.0698    56162.41   90808.78  
   600      2369.95     3.9344     3.9504     0.114      0.0727    55893.72   90367.54  
   650      2580.39     3.9431     3.9612     0.1174     0.0662    55641.39   89952.81  
   700      2790.39     3.9533     3.9671     0.1029     0.0626    55402.96   89561.24  
   750      3000.41     3.9577     3.9753     0.1019     0.0593    55178.61   89192.35  
   800       3210.3     3.9672     3.9845     0.1066     0.0583    54965.9    88842.87  
   850      3420.33     3.9717     3.9949     0.1094     0.0552    54766.85   88515.64  
   900      3630.62     3.9785     3.9984     0.1057     0.0523    54575.89   88202.2   
   950      3840.86     3.9874     4.0065     0.0958     0.0523    54395.62   87906.08  
   1000     4050.83     3.992      4.0114     0.0958     0.0505    54225.0    87625.68  
   1050     4260.75     4.0015     4.0158     0.0934     0.0467    54062.53   87359.11  
   1100      4470.5     4.0019     4.0245     0.0863     0.0482    53906.81   87103.6   
   1150     4681.21      4.01      4.0302     0.0874     0.0444    53758.25   86860.07  
   1200     4893.19     4.0154     4.0339     0.0806     0.0435    53617.09   86628.67  
   1250     5105.73     4.0202     4.0417     0.0927     0.043     53482.32   86407.71  
   1300     5316.89     4.0252     4.046      0.0773     0.0399    53354.63   86198.54  
   1350     5526.69     4.0296     4.0514     0.0792     0.0404    53231.3    85996.39  
   1400     5736.41     4.0357     4.0565     0.0841     0.0406    53113.05   85802.87  
   1450     5946.76     4.0394     4.0584     0.0897     0.0385    53002.0    85621.08  
   1500     6157.37     4.0412     4.0655     0.0786     0.0382    52894.96   85445.98  
   1550      6368.0     4.0469     4.0677     0.082      0.035     52793.52   85280.16  
   1600     6578.37     4.0496     4.0732     0.0734     0.0319    52695.77   85120.31  
   1650     6788.99     4.0554     4.0768     0.073      0.0316    52603.21   84968.88  
   1700     7000.03     4.0593     4.0815     0.0833     0.0321    52513.53   84822.22  
   1750     7210.56     4.0635     4.0828     0.0722      0.03     52428.03   84682.43  
   1800     7420.85     4.0666     4.0889     0.0729     0.0296    52346.78   84549.63  
   1850     7631.19     4.0681     4.0884     0.0723     0.0307    52269.14   84422.73  
   1900     7841.31     4.0733     4.0947     0.067      0.0281    52194.53   84300.85  
   1950     8051.34     4.075      4.0977     0.0721     0.0275    52123.02   84184.13  
   2000     8261.51     4.0799     4.0996     0.0658     0.0266    52055.44   84073.74  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       47.79      3.8254     3.8303     0.1207     0.0877    60162.88  145444.22  
   100       308.53     3.8319     3.8365     0.1192     0.0823    59776.85   144504.0  
   150       569.37     3.8348     3.8495     0.1152     0.0766    59407.71  143604.75  
   200       829.88     3.8483     3.8561     0.1243     0.0787    59051.86  142737.52  
   250      1090.91     3.8523     3.8675     0.1202     0.0731    58708.66  141900.79  
   300      1351.26     3.8627     3.8751     0.1054     0.0687    58380.31  141100.13  
   350      1611.56     3.876      3.8836     0.1021     0.0712    58066.86  140335.18  
   400      1871.53     3.8814     3.8925     0.0997     0.0685    57767.82   139606.2  
   450      2131.64     3.8826     3.8995     0.1047     0.0716    57482.34  138909.43  
   500      2393.33     3.8931     3.9075     0.0989     0.0641    57209.52  138244.04  
   550      2653.32     3.9035     3.9131     0.1034     0.0627    56950.66  137611.71  
   600      2914.51     3.9093     3.9209     0.1069     0.061     56701.44  137002.42  
   650      3178.06     3.9151     3.9295     0.0933     0.0597    56463.75   136420.4  
   700      3442.72     3.9263     3.9351     0.0946     0.0601    56235.27  135861.65  
   750      3706.46     3.9294     3.9398     0.0875     0.0555    56017.66  135329.58  
   800      3967.38     3.9365     3.9481     0.0851     0.0533    55810.62  134822.97  
   850      4227.78     3.9459     3.9559     0.0978     0.053     55611.05  134334.84  
   900      4490.38     3.9452     3.9599     0.0946     0.0508    55418.44  133863.42  
   950      4755.35     3.9541     3.9682     0.0858     0.0491    55236.66  133418.39  
   1000     5020.53     3.9624     3.9752     0.0862     0.0523    55059.37   132984.4  
   1050      5283.2     3.9669     3.9807     0.0779     0.046     54891.93  132574.56  
   1100     5543.82     3.9728     3.9847     0.0867     0.0471    54730.61  132179.53  
   1150     5804.47     3.9776     3.9933     0.0856     0.0441    54574.38   131797.1  
   1200     6064.88     3.9878     3.9983     0.0759     0.0445    54425.74  131433.09  
   1250      6325.5     3.9914     4.0016     0.0829     0.0406    54282.38  131082.29  
   1300     6585.54     3.9962     4.0077     0.0818     0.0401    54145.85  130748.29  
   1350     6845.84     3.9958     4.0133     0.0793     0.0387    54012.65  130422.76  
   1400     7106.21     4.0024     4.0168     0.0697     0.0382    53885.26  130111.38  
   1450     7366.36     4.0093     4.0233     0.0797     0.0358    53762.98  129812.61  
   1500     7626.68     4.0117     4.027      0.0722     0.0377    53644.11  129522.39  
   1550     7886.79     4.0181     4.0317     0.0749     0.0373    53529.0   129241.35  
   1600     8146.93     4.0194     4.0333     0.0685     0.034     53418.93  128972.35  
   1650     8408.26     4.0264     4.0387     0.0651     0.0319    53313.93  128716.06  
   1700     8668.43     4.0303     4.0418     0.0739     0.0325    53211.28   128465.7  
   1750     8928.71     4.0344     4.0472     0.0641     0.0323    53112.82  128225.24  
   1800     9188.86     4.0393     4.0511     0.0708     0.0305    53017.81  127993.59  
   1850     9449.24     4.0414     4.0552     0.071      0.0311    52926.64  127771.43  
   1900     9709.52     4.0468     4.0605     0.0658     0.0322    52838.46  127556.28  
   1950      9969.7     4.0493     4.0633     0.0627     0.0279    52754.2   127351.16  
   2000     10229.67    4.0546     4.0676     0.0666     0.0296    52671.56  127149.93  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       47.81      3.8179     3.8282     0.1009     0.0748    60230.36  194282.02  
   100       358.27     3.8247     3.8339     0.0996     0.0713    59904.9    193226.0  
   150       668.88     3.8316     3.8411     0.1021     0.0699    59586.0   192190.42  
   200       979.68     3.8387     3.8482     0.0982     0.0662    59277.87  191190.12  
   250      1290.34     3.8447     3.8576     0.096      0.0624    58980.79  190225.35  
   300      1600.91     3.8531     3.8621     0.0885     0.0643    58693.93  189292.55  
   350      1911.53     3.8612     3.8667     0.0969     0.0604    58419.74  188401.31  
   400      2221.92     3.8668     3.8773     0.0897     0.0601    58153.33  187534.86  
   450       2536.0     3.8731     3.8837     0.0956     0.0574    57895.2   186695.82  
   500      2846.41     3.883      3.8905     0.0874     0.0567    57646.28  185885.01  
   550      3156.34     3.8886     3.8984     0.085      0.0551    57408.12  185109.65  
   600      3466.12     3.8995     3.9054     0.083      0.0572    57177.46  184358.66  
   650      3782.42     3.9005     3.9129     0.0867     0.0535    56956.04  183637.52  
   700      4101.76     3.9087     3.9177     0.0766     0.0537    56741.25  182938.34  
   750      4420.85     3.9114     3.9242     0.0797     0.0488    56536.44  182270.72  
   800      4733.33     3.9181     3.9291     0.0817     0.0478    56338.65  181625.28  
   850      5043.68     3.9244     3.9357     0.0777     0.049     56146.81  181000.26  
   900      5354.12     3.9313     3.9415     0.0779     0.0465    55960.76  180393.64  
   950      5664.66     3.9354     3.9466     0.0826     0.0476    55782.42  179811.66  
   1000     5983.25     3.9434     3.9547     0.0862     0.0468    55609.91  179248.97  
   1050     6299.85     3.9469     3.9596     0.0762     0.0439    55444.38  178708.72  
   1100     6613.46     3.9514     3.9649     0.0795     0.043     55284.5   178187.24  
   1150     6927.13     3.9591     3.9692     0.0729     0.0424    55130.72  177685.38  
   1200     7241.03     3.9641     3.9752     0.0661     0.0394    54982.63   177202.4  
   1250     7551.01     3.9677     3.9798     0.0702     0.0384    54840.27  176737.81  
   1300     7861.05     3.975      3.9852     0.0662     0.0398    54701.52  176285.56  
   1350     8171.56     3.9786     3.9896     0.0685     0.0379    54567.02  175846.33  
   1400     8481.87     3.9828     3.9951     0.0696     0.0404    54436.53  175420.35  
   1450      8792.7     3.9873     3.9989     0.0669     0.0359    54310.85   175010.4  
   1500     9102.77     3.9956     4.0046     0.0652     0.0349    54189.31  174613.89  
   1550     9410.97     3.9982     4.0095     0.0711     0.0346    54071.98  174231.14  
   1600     9711.84     4.0019     4.0128     0.0683     0.0342    53957.35  173857.79  
   1650     10012.77    4.0052     4.016      0.0723     0.0325    53845.05  173491.98  
   1700     10313.38    4.0084     4.0202     0.0685     0.033     53737.34  173140.85  
   1750     10614.22    4.0143     4.0243     0.065      0.031     53632.91  172800.49  
   1800     10924.52    4.0172     4.0269     0.0694     0.0314    53532.3    172472.8  
   1850     11229.34    4.019      4.0327     0.0634     0.0296    53435.05  172156.08  
   1900     11535.68    4.0242     4.0366     0.0641      0.03     53339.99  171846.72  
   1950     11845.55    4.0313     4.0392     0.0664     0.0291    53248.29  171548.24  
   2000     12155.24    4.0307     4.0426     0.0655     0.0282    53159.08  171257.76  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       48.08      3.8201     3.8291     0.0956     0.0655    60271.13  243088.33  
   100       410.36     3.8232     3.8378     0.0908     0.0608    59983.35  241921.02  
   150       764.37     3.8325     3.8433     0.0976     0.0608    59702.39  240780.86  
   200      1113.43     3.8339     3.8499     0.1029     0.0628    59427.85  239665.59  
   250      1462.38     3.8424     3.8574     0.0882     0.0593    59160.97  238582.22  
   300      1811.56     3.8486     3.8638     0.0844     0.0582    58900.91  237526.01  
   350      2161.23     3.8538     3.8726     0.0857     0.0563    58650.34  236507.33  
   400      2510.97     3.866      3.8769     0.0862     0.0566    58403.82  235506.62  
   450      2860.56     3.8669     3.8833     0.0782     0.0545    58168.77  234551.66  
   500      3210.72     3.8754     3.8899     0.0779     0.0521    57942.97  233634.44  
   550      3565.43     3.884      3.8932     0.0765     0.0525    57719.71  232725.99  
   600      3914.73     3.8857     3.9003     0.0854     0.0481    57504.8   231851.92  
   650      4264.35     3.895      3.9074     0.0698     0.0514    57295.16  231000.02  
   700      4614.08     3.9004     3.9132     0.0777     0.0492    57090.66  230167.99  
   750      4969.12     3.9062     3.9197     0.0777     0.0477    56894.67  229370.59  
   800      5318.85     3.9095     3.9241     0.0738     0.0477    56705.74  228602.06  
   850      5672.94     3.9176     3.9298     0.0727     0.0477    56523.67  227861.41  
   900       6026.4     3.9182     3.9342     0.0682     0.0434    56346.02  227137.42  
   950      6385.59     3.9203     3.9411     0.0712     0.0446    56173.97   226436.7  
   1000     6749.58     3.9291     3.9448     0.0671     0.0446    56009.45  225766.35  
   1050     7104.22     3.9357     3.9512     0.0752     0.0434    55848.61  225110.24  
   1100     7460.25     3.9398     3.955      0.072      0.0401    55693.81  224479.61  
   1150     7815.68     3.9438     3.9613     0.0737     0.0394    55544.16  223869.92  
   1200     8173.59     3.9512     3.9664     0.0693     0.0402    55397.24  223271.22  
   1250     8529.54     3.9544     3.9697     0.0685     0.0387    55255.96  222695.67  
   1300     8885.51     3.9567     3.9742     0.0624     0.0362    55118.84  222137.04  
   1350     9241.18     3.9613     3.9802     0.0627     0.0381    54986.53  221597.86  
   1400      9596.2     3.9681     3.9843     0.064      0.0368    54857.12  221070.09  
   1450     9951.86     3.973      3.9899     0.0609     0.0353    54730.48  220554.03  
   1500     10307.09    3.9772     3.9945     0.0639     0.0341    54607.28   220051.9  
   1550     10661.83    3.981      3.998      0.0632     0.0341    54488.34  219567.34  
   1600     11016.83    3.9879     4.0035     0.065      0.0371    54372.88  219096.73  
   1650     11371.85    3.9911     4.0071     0.0589     0.0323    54259.92  218636.88  
   1700     11727.26    3.9913     4.011      0.0611     0.0332    54150.98  218193.04  
   1750     12082.29    3.9977     4.0148     0.059      0.0317    54045.16  217762.16  
   1800     12438.02    4.0004     4.0162     0.0582     0.0293    53943.25  217347.17  
   1850     12793.59    4.0046     4.0225     0.0568     0.0316    53842.69  216938.24  
   1900     13149.36    4.009      4.026      0.0536     0.0306    53745.28  216541.34  
   1950     13504.57    4.0144     4.0296     0.0535     0.028     53650.29   216154.5  
   2000     13859.62    4.0132     4.0334     0.0603     0.0287    53558.89  215782.88  
Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  8.140228033065796
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       145.18     4.2448     4.2785     0.3162     4.4199   128892.62  104490.08  
   100       476.31     4.2406     4.2743     0.3127     0.2422   127985.48  103752.96  
   150       807.65     4.2243     4.2579     0.3034     0.0487   124996.82  101321.44  
   200      1138.36     4.1858     4.2189     0.2934     0.0234   118852.01   96320.09  
   250      1468.77     4.1114     4.1439     0.2858     0.0262   109181.48   88470.38  
   300      1799.43     3.9891     4.0205     0.2818     0.0217    96917.3    78516.24  
   350      2130.55     3.8107     3.8412     0.2755     0.0186    83549.4    67673.13  
   400      2462.24     3.5835     3.6129     0.2603     0.016     70854.22   57388.04  
   450       2793.5     3.326      3.3537     0.2555     0.0132    59985.58   48586.01  
   500      3125.12     3.0667     3.0912     0.218      0.0108    51558.7    41745.31  
   550      3457.19     2.8141     2.8366     0.2333     0.011     45142.94   36544.09  
   600      3787.97     2.5886     2.6078     0.1804     0.0127    40569.84   32825.77  
   650      4119.38     2.3874     2.4034     0.1934     0.0114    37237.35   30119.22  
   700      4450.36     2.2064     2.2194     0.1578     0.0127    34773.71   28117.07  
   750      4781.03     2.0485     2.0608     0.1221     0.0121    32960.28   26648.54  
   800      5111.73     1.9131     1.9233     0.177      0.0115    31628.94   25559.84  
   850      5442.39     1.7949     1.8024     0.2031     0.0111    30604.99   24723.17  
   900      5773.27     1.6879     1.694      0.2565     0.0106    29761.83   24046.94  
   950      6103.67     1.5956     1.6002     0.1162     0.0105    29127.9    23532.24  
   1000     6433.83     1.5138     1.515      0.1874      0.01     28610.2    23105.7   
   1050     6764.04     1.4452     1.4444     0.1918     0.0099    28196.07   22767.77  
   1100      7094.6     1.3826     1.3826     0.1028     0.0096    27868.5    22504.04  
   1150     7424.45     1.3298     1.3279     0.0788     0.0094    27612.98   22297.71  
   1200     7754.04     1.2846     1.2804     0.0942     0.0091    27398.72   22124.06  
   1250     8082.37     1.2443     1.2392     0.1115     0.009     27232.94   21982.3   
   1300     8409.93     1.2107     1.2055     0.193      0.0088    27092.31   21875.46  
   1350     8738.08     1.1798     1.1731     0.1593     0.0082    26974.6    21779.84  
   1400     9066.52     1.1528     1.1451     0.1587     0.008     26876.45   21690.59  
   1450      9395.3     1.1298     1.1201     0.1383     0.0076    26799.38   21628.55  
   1500     9724.04     1.1077     1.099      0.1744     0.0074    26699.11   21555.36  
   1550     10052.63    1.0913     1.0832     0.1152     0.0073    26646.1    21514.91  
   1600     10381.6     1.0786     1.067      0.2051     0.0069    26597.03   21470.19  
   1650     10710.32    1.0631     1.0528     0.168      0.0067    26546.14   21430.79  
   1700     11038.78    1.0509     1.0388     0.1408     0.0065    26507.7    21393.74  
   1750     11365.84    1.0397     1.0272     0.1054     0.0062    26472.53   21370.95  
   1800     11693.36    1.0286     1.0184     0.1122     0.0061    26464.35   21354.08  
   1850     12021.71    1.0221     1.0084     0.1087     0.0058    26435.59   21332.97  
   1900     12349.89    1.014      1.0022     0.1283     0.0057    26401.48   21313.53  
   1950     12678.13    1.0065     0.9939     0.0843     0.0056    26379.2    21292.85  
   2000     13006.67    1.0026     0.9889     0.1166     0.0053    26374.75   21287.19  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       145.69     4.1997     4.2218     0.0613     3.6176    87709.22  141753.04  
   100       556.05     4.2342     4.2571     0.0325     0.8478   110772.85  179078.56  
   150       965.77     4.2397     4.2626     0.0255     0.4004   117936.36  190619.14  
   200      1376.92     4.2415     4.2643     0.0266     0.2473    121144.5  195785.26  
   250      1785.67     4.2425     4.2651     0.0269     0.1769   122919.03  198642.74  
   300      2195.68     4.2428     4.2655     0.0289     0.1345   124047.29  200457.36  
   350      2604.85     4.2432     4.2658     0.0293     0.1067   124812.02  201688.08  
   400      3014.68     4.2436     4.2662     0.0295     0.0873   125368.51  202582.99  
   450      3423.25     4.2438     4.2665     0.0307     0.073    125789.13  203259.64  
   500      3832.18     4.2438     4.2663     0.0305     0.0629   126117.03  203786.88  
   550      4240.66     4.2439     4.2667     0.031      0.0549   126378.63  204207.49  
   600      4650.24     4.2439     4.2666     0.0321     0.0486   126592.37   204551.1  
   650      5059.56     4.244      4.2666     0.0309     0.0422   126768.81  204834.67  
   700      5468.99     4.2441     4.2667     0.0304     0.0384   126913.93  205067.88  
   750      5877.06     4.244      4.2667     0.0301     0.0349   127036.11  205264.31  
   800      6287.14     4.244      4.2668     0.0304     0.0318   127136.93  205426.24  
   850      6696.26     4.2439     4.2667     0.0318     0.0288   127219.91  205559.45  
   900       7106.2     4.2439     4.2668     0.0341     0.0264   127286.67  205666.58  
   950      7514.57     4.244      4.2667     0.0369     0.0244   127338.39  205749.56  
   1000     7924.15     4.2437     4.2666     0.0388     0.0227   127375.42  205808.77  
   1050     8332.25     4.2435     4.2664     0.0409     0.0215   127396.55  205842.43  
   1100     8740.33     4.2435     4.2662     0.0434     0.0197   127401.18  205849.35  
   1150      9148.3     4.2432     4.2659     0.0461     0.0184   127385.96  205824.45  
   1200     9556.74     4.2427     4.2655     0.0464     0.017    127347.26  205761.46  
   1250     9965.55     4.2421     4.265      0.048      0.0161   127280.34  205652.75  
   1300     10374.28    4.2416     4.2645     0.0518     0.0153   127178.48  205487.85  
   1350     10783.42    4.2409     4.2634     0.0554     0.0145   127026.75  205242.09  
   1400     11192.94    4.2395     4.2621     0.0619     0.0137   126809.47   204890.1  
   1450     11602.71    4.2378     4.2604     0.0686     0.0129   126511.07  204407.16  
   1500     12011.35    4.2356     4.2581     0.0737     0.0125   126088.55  203723.71  
   1550     12419.85    4.2321     4.2548     0.0857     0.012     125497.1  202767.76  
   1600     12828.26    4.2273      4.25      0.0943     0.0117   124647.46  201392.04  
   1650     13235.65     4.22      4.2425     0.1083     0.0115   123421.54  199406.02  
   1700     13632.73    4.2094     4.2316     0.1181     0.0113    121634.1  196510.68  
   1750     14028.7     4.1928     4.215      0.1322     0.0117   119035.98  192306.62  
   1800     14426.1     4.1672     4.1898     0.1697     0.0126   115351.27  186345.59  
   1850     14825.77    4.1294     4.1515     0.1741     0.0139   110335.36  178248.47  
   1900     15225.1     4.0732     4.095      0.1828     0.0158   103802.57  167693.34  
   1950     15614.78    3.994      4.0153     0.1987     0.0189    95932.89  154994.17  
   2000     16019.6     3.8876     3.9091     0.1949     0.0201    87239.06  140956.05  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       145.75     4.1794     4.1935     0.0714     3.455     80192.33  193723.68  
   100       629.98     4.2297     4.2436     0.0361     0.9221   104491.48   252446.2  
   150       1108.2     4.2372     4.2525     0.0238     0.4204   114000.86  275340.89  
   200      1587.06      4.24      4.2553     0.0208     0.2671   118422.47  285973.87  
   250       2074.4     4.2415     4.2566     0.0199     0.1891   120899.82   291925.6  
   300      2556.15     4.2425     4.2575     0.024      0.144    122463.77  295682.44  
   350      3039.16     4.2426     4.2579     0.0226     0.1148   123534.03  298252.28  
   400      3515.87     4.2431     4.2583     0.0227     0.0955    124307.4  300108.49  
   450      3993.74     4.2433     4.2585     0.0234     0.0791   124892.34  301512.69  
   500      4470.63     4.2437     4.2587     0.0236     0.0671   125350.53  302612.21  
   550      4947.35     4.2437     4.2589     0.0232     0.0588   125717.33  303492.79  
   600      5425.28     4.244      4.259      0.0246     0.0523   126017.87  304213.95  
   650      5902.15     4.244      4.2592     0.0242     0.046    126266.68  304811.07  
   700      6379.61     4.2441     4.2591     0.0274     0.0419   126477.21  305316.19  
   750      6857.02     4.244      4.2593     0.0235     0.0374   126657.56  305748.83  
   800      7334.43     4.244      4.2595     0.0242     0.0344   126812.92   306121.6  
   850      7811.31     4.2443     4.2594     0.027      0.0316    126947.4  306444.34  
   900      8288.33     4.2442     4.2593     0.0249     0.029    127065.36  306727.36  
   950      8766.88     4.2443     4.2594     0.0248     0.0267   127168.78  306975.45  
   1000     9234.26     4.2443     4.2594     0.0252     0.025    127259.54  307193.16  
   1050     9697.52     4.2442     4.2595     0.0254     0.0237   127339.26   307384.4  
   1100     10163.44    4.2444     4.2595     0.0256     0.0218   127409.03  307551.75  
   1150     10628.46    4.2442     4.2594     0.0261     0.0207   127469.65  307697.08  
   1200     11092.69    4.2443     4.2593     0.0283     0.0192   127521.36  307820.95  
   1250     11557.93    4.2441     4.2593     0.0277     0.0181   127565.44  307926.56  
   1300     12025.59    4.2441     4.2593     0.0297     0.0172    127601.1   308011.9  
   1350     12490.9     4.244      4.2592     0.0275     0.0163   127628.01  308076.14  
   1400     12957.82    4.244      4.2591     0.0281     0.0155    127646.5  308120.03  
   1450     13423.49    4.2438     4.259      0.0288     0.0148   127655.02  308140.03  
   1500     13892.79    4.2436     4.2588     0.0307     0.014    127651.01  308129.88  
   1550     14362.64    4.2435     4.2586     0.0368     0.0135   127632.72  308085.27  
   1600     14830.53    4.2431     4.2582     0.0383     0.0128   127597.59  307999.99  
   1650     15299.08    4.2427     4.2578     0.0372     0.0123   127538.94  307858.42  
   1700     15762.14    4.242      4.2573     0.0443     0.0117   127450.03  307643.72  
   1750     16227.08    4.2411     4.2565     0.0527     0.0113   127318.66  307326.57  
   1800     16691.83    4.2402     4.2554     0.0555     0.0111   127129.22  306869.22  
   1850     17156.75    4.2386     4.2538     0.0571     0.0106   126847.17  306188.62  
   1900     17620.36    4.2364     4.2515     0.0631     0.0104   126430.53  305182.64  
   1950     18085.04    4.2328     4.2481     0.082      0.0102   125804.44  303673.27  
   2000     18549.84    4.2273     4.2424     0.0841     0.0102   124837.67  301342.18  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       140.52     4.1677     4.1796     0.0891     3.3575    77434.88  249353.57  
   100       681.92     4.226      4.238      0.0418     1.037    101689.37  327452.58  
   150      1223.92     4.2362     4.2488     0.0291     0.4197   112126.89  360979.99  
   200      1763.85     4.2395     4.2525     0.0268     0.2794   117090.39  376897.05  
   250       2304.4     4.2413     4.254      0.023      0.1945   119900.23  385903.99  
   300      2846.46     4.2419     4.2547     0.0222     0.1503   121676.56  391599.89  
   350      3385.63     4.2423     4.2554     0.0241     0.118    122894.27  395501.57  
   400       3925.8     4.243      4.256      0.0218     0.0968   123778.18  398335.19  
   450      4466.07     4.2432     4.2561     0.0228     0.0823   124448.13  400481.55  
   500      5005.45     4.2435     4.2563     0.0243     0.0705   124969.22  402151.68  
   550      5551.81     4.2437     4.2564     0.0212     0.0611   125385.88   403486.2  
   600      6095.52     4.2438     4.2568     0.0225     0.054    125726.89  404578.37  
   650      6637.44     4.2438     4.2567     0.0239     0.0474   126010.98  405488.27  
   700      7179.02     4.2438     4.2569     0.0234     0.0423   126250.35  406254.97  
   750      7719.66     4.244      4.257      0.0244     0.0389    126455.0  406910.43  
   800       8258.0     4.2442     4.257      0.0252     0.0352   126631.85  407476.89  
   850      8795.22     4.244      4.2571     0.023      0.0325    126785.7  407969.48  
   900      9334.13     4.2441     4.2572     0.0245     0.0297    126920.9  408402.44  
   950      9872.16     4.2442     4.2572     0.0222     0.0275   127039.12  408780.95  
   1000     10410.42    4.2442     4.2571     0.0215     0.0257   127143.52  409115.04  
   1050     10948.79    4.2443     4.2572     0.0234     0.024    127236.18  409411.57  
   1100     11487.95    4.2442     4.2572     0.0238     0.0229   127318.31  409674.31  
   1150     12026.56    4.2441     4.2572     0.023      0.0214   127390.48  409905.24  
   1200     12563.69    4.2441     4.2572     0.0271     0.0199   127454.05  410108.51  
   1250     13101.37    4.2442     4.2571     0.0249     0.0191   127509.77  410286.76  
   1300     13639.07    4.2443     4.2571     0.0245     0.018    127556.49  410436.07  
   1350     14177.89    4.2441     4.2571     0.0262     0.0172   127595.12  410559.33  
   1400     14714.55    4.2442     4.257      0.0271     0.0162   127624.94  410654.51  
   1450     15251.68    4.244      4.257      0.0273     0.0153   127645.79  410720.78  
   1500     15788.6     4.2438     4.2567     0.0315     0.0147   127656.58   410754.7  
   1550     16326.42    4.2438     4.2567     0.0345     0.014    127655.37   410750.1  
   1600     16863.91    4.2435     4.2563     0.0362     0.0134   127638.84  410696.27  
   1650     17401.2     4.2431     4.256      0.0411     0.0129   127601.99  410576.88  
   1700     17937.23    4.2427     4.2556     0.0389     0.0125   127540.35  410378.08  
   1750     18474.44    4.2419     4.2549     0.0475     0.012    127441.28  410058.23  
   1800     19009.58    4.2413     4.2541     0.0515     0.0117   127295.17  409587.41  
   1850     19545.59    4.2399     4.2529     0.0527     0.0114   127077.83  408887.68  
   1900     20080.48    4.2381     4.251      0.0649     0.0111   126745.54  407818.63  
   1950     20615.31    4.2352     4.2481     0.0738     0.0111   126235.07  406176.67  
   2000     21149.31    4.2307     4.2436     0.0863     0.011    125428.65  403583.07  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
    50       138.52     4.1625     4.1811     0.0873     3.3187    76046.36  306549.89  
   100       739.19     4.2232     4.2429     0.0563     0.961    100259.86  404260.78  
   150       1340.5     4.235      4.2539     0.0351     0.4399   111120.75  448025.62  
   200      1938.52     4.2394     4.2579     0.0313     0.285     116402.2  469291.98  
   250       2537.6     4.2409     4.2599     0.0254     0.1993   119375.12  481265.68  
   300      3137.21     4.2416     4.2608     0.0247     0.1532   121271.63  488901.59  
   350      3736.55     4.2422     4.2616     0.0262     0.1203   122566.31  494112.75  
   400      4335.66     4.2427     4.2621     0.0253     0.0987   123502.86  497882.87  
   450       4935.8     4.2432     4.2623     0.0228     0.0829   124210.39   500730.6  
   500      5535.24     4.2435     4.2626     0.0225     0.0728   124762.58  502952.51  
   550      6135.13     4.2434     4.2628     0.0234     0.0615   125206.23  504737.87  
   600      6734.97     4.2437     4.2629     0.0217     0.0547   125570.05  506202.02  
   650      7333.62     4.2438     4.2631     0.0251     0.0491   125874.18  507425.45  
   700      7934.27     4.2439     4.2631     0.0214     0.0437   126130.32  508455.83  
   750      8532.88     4.2439     4.2633     0.022      0.0395   126349.66  509337.84  
   800      9131.11     4.2441     4.2633     0.0206     0.036    126538.93  510098.95  
   850       9730.8     4.2442     4.2633     0.0201     0.033    126703.84  510762.36  
   900      10329.96    4.2442     4.2633     0.0212     0.0311   126848.12  511342.58  
   950      10928.46    4.2443     4.2634     0.0239     0.0281   126975.66  511855.58  
   1000     11526.6     4.2444     4.2635     0.0235     0.0263   127088.82  512310.56  
   1050     12126.36    4.2443     4.2636     0.0212     0.0244   127188.98  512713.25  
   1100     12725.27    4.2443     4.2636     0.0231     0.0232   127278.52  513073.13  
   1150     13323.39    4.2444     4.2635     0.022      0.0217   127358.62  513395.02  
   1200     13939.9     4.2443     4.2636     0.0275     0.0204   127429.01  513678.02  
   1250     14545.32    4.2444     4.2634     0.0219     0.0192   127491.46  513928.95  
   1300     15142.65    4.2441     4.2635     0.0266     0.0184   127545.34  514145.13  
   1350     15758.49    4.2443     4.2635     0.0256     0.0174   127591.83  514332.05  
   1400     16379.63    4.2442     4.2634     0.0241     0.0166   127630.46  514487.15  
   1450     17021.42    4.2442     4.2634     0.0293     0.0159   127661.35  514610.98  
   1500     17641.21    4.2441     4.2633     0.0256     0.0152   127682.92  514697.22  
   1550     18260.95    4.2437     4.2632     0.033      0.0145   127693.63  514739.84  
   1600     18879.84    4.2438     4.263      0.0289     0.0137   127693.01  514736.61  
   1650     19498.6     4.2434     4.2628     0.0321     0.0133   127675.18  514664.23  
   1700     20118.35    4.243      4.2624     0.0374     0.0127   127635.89  514505.32  
   1750     20739.11    4.2428     4.262      0.0409     0.0124   127570.81  514242.84  
   1800     21360.09    4.242      4.2613     0.0443     0.0121   127468.52  513830.31  
   1850     21979.69    4.2412     4.2603     0.055      0.0117   127307.83  513182.72  
   1900     22601.17    4.2399     4.259      0.0611     0.0114   127055.13  512163.02  
   1950     23220.36    4.2375     4.2567     0.0671     0.0113   126649.44  510525.67  
   2000     23842.1     4.234      4.2532     0.0827     0.0112    126010.3   507948.9  
