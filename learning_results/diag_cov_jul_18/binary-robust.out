Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  4.861545562744141
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       76.82      1.4098     1.4033     3.1623    30.8778     0.2474  
   100       211.37     1.4137     1.4178     3.1316   5627.5422    0.0541  
   150       347.79     1.4105     1.4137     2.6291   5585.4012    0.2121  
   200       483.94     1.4133     1.4172     2.6786    4154.894    0.0476  
   250       620.01     1.4141     1.4279     3.0073   4080.7416    0.1627  
   300       756.02     1.4166     1.4117     2.9002   4262.0084    0.2316  
   350       891.97     1.4152     1.4116     2.8319   4297.5553    0.0375  
   400      1027.83     1.422      1.4156     3.0108   17123.7529   0.0232  
   450      1163.76     1.4099     1.4146     3.1603   17094.3809   0.2419  
   500      1299.51     1.4131     1.4169     1.6416   9199.0868    0.038   
   550      1435.14     1.4093     1.4206     1.4666   4886.9961    0.0189  
   600      1570.73     1.4148     1.4185      1.99    24572.6337   0.0239  
   650      1706.31     1.4156     1.418      1.3697    45335.27    0.0068  
   700       1842.0     1.4126     1.408      1.5103   49333.4226   0.0513  
   750      1977.44     1.4058     1.411      2.0728   3823.4653     0.01   
   800       2112.8     1.4153     1.4206     2.9803   1667.7969    0.0748  
   850       2248.2     1.4113     1.4146     2.8634   1230.9939    0.0191  
   900      2383.36     1.4152     1.4049     2.7886   1030.0877    0.0119  
   950      2518.33     1.4095     1.4181     3.0359   1868.9172    0.0149  
   1000     2653.29     1.4181     1.4139     2.421    1816.0177    0.0189  
   1050      2788.3     1.4029     1.4209     1.0952   2991.3979    0.0216  
   1100     2923.15     1.4082     1.4115     0.9481   5321.2312    0.0162  
   1150      3057.8     1.4108     1.4071     2.9186   6414.5462    0.0106  
   1200     3192.21     1.4189     1.4108     2.6022   10145.1715   0.0027  
   1250     3326.65     1.4148     1.4032     2.3071   11091.0979   0.0129  
   1300     3461.24     1.4127     1.4164     0.9958   82872.7349   0.0186  
   1350     3595.72     1.4173     1.4192     0.7909   50591.4431   0.0168  
   1400     3730.11     1.419      1.4099     0.9837   9272.9962    0.0147  
   1450      3864.4     1.4161     1.4059     0.8729   6852.2953    0.0094  
   1500     3998.54     1.4175     1.4146     0.7281   1171.6445    0.0083  
   1550     4132.44     1.4078     1.417      0.7085    1070.207    0.0056  
   1600     4266.27     1.4054     1.4042     0.9933    957.6381    0.0052  
   1650     4400.08     1.4213     1.4212     2.9249   4609.2296    0.0042  
   1700     4533.97     1.4149     1.4011     0.9033   1509.5709    0.0016  
   1750     4667.93     1.4001     1.4002     0.7987   2890.1721    0.0061  
   1800     4801.82     1.3904     1.4028     0.9377   6419.4402    0.0023  
   1850      4935.6     1.3836     1.3826     0.7293   9446.2956    0.0038  
   1900     5069.29     1.3804     1.3891     3.1433    2070.882    0.0023  
   1950     5202.95     1.3436     1.3358     0.9827    737.6611    0.0009  
   2000     5336.57     1.2486     1.2595     1.9082    709.8397    0.0033  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       77.53      1.4142     1.4151     0.2478     0.3534     0.0005  
   100       234.52     1.4161     1.4153     0.2384     0.5453      0.0    
   150       391.59     1.4206     1.415      0.2596     1.1997     0.0005  
   200       548.21     1.4164     1.4142     0.3175    15.5435     0.0011  
   250       704.35     1.4149     1.4143     0.8629    568.4079    0.0192  
   300       863.09     1.4068     1.4187     1.0145   1294.2414    0.0121  
   350      1022.29     1.4168     1.4172     0.9896   1821.8242    0.0698  
   400      1181.45     1.4144     1.4212     0.9721   1810.9459    0.0191  
   450      1340.77     1.4164     1.4137     1.2235   7286.1294    0.0383  
   500      1500.11     1.4114     1.4173     0.9616   7136.9992    0.0101  
   550      1659.57     1.4188     1.4101     1.0941   1554.7131    0.1348  
   600      1819.13     1.4148     1.4135     0.9898   1578.2453    0.1123  
   650      1978.66     1.4175     1.4126     0.9033   1319.8617    0.0763  
   700      2138.09     1.4133     1.4202     0.9407   2403.9865    0.0233  
   750      2297.62     1.4167     1.416      0.9561   2340.3039    0.0356  
   800      2457.15     1.4143     1.4091     1.143    65357.0814   0.0968  
   850       2616.8      1.41      1.4051     0.9845   65470.1186   0.0495  
   900       2776.4     1.4177     1.4156     0.9377   2861.8377    0.0685  
   950      2935.97     1.4087     1.4169     1.0543   2216.8771    0.0487  
   1000     3095.54     1.4112     1.419      0.9843   2305.6889    0.0136  
   1050     3255.08     1.4133     1.4148     0.9632   8394.0493    0.0443  
   1100     3414.65     1.4121     1.4176     3.0135    8336.128    0.0196  
   1150     3574.07     1.4146     1.4082     2.5551    800.1365    0.0098  
   1200     3733.36     1.4161     1.4111     1.1505   1351.5961    0.0192  
   1250     3892.66     1.4112     1.4075     0.8728   153571.8572   0.0582  
   1300     4052.29     1.4108     1.4137     0.9852   154186.3483   0.1354  
   1350     4211.88     1.4038     1.4083     0.699    1764.1233    0.0165  
   1400     4371.07     1.3919     1.3942     0.9217   7224.0201    0.0094  
   1450     4530.49     1.3834     1.3891     1.0038   7982.5589    0.0265  
   1500     4689.89     1.3876     1.3852     0.8684   3313.3325    0.0725  
   1550     4849.16     1.3742     1.3746     1.3941   3267.4345    0.2526  
   1600     5008.41     1.3563     1.3655     2.4625    586.5817    0.0411  
   1650     5167.57     1.3472     1.3454     2.0152   7756.3535    0.0203  
   1700     5326.97     1.3349     1.3317     1.0155   11544.4663   0.0301  
   1750     5486.44     1.3216     1.3184     0.8566   11407.6982   0.0228  
   1800     5645.76     1.2657     1.269      0.7311    875.7693    0.0003  
   1850     5805.03     1.2238     1.222      0.7668   1019.9924    0.0157  
   1900     5964.41      1.2       1.1949     1.0489   1935.5099    0.0414  
   1950     6123.88     1.1733     1.1702     2.5716   2479.9932    0.0214  
   2000     6283.34     1.2177     1.2059     0.9588   2569.2966    0.0335  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       78.59      1.4188     1.4179     0.1072     0.0641     0.0102  
   100       259.69     1.4034     1.4159     0.1357     0.1233     0.0041  
   150       440.22     1.4227     1.4155     0.1689     0.188      0.0019  
   200       620.5      1.4153     1.4123     0.1856     0.2864     0.0007  
   250       800.66     1.4122     1.4142     0.1854     0.507      0.0003  
   300       980.82     1.4156     1.4131     0.2424     0.8943      0.0    
   350      1161.02     1.4177     1.4154     0.2643     2.6579     0.0007  
   400      1341.29     1.4159     1.4182     0.2916    16.4836     0.0017  
   450      1522.06     1.4166     1.4143     0.9435    401.7219    0.0093  
   500      1705.24     1.4137     1.4123     1.0816   1129.8647    0.0117  
   550      1889.78     1.4172     1.4092     2.3556   1068.8669    0.0394  
   600       2074.7     1.4077     1.413      1.5637   5744.4771    0.0103  
   650      2259.69     1.4084     1.4138     3.0551   5736.2711    0.1324  
   700      2444.98     1.4203     1.4161     1.1043   1968.8091    0.0547  
   750      2630.36     1.4121     1.4156     2.6176   9959.2581    0.0482  
   800      2815.88     1.4169     1.4166     2.3982   10128.3746   0.0588  
   850      3001.38     1.4105     1.4168     0.859     541.9537    0.0203  
   900      3186.62     1.4108     1.4176     1.1807    2783.376    0.0362  
   950       3372.0     1.4187     1.4177     2.4522   4643.4941    0.0692  
   1000      3557.5     1.4086     1.4153     1.0667   5267.6784    0.0557  
   1050     3742.96     1.4199     1.4129     1.1873   8930.8735    0.0559  
   1100     3928.45     1.4122     1.4184     3.044    17841.1686   0.0226  
   1150     4114.28     1.4214     1.4116     1.1832   37411.0287   0.1118  
   1200      4299.9     1.403      1.4141     1.3451   20840.7622   0.045   
   1250      4485.4     1.4136     1.4206     0.9613   19672.748    0.1435  
   1300     4670.94     1.4259     1.4124     1.166    28460.0411   0.0215  
   1350     4856.37     1.4174     1.4155     0.9432   28123.221    4.1122  
   1400     5041.96     1.4142     1.4183     1.3417   1108.2927    0.1599  
   1450     5227.16     1.4063     1.4132     0.8406   3891.6537    0.3872  
   1500     5412.49     1.4133     1.4124     0.8355   3204.3327    0.0484  
   1550     5597.65     1.4153     1.4144     0.9419   2543.4674    0.2557  
   1600     5782.87     1.4113     1.4164     1.2075   6732.7299    0.1129  
   1650     5968.22     1.4106     1.4152     0.9114   25428.5112   0.0051  
   1700     6153.98     1.4086     1.4148     0.9773   26235.0501   0.0519  
   1750     6339.89     1.4204     1.4176     3.1359   11030.9024   0.0344  
   1800     6525.43     1.4137     1.4176     3.0483   11341.8009   0.0216  
   1850     6710.91     1.4168     1.4155     0.7988   1785.1656    0.0012  
   1900     6896.16     1.4168     1.413      0.8952   217973.1226   0.012   
   1950     7081.73     1.4114     1.4125     0.9408   220221.6489   0.1071  
   2000     7267.55     1.4139     1.4175     2.9054   3798.5687    0.022   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       78.95      1.4192     1.4167     0.0605     0.0199     0.0127  
   100       285.77     1.4117     1.4094     0.085      0.0584     0.0057  
   150       491.88     1.4136     1.4153     0.1131     0.0613     0.0036  
   200       697.63     1.4213     1.4152     0.1074     0.0862     0.0022  
   250       903.05     1.414      1.4167     0.1309     0.1309     0.0009  
   300      1108.29     1.4133     1.4169     0.1524     0.1575     0.0007  
   350      1313.36     1.4113     1.4137     0.133      0.2128     0.0003  
   400      1518.45     1.4145     1.4165     0.1472     0.413      0.0007  
   450      1723.56     1.4146     1.4136     0.1899     0.8607     0.0002  
   500      1928.56     1.4134     1.4144     0.1917     1.5382      0.0    
   550       2133.5     1.4074     1.4132     0.2127     14.239     0.0001  
   600      2338.59     1.4155     1.4115     0.2786    28.5739     0.0003  
   650      2543.96     1.4114     1.4124     0.7655    7147.31     0.0038  
   700      2751.65     1.4081     1.4164     0.9355   7080.4625    0.0023  
   750      2960.48     1.4162     1.4141     0.9234   1957.0202    0.0437  
   800      3169.56     1.4166     1.4157     0.898    1346.4695    0.0084  
   850      3378.89     1.4175     1.4206     1.3384    802.4538    0.0104  
   900      3588.29     1.4083     1.4186     0.9721   3245.4066    0.2183  
   950      3797.97     1.4117     1.4161     0.9776    3142.291    0.0234  
   1000     4007.63     1.4094     1.4128     1.7128   1405.0532    0.011   
   1050     4217.14     1.4195     1.4104     0.6883   1269.9573    0.0012  
   1100     4426.63     1.4076     1.416      0.8189   5393.6574     0.0    
   1150     4636.28     1.4132     1.4169     0.9375   5352.0097    0.0152  
   1200      4845.9     1.4152     1.4136     0.7521    927.225     0.0027  
   1250      5055.4     1.4174     1.4116     3.158     878.5865    0.001   
   1300      5265.1     1.4051     1.4146     0.4492    450.0456    0.0181  
   1350     5474.51     1.4106     1.4112     0.6955    3428.513    0.0098  
   1400     5684.17     1.4123     1.4188     0.8816   14492.9197   0.0023  
   1450     5894.15     1.4191     1.4138     0.937    14352.9556   0.0595  
   1500     6104.56     1.4114     1.4145     0.769    2466.5223    0.0313  
   1550      6314.6     1.4226     1.4145     0.9259    3154.495    0.026   
   1600     6524.58     1.4135     1.4154     2.9082   1317.8336    0.0086  
   1650     6734.32     1.4164     1.417      0.922    1393.6178    0.0124  
   1700     6944.33     1.4157     1.4106     0.9164    1124.328    0.0158  
   1750     7154.36     1.4163     1.4125     0.9173   212618.8723   0.0757  
   1800     7362.69     1.4152     1.4164     0.7882   212625.6013   0.0736  
   1850     7566.28     1.4094     1.4203     2.6934    747.198     0.0224  
   1900     7769.67     1.412      1.412      0.795    1401.1721    0.0218  
   1950     7972.96     1.413      1.4154     0.8701   1411.4613    0.0195  
   2000     8176.07     1.4141     1.4158     0.7371   8751.6444    0.005   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       76.33      1.4139     1.411      0.0404     0.0113     0.0097  
   100       299.57     1.4133     1.4169     0.0489     0.0249     0.0052  
   150       523.55     1.4155     1.4172     0.0697     0.034      0.0038  
   200       747.15     1.4064     1.4147     0.0589     0.039      0.0024  
   250       969.76     1.4075     1.4146     0.0879     0.0585     0.0017  
   300      1186.49     1.4175     1.4179     0.087      0.0787     0.0014  
   350      1401.88     1.4157     1.4143     0.0711     0.0805     0.001   
   400      1625.38     1.4208     1.4135     0.0951     0.1071     0.001   
   450      1854.68     1.4179     1.4161     0.0861     0.1421     0.0006  
   500      2083.95     1.4206     1.4133     0.0907     0.2352     0.0002  
   550      2313.26     1.4025     1.4179     0.1035     0.1866     0.0004  
   600       2537.9     1.4117     1.4148     0.1085     0.3941     0.0003  
   650      2759.92     1.4113     1.4171     0.0986     0.7368     0.0001  
   700      2982.03     1.4054     1.4145     0.1179     1.279      0.0002  
   750      3204.24     1.4177     1.4151     0.1312     3.059       0.0    
   800      3426.39     1.4176     1.4182     0.166     25.3078     0.0004  
   850       3649.9     1.4134     1.4106     0.2311    40.8773     0.0001  
   900      3877.48     1.416      1.4148     0.2141    18.0106     0.0001  
   950      4104.98     1.4124     1.4158     0.5544    117.9791    0.0032  
   1000      4328.1      1.41      1.4158     0.6946    244.0517    0.0135  
   1050     4555.99     1.4143     1.416      0.6945    911.9757    0.0001  
   1100     4784.59     1.4131     1.414      0.696    3260.7081    0.0106  
   1150     5013.71     1.4175     1.412      0.7905   3010.3697    0.0098  
   1200     5239.73     1.4112     1.4137     0.8623    255.727     0.0065  
   1250     5465.76     1.4181     1.4104     0.6673    191.5222    0.0065  
   1300     5691.95     1.4165     1.4142     0.7622   1560.4605    0.0074  
   1350     5918.88     1.4195     1.4123     0.7364   1659.6417    0.0112  
   1400     6145.72     1.4149     1.4127     1.0246   1872.3927    0.0122  
   1450      6372.7     1.4023     1.4128     2.1894   5667.7142    0.0211  
   1500     6599.91     1.4098     1.4187     0.8926   5931.5822    0.0176  
   1550     6827.23     1.4093     1.4134     2.0759    756.9423    0.0173  
   1600     7054.28     1.4197     1.4158     0.6866    856.9397    0.0126  
   1650     7281.24     1.4142     1.4134     0.7637   176419.9543   0.0203  
   1700     7508.56     1.4187     1.4106     0.7583   176331.8856   0.0187  
   1750     7736.09     1.4126     1.4142     0.7587   1523.8637    0.0107  
   1800     7963.31     1.4064     1.415      0.6507    422.7822    0.0065  
   1850     8190.55     1.4136     1.4149     0.7414    962.8435    0.007   
   1900     8417.72     1.4156     1.4112     0.6798    853.3053    0.0071  
   1950     8645.11     1.4129     1.414      0.6947   4412.9516    0.0089  
   2000     8872.28     1.4108     1.4158     0.7071   4367.3047    0.0094  
Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.395843505859375
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       150.14     1.2441      1.21      3.1623     1.0239     0.013   
   100       442.74     0.7865     0.7694     3.103      0.6574     0.0007  
   150       734.26     0.7452     0.7281     2.2678     1.8765     0.0034  
   200      1025.04     0.7445     0.7336     2.1065    13.3958     0.0058  
   250      1314.59     0.7476     0.7269     2.0154    58.5807     0.0075  
   300      1603.69     0.7463     0.735      1.8013    28.0035     0.0032  
   350      1892.34     0.7475     0.7309     1.6143    17.9807     0.0046  
   400      2180.69     0.7748     0.7575     1.945     512.5755    0.0178  
   450      2468.84     0.7549      0.74      1.9452    376.0666    0.0027  
   500      2756.25     0.745      0.7301     1.6488    315.3111    0.007   
   550      3043.18     0.7343     0.7155     2.1533   5244.1535    0.0164  
   600      3329.44     0.743      0.7298     1.943    4695.3438    0.0144  
   650      3615.57     0.7447     0.7225     1.908     4077.93     0.0013  
   700       3901.2     0.7328     0.7225     1.8599   5106.4765    0.0041  
   750      4186.18     0.7381     0.7251     2.028    2315.7428    0.0069  
   800      4470.76     0.7254     0.7127     1.6921   1212.5322    0.0004  
   850      4755.35     0.7378     0.7219     1.5308    818.9269    0.0179  
   900      5041.65     0.7311     0.7123     1.8012   2172.6331    0.0045  
   950      5326.86     0.7291     0.7192     1.7936    776.1833    0.0081  
   1000     5608.25     0.7286     0.7105     1.4567    198.4653    0.0092  
   1050     5889.54     0.7321     0.7175     1.715     83.4157     0.0019  
   1100     6170.03     0.7231     0.705      1.5597    195.1602    0.0048  
   1150      6450.6     0.7194     0.6971     1.6338   1931.2836    0.0018  
   1200     6731.59     0.7287     0.7063     1.8408    3048.013    0.0005  
   1250     7012.34     0.7224     0.6981     1.7676   2088.5818    0.0005  
   1300     7292.94     0.7179     0.7001     1.6536    235.4231    0.0006  
   1350     7573.36     0.7202     0.6953     1.9854    323.1024    0.0027  
   1400     7853.61     0.7109     0.6875     1.5779    498.492     0.0016  
   1450     8133.39     0.7102     0.6912     1.7858    42.0013     0.0051  
   1500     8413.44     0.7109     0.6922     1.7893   1124.4835    0.0007  
   1550     8693.48     0.7094     0.6877     2.0259   5235.1592    0.0094  
   1600     8973.64     0.7123     0.6984     1.9812   2811.8492    0.0058  
   1650     9253.54     0.712      0.6851     1.7899    581.5052    0.0021  
   1700     9533.61     0.7073     0.6872     1.4826    38.3429     0.0003  
   1750     9812.95     0.7097     0.6818     1.658     27.2678     0.0002  
   1800     10092.06    0.7022     0.6854     1.5061    12.5892     0.0015  
   1850     10371.66    0.7052     0.6813     1.3231    21.0211     0.0031  
   1900     10651.3     0.714      0.6905     1.1873    13.6215     0.0026  
   1950     10930.62    0.7057     0.6902     1.1999     8.8238     0.0044  
   2000     11210.65    0.707      0.6853     1.1917     8.816      0.0018  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       149.26     1.4226     1.4146     1.0046     0.0055     0.0019  
   100       492.08     1.4048     1.4157     1.1034     0.0064     0.0001  
   150       835.98     1.3912     1.3948     1.1013     0.0069     0.0001  
   200      1179.47     1.0697     1.0754     1.4817     0.0076     0.0006  
   250       1521.8     0.7347     0.725      1.5541     0.0314     0.0015  
   300      1863.13      0.76      0.7461     1.4145     0.059      0.0002  
   350      2204.18     0.7416     0.7338     1.4114     0.055      0.0003  
   400      2544.32     0.7384     0.7348     1.378      0.0641     0.0023  
   450      2883.41     0.7541     0.7423     1.248      0.0665     0.0037  
   500      3222.45     0.7318     0.7208     1.1873     0.0625     0.0012  
   550      3561.52     0.7344     0.7217     1.1839     0.0687     0.0055  
   600      3898.56     0.726      0.7173     1.1695     0.0691     0.0041  
   650      4235.25     0.7318     0.718      0.9494     0.0722     0.0027  
   700      4571.65     0.7292     0.7208     1.0832     0.0756     0.0005  
   750      4908.51     0.7485     0.7374     1.0115     0.0765     0.0019  
   800      5244.16     0.7306     0.719      0.9898     0.0783     0.0012  
   850       5580.4     0.7292     0.7188     1.1552     0.0862     0.0035  
   900      5916.06     0.7323     0.7207     0.9067     0.1002     0.0017  
   950      6251.15     0.7239     0.7148     0.9954     0.1034     0.0011  
   1000     6586.74     0.7339     0.7208     1.129      0.1194     0.0072  
   1050     6921.66     0.7343     0.7183     0.9238     0.1189     0.0024  
   1100     7256.12     0.7303     0.7153     1.0883     0.1144     0.002   
   1150     7592.06     0.7233     0.7144     0.9472     0.1449     0.0002  
   1200     7927.02     0.7331     0.7262     0.9925     0.1525     0.0017  
   1250     8260.84     0.7343     0.718      0.8274     0.1764     0.0043  
   1300     8595.22     0.7386     0.7207     0.8197     0.1697     0.0022  
   1350     8929.01     0.7311     0.7226     0.9084     0.1937     0.0014  
   1400     9262.85     0.7429     0.7296     0.8586     0.2326     0.0018  
   1450     9595.89     0.7368     0.7239     1.0282     0.2126     0.0009  
   1500     9929.49     0.7242     0.7172     0.8655     0.2252     0.0012  
   1550     10264.13    0.7271     0.7199     0.7585     0.2658     0.0006  
   1600     10598.12    0.7341     0.7244     0.9263     0.323      0.0028  
   1650     10931.56    0.733      0.7185     0.8498     0.3473     0.001   
   1700     11264.29    0.7302     0.723      0.7878     0.4443     0.003   
   1750     11598.53    0.7298     0.721      0.6951     0.4177     0.0027  
   1800     11932.55    0.7357     0.7224     0.8335     0.5132     0.0005  
   1850     12265.9     0.7281     0.7223     0.8189     0.5918     0.0006  
   1900     12599.17    0.7356     0.719      0.8618     0.6846     0.0006  
   1950     12932.03    0.7345     0.7216     0.8023     0.7319     0.001   
   2000     13264.54    0.7335     0.7231     0.9475     0.8627     0.003   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       149.61     1.4151     1.415      0.6776     0.0037     0.0002  
   100       551.51     1.4141     1.4123     0.8438     0.004      0.0005  
   150       953.93     1.4166     1.4153     0.9097     0.004      0.0002  
   200      1356.56     1.4166     1.4119     0.6357     0.0041     0.0014  
   250      1758.47     1.4121     1.4081     0.7155     0.004      0.0007  
   300      2162.82     1.4191     1.4096     0.7599     0.0041     0.0002  
   350      2566.96     1.3177     1.3135     0.9055     0.0042     0.0004  
   400      2970.32     0.849      0.8393     0.9042     0.0067     0.0003  
   450      3374.49     0.7415     0.7309     0.8508     0.0218     0.0013  
   500      3777.19     0.7318     0.7167     0.745      0.0409     0.0028  
   550      4180.13     0.7394     0.7252     0.7322     0.057      0.0018  
   600      4582.11     0.723      0.7111     0.686      0.062      0.0031  
   650      4984.83     0.7435     0.7292     0.6883     0.0626     0.0008  
   700      5386.54     0.7377     0.7248     0.6171     0.0704     0.0023  
   750      5790.74     0.7331     0.7265     0.6269     0.0845     0.0017  
   800      6195.09     0.7308     0.7236     0.5781     0.085      0.0054  
   850      6598.37     0.7288     0.721      0.6556     0.091      0.0011  
   900      7001.35     0.7446     0.7313     0.6176     0.094      0.0005  
   950      7403.49     0.7351     0.7207     0.5573     0.1203     0.0002  
   1000     7810.22     0.7342     0.7257     0.5576     0.1125     0.0008  
   1050     8213.78     0.7308     0.7195     0.5269     0.1135     0.0003  
   1100     8619.35     0.7298     0.7247     0.4539     0.1392     0.0021  
   1150     9022.87     0.7334     0.7243     0.5155     0.1316     0.0036  
   1200     9425.26     0.7269     0.7163     0.5348     0.1601     0.0037  
   1250     9828.11     0.7334     0.7241     0.5096     0.1866     0.0021  
   1300     10229.89    0.7316     0.7204     0.5062     0.2061     0.0046  
   1350     10634.52    0.7327     0.7207     0.4645     0.2434     0.0023  
   1400     11039.04    0.7353     0.7222     0.494      0.2647     0.0025  
   1450     11443.05    0.7284     0.7176     0.5332     0.3366     0.003   
   1500     11848.81    0.7293     0.7147     0.486      0.312      0.0002  
   1550     12251.38    0.7281     0.7206     0.5152     0.3762     0.0046  
   1600     12653.79    0.7333     0.7172     0.491      0.4666     0.0001  
   1650     13057.82    0.7295     0.7193     0.4506     0.4544     0.0029  
   1700     13461.36    0.7364     0.7254     0.498      0.6246     0.0029  
   1750     13866.29    0.7388     0.7248     0.6005     0.7513     0.0099  
   1800     14270.23     0.73      0.7267     0.6838     1.0057     0.002   
   1850     14673.65    0.7377     0.7203     0.7187     0.9041     0.0019  
   1900     15076.69    0.7305     0.7191     0.8374     1.386      0.0052  
   1950     15479.76    0.7294     0.7187     0.7609     1.2892     0.0005  
   2000     15882.67    0.7274     0.714      0.9653     1.7279     0.0015  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       151.88     1.4108     1.4174     0.4968     0.0026     0.0025  
   100       611.54     1.4089     1.4119     0.5527     0.0029      0.0    
   150      1074.71     1.4113     1.4122     0.6072     0.0031      0.0    
   200       1538.4     1.4189     1.4143     0.6871     0.0031     0.0006  
   250      2001.69     1.412      1.4144     0.6908     0.0032     0.0005  
   300      2465.37     1.4174     1.4113     0.6024     0.0031     0.0004  
   350      2931.55     1.4142     1.4153     0.5508     0.003      0.0004  
   400      3397.36     1.416      1.4115     0.6305     0.0031     0.0003  
   450      3863.11     1.4039     1.4127     0.6095     0.003      0.0003  
   500       4327.8     1.4135     1.4106     0.6116     0.003      0.0003  
   550      4792.64     1.4054     1.4091     0.5277     0.003      0.0006  
   600      5257.91     1.3921     1.3827     0.6892     0.0032     0.0001  
   650      5723.94     1.1644     1.1643     0.9839     0.0032     0.0003  
   700      6188.98     0.7405     0.731      1.0726     0.008      0.0014  
   750      6653.59     0.7375     0.7244     1.0148     0.0271     0.0006  
   800       7118.6     0.7346     0.7248     0.9842     0.0333     0.0065  
   850      7582.49     0.7328     0.7242     0.9915     0.0341     0.0043  
   900      8043.33     0.7359     0.7203     0.9713     0.0359     0.007   
   950      8503.86     0.7267     0.7136     0.8791     0.0427     0.0045  
   1000     8965.27     0.7292     0.713      0.8349     0.0429     0.002   
   1050     9426.39     0.7319     0.7242     0.9126     0.0454     0.0069  
   1100     9884.32     0.7302     0.7194     0.8172     0.041      0.0004  
   1150     10343.95    0.7322     0.7173     0.8287     0.0479     0.0011  
   1200     10803.04    0.7307     0.7163     0.775      0.0498     0.0046  
   1250     11261.71    0.7312     0.7169     0.9215     0.0534     0.0037  
   1300     11719.77    0.7239     0.7114     0.8106     0.0491     0.0017  
   1350     12178.17    0.7315     0.7226     0.8591     0.0546     0.0042  
   1400     12635.75    0.7305     0.7147     0.7668     0.0529     0.0003  
   1450     13094.67    0.7321     0.7197     0.8817     0.0607     0.0035  
   1500     13553.11    0.7329     0.7194     0.768      0.0622     0.0019  
   1550     14010.46    0.7264     0.7167     0.8518     0.0545     0.0056  
   1600     14468.78    0.7292     0.7165     0.8316     0.076      0.0013  
   1650     14927.19    0.7264     0.7119     0.9209     0.0682     0.0019  
   1700     15383.47    0.7295     0.7138     0.9465     0.0636     0.0118  
   1750     15837.87    0.7242     0.7166     0.8085     0.0671     0.0021  
   1800     16293.6     0.7285     0.7165     1.0887     0.0667     0.003   
   1850     16747.77    0.7228     0.7088     0.9254     0.0673     0.0016  
   1900     17202.68    0.7294     0.7148     1.0643     0.0737     0.0026  
   1950     17658.89    0.7197     0.7103     1.101      0.0719     0.0044  
   2000     18113.69    0.7292     0.7184     0.8966     0.0728     0.0053  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       150.44     1.4209     1.4114     0.4458     0.0021     0.0017  
   100       665.86     1.4131     1.4118     0.3942     0.0027     0.0001  
   150      1185.98     1.4079     1.4163     0.4086     0.0025     0.0007  
   200       1706.8     1.4084     1.4141      0.4       0.0028     0.001   
   250      2228.63     1.4128     1.4136     0.474      0.0026     0.0003  
   300      2751.19     1.4145     1.4149     0.6099     0.0025     0.0001  
   350      3275.24     1.4175     1.4162     0.528      0.0024     0.0002  
   400      3798.34     1.4164     1.4115     0.5004     0.0025     0.0006  
   450      4321.79     1.4053     1.4174     0.4307     0.0024      0.0    
   500      4845.73      1.41      1.4126     0.4973     0.0025     0.0005  
   550      5369.84     1.4126     1.4174     0.5156     0.0026     0.0005  
   600      5895.18     1.4072     1.4058     0.4716     0.0026     0.0006  
   650      6419.68     1.3866     1.381      0.6437     0.0026     0.0001  
   700      6944.03     1.1092     1.1119     0.8108     0.003      0.001   
   750       7470.5     0.7459     0.7366     0.8236     0.0074     0.0006  
   800      7994.06     0.7356     0.7295     0.736      0.025      0.0003  
   850       8515.0     0.7391     0.7251     0.725      0.0288     0.002   
   900      9035.89     0.7311      0.72      0.7298     0.0354     0.0051  
   950      9556.26     0.7308     0.7136     0.6972     0.0349     0.0082  
   1000     10076.56    0.7381     0.7256     0.6772     0.0463     0.0055  
   1050     10599.74    0.7298     0.7157     0.7109     0.0481     0.0047  
   1100     11119.02     0.73      0.7205     0.6243     0.0527     0.0061  
   1150     11637.65    0.7318     0.7209     0.5989     0.0504     0.0047  
   1200     12159.53    0.7341     0.7201     0.5701     0.049      0.005   
   1250     12678.81    0.7305     0.7216     0.6029     0.0671     0.0024  
   1300     13198.62    0.7401     0.7287     0.585      0.0785     0.0018  
   1350     13719.06    0.7341     0.7212     0.5004     0.0695     0.0028  
   1400     14239.56    0.7266     0.7162     0.6114     0.0734     0.0014  
   1450     14758.79    0.7297     0.7162     0.5338     0.0702     0.0074  
   1500     15279.54    0.7321     0.7209     0.5464     0.0659     0.0036  
   1550     15799.61    0.7323     0.719      0.4821     0.0826     0.0047  
   1600     16319.93    0.7298     0.7165     0.5599     0.0809     0.0021  
   1650     16838.88    0.7282     0.7197     0.5186     0.0904     0.0003  
   1700     17358.18    0.727      0.713       0.39      0.1005     0.0072  
   1750     17878.54    0.7295     0.715      0.4794     0.0964     0.006   
   1800     18398.03    0.7311     0.7184     0.4823     0.1017     0.0134  
   1850     18918.57    0.7322     0.7189     0.5434     0.1427     0.0008  
   1900     19439.35    0.728      0.7175     0.5165     0.1308     0.0072  
   1950     19958.07    0.7243     0.7171     0.4492     0.1149     0.0012  
   2000     20476.95    0.7345     0.7177     0.3945     0.155      0.0033  
