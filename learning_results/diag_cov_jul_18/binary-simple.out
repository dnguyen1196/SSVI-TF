Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.205643177032471
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       40.68      1.3267      1.29      3.1623     1.7273     0.0001  
   100       134.91     0.7833     0.7506     3.1127     0.703       0.0    
   150       229.61     0.7275     0.7031     2.9497     0.3564      0.0    
   200       324.34     0.7141     0.696      2.8378     0.2191      0.0    
   250       418.85     0.7134     0.7019     2.8606     0.1619      0.0    
   300       513.74     0.7122     0.6917     2.9136     0.1336      0.0    
   350       608.64     0.7256     0.702      2.9212     0.1141      0.0    
   400       703.92     0.7178     0.702      2.9741     0.111       0.0    
   450       798.34     0.7189     0.6933     2.9053     0.1017      0.0    
   500       892.64     0.7211     0.7021     2.8591     0.0955      0.0    
   550       986.76     0.7132     0.6918     2.8222     0.0936      0.0    
   600      1080.88     0.7234     0.6984     2.7649     0.0897      0.0    
   650      1174.49     0.7201     0.6914     2.6825     0.0868      0.0    
   700       1267.9     0.7192     0.6935     2.7057     0.097       0.0    
   750      1361.24     0.7153     0.6967     2.6571     0.0839      0.0    
   800      1454.62     0.7148     0.6825     2.7588     0.0963      0.0    
   850      1547.91     0.712      0.6888     2.4185     0.076       0.0    
   900      1641.25     0.7115     0.6888     2.5539     0.0899      0.0    
   950       1734.6     0.7111     0.6956     2.5622     0.0767      0.0    
   1000     1827.71     0.7202     0.6976     2.6332     0.0748      0.0    
   1050     1920.66     0.7209     0.6902     2.7878     0.0656      0.0    
   1100     2013.64     0.7194     0.7011     2.2659     0.0848      0.0    
   1150     2106.64     0.7192     0.6957     2.4956     0.0516      0.0    
   1200     2199.73     0.719      0.6914     2.2892     0.0861      0.0    
   1250     2292.75     0.7168     0.6938     2.4205     0.0507      0.0    
   1300     2385.86     0.7122     0.6892     2.5226     0.0455      0.0    
   1350     2478.99     0.7146     0.6958     2.2051     0.055       0.0    
   1400      2572.0     0.7148     0.687      2.2352     0.0469      0.0    
   1450     2665.32     0.7189     0.6888     2.6353     0.0414      0.0    
   1500     2758.58     0.7129     0.6822     2.3841     0.0377      0.0    
   1550     2851.78     0.7106     0.6905     2.0943     0.0368      0.0    
   1600     2945.15     0.716      0.7003     2.0175     0.044       0.0    
   1650     3038.44     0.7171     0.7054     2.4014     0.0316      0.0    
   1700     3131.68     0.7131     0.6984     2.0364     0.0436      0.0    
   1750     3224.98     0.714      0.6993     2.137      0.0483      0.0    
   1800     3318.35     0.7132     0.694      2.1573     0.0267      0.0    
   1850     3411.41     0.7114     0.6933     2.5908     0.0698      0.0    
   1900     3504.35     0.7113     0.6953     1.9164     0.0351      0.0    
   1950     3597.33     0.713      0.6996     2.6447     0.0354      0.0    
   2000      3690.3     0.7117     0.6956     2.3332     0.0453      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       40.49      1.4166     1.4129     0.0053     0.3926      0.0    
   100       158.73     1.4136     1.4186     0.0031     0.2781      0.0    
   150       276.89     1.3987     1.4089     0.0029     0.2302      0.0    
   200       395.22     1.4126     1.4161     0.0026     0.2013      0.0    
   250       513.17     1.4171     1.4212     0.0038     0.1816      0.0    
   300       631.05     1.408      1.4191     0.0024     0.1662      0.0    
   350       752.08     1.4155     1.4175     0.0027     0.1536      0.0    
   400       872.39     1.4128     1.4084     0.0026     0.1417      0.0    
   450       990.29     1.4183     1.415      0.0033     0.1303      0.0    
   500      1108.17     1.4222     1.413      0.0025      0.12       0.0    
   550      1225.91     1.4155     1.4144     0.0025     0.1123      0.0    
   600       1344.2      1.41      1.4095     0.0028     0.1027      0.0    
   650      1462.12     1.4139     1.4144     0.0023     0.0954      0.0    
   700      1580.27     1.4147     1.4177     0.002      0.0884      0.0    
   750      1697.18     1.4159     1.4145     0.0029     0.082       0.0    
   800      1813.91     1.4075     1.4109     0.0024     0.0758      0.0    
   850      1930.48     1.4064     1.4124     0.0024     0.071       0.0    
   900       2048.4     1.4091     1.4155     0.0018     0.0659      0.0    
   950      2164.96     1.4155     1.4148     0.0024     0.0618      0.0    
   1000     2281.42     1.4099     1.4129     0.0022     0.0582      0.0    
   1050     2398.28     1.4179     1.4115     0.0027     0.0544      0.0    
   1100     2518.11     1.4128     1.4128     0.0023     0.0518      0.0    
   1150     2634.74     1.4101     1.413      0.0023     0.0485      0.0    
   1200     2750.28     1.4136     1.4089     0.0024     0.046       0.0    
   1250     2864.99     1.4134     1.4099     0.0021     0.0436      0.0    
   1300      2980.4     1.4084     1.4176     0.0018     0.0413      0.0    
   1350     3097.99     1.4226     1.4094     0.003      0.0393      0.0    
   1400     3214.55     1.4169     1.4091     0.0021     0.0372      0.0    
   1450     3334.03     1.4147     1.4116     0.0023     0.0353      0.0    
   1500     3453.64     1.4095     1.4142     0.0022     0.0337      0.0    
   1550     3570.06     1.4181     1.4163     0.0033     0.0322      0.0    
   1600     3686.35     1.4127     1.4162     0.002      0.0309      0.0    
   1650     3803.95     1.4162     1.413      0.0017     0.0293      0.0    
   1700     3920.36     1.4096     1.4254     0.0021     0.028       0.0    
   1750     4036.72     1.4087     1.4122     0.0024     0.0268      0.0    
   1800     4153.05     1.4136     1.4159     0.0025     0.0256      0.0    
   1850     4269.28     1.4172     1.4146     0.0022     0.0248      0.0    
   1900     4385.65     1.4189     1.4156     0.0026     0.024       0.0    
   1950     4501.93     1.414      1.4154     0.0023     0.0228      0.0    
   2000     4617.86     1.4102     1.4112     0.0024     0.0221      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       40.06      1.4199     1.4131     0.007      0.3875      0.0    
   100       180.04     1.4147     1.4153     0.0058     0.3172      0.0    
   150       320.32     1.4073     1.4103     0.0053     0.2739      0.0    
   200       460.37     1.4179     1.4155     0.0052     0.2425      0.0    
   250       600.6      1.416      1.4117     0.0035     0.216       0.0    
   300       740.35     1.4232     1.4131     0.0052     0.1938      0.0    
   350       880.14     1.4146     1.4187     0.003      0.1741      0.0    
   400      1019.94     1.4178     1.4156     0.0037     0.1569      0.0    
   450      1159.57     1.4135     1.4147     0.0047     0.141       0.0    
   500      1299.09     1.416      1.4146     0.003      0.1263      0.0    
   550      1438.75     1.4054     1.4158     0.0024     0.1138      0.0    
   600      1578.83     1.4168     1.4152     0.0027     0.1025      0.0    
   650      1718.47     1.4181     1.4104     0.004      0.0929      0.0    
   700      1858.26     1.417      1.413      0.0035     0.0844      0.0    
   750      1997.96     1.4132     1.4122     0.0027     0.0772      0.0    
   800      2137.94     1.408      1.4111     0.0026     0.0713      0.0    
   850      2277.85     1.4164     1.4176     0.0027     0.0654      0.0    
   900       2417.7     1.4149     1.4143     0.0033     0.0606      0.0    
   950      2557.52     1.4106     1.4173     0.0033     0.0561      0.0    
   1000     2698.18     1.4219     1.412      0.0024     0.052       0.0    
   1050      2839.2     1.4213     1.4147     0.0029     0.0486      0.0    
   1100     2981.34     1.4122     1.4136     0.002      0.0452      0.0    
   1150     3122.57     1.4125     1.4142     0.0026     0.0421      0.0    
   1200     3263.84     1.4134     1.4146     0.0027     0.0397      0.0    
   1250     3403.75     1.4199     1.4135     0.0026     0.0376      0.0    
   1300      3543.6     1.4085     1.4138     0.0027     0.0351      0.0    
   1350     3683.51     1.4156     1.4124     0.0031     0.0334      0.0    
   1400     3823.26     1.408      1.4134     0.0027     0.0319      0.0    
   1450     3963.78     1.4188     1.4145     0.0033     0.0302      0.0    
   1500     4104.26     1.4187     1.4106     0.0027     0.0285      0.0    
   1550     4244.59     1.4208     1.4172     0.0025     0.0275      0.0    
   1600     4384.88     1.4077     1.415      0.003      0.0263      0.0    
   1650     4525.09     1.4179     1.4179     0.0034     0.025       0.0    
   1700     4665.24     1.4126     1.4116     0.0026     0.0238      0.0    
   1750     4805.61     1.4208     1.4122     0.0022     0.0232      0.0    
   1800     4946.01     1.4041     1.4129     0.0027     0.0222      0.0    
   1850     5086.71     1.4131     1.4136     0.003      0.0211      0.0    
   1900     5226.72     1.4159     1.4142     0.0029     0.0204      0.0    
   1950     5366.79     1.4088     1.4128     0.0026     0.0196      0.0    
   2000     5506.87     1.4122     1.4174     0.0036     0.0189      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50        40.2      1.4157     1.4149     0.0129     0.4141      0.0    
   100       204.52     1.4186     1.4124     0.0115     0.3429      0.0    
   150       368.67     1.4145     1.4158     0.0074     0.299       0.0    
   200       532.67     1.4191     1.4179     0.0073     0.2643      0.0    
   250       696.76     1.4192     1.4113     0.0053     0.2317      0.0    
   300       860.94     1.4136     1.4119     0.0057     0.2047      0.0    
   350       1025.0     1.4096     1.4103     0.0042     0.1809      0.0    
   400      1188.94     1.4109     1.4168     0.0055     0.1601      0.0    
   450      1353.01     1.4079     1.4143     0.0058     0.1413      0.0    
   500      1516.97     1.4223     1.4126     0.0037     0.1252      0.0    
   550      1680.74     1.4155     1.4156     0.0043     0.1112      0.0    
   600      1845.21     1.4148     1.4151     0.0031     0.0998      0.0    
   650      2009.05     1.4113     1.4127     0.0038     0.0899      0.0    
   700      2172.94     1.4183     1.4129     0.004      0.0811      0.0    
   750      2336.83     1.4063     1.4138     0.0029     0.0736      0.0    
   800      2500.68     1.4166     1.4164     0.0035     0.0677      0.0    
   850      2664.46     1.4196     1.4111     0.0033     0.061       0.0    
   900      2828.27     1.4169     1.4139     0.0031     0.0561      0.0    
   950      2992.33     1.418      1.4133     0.0038     0.0516      0.0    
   1000     3157.32     1.412      1.416      0.0044     0.0477      0.0    
   1050     3321.36     1.4086     1.4122     0.0042     0.0447      0.0    
   1100     3484.97     1.4125     1.4114     0.0029     0.042       0.0    
   1150     3648.64     1.4203     1.4094     0.0055     0.0394      0.0    
   1200     3814.75     1.4109     1.4128     0.0027     0.0366      0.0    
   1250     3980.73     1.4157     1.4169     0.0037     0.0346      0.0    
   1300     4147.64     1.4198     1.4116     0.003      0.0328      0.0    
   1350     4314.64     1.417      1.4086     0.0034     0.031       0.0    
   1400     4481.01     1.4107     1.4209     0.0035     0.0291      0.0    
   1450     4646.02     1.418      1.4133     0.0039     0.0279      0.0    
   1500     4810.03     1.4178     1.4154     0.0024     0.0263      0.0    
   1550     4974.06     1.4189     1.4173     0.0024     0.0252      0.0    
   1600     5137.95     1.4139     1.4124     0.0032     0.0238      0.0    
   1650     5303.75     1.4141     1.4109     0.003      0.0227      0.0    
   1700     5470.19     1.4079     1.4142     0.0034     0.0219      0.0    
   1750     5636.76     1.4145     1.4113     0.0026     0.0208      0.0    
   1800      5803.4     1.4186     1.4138     0.0026     0.0199      0.0    
   1850      5969.8     1.4135     1.4153     0.0028     0.0192      0.0    
   1900     6134.68     1.4119     1.4144     0.0032     0.0184      0.0    
   1950     6298.68     1.4159     1.4155     0.0034     0.018       0.0    
   2000     6462.72     1.4212     1.4141     0.0032     0.0169      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       40.42      1.4026     1.4141     0.0105     0.4115      0.0    
   100       228.31     1.4162     1.4124     0.0071     0.354       0.0    
   150       416.31     1.4242     1.4145      0.01      0.308       0.0    
   200       604.77     1.4139     1.4158     0.009      0.2712      0.0    
   250       800.93     1.4157     1.4139     0.0056     0.2371      0.0    
   300       988.58     1.408      1.4181     0.0055     0.2083      0.0    
   350      1176.01     1.4191     1.4158     0.0075     0.1834      0.0    
   400      1363.38     1.4149     1.4156     0.007      0.1606      0.0    
   450      1550.75     1.4149     1.4198     0.0044     0.1411      0.0    
   500      1738.12     1.4149     1.4104     0.0055     0.1246      0.0    
   550       1925.6     1.4105     1.4136     0.0059     0.1101      0.0    
   600      2112.87     1.4238     1.4146     0.0048     0.0984      0.0    
   650      2300.25     1.4093     1.4152     0.0067     0.0883      0.0    
   700      2488.63     1.4117     1.4139     0.0047     0.0784      0.0    
   750      2675.95     1.4093     1.4177     0.0048     0.0714      0.0    
   800      2863.19     1.4213     1.4185     0.0049     0.065       0.0    
   850       3050.5     1.413      1.4134     0.0036     0.0584      0.0    
   900      3237.81     1.4105     1.4161     0.0041     0.054       0.0    
   950      3425.07     1.4198     1.4147     0.005      0.0497      0.0    
   1000     3612.29     1.4127     1.4111     0.0041     0.0458      0.0    
   1050     3799.81     1.4131     1.4158     0.0043     0.0433      0.0    
   1100     3987.18     1.409      1.4166     0.0039      0.04       0.0    
   1150     4174.54     1.4052     1.4135     0.0036     0.0369      0.0    
   1200     4361.71     1.4068     1.4122     0.004      0.0351      0.0    
   1250     4548.85     1.4145     1.4136     0.0033     0.0323      0.0    
   1300     4735.99     1.4143     1.4115     0.0037     0.0308      0.0    
   1350     4923.24     1.4156     1.4141     0.0036     0.0292      0.0    
   1400     5110.56     1.418      1.4112     0.0046     0.0276      0.0    
   1450     5297.92     1.4125     1.4138     0.006      0.026       0.0    
   1500     5485.54     1.4139     1.4135     0.004      0.0246      0.0    
   1550     5672.85     1.4152     1.4155     0.0048     0.023       0.0    
   1600     5860.26     1.4073     1.4196     0.005      0.0221      0.0    
   1650     6047.67     1.413      1.4172     0.0031     0.0209      0.0    
   1700     6235.02     1.4191     1.4137     0.0026     0.0199      0.0    
   1750      6422.6     1.4141     1.4156     0.0045     0.0193      0.0    
   1800     6610.06     1.4184     1.4146     0.0033     0.0181      0.0    
   1850     6797.62     1.4159     1.4195     0.0037     0.0175      0.0    
   1900     6985.08     1.4172     1.4148     0.0036     0.0166      0.0    
   1950     7172.77     1.4151     1.4185     0.0033     0.0161      0.0    
   2000      7360.5     1.4177     1.4149     0.0029     0.0154      0.0    
Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.36422324180603
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       136.36     1.2332     1.1774     3.1623     3.9773     0.0011  
   100       394.75     0.7744     0.7444     3.1407     0.2509     0.0001  
   150       649.83     0.7634     0.7193     2.6256     0.163      0.0001  
   200       904.32     0.7146     0.6574     2.355      0.1296      0.0    
   250       1159.0     0.6823     0.6189     2.4466     0.102       0.0    
   300      1413.94     0.6636     0.5987     2.1365     0.0916      0.0    
   350      1667.99     0.6463     0.5636     2.2272     0.0754      0.0    
   400       1922.5     0.6369     0.5564     1.9125     0.0593      0.0    
   450      2177.69     0.6621     0.5463     1.8941     0.0618      0.0    
   500      2433.08     0.6205     0.5122     2.021      0.0784      0.0    
   550      2688.95     0.617      0.4915     2.1083     0.0573      0.0    
   600      2944.11     0.6076     0.4919     1.9987     0.0564      0.0    
   650      3199.68     0.6157     0.4923     2.1357     0.0507      0.0    
   700      3454.66     0.6106     0.4876     2.0005     0.0433      0.0    
   750      3710.46     0.6058     0.4771     2.1281     0.0451      0.0    
   800       3966.4     0.6066     0.4545     1.8504     0.0455      0.0    
   850      4222.15     0.5925     0.4545     1.6926     0.0453      0.0    
   900      4477.61     0.5903     0.4528     1.6751      0.04       0.0    
   950      4733.32     0.5905     0.442      1.7966     0.039       0.0    
   1000     4988.67     0.6036     0.4656     2.0515     0.0382      0.0    
   1050     5244.15     0.5959     0.4503     2.0961     0.0369      0.0    
   1100     5498.37     0.5956     0.4541     1.8359     0.0394      0.0    
   1150     5753.44     0.6052     0.4697     1.9331     0.0353      0.0    
   1200     6007.85     0.6199     0.4785     2.3911     0.0367      0.0    
   1250     6262.17     0.6145     0.4942     2.3435     0.0392      0.0    
   1300     6516.71     0.6157     0.4727     2.3769     0.0334      0.0    
   1350     6770.06      0.61      0.4889     2.2508     0.0359      0.0    
   1400     7023.46     0.6138     0.4966     2.4842     0.0358      0.0    
   1450      7276.9     0.6027     0.4707     2.7142     0.0385      0.0    
   1500     7530.58     0.611      0.471      2.417      0.0385      0.0    
   1550     7784.27     0.6044     0.4946     2.8557     0.0377      0.0    
   1600     8036.86     0.6028      0.48      2.617      0.0381      0.0    
   1650     8288.96     0.6019     0.4885     2.5915     0.0372      0.0    
   1700     8539.45     0.5984     0.4693     2.5336     0.0404      0.0    
   1750     8790.19     0.5937     0.4737     2.4566     0.0455      0.0    
   1800     9040.33     0.5969     0.4686     2.6577     0.0362      0.0    
   1850     9289.86     0.5886     0.4609     2.6408     0.036       0.0    
   1900      9538.9     0.5858     0.4587     2.5989     0.0511      0.0    
   1950     9788.13     0.5788     0.4481     2.4161     0.0575      0.0    
   2000     10036.94    0.579      0.4486     2.4413     0.0487      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       135.79     1.4214     1.4144     0.3519     1.2164      0.0    
   100       443.11     1.411      1.4158     0.1913     0.7935     0.0001  
   150       750.75     1.4075     1.4171     0.1933     0.4997     0.0001  
   200       1058.9     1.4058     1.4135     0.1421     0.3167     0.0001  
   250      1366.17     1.415      1.4087     0.1576     0.2113     0.0001  
   300      1673.02     1.414      1.4185     0.1048     0.1517     0.0001  
   350      1980.51     1.416      1.4155     0.1391     0.1152     0.0001  
   400      2287.92     1.4225     1.4147     0.1433     0.0909     0.0001  
   450      2596.36     1.4118     1.4101     0.161      0.0728      0.0    
   500      2904.09     1.4134     1.4083     0.134      0.0601      0.0    
   550      3212.54     1.3892     1.3925     0.1994     0.0513      0.0    
   600      3521.19     1.3564     1.3493     0.2324     0.0448      0.0    
   650      3829.59     1.2335     1.2374     0.4109      0.04       0.0    
   700      4135.44     0.9051     0.9013     0.7741     0.046       0.0    
   750      4438.34     0.7409     0.7429     1.0213     0.0824      0.0    
   800      4738.04     0.7203     0.7101     1.0191     0.0914      0.0    
   850      5037.39     0.7356     0.7215     0.6424     0.0678      0.0    
   900      5335.64     0.7259     0.7113     0.5962     0.0351      0.0    
   950      5633.13     0.7355     0.7229     0.597      0.0285      0.0    
   1000     5930.68     0.7248     0.7065     0.7485     0.0217      0.0    
   1050     6227.63     0.7352     0.7201     0.753      0.0199      0.0    
   1100     6525.81     0.7167     0.7041     0.699      0.0175      0.0    
   1150     6823.48     0.713      0.702      0.7475     0.0187      0.0    
   1200      7121.2     0.7163     0.708      0.5657     0.0141      0.0    
   1250     7418.74     0.7216     0.7162     0.7043     0.0131      0.0    
   1300     7716.63     0.7209     0.7003     0.6011     0.0122      0.0    
   1350     8015.96     0.7088     0.6974     0.5722     0.0141      0.0    
   1400     8314.61     0.7105     0.6905     0.5755     0.0119      0.0    
   1450     8613.75     0.6963     0.6804     0.6698     0.0104      0.0    
   1500      8913.4     0.6958     0.6785     0.6634     0.0123      0.0    
   1550     9214.03     0.6897     0.6779     0.5381     0.0097      0.0    
   1600     9514.58     0.6846     0.6731     0.4852     0.0096      0.0    
   1650     9813.24     0.6907     0.6666     0.4294     0.0092      0.0    
   1700     10112.37    0.6856     0.6654     0.515      0.0098      0.0    
   1750     10411.25    0.6789     0.6593     0.5479     0.0085      0.0    
   1800     10711.74    0.6778     0.6625     0.7475     0.0093      0.0    
   1850     11012.62    0.6802     0.6602     0.5059     0.0079      0.0    
   1900     11313.15    0.6822     0.6563     0.6228     0.0076      0.0    
   1950     11614.45    0.6706     0.6474     0.4488     0.0082      0.0    
   2000     11916.33    0.6662     0.6502     0.5273     0.0076      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       134.74     1.4105     1.4147     0.1869     0.993       0.0    
   100       494.8      1.4166     1.4153     0.1576     0.7118      0.0    
   150       855.76     1.4071     1.4144     0.1583     0.5026      0.0    
   200      1215.96     1.425      1.4134     0.1153     0.3485      0.0    
   250      1576.12     1.4204     1.4116     0.1322     0.2451      0.0    
   300      1937.04     1.4249     1.4137     0.0954     0.1778      0.0    
   350      2298.26     1.4112     1.4149     0.1039     0.1345      0.0    
   400      2658.82     1.4167     1.4071     0.0981     0.1055      0.0    
   450      3019.68     1.4143     1.4134      0.12      0.0863      0.0    
   500      3379.25     1.4129      1.41      0.0913     0.0713      0.0    
   550      3739.97     1.4154     1.4129     0.1006     0.0596      0.0    
   600      4100.19     1.4066     1.3995     0.1506     0.0516      0.0    
   650      4452.63     1.3935     1.3961     0.1406     0.0452      0.0    
   700      4803.93     1.3656     1.3619     0.1985     0.0409      0.0    
   750      5154.55     1.2748     1.2777     0.3178     0.0367      0.0    
   800      5506.62     0.9709     0.9704     0.5792     0.0381      0.0    
   850      5859.69     0.7455     0.7388     0.8519     0.073       0.0    
   900       6205.5     0.7218     0.7098     0.9001     0.0944      0.0    
   950      6544.59     0.7356     0.7225     0.5017     0.0712      0.0    
   1000      6891.6     0.7277     0.7123     0.5985     0.035       0.0    
   1050     7247.92     0.7276     0.7223     0.5972     0.0266      0.0    
   1100     7603.14     0.7164     0.7076     0.6859     0.0249      0.0    
   1150     7950.44     0.7277     0.7133     0.5303     0.0197      0.0    
   1200     8296.79     0.7182     0.712      0.5695     0.0194      0.0    
   1250     8643.68     0.7228     0.7096     0.4053     0.0174      0.0    
   1300     8994.63     0.7186     0.7093     0.5585     0.0158      0.0    
   1350     9348.01     0.7251     0.7137     0.6274     0.0152      0.0    
   1400     9698.06      0.71      0.6949     0.511      0.0156      0.0    
   1450     10049.11    0.7141     0.7072     0.5779     0.0147      0.0    
   1500     10400.27    0.7079     0.6956     0.6289     0.0152      0.0    
   1550     10747.83    0.6972     0.683      0.4417     0.0124      0.0    
   1600     11096.7     0.7096     0.693      0.5724     0.0118      0.0    
   1650     11444.76    0.6958     0.6831     0.4412     0.0119      0.0    
   1700     11793.6     0.6929     0.6838     0.4271     0.0114      0.0    
   1750     12141.83    0.6997     0.6872     0.4728     0.0111      0.0    
   1800     12490.46    0.6869     0.6694     0.4684     0.0098      0.0    
   1850     12838.89    0.6933     0.6781      0.51      0.0096      0.0    
   1900     13187.08    0.6878     0.6739     0.5414     0.0092      0.0    
   1950     13535.79    0.6856     0.6689     0.5169     0.0084      0.0    
   2000     13884.49    0.6816     0.6697     0.3325     0.0086      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       134.63     1.4099     1.4146     0.2117     0.9013      0.0    
   100       535.23     1.419      1.415      0.1723     0.6766      0.0    
   150       935.62     1.4146     1.4167     0.1593     0.4976      0.0    
   200      1335.96     1.4184     1.418      0.1276     0.358       0.0    
   250      1735.88     1.4174     1.4139     0.1351     0.2583      0.0    
   300      2134.85     1.4155     1.4148     0.1179     0.1887      0.0    
   350      2524.92     1.4157     1.4132     0.0892     0.1442      0.0    
   400      2914.64     1.414      1.4135     0.0853     0.1134      0.0    
   450      3305.08     1.4204     1.4158     0.0908     0.0906      0.0    
   500      3695.57     1.4072     1.4153     0.0748     0.075       0.0    
   550      4084.75     1.4077     1.4069     0.0832     0.0634      0.0    
   600      4476.57     1.4091     1.4094     0.0833     0.0543      0.0    
   650      4867.97     1.4003     1.4014     0.097      0.0476      0.0    
   700      5258.63     1.3889     1.3868     0.1303     0.0421      0.0    
   750      5649.16     1.3453     1.3435     0.1785     0.0377      0.0    
   800      6040.48     1.2121     1.2116     0.3204     0.0354      0.0    
   850      6433.51     0.8444     0.8407     0.6812     0.0433      0.0    
   900      6824.71     0.7276     0.7179     0.9453     0.0975      0.0    
   950      7218.98     0.7222     0.7093     0.8571     0.0978      0.0    
   1000     7612.95     0.7447     0.7287     0.7529     0.0574      0.0    
   1050     8009.04     0.7249     0.7119     0.586      0.0362      0.0    
   1100     8406.27     0.7193     0.7112     0.5906     0.0259      0.0    
   1150     8798.74     0.7241     0.7098     0.5515     0.0233      0.0    
   1200     9191.95     0.7262     0.7097     0.6446     0.0213      0.0    
   1250     9585.49     0.7218     0.7091     0.5132     0.0204      0.0    
   1300     9977.46     0.7262     0.7119     0.5181     0.0174      0.0    
   1350     10371.18    0.7198     0.7077     0.4677     0.0176      0.0    
   1400     10765.26    0.7438     0.7299     0.4977     0.0163      0.0    
   1450     11159.95    0.7209     0.7041     0.5679     0.0159      0.0    
   1500     11554.4     0.7208     0.7053     0.4817     0.0143      0.0    
   1550     11949.63    0.7254     0.7072     0.4109     0.0136      0.0    
   1600     12344.67    0.7201     0.7056     0.5031     0.0138      0.0    
   1650     12739.47    0.7224     0.7067     0.6262     0.0129      0.0    
   1700     13133.25    0.7238     0.7097     0.5553     0.0122      0.0    
   1750     13528.37    0.7167     0.6982     0.5635     0.0125      0.0    
   1800     13923.98    0.7099     0.696      0.652      0.0107      0.0    
   1850     14319.66    0.7149     0.6951     0.5181     0.0108      0.0    
   1900     14715.91    0.7085     0.6916     0.3726      0.01       0.0    
   1950     15111.82    0.7112     0.6954     0.4526     0.0094      0.0    
   2000     15507.38    0.7123     0.6921     0.4721     0.0111      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
    50       132.7      1.4143     1.4108     0.2728     0.8624      0.0    
   100       569.2      1.414      1.4147     0.209      0.656       0.0    
   150      1005.03     1.4212     1.4129     0.2114     0.4908      0.0    
   200      1440.84     1.4124      1.41      0.1491     0.3601      0.0    
   250      1883.99     1.4063     1.415      0.1394     0.2648      0.0    
   300      2324.21     1.4131     1.4151     0.1234     0.1962      0.0    
   350      2763.62     1.416      1.4192     0.1177     0.1495      0.0    
   400      3204.15     1.4143     1.4183     0.1168     0.1171      0.0    
   450      3643.46     1.4172     1.4156     0.1066     0.0946      0.0    
   500      4081.04     1.4116     1.4158     0.0903     0.0779      0.0    
   550      4516.69     1.401      1.412      0.1076     0.0657      0.0    
   600      4954.91     1.4093     1.4114     0.1018     0.056       0.0    
   650      5391.53     1.4063     1.4077     0.0947     0.0485      0.0    
   700      5828.33     1.3967     1.4017     0.1015     0.0431      0.0    
   750      6265.13     1.3761     1.3811     0.1354     0.0385      0.0    
   800      6701.22     1.3154     1.306      0.2336     0.0354      0.0    
   850      7138.17     0.9594     0.9556     0.5467     0.0388      0.0    
   900      7572.17     0.7339     0.7251     0.7055     0.0873      0.0    
   950      8003.24     0.7194     0.705      0.727      0.0936      0.0    
   1000     8436.89     0.7457     0.7348     0.6459     0.062       0.0    
   1050     8869.35     0.717      0.7042     0.5944     0.0288      0.0    
   1100     9300.76     0.7346     0.7246     0.5402     0.0234      0.0    
   1150     9732.25     0.7426     0.731      0.6088     0.0214      0.0    
   1200     10164.83    0.7381     0.7267     0.5734     0.0201      0.0    
   1250     10597.67    0.7305     0.7165     0.5123     0.0194      0.0    
   1300     11029.7     0.7242     0.7127     0.507      0.018       0.0    
   1350     11462.58    0.7255     0.7118     0.3727     0.017       0.0    
   1400     11897.14    0.7193     0.7081     0.4334     0.0164      0.0    
   1450     12331.09    0.7176     0.7042     0.371      0.0156      0.0    
   1500     12766.08    0.7171     0.7046     0.5307     0.0147      0.0    
   1550     13201.94    0.7253     0.7105     0.3387     0.0138      0.0    
   1600     13638.12    0.7319     0.7199     0.4209     0.0138      0.0    
   1650     14071.33    0.7181     0.7057     0.4295     0.0133      0.0    
   1700     14506.4     0.7189     0.7055     0.334      0.0122      0.0    
   1750     14941.7     0.721      0.7128     0.3246     0.0119      0.0    
   1800     15376.83    0.7193     0.7073     0.349      0.0122      0.0    
   1850     15811.62    0.7228     0.7147     0.3997     0.011       0.0    
   1900     16246.12    0.7248     0.715      0.327      0.0107      0.0    
   1950     16681.09    0.7217     0.7097     0.3262     0.0104      0.0    
   2000     17116.3     0.7223     0.7098     0.3567     0.0101      0.0    
