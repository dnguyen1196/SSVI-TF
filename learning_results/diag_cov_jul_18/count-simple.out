Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  7.554339170455933
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       47.56      3.8253     3.8511     0.3162     0.2922    58957.92   47619.81    0.0001  
   100       216.48     3.8499     3.8736     0.2784     0.2251    57834.25   46707.59     0.0    
   150       385.35     3.8688     3.8994     0.2515     0.1834    56983.92   46015.69     0.0    
   200       554.11      3.89      3.9179     0.2308     0.1543    56309.64   45465.82     0.0    
   250       722.79     3.9096     3.9364     0.2039     0.1357    55748.99   45008.32     0.0    
   300       891.91     3.9213     3.9519     0.1894     0.1162    55282.74   44627.17     0.0    
   350       1061.7     3.935      3.965      0.1923     0.1062    54878.76   44297.19     0.0    
   400      1230.78     3.9474     3.9775     0.1692     0.0941    54527.84   44010.34     0.0    
   450      1399.68     3.9574     3.9868     0.1714     0.0859    54217.53   43756.98     0.0    
   500      1568.52     3.9672     3.9997     0.1603     0.0776    53940.37   43530.94     0.0    
   550      1737.52     3.9764     4.0087     0.1459     0.0703    53695.89   43331.53     0.0    
   600      1906.04     3.9831     4.0157     0.1468     0.0663    53473.35   43150.36     0.0    
   650      2074.54     3.992      4.0203     0.1483     0.0613    53272.09   42986.6      0.0    
   700      2243.28     3.999      4.0292     0.1517     0.0579    53089.45   42838.14     0.0    
   750      2412.05     4.0073     4.0349     0.1392     0.0558    52922.59   42702.44     0.0    
   800      2580.98     4.0118     4.0425     0.1276     0.0527    52769.57   42578.35     0.0    
   850       2750.2     4.0164     4.0502     0.1217     0.0482    52628.64   42463.84     0.0    
   900      2919.38     4.0232     4.054      0.1231     0.0456    52499.61   42359.19     0.0    
   950      3088.45     4.0293     4.0579     0.1217     0.0422    52381.43   42263.58     0.0    
   1000     3257.66     4.0345     4.0607     0.1152     0.0428    52271.78   42174.91     0.0    
   1050      3426.7     4.0342     4.0647     0.1256     0.038     52170.72   42093.21     0.0    
   1100     3595.36     4.0384     4.0671     0.118      0.0386    52074.92   42015.91     0.0    
   1150     3764.08     4.0392     4.0692     0.1151     0.0347    51983.12   41942.06     0.0    
   1200      3932.8     4.0365     4.0678     0.1217     0.0332    51892.99   41869.51     0.0    
   1250     4101.81     4.0311     4.0628     0.1248     0.0319    51797.33   41792.54     0.0    
   1300     4270.77     4.0239     4.057      0.1157     0.0315    51697.39   41712.16     0.0    
   1350     4441.15     4.0109     4.0405     0.1279     0.0303    51585.88   41622.81     0.0    
   1400     4613.91     3.9939     4.0254     0.1235     0.0303    51465.34   41526.27     0.0    
   1450      4782.8     3.9708      4.0       0.134      0.0284    51327.86   41416.48     0.0    
   1500     4951.66     3.9386     3.9677     0.1396     0.0274    51169.64   41290.43     0.0    
   1550     5120.43     3.8926     3.9244     0.1386     0.0259    50987.23   41145.35     0.0    
   1600      5288.1     3.8414     3.8707     0.1526     0.027     50784.04   40984.13     0.0    
   1650     5455.91     3.7754     3.8055     0.1495     0.0249    50566.82   40812.18     0.0    
   1700     5623.55     3.6902     3.7215     0.1611     0.025     50345.52   40637.1      0.0    
   1750     5792.49     3.5935     3.6166     0.1628     0.0244    50131.93   40467.96     0.0    
   1800      5960.0     3.4743     3.5012     0.1631     0.0232    49939.57   40314.95     0.0    
   1850     6127.35     3.3439     3.3686     0.1605     0.0222    49780.2    40187.49     0.0    
   1900     6298.78     3.1891     3.2176     0.1658     0.0221    49645.9    40078.29     0.0    
   1950     6465.47      3.02      3.0427     0.1741     0.0223    49539.12   39989.45     0.0    
   2000      6630.4     2.8431     2.8657     0.1697     0.0216    49456.27   39920.3      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       47.71      3.8121     3.8231     0.162      0.1338    59984.28   97065.51     0.0    
   100       271.17     3.8212     3.8356     0.1751     0.1141    59444.49   96183.84     0.0    
   150       496.82     3.8303     3.8472     0.1375     0.1075    58943.9    95366.54     0.0    
   200       717.51     3.8445     3.8604     0.1369     0.0983    58474.81   94599.51     0.0    
   250       939.67     3.8569     3.8691     0.1375     0.1002    58042.49   93891.94     0.0    
   300      1160.66     3.8662     3.8835     0.1214     0.0916    57640.47   93233.37     0.0    
   350      1381.46     3.8737     3.8889     0.1125     0.0846    57271.28   92628.34     0.0    
   400       1601.8     3.8837     3.9013     0.1218     0.0835    56922.04   92055.89     0.0    
   450      1821.65     3.8915     3.9075     0.1248     0.0779    56598.85   91525.28     0.0    
   500      2042.06     3.9002     3.9163     0.1006     0.0746    56297.71   91031.35     0.0    
   550      2262.62     3.9095     3.9245     0.1085     0.0707    56014.32   90566.08     0.0    
   600       2482.7     3.9168     3.9365     0.1052     0.0712    55752.04   90135.24     0.0    
   650      2703.03     3.9231     3.941      0.105      0.0662    55505.0    89729.17     0.0    
   700      2923.07     3.9318     3.9498     0.1037     0.0626    55271.52   89345.01     0.0    
   750      3143.05     3.9385     3.9564     0.0938     0.0574    55054.01   88987.75     0.0    
   800      3363.53     3.9445     3.961      0.0956     0.0551    54848.03   88649.1      0.0    
   850      3583.59     3.9505     3.9707     0.0945     0.0549    54650.53   88324.66     0.0    
   900      3803.61     3.9566     3.975      0.0826     0.0519    54468.41   88025.57     0.0    
   950      4023.56     3.9631     3.9839     0.089      0.0495    54293.21   87737.86     0.0    
   1000     4243.35     3.9689     3.9882     0.0838     0.049     54126.73   87464.47     0.0    
   1050     4464.71     3.9747     3.9936     0.0868     0.0487    53969.59   87206.58     0.0    
   1100     4687.59     3.9785     3.9981     0.089      0.0455    53820.15   86961.29     0.0    
   1150     4909.49     3.9878     4.0022     0.0782     0.0443    53677.16   86726.49     0.0    
   1200     5129.64     3.9904     4.0093     0.0823     0.0431    53540.9    86503.07     0.0    
   1250     5349.56     3.9949     4.015      0.0826     0.0414    53412.39   86292.44     0.0    
   1300     5569.43     3.9982     4.0201     0.0801     0.0384    53289.11   86090.37     0.0    
   1350     5790.38     4.0018     4.0247     0.0776     0.0373    53171.87   85898.37     0.0    
   1400     6011.05     4.0082     4.0295     0.0755     0.0376    53059.79   85714.8      0.0    
   1450     6231.67     4.0132     4.0331     0.0672     0.0346    52952.25   85538.81     0.0    
   1500     6452.14     4.0146     4.0362     0.0711     0.0334    52849.69   85371.02     0.0    
   1550     6673.25     4.0209     4.0405     0.082      0.0336    52750.33   85208.49     0.0    
   1600     6893.93     4.0245     4.0463     0.0759     0.0325    52656.22   85054.43     0.0    
   1650     7114.35     4.0252     4.0462     0.0758     0.0314    52566.59   84908.06     0.0    
   1700     7335.05     4.0291     4.0528     0.0789     0.0296    52481.14   84768.41     0.0    
   1750     7555.48     4.0354     4.0552     0.0726     0.0306    52399.18   84634.46     0.0    
   1800     7775.74     4.036      4.0584     0.075      0.0296    52322.62   84509.52     0.0    
   1850     7996.39     4.0426     4.0606     0.0649     0.0288    52248.03   84387.52     0.0    
   1900     8216.97     4.0459     4.0644     0.0694     0.0267    52177.25   84271.92     0.0    
   1950     8437.31     4.0484     4.0667      0.06      0.027     52108.65   84159.84     0.0    
   2000     8657.96     4.047      4.0712     0.068      0.0252    52043.27   84052.95     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50        47.1      3.8138     3.8173     0.1232     0.0965    60138.21  145383.83     0.0    
   100       321.76     3.8185     3.8286     0.1251     0.0907    59731.41   144392.8     0.0    
   150       595.8      3.8252     3.838      0.1137     0.0903    59346.27  143454.08     0.0    
   200       869.83     3.8334     3.8463     0.1125     0.0826    58977.53  142555.76     0.0    
   250      1143.88     3.8488     3.8514     0.1111     0.0789    58624.49  141695.08     0.0    
   300      1417.95     3.8494     3.8619     0.1126     0.0788    58287.66   140873.8     0.0    
   350      1693.57     3.8588     3.8709     0.1016     0.0751    57964.59  140086.54     0.0    
   400      1967.89     3.8641     3.8777     0.1099     0.0717    57659.36  139342.05     0.0    
   450      2243.57     3.8724     3.8893     0.0985     0.0668    57368.17  138630.79     0.0    
   500       2522.1     3.8815     3.8945     0.0973     0.0655    57092.13  137956.86     0.0    
   550      2801.38     3.891      3.9028     0.0957     0.0612    56830.86  137317.99     0.0    
   600      3078.55     3.8939     3.9086     0.089      0.0607    56577.73  136699.21     0.0    
   650      3353.09     3.9024     3.9178     0.1025     0.0644    56340.53  136119.45     0.0    
   700      3627.16     3.9077     3.9207     0.0906     0.058     56115.47  135569.18     0.0    
   750      3904.75     3.917      3.9264     0.0901     0.057     55901.61  135046.03     0.0    
   800      4183.03     3.9236     3.9349     0.0818     0.0522    55696.88  134544.97     0.0    
   850      4461.18     3.9262     3.9411     0.084      0.0532    55500.77  134065.05     0.0    
   900      4736.09     3.9339     3.9462     0.0872     0.0497    55313.2   133605.83     0.0    
   950      5010.35     3.9396     3.9495     0.0808     0.0493    55133.19   133165.2     0.0    
   1000     5284.71     3.9452     3.9571     0.0794     0.0462    54960.18  132741.49     0.0    
   1050     5558.89     3.9509     3.9631     0.0772     0.0474    54794.98  132337.12     0.0    
   1100     5833.14     3.9542     3.9694     0.0829     0.0465    54638.33   131953.8     0.0    
   1150     6107.18     3.9593     3.9727     0.0857     0.0444    54486.79  131583.46     0.0    
   1200     6381.08     3.9606     3.9769     0.0709     0.0417    54342.87  131231.43     0.0    
   1250     6654.94     3.9714     3.9825     0.0754     0.0434    54203.23  130889.92     0.0    
   1300     6929.18     3.9763     3.9895     0.0797     0.0392    54068.73  130561.08     0.0    
   1350     7203.09     3.9793     3.9924     0.0687     0.0392    53938.77  130243.12     0.0    
   1400     7477.15     3.983      3.9977     0.0696     0.0378    53815.02  129940.76     0.0    
   1450     7752.19     3.9906     4.0017     0.0718     0.0379    53695.19  129647.93     0.0    
   1500     8026.09     3.9915     4.0047     0.0682     0.0351    53580.82  129368.71     0.0    
   1550     8300.06     3.9973     4.0087     0.0652     0.0374    53470.92  129100.45     0.0    
   1600     8573.98     4.0029     4.0136     0.0633     0.0346    53364.62  128841.16     0.0    
   1650     8848.25     4.0057     4.0191     0.0643     0.0328    53262.19  128591.39     0.0    
   1700     9122.54     4.0069     4.0214     0.0658     0.0322    53162.53  128348.14     0.0    
   1750     9396.71     4.0099     4.0269     0.0629     0.0326    53067.95  128117.56     0.0    
   1800     9670.48     4.0167     4.0294     0.0659     0.0298    52975.78  127892.87     0.0    
   1850     9944.32     4.0154     4.0328     0.0644     0.0289    52887.41  127677.32     0.0    
   1900     10218.35    4.0234     4.0369     0.0628     0.0288    52801.85  127468.91     0.0    
   1950     10492.61    4.0281     4.0406     0.063      0.0282    52719.53  127268.37     0.0    
   2000     10766.55    4.0315     4.0434     0.0665     0.0288    52640.11  127074.93     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       47.31      3.8079     3.8209     0.1057     0.0768    60212.58  194224.48     0.0    
   100       375.31     3.8121     3.8255     0.1087     0.0752    59870.44  193112.94     0.0    
   150       703.17     3.821      3.8314     0.0957     0.0705    59537.39  192031.93     0.0    
   200      1031.07     3.8337     3.8418     0.0902     0.0717    59221.45   191005.7     0.0    
   250       1359.4     3.8389     3.8472     0.0946     0.0681    58911.44  189998.09     0.0    
   300      1686.97     3.8438     3.8537     0.1026     0.0645    58614.16  189031.87     0.0    
   350      2013.97     3.8455     3.858      0.0879     0.0626    58329.48  188106.39     0.0    
   400      2341.14     3.8592     3.8657     0.0871     0.0618    58053.82  187209.29     0.0    
   450      2677.17     3.8697     3.8748     0.0936     0.0582    57792.85  186360.01     0.0    
   500      3012.43     3.8711     3.8793     0.0826     0.0618    57540.91  185540.38     0.0    
   550      3345.05     3.8777     3.8855     0.085      0.058     57297.74  184748.44     0.0    
   600      3672.84     3.8839     3.8903     0.0812     0.0559    57063.56  183985.43     0.0    
   650      4000.67     3.8868     3.8991     0.0828     0.0536    56840.17   183258.0     0.0    
   700      4328.62     3.898      3.9058     0.0855     0.0516    56628.2   182567.39     0.0    
   750      4661.86     3.9046     3.9124      0.08      0.0494    56421.98   181895.0     0.0    
   800       4996.7     3.908      3.919      0.0787     0.0532    56224.09  181249.74     0.0    
   850      5327.01     3.9134     3.922      0.0802     0.0478    56032.67   180625.1     0.0    
   900      5657.24     3.9165     3.9282     0.0764     0.045     55850.04  180029.51     0.0    
   950      5987.67     3.9262     3.9344     0.0775     0.0468    55674.64  179457.36     0.0    
   1000     6314.76     3.9301     3.9394     0.0751     0.0426    55504.92   178904.1     0.0    
   1050     6642.05     3.9348     3.9436     0.0737     0.0434    55342.58  178374.43     0.0    
   1100     6969.71     3.9395     3.9499     0.0939     0.0438    55184.78  177859.69     0.0    
   1150     7297.56     3.947      3.9563     0.069      0.0415    55032.01  177361.68     0.0    
   1200      7625.5     3.951      3.9606     0.077      0.0413    54884.9   176881.85     0.0    
   1250     7952.55     3.9543     3.9658     0.0787     0.0394    54744.14  176422.92     0.0    
   1300     8274.74     3.9583     3.9691     0.0704     0.0392    54607.93  175978.27     0.0    
   1350     8592.77     3.9609     3.9745     0.0714     0.0378    54474.88  175544.23     0.0    
   1400     8910.04     3.9654     3.9795     0.0668     0.0356    54347.44  175128.94     0.0    
   1450     9227.39     3.9728     3.9851     0.0692     0.0354    54224.77  174729.11     0.0    
   1500     9551.02     3.9758     3.9857     0.0594     0.0356    54106.19  174342.59     0.0    
   1550     9876.41     3.9794     3.9904     0.0624     0.0346    53991.06  173967.19     0.0    
   1600     10196.95    3.9845     3.9963     0.0616     0.0325    53881.06   173609.0     0.0    
   1650     10524.21    3.9887     4.0002     0.0712     0.0325    53774.28  173261.13     0.0    
   1700     10851.75    3.9922     4.0033     0.0672     0.0318    53669.86  172920.72     0.0    
   1750     11179.57    3.9928     4.0072     0.067      0.0293    53570.07  172595.59     0.0    
   1800     11508.41    4.0002     4.0108     0.0624     0.0317    53472.13  172276.71     0.0    
   1850     11832.65    4.0041     4.0131     0.0606      0.03     53376.84  171966.53     0.0    
   1900     12149.76    4.0089     4.0165     0.0664     0.0296    53284.3   171665.32     0.0    
   1950     12466.78    4.0112     4.021      0.0552     0.0291    53196.23  171378.56     0.0    
   2000     12783.81    4.0151     4.0234     0.0583     0.0272    53110.88  171100.83     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       45.85      3.8027     3.8233     0.0928     0.0639    60260.33  243045.18     0.0    
   100       415.0      3.8106     3.8288     0.0934     0.0653    59960.31  241827.66     0.0    
   150       784.12     3.8213     3.8373     0.0856     0.0635    59667.17   240638.5     0.0    
   200       1153.9     3.8249     3.8434     0.087      0.0629    59382.9    239484.4     0.0    
   250      1526.27     3.8359     3.8455     0.0937     0.0614    59107.87  238368.86     0.0    
   300      1898.35     3.8407     3.8545     0.0817     0.0577    58843.29  237294.91     0.0    
   350      2267.63     3.8471     3.8619     0.0834     0.0582    58586.97  236253.65     0.0    
   400      2637.22     3.8478     3.8667     0.0753     0.0548    58334.77  235228.03     0.0    
   450      3011.99     3.8542     3.8731     0.0764     0.0548    58095.38  234255.26     0.0    
   500      3381.51     3.8635     3.8785     0.0812     0.0548    57860.42  233299.71     0.0    
   550       3755.1     3.8726     3.8828     0.0723     0.0524    57636.47  232388.68     0.0    
   600      4128.77     3.8733     3.8909     0.0764     0.0494    57419.9   231507.52     0.0    
   650       4508.0     3.8817     3.8951     0.0749     0.0486    57210.69   230656.5     0.0    
   700      4891.67     3.8876     3.9005     0.0839     0.0507    57004.88  229818.68     0.0    
   750      5266.78     3.8909     3.9068     0.0745     0.0474    56808.48  229018.76     0.0    
   800      5642.19     3.8992     3.9147     0.0763     0.0456    56617.58  228241.92     0.0    
   850      6017.52     3.9044     3.9194     0.0724     0.0464    56435.62  227501.37     0.0    
   900      6395.34     3.905      3.9247     0.0719     0.043     56259.25  226783.17     0.0    
   950      6770.45     3.9141     3.9299     0.0754     0.0462    56087.11  226082.16     0.0    
   1000     7146.04     3.9181     3.9331     0.0692     0.0424    55920.94  225405.24     0.0    
   1050     7521.34     3.9276     3.9371     0.0696     0.0427    55760.91  224752.87     0.0    
   1100     7897.05     3.9274     3.9456     0.0735      0.04     55605.4   224118.91     0.0    
   1150     8273.16     3.9334     3.9474     0.0695     0.0412    55455.97  223510.14     0.0    
   1200     8648.65     3.9351     3.953      0.0767     0.0404    55310.49  222916.74     0.0    
   1250     9024.09     3.9427     3.9597     0.0664     0.038     55170.74  222347.56     0.0    
   1300     9399.92     3.9487     3.9618     0.0716     0.0376    55036.5   221801.14     0.0    
   1350     9775.69     3.9471     3.969      0.0625     0.0373    54905.88  221269.06     0.0    
   1400     10151.52    3.9511     3.9721     0.0709     0.0364    54777.49  220745.54     0.0    
   1450     10527.94    3.9626     3.9743     0.0662     0.0352    54653.26  220239.32     0.0    
   1500     10903.93    3.9629     3.9781     0.0639     0.0388    54532.83  219748.39     0.0    
   1550     11280.32    3.9688     3.9827     0.0594     0.0347    54416.71  219275.41     0.0    
   1600     11656.1     3.9721     3.9875     0.0656     0.0346    54303.91  218816.46     0.0    
   1650     12031.45    3.9757     3.9922     0.0575     0.0318    54194.07  218368.95     0.0    
   1700     12409.77    3.9801     3.995      0.0601     0.0312    54087.55  217935.01     0.0    
   1750     12855.35    3.984      3.9993     0.0663     0.0316    53983.97   217513.1     0.0    
   1800     13299.66    3.9846     4.0033      0.06      0.0294    53882.99   217101.9     0.0    
   1850     13748.53    3.9896     4.0064     0.0593     0.0279    53785.77  216706.21     0.0    
   1900     14178.47    3.9928     4.0099     0.0506     0.0288    53691.09  216320.91     0.0    
   1950     14605.9     3.9979     4.0143     0.0581     0.029     53598.76  215945.06     0.0    
   2000     15055.67    4.0002     4.0159     0.0571     0.0275    53508.89  215579.55     0.0    
Generating synthetic  count valued data ... 
Generating synthetic  count valued data took:  7.9813432693481445
max_count =  16  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       102.72     4.2006     4.234      0.3162     4.415    128772.22  104392.59    0.0006  
   100       410.17     4.2029     4.2363     0.3136     0.2913   127950.55  103720.16    0.0001  
   150       718.27     4.1895     4.2222     0.3004     0.0588   125126.06  101412.68     0.0    
   200      1026.44     4.1543     4.1865     0.2959     0.0258   119176.96   96559.11     0.0    
   250      1334.29     4.0836     4.1151     0.2865     0.0273   109548.16   88727.1      0.0    
   300      1642.09     3.9662     3.9974     0.2855     0.0234    97277.78   78761.55     0.0    
   350      1950.53     3.7942     3.8241     0.2805     0.0205    83827.95   67845.2      0.0    
   400      2259.19     3.5719     3.5995     0.2645     0.017     70936.73   57406.28     0.0    
   450      2567.99     3.3152     3.3407     0.2507     0.0138    59824.69   48403.86     0.0    
   500      2876.82     3.0483     3.0723     0.2294     0.0103    51083.24   41330.62     0.0    
   550      3185.65     2.7961     2.8173     0.2205     0.0107    44698.72   36160.04     0.0    
   600      3494.08     2.5671     2.5842     0.1979     0.0115    40098.51   32437.64     0.0    
   650      3802.34     2.3628     2.3775     0.2111     0.0114    36804.19   29760.46    0.0001  
   700      4111.16     2.1843     2.1957     0.1735     0.0115    34403.56   27811.65    0.0001  
   750      4420.17     2.0231     2.0323     0.1694     0.011     32597.38   26344.09    0.0001  
   800      4729.03     1.8829     1.8914     0.208      0.0116    31245.24   25250.15     0.0    
   850      5037.42     1.7634     1.7696     0.1007     0.011     30233.45   24434.11     0.0    
   900      5345.03     1.6581     1.6617     0.1618     0.0107    29452.91   23791.52     0.0    
   950      5653.36     1.5719     1.5735     0.1202     0.0104    28873.32   23317.13     0.0    
   1000     5961.94     1.4935     1.4942     0.1323     0.011     28401.43   22943.15     0.0    
   1050     6271.43     1.4269     1.4256     0.1499     0.0094    28021.95   22628.32     0.0    
   1100      6581.0     1.3705     1.3664     0.1023     0.0092    27728.78   22385.77     0.0    
   1150     6890.04     1.3187     1.316      0.0942     0.0089    27486.83   22186.81     0.0    
   1200     7199.32     1.279      1.2731     0.0935     0.0088    27301.34   22040.45     0.0    
   1250     7508.91     1.2419     1.2369     0.1139     0.0089    27148.12   21918.6      0.0    
   1300     7818.11     1.209      1.2029     0.1281     0.0083    27026.4    21813.51     0.0    
   1350     8127.86     1.1829     1.1758     0.0696     0.0081    26920.72   21729.7      0.0    
   1400     8436.51     1.1595     1.1528     0.1332     0.008     26824.23   21655.73     0.0    
   1450     8745.12     1.1371     1.1295     0.0962     0.0078    26730.09   21577.22     0.0    
   1500     9053.62     1.1207     1.1121     0.1077     0.0074    26688.36   21542.52     0.0    
   1550      9362.4     1.1029     1.0937     0.1146     0.0071    26618.48   21480.61     0.0    
   1600     9670.92     1.0915     1.0803     0.1358     0.0073    26588.99   21456.0      0.0    
   1650     9980.38     1.0789     1.0717     0.1609     0.0067    26553.59   21429.79     0.0    
   1700     10289.51    1.0707     1.0597     0.1765     0.0066    26526.33   21416.6      0.0    
   1750     10598.62    1.0615     1.0502     0.1158     0.0063    26504.31   21401.19     0.0    
   1800     10908.34    1.0527     1.0433     0.0913     0.0063    26476.31   21368.63     0.0    
   1850     11218.28    1.0476     1.037      0.0832     0.0061    26465.58   21363.55     0.0    
   1900     11528.48    1.0382     1.027      0.0972     0.0058    26414.03   21328.1      0.0    
   1950     11838.94    1.0328     1.0216     0.1666     0.006     26392.42   21304.42     0.0    
   2000     12148.25    1.0264     1.0129     0.0874     0.0055    26365.56   21280.71     0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       102.76     4.1402     4.1625     0.0697     3.5915    84672.4   136786.11    0.0001  
   100       499.11     4.1745     4.1957      0.03      0.7942   107066.59  173054.43    0.0001  
   150       896.83     4.1825     4.2038     0.0258     0.3844   115325.41  186389.46    0.0001  
   200      1293.81     4.1868     4.2093     0.0239     0.2549   119264.17  192744.48    0.0001  
   250       1692.0     4.1907     4.2127     0.0281     0.1893   121473.97  196307.07    0.0001  
   300      2088.77     4.1935     4.2156     0.0288     0.1473   122890.12  198589.54    0.0001  
   350      2487.82     4.1957     4.2179     0.0287     0.1169   123857.39  200147.42     0.0    
   400      2886.97     4.1973     4.2197     0.0306     0.0959   124558.58  201276.07     0.0    
   450      3285.38     4.1995     4.2215     0.0327     0.0818   125086.23  202125.67     0.0    
   500      3685.15     4.2006     4.2228     0.0334     0.0701   125497.42  202787.39     0.0    
   550      4085.89     4.2022     4.2244     0.0328     0.0611   125828.09  203319.46     0.0    
   600      4483.29     4.2031     4.2257     0.0324     0.0542   126096.41   203751.3     0.0    
   650      4881.38     4.2044     4.2263     0.034      0.0482   126318.41  204108.59     0.0    
   700      5281.25     4.2054     4.2274     0.0319     0.0439   126504.67   204408.3     0.0    
   750      5680.69     4.2061     4.2285     0.0345     0.039    126660.01  204658.07     0.0    
   800      6080.31     4.2067     4.2291     0.0321     0.0357   126792.03  204870.34     0.0    
   850      6479.84     4.2075     4.2299     0.0361     0.0325    126904.2  205050.73     0.0    
   900      6879.07     4.2083     4.2304     0.038      0.0301   126997.01  205199.93     0.0    
   950      7276.45     4.2085     4.231      0.0388     0.0278   127075.08  205325.38     0.0    
   1000     7675.33     4.2092     4.2316     0.0359     0.0258   127138.17   205426.7     0.0    
   1050     8072.41     4.2096     4.232      0.0365     0.0237   127186.66  205504.52     0.0    
   1100     8469.78     4.2101     4.2323     0.038      0.0221   127221.78  205560.74     0.0    
   1150     8867.58     4.2103     4.2327     0.0399     0.0207   127241.75  205592.53     0.0    
   1200     9266.46     4.2104     4.2327     0.0415     0.0194    127244.2  205595.97     0.0    
   1250     9663.03     4.2105     4.2327     0.0426     0.0182   127229.47  205571.68     0.0    
   1300     10061.13    4.2105     4.2328     0.0441     0.017    127192.72  205511.75     0.0    
   1350     10458.77    4.2102     4.2328     0.0478     0.0162    127130.4  205410.37     0.0    
   1400     10856.7     4.2101     4.2324     0.0517     0.0152   127034.83  205255.57     0.0    
   1450     11254.14    4.2096     4.2321     0.0536     0.0145   126895.48  205029.44     0.0    
   1500     11650.32    4.2088     4.2313     0.0583     0.0136   126703.64  204719.22     0.0    
   1550     12048.37    4.2074     4.2299     0.0665     0.0133   126432.91   204280.8     0.0    
   1600     12445.67    4.2058     4.2282     0.0724     0.0124   126063.57  203682.68     0.0    
   1650     12842.75    4.2032     4.2256     0.0807     0.012     125548.4  202849.52     0.0    
   1700     13239.56    4.1995     4.222      0.0831     0.0117   124834.86  201695.53     0.0    
   1750     13635.54    4.1939     4.216      0.1028     0.0113   123805.53  200031.89     0.0    
   1800     14030.64    4.1861     4.208      0.112      0.0112   122393.25  197746.66     0.0    
   1850     14428.89    4.1737     4.1957     0.1216     0.0113   120304.83  194375.71     0.0    
   1900     14827.71    4.1547     4.1768     0.1459     0.0118   117379.01  189645.43     0.0    
   1950     15226.8     4.1259     4.1477     0.1621     0.0125   113220.79  182915.68     0.0    
   2000     15625.21    4.0838     4.1051     0.1724     0.0144   107708.68  174014.35     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       103.16     4.1508     4.1653     0.0705     3.4293    78519.45  189639.49     0.0    
   100       591.25     4.1993     4.2154     0.0426     0.8915   102033.98  246455.23     0.0    
   150       1079.0     4.2092     4.2239     0.0265     0.4053    112031.1  270564.65     0.0    
   200      1567.19     4.2123     4.2274     0.0244     0.2704   116917.44   282325.0     0.0    
   250      2052.13     4.2141     4.2288     0.0254     0.1921   119733.53   289098.5     0.0    
   300       2534.7     4.2151     4.2301     0.0238     0.1494   121531.92   293424.3     0.0    
   350      3016.76     4.2158     4.2308     0.0238     0.1208   122766.84  296393.22     0.0    
   400      3499.05     4.2165     4.2313     0.0235     0.0995   123656.67  298532.95     0.0    
   450      3982.81     4.2172     4.2319     0.0276     0.0831   124331.71  300155.45     0.0    
   500      4466.19     4.2172     4.2323     0.026      0.073    124857.99   301420.2     0.0    
   550      4949.55     4.2177     4.2327      0.03      0.0631   125277.22  302428.02     0.0    
   600      5433.21     4.2178     4.2331     0.0262     0.0556   125621.72   303255.9     0.0    
   650      5916.55     4.2182     4.2334     0.0267     0.0498   125906.92   303941.1     0.0    
   700      6399.02     4.2187     4.2337     0.0264     0.0445   126146.78  304517.45     0.0    
   750      6882.24     4.2189     4.2342     0.0305     0.0408   126350.62   305007.2     0.0    
   800      7365.08     4.2191     4.2342     0.0263     0.0371    126525.9  305428.26     0.0    
   850      7847.77     4.2193     4.2344     0.0272     0.0339   126677.53  305792.49     0.0    
   900      8329.71     4.2197     4.2347     0.0298     0.0318    126810.0  306110.52     0.0    
   950      8811.95     4.2198     4.2349     0.0304     0.0289   126925.34  306387.63     0.0    
   1000      9293.7     4.2201     4.2351     0.0292     0.027    127025.47  306628.03     0.0    
   1050     9775.23     4.2201     4.2352     0.0337     0.025    127112.48  306836.93     0.0    
   1100     10257.58    4.2205     4.2354     0.0288     0.0236   127187.51  307016.99     0.0    
   1150     10739.5     4.2204     4.2354     0.0307     0.0221   127250.54  307168.23     0.0    
   1200     11220.73    4.2205     4.2357     0.0316     0.0209   127302.82  307293.62     0.0    
   1250     11701.65    4.2204     4.2356     0.0316     0.0197   127343.89  307391.94     0.0    
   1300     12182.67    4.2207     4.2356     0.0317     0.0187   127372.96  307461.44     0.0    
   1350     12664.21    4.2207     4.2358     0.0333     0.0177   127390.48  307503.05     0.0    
   1400     13145.85    4.2204     4.2357     0.0338     0.0168   127393.69  307510.23     0.0    
   1450     13627.02    4.2206     4.2355     0.0371     0.0161   127381.05  307479.33     0.0    
   1500     14108.15    4.2202     4.2353      0.04      0.0152   127346.95  307396.38     0.0    
   1550     14588.68     4.22      4.235      0.0398     0.0146    127287.9  307253.64     0.0    
   1600     15070.86    4.2194     4.2345     0.0431     0.0141   127194.54  307027.65     0.0    
   1650     15552.77    4.2188     4.2337     0.047      0.0135   127053.92  306688.21     0.0    
   1700     16036.43    4.2176     4.2327     0.054      0.013    126848.69  306192.68     0.0    
   1750     16522.32    4.2161     4.2312     0.0617     0.0127   126555.47  305485.28     0.0    
   1800     17007.53    4.2139     4.2289     0.0709     0.0123   126119.79  304434.42     0.0    
   1850     17493.83    4.2101     4.2255     0.0766     0.0121   125476.49  302882.39     0.0    
   1900     17980.59    4.2051     4.2201     0.0967     0.012     124504.2  300537.59     0.0    
   1950     18466.09    4.1965     4.2118     0.1078     0.012    123024.43  296968.61     0.0    
   2000     18949.23    4.1831     4.1981     0.1292     0.0126   120746.96  291475.74     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       102.47     4.1492     4.1603     0.0827     3.3463    76094.11  244962.59     0.0    
   100       673.16     4.2032     4.2162     0.0519     0.9911    99449.69  320182.14     0.0    
   150      1243.53     4.2142     4.2271     0.0337     0.4143    110288.9  355042.23     0.0    
   200      1816.97     4.2182     4.2309     0.0314     0.2666   115702.25  372429.49     0.0    
   250      2388.72     4.2197     4.2325     0.0258     0.1972   118811.96  382408.13     0.0    
   300      2958.77     4.2209     4.2337     0.0258     0.1517   120802.38  388794.59     0.0    
   350      3530.18     4.2215     4.2345     0.0259     0.1238   122178.78  393207.44     0.0    
   400      4101.62     4.2223     4.2351     0.0244     0.1006   123177.82  396410.42     0.0    
   450      4674.79     4.2228     4.2354     0.0281     0.0854   123925.95  398808.31     0.0    
   500      5247.18     4.2227     4.2355     0.0228     0.0732   124509.99  400679.72     0.0    
   550      5820.07     4.2233     4.2361     0.0251     0.0642   124978.19  402179.48     0.0    
   600       6392.0     4.2234     4.2361     0.0237     0.0569   125360.53  403403.74     0.0    
   650      6965.22     4.2236     4.2363     0.0248     0.0507   125679.28  404425.01     0.0    
   700      7537.72     4.2238     4.2365     0.0259     0.0451   125947.53   405284.2     0.0    
   750      8110.58     4.2242     4.2368     0.0261     0.0417   126177.08  406019.42     0.0    
   800      8682.47     4.2241     4.2369     0.0247     0.0381   126374.38  406651.34     0.0    
   850      9254.08     4.2243     4.237      0.0261     0.0345   126546.26  407201.88     0.0    
   900       9826.5     4.2244     4.2371     0.0269     0.0319   126695.74  407680.32     0.0    
   950      10398.38    4.2246     4.2373     0.0281     0.0299   126826.82  408099.96     0.0    
   1000     10971.09    4.2246     4.2374     0.0268     0.0277   126942.34   408469.6     0.0    
   1050     11542.79    4.2246     4.2375     0.027      0.026    127044.26  408795.69     0.0    
   1100     12115.26    4.2249     4.2374     0.0276     0.0241   127133.71  409081.78     0.0    
   1150     12687.73    4.2249     4.2377     0.0295     0.0227   127212.19   409332.8     0.0    
   1200     13260.04    4.2247     4.2377     0.0294     0.0215   127280.97  409552.65     0.0    
   1250     13832.59    4.225      4.2378     0.0271      0.02    127338.85  409737.55     0.0    
   1300     14405.96    4.2249     4.2377     0.0299     0.0192   127386.56  409889.86     0.0    
   1350     14978.68    4.225      4.2378     0.0287     0.0182   127423.31  410007.04     0.0    
   1400     15551.24    4.2249     4.2378     0.0321     0.0173   127449.01  410088.49     0.0    
   1450     16124.4     4.2247     4.2377     0.0298     0.0165   127463.14  410132.91     0.0    
   1500     16698.34    4.2247     4.2375     0.033      0.0158   127462.07  410128.55     0.0    
   1550     17271.01    4.2247     4.2373     0.0367     0.0151   127443.36  410067.31     0.0    
   1600     17843.39    4.2245     4.2371     0.0378     0.0146   127401.69  409932.33     0.0    
   1650     18416.99    4.2238     4.2366     0.0435     0.0139   127328.28  409695.01     0.0    
   1700     18990.25    4.2233     4.236      0.0481     0.0134   127211.78  409318.47     0.0    
   1750     19562.45    4.2222     4.235      0.0497     0.0131   127036.02  408751.19     0.0    
   1800     20135.29    4.2209     4.2335     0.0632     0.0126   126766.62  407882.31     0.0    
   1850     20708.17    4.2186     4.2314     0.0724     0.0126   126351.17  406543.15     0.0    
   1900     21278.83    4.2151     4.2279     0.0871     0.0124   125701.65  404452.34     0.0    
   1950     21851.91    4.2096     4.2223     0.099      0.0124   124696.93  401214.27     0.0    
   2000     22425.75    4.2003     4.2131     0.1225     0.0127   123080.88  396007.88     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll     dw   
    50       102.7      4.1452     4.1629     0.0948     3.2688    74894.2    301852.9     0.0    
   100       766.76     4.205      4.2239     0.0553     0.9328    98063.56  395361.51     0.0    
   150      1429.27     4.2171     4.2358     0.0386     0.4368   109252.67  440469.94     0.0    
   200      2092.21     4.2213     4.2398     0.0353     0.2788   114960.42  463459.29     0.0    
   250      2754.86     4.2226     4.2418     0.029      0.2008   118264.79  476776.17     0.0    
   300      3415.91     4.224      4.2428     0.0267     0.1549   120379.92  485296.27     0.0    
   350      4078.64     4.2248     4.2437     0.0235     0.126    121834.85  491156.59     0.0    
   400      4740.37     4.225      4.2441     0.0291     0.1026   122894.06   495423.7     0.0    
   450      5402.35     4.2255     4.2446     0.0266     0.086     123687.0  498616.21     0.0    
   500      6065.29     4.226      4.2448     0.0237     0.075    124310.32   501125.4     0.0    
   550      6727.05     4.2262     4.2451     0.0268     0.0648   124810.76  503139.61     0.0    
   600      7387.43     4.2262     4.2453     0.0263     0.0573   125215.54  504769.25     0.0    
   650      8048.86     4.2264     4.2454     0.0269     0.0512   125554.88  506134.91     0.0    
   700      8709.61     4.2266     4.2457     0.0271     0.0454   125840.41  507284.03     0.0    
   750      9370.63     4.2268     4.2458     0.0282     0.042    126084.72  508267.18     0.0    
   800      10031.64    4.2267     4.2459     0.026      0.0378   126294.61  509111.53     0.0    
   850      10690.41    4.2268     4.246      0.0237     0.0353   126477.11  509845.67     0.0    
   900      11350.39    4.2271     4.2462     0.0249     0.0327   126636.86  510488.38     0.0    
   950      12009.59    4.2272     4.2462     0.027      0.0303   126777.48  511054.09     0.0    
   1000     12669.74    4.2272     4.2463     0.0321     0.0278   126902.31  511556.32     0.0    
   1050     13330.17    4.2273     4.2463     0.0243     0.0262   127013.26  512002.75     0.0    
   1100     13989.02    4.2273     4.2465     0.0246     0.0245   127111.64  512398.36     0.0    
   1150     14650.47    4.2274     4.2466     0.028      0.0231   127198.26  512746.57     0.0    
   1200     15311.49    4.2275     4.2466     0.0254     0.0215   127275.53   513057.3     0.0    
   1250     15975.12    4.2274     4.2466     0.0285     0.0204   127343.35  513329.96     0.0    
   1300     16635.39    4.2275     4.2466     0.0258     0.0194    127401.3  513562.62     0.0    
   1350     17294.12    4.2274     4.2467     0.0263     0.0185   127450.47  513759.82     0.0    
   1400     17953.58    4.2276     4.2466     0.0281     0.0175   127490.29  513919.55     0.0    
   1450     18611.39    4.2275     4.2467     0.0266     0.0167   127519.65  514037.07     0.0    
   1500     19269.25    4.2274     4.2465     0.0325     0.0161   127536.26  514103.19     0.0    
   1550     19925.81    4.2273     4.2463     0.0307     0.0153   127539.75  514116.33     0.0    
   1600     20584.58    4.227      4.2462     0.0338     0.0146    127526.4  514061.97     0.0    
   1650     21243.75    4.2269     4.2459     0.0367     0.0142   127490.46  513916.73     0.0    
   1700     21902.28    4.2265     4.2454     0.0393     0.0136   127425.86  513654.69     0.0    
   1750     22563.23    4.2257     4.2448     0.0442     0.0132   127318.18  513218.82     0.0    
   1800     23223.42    4.2249     4.2438     0.0495     0.0128   127146.59   512524.6     0.0    
   1850     23881.86    4.2235     4.2425     0.0586     0.0125   126881.25  511455.13     0.0    
   1900     24541.85    4.2211     4.2402     0.0728     0.0125   126456.78  509743.15     0.0    
   1950     25201.24    4.2174     4.2365     0.0875     0.0125   125759.38  506928.34     0.0    
   2000     25860.62    4.2111      4.23      0.1035     0.0126   124599.56  502253.06     0.0    
