Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.187870264053345
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       40.34      1.3485     1.2807     3.1623     1.7356   
   100       80.92      0.8013     0.7574     3.1233     0.7071   
   150       121.71     0.7237     0.7011     2.8386     0.3527   
   200       162.68     0.7245     0.6979     2.8764     0.208    
   250       203.64     0.7222     0.6997     2.8714     0.1655   
   300       244.94     0.7039     0.6831     2.9641     0.1403   
   350       286.43     0.717      0.6944     3.0105     0.1269   
   400       327.87     0.7171     0.6958     3.0056     0.1183   
   450       368.86     0.7123     0.6909     2.9934     0.115    
   500       409.57     0.7155     0.6953     2.8508     0.0985   
   550       450.19     0.7169     0.7017     2.8659     0.0901   
   600       490.57     0.7165     0.6987     2.9655     0.081    
   650       530.75     0.7251     0.7078     2.6468     0.0752   
   700       570.86     0.7109     0.6899     2.9219     0.0663   
   750       610.97     0.7183     0.6948     2.5333     0.0672   
   800       650.98     0.7165     0.7006     2.8853     0.0607   
   850       690.95     0.7134     0.6941     2.8146     0.0546   
   900       731.06     0.717      0.7064     2.7534     0.0464   
   950       770.99     0.7172     0.707      2.5596     0.0466   
   1000      810.99     0.7173     0.706      2.7977     0.0394   
   1050      850.99     0.7171     0.7063     2.637      0.0434   
   1100      891.22     0.7103     0.696      2.7585     0.0361   
   1150      931.36     0.7201     0.706      2.3996     0.0363   
   1200      971.31     0.7139     0.7007     2.6017     0.0418   
   1250     1011.82     0.7217     0.7078     2.5119      0.03    
   1300     1051.77     0.7206     0.7044     2.5824     0.0383   
   1350      1091.8     0.7096     0.6956     2.089      0.0323   
   1400     1131.81     0.7109     0.6971     2.4386     0.0261   
   1450     1171.73     0.7189     0.7044     2.3148     0.0301   
   1500     1211.58     0.7217     0.7078     2.2646     0.0309   
   1550     1251.43     0.721      0.7075     2.0797     0.0236   
   1600     1291.28     0.7211     0.7075     2.2126     0.0289   
   1650     1331.22     0.7203     0.7037     2.0987     0.0202   
   1700     1371.04     0.7106     0.6966     2.6208     0.0233   
   1750     1410.98     0.7138     0.7004     1.8889     0.0237   
   1800     1450.85     0.7113     0.6956     2.4849     0.0233   
   1850     1490.69     0.715      0.701      2.547      0.0207   
   1900     1530.54     0.7094     0.6976     1.8547     0.0229   
   1950     1570.44     0.7162     0.7063     2.3345     0.023    
   2000     1610.36     0.7139     0.7024     2.1549     0.0161   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       40.28      1.4116     1.4163     0.0018     0.3386   
   100        81.6      1.4067     1.4165     0.0019     0.2793   
   150       122.82     1.4043     1.4068     0.0017     0.2445   
   200       163.92     1.4242     1.4185     0.0019     0.2185   
   250       204.87     1.4356     1.4272     0.0017      0.2     
   300       245.76     1.4345     1.425      0.0016     0.1846   
   350       286.62     1.4516     1.447      0.0018     0.1702   
   400       327.47     1.4559     1.4539     0.0012     0.1578   
   450       368.23     1.4625     1.4622     0.0013     0.1451   
   500       408.99     1.4686     1.4668     0.0013     0.1335   
   550       449.73     1.4777     1.4754     0.0014     0.123    
   600       490.51     1.483      1.4831     0.0016     0.1132   
   650       531.33     1.4876     1.4856     0.0015     0.1039   
   700       572.17     1.4867     1.4849     0.0012     0.0954   
   750       612.96     1.4892     1.4863     0.0012     0.0877   
   800       653.7      1.489      1.4862     0.0015     0.0812   
   850       694.52     1.4926      1.49      0.0013     0.0752   
   900       735.29     1.4955     1.4933     0.001      0.0696   
   950       776.1      1.4956     1.4933     0.0011     0.0653   
   1000      816.85     1.4956     1.4933     0.0012     0.0605   
   1050      857.67     1.4956     1.4933     0.0012     0.057    
   1100      898.44     1.4956     1.4933     0.0013     0.0527   
   1150      939.18     1.4956     1.4933     0.0012      0.05    
   1200      979.97     1.4956     1.4933     0.0011     0.046    
   1250     1020.78     1.4956     1.4933     0.0012     0.043    
   1300     1061.79     1.4956     1.4933     0.0015     0.0406   
   1350     1102.56     1.4956     1.4933     0.0014     0.0385   
   1400      1143.5     1.4956     1.4933     0.001      0.0363   
   1450     1184.32     1.4956     1.4933     0.0013     0.0343   
   1500     1225.14     1.4956     1.4933     0.0015     0.0324   
   1550     1266.07     1.4956     1.4933     0.0011     0.0308   
   1600     1306.98     1.4956     1.4933     0.0013     0.0293   
   1650     1347.87     1.4956     1.4933     0.0012     0.0279   
   1700     1388.77     1.4956     1.4933     0.0011     0.0264   
   1750      1429.6     1.4956     1.4933     0.0011     0.0254   
   1800     1470.49     1.4956     1.4933     0.0014     0.0242   
   1850     1511.31     1.4956     1.4933     0.0015     0.0231   
   1900     1552.18     1.4956     1.4933     0.0014     0.022    
   1950     1593.12     1.4956     1.4933     0.0015     0.0214   
   2000     1634.03     1.4956     1.4933     0.0014     0.0203   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       40.38      1.4108     1.4199     0.0027     0.3599   
   100       82.35      1.4224     1.4207     0.0028     0.3188   
   150       124.02     1.4361     1.4253     0.0025     0.2875   
   200       165.65     1.453      1.4449     0.0022     0.259    
   250       207.18     1.4676     1.4623     0.0026     0.2337   
   300       248.61     1.487      1.4825     0.0024     0.2109   
   350       289.99     1.4853     1.4788     0.0016     0.1888   
   400       331.37     1.4846     1.4785     0.0017     0.1697   
   450       372.77     1.4861     1.4808     0.0024     0.1524   
   500       414.1      1.4921     1.4912     0.0017     0.136    
   550       455.51      1.49      1.4932     0.0014     0.1221   
   600       496.86     1.5016     1.5012     0.0015     0.1098   
   650       538.19     1.5014     1.503      0.0014     0.099    
   700       579.53     1.4968     1.5011     0.0016     0.0891   
   750       621.01     1.4953     1.4982     0.0013     0.0809   
   800       662.43     1.4953     1.4987     0.0019     0.0745   
   850       703.88     1.4956     1.4989     0.0012     0.0676   
   900       745.28     1.4956     1.499      0.0014     0.0615   
   950       786.71     1.4956     1.499      0.0019     0.0566   
   1000      828.12     1.4956     1.499      0.0012     0.0525   
   1050      869.71     1.4956     1.499      0.0017     0.0485   
   1100      910.89     1.4956     1.499      0.0014     0.0452   
   1150      951.88     1.4956     1.499      0.0015     0.0419   
   1200      992.85     1.4956     1.499      0.0013     0.0389   
   1250     1033.88     1.4956     1.499      0.0014     0.0367   
   1300     1074.85     1.4956     1.499      0.0014     0.0338   
   1350     1115.84     1.4956     1.499      0.0013     0.0319   
   1400     1157.17     1.4956     1.499      0.002      0.0304   
   1450     1199.46     1.4956     1.499      0.0013     0.0288   
   1500     1241.75     1.4956     1.499      0.0016     0.0269   
   1550     1284.03     1.4956     1.499      0.0016     0.0253   
   1600     1326.29     1.4956     1.499      0.0015     0.0238   
   1650     1367.65     1.4956     1.499      0.0015     0.0228   
   1700     1408.65     1.4956     1.499      0.0013     0.0219   
   1750     1449.76     1.4956     1.499      0.0018     0.0208   
   1800     1490.83     1.4956     1.499      0.0013     0.0199   
   1850     1531.93     1.4956     1.499      0.0014     0.0191   
   1900     1573.03     1.4956     1.499      0.0014     0.018    
   1950     1614.14     1.4956     1.499      0.0014     0.0172   
   2000     1655.21     1.4956     1.499      0.0015     0.0165   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       39.98      1.399      1.4056     0.0034     0.3824   
   100       81.99      1.416      1.4135     0.0034     0.3428   
   150       123.91     1.4231     1.4261     0.0035     0.3054   
   200       166.06     1.4269     1.4337     0.0032     0.2736   
   250       208.06     1.4444     1.4441     0.0036     0.243    
   300       249.83     1.4453     1.4395     0.0033     0.2177   
   350       291.6      1.4546     1.4583     0.0022     0.1935   
   400       333.42     1.4762     1.4751     0.0027     0.1717   
   450       375.2      1.4767     1.4758     0.002      0.1515   
   500       416.94     1.4872     1.4908     0.0021     0.134    
   550       458.74     1.4911     1.4925     0.0026     0.1196   
   600       500.5      1.4932     1.4973     0.0019     0.1055   
   650       542.32     1.4995     1.4987     0.0018     0.0944   
   700       584.16     1.4954     1.497      0.0021     0.0846   
   750       625.96     1.4965     1.497      0.0018     0.0755   
   800       667.65     1.4872     1.4892     0.0015     0.0687   
   850       709.33     1.4857     1.487      0.0018     0.0634   
   900       751.08     1.4877     1.4884     0.0016     0.0573   
   950       793.19     1.4901     1.4921     0.0019     0.0524   
   1000      835.96     1.492      1.4937     0.0018     0.0485   
   1050      877.68     1.4935     1.4954     0.002      0.044    
   1100      919.37     1.4951     1.4965     0.002      0.0408   
   1150      961.12     1.4954     1.497      0.0017     0.0378   
   1200     1002.84     1.4956     1.4975     0.0019     0.0352   
   1250     1044.55     1.4956     1.4975     0.0014     0.0332   
   1300     1086.31     1.4956     1.4975     0.0019     0.0307   
   1350     1127.96     1.4956     1.4975     0.0017     0.0289   
   1400     1169.62     1.4956     1.4975     0.0018     0.027    
   1450     1211.27     1.4956     1.4975     0.0017     0.0252   
   1500     1253.79     1.4956     1.4975     0.0017     0.0239   
   1550     1296.72     1.4956     1.4975     0.0019     0.0227   
   1600     1339.65     1.4956     1.4975     0.0016     0.0214   
   1650     1382.21     1.4956     1.4975     0.0017     0.0205   
   1700     1423.94     1.4956     1.4975     0.0018     0.0194   
   1750      1465.5     1.4956     1.4975     0.0017     0.0186   
   1800     1506.95     1.4956     1.4975     0.0018     0.0175   
   1850     1548.42     1.4956     1.4975     0.0017     0.0167   
   1900     1589.69     1.4956     1.4975     0.0027     0.0161   
   1950     1630.18     1.4956     1.4975     0.0016     0.0153   
   2000     1671.36     1.4956     1.4975     0.0015     0.0146   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       39.98      1.4148     1.416      0.0046     0.3845   
   100       82.53      1.4102     1.413      0.0045     0.3431   
   150       124.57     1.411      1.4127     0.0042     0.3092   
   200       166.75     1.4106     1.4164     0.0041     0.2747   
   250       209.16     1.4186     1.4167     0.0047     0.2464   
   300       252.82     1.4202     1.4205     0.0034     0.2182   
   350       295.35     1.4194     1.4212     0.0032     0.1932   
   400       337.73     1.4234     1.4259     0.0035     0.1711   
   450       379.99     1.4348     1.4329     0.0027     0.1508   
   500       423.15     1.4412     1.4409     0.003      0.1325   
   550       466.73     1.4427     1.4427     0.0022     0.1171   
   600       510.27     1.4515     1.4519     0.0026     0.1036   
   650       553.6      1.468      1.4685     0.0024     0.0916   
   700       597.15     1.4797     1.4816     0.0021     0.0821   
   750       640.19     1.4785     1.4814     0.0019     0.0737   
   800       682.41     1.475      1.4779     0.0018     0.0668   
   850       724.61     1.4732     1.4774     0.0018     0.0597   
   900       766.89     1.4835     1.4834     0.0019     0.0545   
   950       809.09     1.4863     1.4872     0.0018     0.0497   
   1000      851.29     1.4874     1.4884     0.0019     0.0454   
   1050      893.54     1.4903     1.4918     0.0024     0.0416   
   1100      936.06     1.4924     1.4947     0.0025     0.0381   
   1150      979.81     1.4937     1.4963     0.0017     0.0354   
   1200     1021.94     1.4943     1.4968     0.0021     0.0327   
   1250     1064.09     1.4946     1.4971     0.0018     0.0307   
   1300     1106.26     1.495      1.4971     0.0019     0.0284   
   1350     1148.49     1.495      1.4972     0.0016     0.0267   
   1400     1190.74     1.495      1.4974     0.0016     0.0249   
   1450     1233.03     1.4955     1.4974     0.0017     0.0235   
   1500     1275.28     1.4956     1.4976     0.002      0.0222   
   1550     1317.57     1.4956     1.4976     0.002      0.0209   
   1600     1359.86     1.4956     1.4976     0.0019     0.0196   
   1650     1402.13     1.4956     1.4976     0.0017     0.0188   
   1700     1444.32     1.4956     1.4976     0.0017     0.0179   
   1750     1486.74     1.4956     1.4976     0.0018     0.017    
   1800     1528.97     1.4956     1.4976     0.0018     0.0161   
   1850     1571.22     1.4956     1.4976     0.0017     0.0154   
   1900     1613.46     1.4956     1.4976     0.002      0.0147   
   1950     1655.75     1.4956     1.4976     0.0018     0.014    
   2000     1698.02     1.4956     1.4976     0.0017     0.0134   
Generating synthetic  binary valued data ... 
Generating synthetic  binary valued data took:  5.3518781661987305
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       136.17     1.2727     1.2047     3.1623     4.0128   
   100       278.84     0.7668     0.7279     3.1239     0.2316   
   150       419.94     0.7635     0.732      2.5554     0.1459   
   200       561.67     0.7346     0.6892     2.6882     0.1203   
   250       703.55     0.6884     0.6459     2.5261     0.0908   
   300       844.84     0.6906     0.6106     2.0299     0.091    
   350       986.22     0.6736     0.607      2.1206     0.0795   
   400      1128.13     0.6614     0.5741     2.3325     0.0697   
   450       1270.1     0.6412     0.5662     1.9338     0.0592   
   500      1412.07     0.6447     0.5474     1.8157     0.0563   
   550      1554.08     0.6385     0.5535     2.1555     0.0516   
   600      1695.56     0.627      0.524      2.0237     0.0457   
   650      1837.32     0.6162     0.5257     1.8915     0.0488   
   700      1979.16     0.6151     0.4982     1.8023     0.0565   
   750      2120.76     0.6108     0.4874     1.7329     0.0409   
   800      2261.98     0.6025     0.4812     1.9356     0.0382   
   850      2403.54     0.6001     0.4779     1.6707     0.0398   
   900      2545.11     0.6056     0.4994     1.6583     0.0343   
   950      2686.89     0.6094     0.4743     1.8863     0.0346   
   1000     2828.48     0.5899     0.4632     1.6241     0.0311   
   1050     2969.92     0.5909     0.4492     1.7456     0.0345   
   1100     3111.38     0.5946     0.4578     1.7341     0.0327   
   1150     3253.62     0.5993     0.4539     1.9117     0.0307   
   1200     3395.65     0.5842     0.4379     2.0045     0.0314   
   1250     3537.59     0.6012     0.4463     1.5186     0.0307   
   1300     3679.19     0.5806     0.4429     1.8408     0.0293   
   1350     3821.42     0.5828     0.431      1.716      0.028    
   1400     3962.86     0.5819     0.4427     1.6503     0.0309   
   1450     4104.41     0.6039     0.4583     2.1208     0.0274   
   1500     4246.56     0.5814     0.429      1.7762     0.0302   
   1550      4388.4     0.583      0.4308     1.9655     0.0309   
   1600     4530.15     0.5801     0.432      1.7287     0.0289   
   1650     4671.74     0.5838     0.437      1.6019     0.0299   
   1700     4814.06     0.5981     0.4545     1.687       0.03    
   1750     4956.34     0.5932     0.4682     2.0972     0.0325   
   1800      5098.4     0.6004     0.4756     2.2254     0.0309   
   1850     5239.91     0.5989     0.4552     2.5948     0.029    
   1900     5381.78     0.5907     0.4652     1.8693     0.0333   
   1950     5523.49     0.6249     0.5146     2.5347     0.0352   
   2000     5664.88     0.6545     0.5771     2.6462     0.0341   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       136.86     1.4095     1.4135     1.4685     1.291    
   100       280.14     1.3711     1.3631     1.7137     0.8691   
   150       423.02      1.23      1.2156     1.2679     0.5463   
   200       565.96     0.773      0.769      1.5507     0.3584   
   250       708.0      0.7209     0.7114     1.8802     0.3163   
   300       849.76     0.7701     0.759      1.6877     0.2122   
   350       991.23     0.7481     0.7375     1.5565     0.1541   
   400      1132.93     0.7304     0.7137     1.3901     0.1029   
   450      1274.36     0.738      0.7301     1.1441     0.112    
   500      1415.83     0.7354     0.721      1.1987     0.0918   
   550      1556.77     0.7269     0.7191     1.2279     0.0676   
   600      1697.41     0.7032     0.6911     1.4753     0.0631   
   650      1838.94     0.7171     0.7062     1.0752     0.0525   
   700      1980.14     0.7139     0.6916     1.1836     0.0573   
   750      2121.62     0.6937     0.6793     1.3228     0.0509   
   800      2263.05     0.6969     0.6873     0.9929     0.0547   
   850      2404.52     0.6804     0.6568     0.9245     0.039    
   900      2545.39     0.6793     0.6617     1.1078     0.0434   
   950      2686.74     0.6706     0.6448     1.4222     0.0491   
   1000      2828.6     0.6567     0.6384     0.8233     0.0342   
   1050     2970.13     0.6663     0.6324     1.051      0.0338   
   1100     3111.88     0.6519     0.6283     0.9822     0.0277   
   1150     3253.56     0.6758     0.6518     1.2421     0.0261   
   1200     3395.65     0.6355     0.6122     1.1797     0.031    
   1250     3537.45     0.6396     0.6122     0.8785     0.0223   
   1300     3678.61     0.6323     0.6017     0.8092     0.0204   
   1350     3820.79     0.6407     0.6065     1.2041     0.0234   
   1400     3962.83     0.6148     0.5818     0.7507     0.0218   
   1450     4104.62     0.6237     0.5758     0.895      0.0215   
   1500      4246.8     0.6266     0.5951     0.8994     0.0203   
   1550     4388.78     0.6328     0.5977     1.2439     0.0196   
   1600     4530.78     0.6158     0.5846     1.0518      0.02    
   1650      4672.9     0.6148     0.576      1.1409     0.0223   
   1700      4815.3     0.6015     0.5769     1.0869     0.0183   
   1750     4956.96     0.6123     0.5679     0.9445     0.0179   
   1800     5099.11     0.6081     0.5707     1.051      0.0228   
   1850     5241.56     0.5918     0.5496     0.813      0.0166   
   1900     5384.04     0.6017     0.5655     1.2926     0.0232   
   1950     5525.85     0.5999     0.5575     1.0266     0.0179   
   2000     5667.81     0.5988     0.5625     0.8836     0.0134   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       137.03      1.42      1.4139     1.2389     1.0443   
   100       280.0      1.3783     1.3755     1.3463     0.7718   
   150       423.13     1.3239     1.3212     1.2382     0.5496   
   200       566.28     1.1449     1.1529     1.0427     0.3737   
   250       709.06     0.7795     0.7722     1.4041     0.2582   
   300       851.16     0.7296     0.7162     1.6414     0.2726   
   350       993.73     0.7861     0.7753     1.5977     0.2226   
   400      1136.31     0.745      0.7333     1.4637     0.1395   
   450      1278.52     0.7513     0.7421     1.332      0.1108   
   500      1420.45     0.7529     0.7435     1.3497     0.0948   
   550      1562.63     0.7267     0.7188     1.1622     0.0735   
   600      1705.04     0.7144     0.705      0.9146     0.0641   
   650      1847.07     0.7097     0.6993     0.8571     0.0569   
   700      1989.05     0.7415     0.7271     0.8749     0.0535   
   750      2131.15     0.7171     0.7029     1.0048     0.0497   
   800      2273.84     0.7094     0.6917     1.0185     0.0432   
   850      2416.49     0.692      0.6816     0.9293     0.0409   
   900       2559.0     0.6884     0.6597     1.0651     0.0455   
   950      2701.53     0.6921     0.6777     0.8625     0.0362   
   1000     2843.16     0.6936     0.6743     0.928      0.0324   
   1050     2985.47     0.6714     0.6556     0.9012     0.0312   
   1100     3128.49      0.67      0.6536     1.0898     0.0345   
   1150     3271.42     0.6567     0.6428     0.9229     0.0298   
   1200     3414.14     0.6565     0.6327     0.9123     0.0255   
   1250     3557.24     0.6437     0.6209     0.6901     0.0247   
   1300     3700.15     0.6412     0.623      0.8641     0.0221   
   1350     3842.85     0.6429     0.6194     1.0074     0.0225   
   1400     3985.93     0.6473     0.6273     0.9378     0.0198   
   1450     4128.31     0.6322     0.6132     1.174      0.023    
   1500     4271.08     0.6284     0.609      0.8229     0.0184   
   1550     4413.86     0.6265     0.6065     0.7448     0.0156   
   1600     4556.61     0.6338      0.61      1.0134     0.0254   
   1650     4699.14     0.6281     0.605      0.9514     0.0197   
   1700     4841.49     0.6361     0.6155     0.849      0.0149   
   1750     4984.43     0.6163     0.5961     0.8201     0.0157   
   1800     5126.52     0.6272     0.6034     0.7552     0.0144   
   1850     5269.08     0.6149     0.5941     0.8888     0.0128   
   1900      5412.1     0.6153     0.5908     0.6939     0.0204   
   1950     5554.43     0.6248     0.6018     0.7298     0.0161   
   2000     5697.36     0.6127     0.5901     0.8002     0.014    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       135.95     1.4104     1.4119     0.986      0.9516   
   100       279.58     1.4154     1.4066     1.0599     0.7276   
   150       422.64     1.4039     1.4037     1.2308     0.5393   
   200       565.74     1.3379     1.3323     0.9389     0.3849   
   250       708.76     1.1744     1.1718     0.6511     0.2671   
   300       851.7      0.8106     0.807      1.0049     0.1927   
   350       993.73     0.7172     0.7035     1.4348     0.2456   
   400      1135.89     0.7276     0.7161     1.3937     0.2097   
   450      1278.04     0.7735     0.763      1.2383     0.119    
   500      1421.01     0.7262     0.7143     1.2589     0.0972   
   550      1563.28     0.7248     0.7141     1.0262     0.0763   
   600      1706.14     0.731      0.721      0.984      0.0679   
   650      1848.24     0.7388     0.7328     1.2938     0.0736   
   700      1990.33     0.7253      0.71      0.7589     0.0501   
   750      2132.83     0.7549     0.752      0.9452     0.0446   
   800       2275.6     0.7098     0.6931     0.9709     0.0414   
   850      2417.41     0.6924     0.6791     0.8698     0.038    
   900      2559.41     0.6969     0.6839     1.0008     0.0354   
   950      2702.58     0.6911     0.6771      0.67      0.0375   
   1000     2844.98     0.6974     0.6785     0.6952     0.0272   
   1050     2987.61     0.6951     0.6856     0.9066     0.029    
   1100     3130.05     0.7018     0.6825     0.8339     0.0274   
   1150     3272.26     0.6708     0.6557     0.8487     0.0234   
   1200     3414.61     0.6749     0.6498     0.8319     0.0232   
   1250     3556.85     0.6615     0.6439     0.8658     0.0197   
   1300     3699.29     0.6743     0.6507     0.7205     0.0184   
   1350     3841.94     0.6818     0.6612     0.7733     0.0209   
   1400     3983.73     0.6582     0.638      0.9209     0.0261   
   1450      4126.8     0.6661     0.6409     0.8658     0.0178   
   1500     4269.25     0.6492     0.6308     0.6146     0.019    
   1550      4411.6     0.6519     0.625      0.5621     0.0168   
   1600     4554.36     0.6394     0.6241     0.7638     0.0188   
   1650     4696.93     0.6406     0.6187     0.7265     0.0178   
   1700     4839.58     0.6475     0.6205     0.7525     0.0147   
   1750     4982.53     0.6371     0.6058     0.7536     0.0217   
   1800     5125.94     0.6355     0.6061     0.7994     0.0268   
   1850     5268.32     0.6396     0.6129     0.6458     0.015    
   1900      5410.7     0.6327     0.6086     0.8008     0.0153   
   1950     5553.69     0.6272     0.602      0.838      0.0206   
   2000     5696.12     0.6158     0.593      0.7107     0.0163   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
    50       135.78     1.4129     1.4145     0.918      0.8984   
   100       279.84     1.4117     1.4056     1.1635     0.7011   
   150       423.48     1.3679     1.3641     1.0303     0.5327   
   200       565.78     1.1166     1.1116     0.7478     0.3894   
   250       708.3      0.7375     0.7272     0.9431     0.2758   
   300       851.56     0.7172     0.7044     0.8745     0.2505   
   350       994.91     0.7356     0.7259     0.743      0.1964   
   400      1138.05     0.7443     0.7357     0.6963     0.1158   
   450      1281.52     0.7295     0.7194     0.6748     0.0994   
   500      1423.92     0.7393     0.7266     0.6661     0.0892   
   550      1566.87     0.7253     0.7131     0.6303     0.079    
   600      1709.94     0.7515     0.7388     0.6267     0.0657   
   650      1852.84     0.7217     0.7081     0.6932     0.0604   
   700      1996.75     0.7312     0.7224     0.5111     0.0507   
   750      2139.83     0.7305     0.7166     0.4507     0.0479   
   800      2283.61     0.734      0.7199      0.4       0.0428   
   850      2426.51     0.7172     0.7044     0.3639     0.0362   
   900      2570.46     0.726      0.7113     0.3574     0.034    
   950      2713.23     0.7253     0.7131     0.3786     0.0348   
   1000     2856.68     0.726      0.7113     0.5115     0.0376   
   1050     3000.34     0.7209     0.7095     0.5055     0.0297   
   1100     3144.13     0.7284     0.714      0.3798     0.0252   
   1150     3287.57     0.7172     0.7044     0.3405     0.0283   
   1200     3431.34     0.7172     0.7022     0.4168     0.0268   
   1250     3574.82     0.7271     0.7183     0.3845     0.0223   
   1300     3717.58     0.7203     0.713      0.3931     0.0218   
   1350     3857.42     0.7206     0.7071      0.34      0.0246   
   1400     3998.08     0.7244     0.7126     0.4129     0.0202   
   1450      4138.8     0.7115     0.6977     0.4521     0.0198   
   1500     4279.51     0.7253     0.7143     0.5331     0.0232   
   1550     4419.58     0.7136     0.7059     0.5385     0.0218   
   1600     4560.46     0.7198     0.7017     0.6076     0.0209   
   1650      4701.4     0.703      0.6846     0.5918     0.0213   
   1700     4841.91     0.7068     0.6891     0.638      0.0229   
   1750     4982.92     0.6996     0.684      0.6675     0.0192   
   1800     5125.15     0.6932     0.6843     0.8253     0.0249   
   1850     5267.09     0.691      0.6741     0.6075     0.0222   
   1900     5409.36     0.685      0.6729     0.6716     0.0175   
   1950     5550.65     0.6767     0.6631     0.7705     0.0217   
   2000     5691.22     0.7022     0.6848     0.8478     0.0247   
