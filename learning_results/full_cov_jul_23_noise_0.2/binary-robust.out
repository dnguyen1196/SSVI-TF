Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.706270456314087
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       382.42     0.8034     0.8021     3.0859     0.0297     0.0043  
   200       901.11     0.7651     0.7519     2.119      0.0393     0.005   
   300      1412.21     0.7593     0.7385     1.9237     0.0313     0.0034  
   400      1924.17     0.7477     0.7239     1.6477     0.0216     0.0015  
   500      2433.94     0.742      0.7179     1.9702     0.0185     0.0021  
   600      2942.44     0.7249     0.7123     1.5766     0.0171     0.0012  
   700       3453.2     0.7322     0.7141     1.4055     0.0139      0.0    
   800       3961.4     0.7292     0.7141     1.4903     0.0116     0.0004  
   900      4473.49     0.7307     0.7144     1.3056     0.0108     0.0005  
   1000     4982.59     0.7447     0.7182     1.2311     0.0103     0.0016  
   1100     5494.79     0.7356     0.7154     1.4716     0.0092     0.0003  
   1200     6010.62     0.7308     0.7171     1.1438     0.0091     0.0006  
   1300     6529.82     0.7354     0.7197     1.0269     0.0076     0.001   
   1400     7043.23     0.7248     0.7129     1.2393     0.0075     0.0016  
   1500     7553.73     0.7278     0.7102     1.1577     0.0068     0.0027  
   1600     8062.34     0.7322     0.7136     1.1724     0.0068     0.0032  
   1700     8576.84     0.7314     0.7146     0.9331     0.006      0.0001  
   1800     9086.19     0.7304     0.719      1.1462     0.0054     0.0013  
   1900      9594.3     0.7388     0.715      0.9024     0.0059     0.001   
   2000     10101.07    0.7254     0.7197     1.0627     0.0048     0.0037  
   2100     10608.24    0.7295     0.7161     0.8897     0.0054     0.0007  
   2200     11113.94    0.7305     0.7141     1.066      0.0047     0.0011  
   2300     11620.55     0.73      0.7181     1.0618     0.0044     0.0005  
   2400     12126.28    0.7311     0.7137     0.9765     0.0045     0.0008  
   2500     12631.3     0.7329     0.713      0.7862     0.0046     0.0014  
   2600     13138.03    0.7347     0.7264     0.9553     0.0045     0.0005  
   2700     13643.62    0.7366     0.7219     0.8382     0.0043     0.0003  
   2800     14149.54    0.7311     0.7139     0.787      0.0043     0.0006  
   2900     14654.8     0.7276     0.7082     0.7747     0.0036     0.0008  
   3000     15155.52    0.7342     0.7211     0.9201     0.0036     0.0021  
   3100     15635.54    0.7343     0.7185     0.8528     0.0037     0.0006  
   3200     16103.93    0.7267     0.7106     0.6466     0.0035     0.0008  
   3300     16564.25    0.7347     0.7167     0.8502     0.0033     0.0018  
   3400     17021.17    0.7317     0.7129     0.8231     0.0033     0.0009  
   3500     17478.33    0.7293     0.7122     0.8289     0.0035     0.0005  
   3600     17935.4     0.7294     0.7167     0.7731     0.0034     0.0012  
   3700     18393.29    0.7324     0.714      0.7655     0.0033     0.0005  
   3800     18845.38    0.7277     0.7126     0.6801     0.0029     0.0015  
   3900     19299.31    0.7311     0.7146     0.777      0.003      0.0011  
   4000     19751.86    0.7356     0.7148     0.7152     0.0031     0.0001  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       342.11     0.8928     0.8916     3.1547     0.2188     0.0006  
   200       846.87     0.7503     0.7416     1.4904     0.3354     0.0002  
   300       1348.2     0.7364     0.7304     1.2459     0.3963     0.0002  
   400      1848.12     0.7452     0.7299     1.1465     0.4415     0.0002  
   500      2346.39     0.7366     0.7234     1.0106     0.4805     0.0002  
   600      2846.91     0.7408     0.732      0.9217     0.6103     0.0002  
   700      3345.29     0.7396     0.7284     0.8809     0.6835     0.0002  
   800      3843.19     0.7357     0.7286     0.7503     1.061      0.0002  
   900      4340.92     0.7368     0.7295     0.8698     1.2435     0.0002  
   1000     4839.56     0.7406     0.729      0.7144     1.5389     0.0002  
   1100     5337.93     0.7366     0.7266     0.6666     1.7112     0.0002  
   1200     5835.97     0.7394     0.7361     0.8363     2.9214     0.0002  
   1300     6333.49     0.7432     0.731      0.7578     3.4276     0.0002  
   1400     6831.66     0.7446     0.7338     0.6312     4.8038     0.0002  
   1500     7329.18     0.7504     0.7393     0.708      6.8069     0.0002  
   1600     7827.72     0.7495     0.7389     0.7265    10.2129     0.0002  
   1700     8323.93     0.7511     0.7424     0.586     13.6268     0.0002  
   1800     8821.09     0.7513     0.7424     0.6218     18.912     0.0002  
   1900     9319.51     0.765      0.7482     0.6398    28.6592     0.0001  
   2000     9817.52     0.7724     0.7626     0.6601    78.6383     0.0001  
   2100     10313.79    0.7971     0.7814     0.6158    98.7962     0.0001  
   2200     10810.38    0.8429     0.8261     0.6933    204.5063    0.0002  
   2300     11307.42    0.9428     0.9401     0.6474    399.7795    0.0002  
   2400     11804.96    1.1016     1.1007     0.8244   1356.0819    0.0001  
   2500     12302.59    1.2793     1.2756     0.8083   3946.6715    0.0001  
   2600     12799.52    1.3933     1.3974     1.018    40598.7169   0.0001  
   2700     13297.73    1.4014     1.4124     1.2606   579858.4453   0.0002  
   2800     13795.43    1.4097     1.4084     0.9286   12099.0117   0.0001  
   2900     14293.04    1.4048     1.4169     0.5077   2319.3622    0.0001  
   3000     14789.68    1.4123     1.4123     0.4322    802.9168    0.0001  
   3100     15286.93    1.4063     1.4075     0.4805    465.3655    0.0001  
   3200     15785.02    1.4086     1.414      2.8406     96.313      0.0    
   3300     16282.23    1.4117     1.4081     0.5247    60.1167     0.0001  
   3400     16779.6     1.411      1.4162     0.372     33.1201     0.0001  
   3500     17276.2     1.4081     1.4078     0.2889    101.3188    0.0001  
   3600     17773.52    1.4042     1.4122     0.3027    66.9762      0.0    
   3700     18269.36    1.4135     1.4079     0.2829    53.8399      0.0    
   3800     18766.82    1.4077     1.4127     0.4339    53.8902      0.0    
   3900     19263.87    1.4047     1.4002     0.2763    32.0115      0.0    
   4000     19759.74    1.3603     1.3538     0.371     18.5098      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       342.4      1.4092     1.414      3.1579     0.2657     0.0001  
   200       896.74     1.4138     1.4151     1.2835     0.1165     0.0001  
   300      1452.23     1.4071     1.4107     1.2323     0.0876     0.0001  
   400      2007.76     1.4149     1.4161     1.092      0.0699     0.0001  
   500      2563.52     1.4121     1.4133     1.0447     0.0628      0.0    
   600      3119.73     1.4174     1.4148     1.0613     0.0556      0.0    
   700      3674.51     1.416      1.4122     1.0012     0.0475      0.0    
   800      4229.62     1.4026     1.4131     1.0741     0.0453      0.0    
   900      4786.06     1.4184     1.4146     0.9752     0.0405      0.0    
   1000     5341.65     1.4097     1.4151     0.9432     0.0377      0.0    
   1100     5898.46     1.4184     1.4129     0.8244     0.0362      0.0    
   1200     6455.96     1.4152     1.4089     0.9162     0.0343      0.0    
   1300      7011.8     1.4097     1.4118     0.9563     0.0327      0.0    
   1400     7568.19     1.4155     1.4145     0.9435     0.0318      0.0    
   1500      8122.9     1.4129     1.4158     0.8368     0.0298      0.0    
   1600     8692.82     1.4126     1.4074     0.8263     0.0283      0.0    
   1700     9249.52     1.4143     1.4153     0.7555     0.0286      0.0    
   1800     9806.96     1.4128     1.4136     0.7798     0.0275      0.0    
   1900     10363.96    1.4206     1.4171     0.7841     0.0279      0.0    
   2000     10920.33    1.4176     1.413      0.7713     0.0265      0.0    
   2100     11474.86    1.4064     1.4197     0.7766     0.0247      0.0    
   2200     12051.77    1.4094     1.4165     0.7357     0.0245      0.0    
   2300     12621.59    1.4184     1.4121     0.8013     0.0234      0.0    
   2400     13185.36    1.4164     1.4126     0.829      0.0231      0.0    
   2500     13741.51    1.4192     1.4116     0.6731     0.0225      0.0    
   2600     14297.35    1.4131     1.4122     0.7207     0.0216      0.0    
   2700     14925.43    1.4157     1.4164     0.761      0.0214      0.0    
   2800     15552.7     1.4187     1.4119     0.737      0.0213      0.0    
   2900     16180.24    1.4119     1.4154     0.7167     0.0211      0.0    
   3000     16806.47    1.4176     1.4181     0.6571     0.0205      0.0    
   3100     17439.43    1.4196     1.4159     0.6586     0.0202      0.0    
   3200     18075.6     1.424      1.4181     0.6577     0.0196      0.0    
   3300     18676.48    1.4186     1.4184     0.6548     0.019       0.0    
   3400     19233.2     1.4169     1.4131     0.716      0.019       0.0    
   3500     19789.75    1.4217     1.4129     0.6918     0.0188      0.0    
   3600     20345.06    1.4157     1.4211     0.6472     0.0195      0.0    
   3700     20956.53    1.4136     1.4108     0.684      0.0182      0.0    
   3800     21581.68    1.4106     1.4172     0.6279     0.0185      0.0    
   3900     22206.71    1.4177     1.4252     0.6018     0.0194      0.0    
   4000     22831.37    1.4169     1.4196     0.6223     0.0177      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       378.1      1.1936     1.1859     3.1576     0.4156     0.0001  
   200      1057.97     0.7942     0.7891     1.355      2.2946      0.0    
   300       1735.2     0.8908     0.8731     1.1423    91.9385      0.0    
   400      2411.61     1.3809     1.3732     2.7519   1284.0104     0.0    
   500      3087.64     1.4244     1.4116     1.4585    1042.97      0.0    
   600      3764.48     1.4091     1.4181     1.1986   5383.6722     0.0    
   700      4442.61     1.4091     1.4177     3.1046   1870.8709     0.0    
   800      5121.96     1.4104     1.4131     1.0952    715.7189     0.0    
   900      5801.15     1.412      1.4188     1.0013   8237.9306     0.0    
   1000     6480.27     1.4098     1.4159     1.0489    1514.595     0.0    
   1100     7159.68     1.4168     1.4157     1.0548    374.2201     0.0    
   1200     7839.25     1.4185     1.4195     1.0347    51.8858      0.0    
   1300     8519.58     1.4138     1.4101     0.9322     35.042      0.0    
   1400     9199.61     1.4108     1.4144     0.8736     20.959      0.0    
   1500     9878.52     1.4147     1.411      0.9223    11.1739      0.0    
   1600     10557.68    1.4102     1.4113     0.874      4.852       0.0    
   1700     11236.78    1.413      1.4097     0.8782     5.6626      0.0    
   1800     11917.0     1.414      1.4146     0.848      4.5671      0.0    
   1900     12598.49    1.4042     1.4192     0.8755     2.7433      0.0    
   2000     13278.38    1.4108     1.4121     0.8502     1.8217      0.0    
   2100     13957.12    1.415      1.4132     0.8359     2.0678      0.0    
   2200     14637.02    1.4161     1.4142     0.897      0.9331      0.0    
   2300     15316.9     1.4195     1.4135     0.8234     0.974       0.0    
   2400     15999.88    1.4183     1.4183     0.8094     0.7405      0.0    
   2500     16680.41    1.411      1.4174     0.8279     0.6803      0.0    
   2600     17360.71    1.4125     1.4135     0.7582     0.5943      0.0    
   2700     18041.56    1.4042     1.4117     0.8103     0.5007      0.0    
   2800     18723.1     1.4124     1.4173     0.7561      0.46       0.0    
   2900     19404.03    1.4119     1.4152     0.7886     0.5289      0.0    
   3000     20086.17    1.4079     1.4156     0.7815     0.3703      0.0    
   3100     20767.45     1.42      1.4158     0.7656     0.3326      0.0    
   3200     21446.46    1.4088     1.411      0.8935     0.3096      0.0    
   3300     22128.64    1.4218     1.4184     0.7525     0.3486      0.0    
   3400     22812.52    1.4148     1.4113     0.7645     0.261       0.0    
   3500     23494.62    1.4097     1.4139     0.7081     0.2646      0.0    
   3600     24176.97    1.408      1.4144     0.745      0.2194      0.0    
   3700     24863.01    1.4138     1.4147     0.6824     0.2188      0.0    
   3800     25551.23    1.4175     1.4175     0.6887     0.1973      0.0    
   3900     26239.69    1.4136     1.4119     0.7008     0.2062      0.0    
   4000     26928.03    1.4149     1.408      0.7099     0.1951      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       380.38     1.421      1.4142     3.157      0.4949     0.0001  
   200      1122.91     1.4147     1.4127     1.4178     0.3373      0.0    
   300      1864.94     1.4166     1.4135     1.3452     0.2795      0.0    
   400      2608.23     1.4086     1.4184     1.3006     0.2453      0.0    
   500      3351.09     1.4066     1.4117     1.2915     0.2304      0.0    
   600       4093.4     1.4166     1.4145     1.2539     0.2323      0.0    
   700      4839.28     1.4153     1.4139     1.233      0.2306      0.0    
   800      5589.92     1.4147     1.414      1.1733     0.2254      0.0    
   900      6344.11     1.414      1.4181     1.2401     0.2355      0.0    
   1000     7098.75     1.4107     1.415      1.1997     0.2349      0.0    
   1100     7852.38     1.4119     1.4181     1.1341     0.2402      0.0    
   1200     8606.51     1.4149     1.4159     1.2574     0.2386      0.0    
   1300     9358.61     1.4032     1.4132     1.1258     0.2506      0.0    
   1400     10110.73    1.4146     1.4173     1.0828     0.2657      0.0    
   1500     10860.02    1.4041     1.415      1.0806     0.2697      0.0    
   1600     11607.85    1.4126     1.415      1.0813     0.301       0.0    
   1700     12357.41    1.421      1.4169     1.0581     0.3146      0.0    
   1800     13104.14    1.4097     1.4129     1.0419     0.3258      0.0    
   1900     13849.58     1.42      1.4138     1.1475     0.3568      0.0    
   2000     14590.89    1.4093     1.4151     1.1216     0.3725      0.0    
   2100     15330.74    1.414      1.4159     1.1306     0.4107      0.0    
   2200     16065.6     1.4097     1.4156     1.1111     0.463       0.0    
   2300     16790.44    1.4203     1.4181     1.0886     0.4831      0.0    
   2400     17475.24    1.416      1.416      1.0697     0.5327      0.0    
   2500     18144.73    1.423      1.4147     1.0702     0.7634      0.0    
   2600     18799.53    1.4144     1.4134     1.1185     1.0768      0.0    
   2700     19445.45    1.4104     1.4168     1.1174     2.0171      0.0    
   2800     20090.47    1.413      1.4128     1.1055     3.4733      0.0    
   2900     20733.83    1.4211     1.4141     1.1023    34.5734      0.0    
   3000     21379.18    1.4202     1.4165     1.1515   7186.7995     0.0    
   3100     22021.72    1.4088     1.4151     1.1113   1910.4562     0.0    
   3200     22665.59    1.4205     1.4128     1.0274    948.3228     0.0    
   3300     23310.6     1.4159     1.4148     1.0485    66.1821      0.0    
   3400     23955.11    1.4154     1.4137      1.01     106.7975     0.0    
   3500     24601.95    1.4199     1.411      1.066     193.9605     0.0    
   3600     25247.19    1.4085     1.412      1.169    58061.1412    0.0    
   3700     25893.58    1.4208     1.4174     1.0998   17241.5289    0.0    
   3800     26537.58    1.4109     1.4144     1.1151    638.1392     0.0    
   3900     27180.49    1.4224     1.4169     1.1585   8553.0951     0.0    
   4000     27824.96    1.4194     1.417      1.1006   3190.0677     0.0    
