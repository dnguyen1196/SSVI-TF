Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  9.500545024871826
max_count =  24  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       156.12     4.3415     4.3756     0.3126     0.267    129449.07  104882.96  
   200       506.27     4.2907     4.3242     0.2974     0.0234   120621.86   97702.34  
   300       855.5      4.1034     4.1358     0.2814     0.0215    98884.66   80066.09  
   400       1206.1     3.7122     3.7427     0.2593     0.0162    72658.5    58805.78  
   500      1557.89     3.2163     3.2434     0.2237     0.0105    53198.61   43067.03  
   600      1909.92     2.7627     2.7853     0.1957     0.0112    42082.17   34067.77  
   700      2261.68     2.4085     2.4284     0.2163     0.0117    36261.39   29354.14  
   800      2612.72     2.1367     2.1565     0.1621     0.0115    32999.48   26709.2   
   900      2964.89     1.9371     1.9539     0.2159     0.0112    31113.82   25178.56  
   1000     3317.26     1.7837     1.7973     0.0774     0.0104    29901.86   24205.1   
   1100     3668.75     1.6707     1.683      0.1496     0.0096    29177.48   23609.14  
   1200     4020.03     1.5894     1.6002     0.1098     0.0097    28660.06   23197.55  
   1300     4372.05     1.5285     1.5376     0.1775     0.0089    28335.65   22933.5   
   1400     4723.72     1.4834     1.4912     0.1463     0.0082    28103.87   22741.04  
   1500     5076.15     1.4472     1.4539     0.0949     0.0075    27941.38   22601.74  
   1600     5427.77     1.4225     1.4271     0.1783     0.007     27836.63   22509.77  
   1700     5779.58     1.4019     1.4048     0.0734     0.0065    27748.55   22434.65  
   1800     6131.73     1.3879     1.3898     0.125      0.006     27686.76   22386.98  
   1900     6482.03     1.3733     1.375      0.101      0.0058    27616.24   22339.73  
   2000     6831.36     1.3645     1.3681     0.0894     0.0054    27575.69   22307.16  
   2100     7180.78     1.3542     1.3589     0.1091     0.0055    27548.63   22281.0   
   2200     7529.81      1.35      1.3507     0.1221     0.0049    27523.15   22253.32  
   2300     7878.71     1.344      1.3464     0.0801     0.0043    27506.79   22243.76  
   2400     8228.02     1.3396     1.3407     0.155      0.0045    27486.51   22223.74  
   2500     8577.42     1.336      1.3357     0.0716     0.0043    27470.07   22205.74  
   2600     8926.46     1.3336     1.3333     0.1283     0.004     27477.21   22205.63  
   2700     9275.73     1.3289     1.3321     0.1151     0.0038    27443.19   22188.03  
   2800     9625.25     1.3274     1.3298     0.0767     0.004     27435.74   22178.69  
   2900     9974.63     1.3265     1.3279     0.0847     0.0033    27430.61   22172.8   
   3000     10324.61    1.324      1.3239     0.1094     0.0035    27414.37   22164.02  
   3100     10674.12    1.3227     1.3225     0.0956     0.0033    27418.03   22159.88  
   3200     11023.79    1.3216     1.3243     0.0673     0.0037    27410.79   22164.39  
   3300     11373.38    1.3214     1.3201     0.1302     0.0029    27426.5    22165.73  
   3400     11722.84    1.3217     1.3196     0.1183     0.0029    27407.95   22157.06  
   3500     12071.95    1.3198     1.3188     0.1085     0.003     27416.82   22159.97  
   3600     12421.5      1.32      1.3189     0.0675     0.003     27411.67   22155.27  
   3700     12770.81    1.3187     1.3169     0.103      0.0025    27408.34   22153.64  
   3800     13120.63    1.3183     1.3156     0.0498     0.0028    27394.55   22141.69  
   3900     13470.19    1.3187     1.3184     0.0873     0.0022    27407.13   22147.67  
   4000     13819.73    1.3168     1.3167     0.0699     0.0024    27402.76   22149.87  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       154.7      4.3428     4.3599     0.3132     0.236    130159.86  209949.01  
   200       585.79     4.2965     4.3133     0.3007     0.0211   122569.37  197645.77  
   300      1015.65     4.113      4.1285     0.2883     0.0236   101006.36  162823.24  
   400      1445.94     3.7256     3.7388     0.2777     0.0133    73711.89  118822.38  
   500      1875.04     3.2291      3.24      0.2301     0.0085    53358.29   86086.5   
   600       2303.7     2.7799     2.7878     0.1916     0.009     42114.31   68006.12  
   700      2732.33     2.4243     2.4318     0.1772     0.0095    36222.1    58547.54  
   800       3160.8     2.1486     2.1553     0.2244     0.0106    32902.04   53230.29  
   900       3590.0     1.9416     1.9494     0.1537     0.0107    30955.02   50103.75  
   1000     4019.55     1.7891     1.7975     0.1834     0.0104    29776.84   48208.39  
   1100      4449.1     1.6805     1.6873     0.1601      0.01     29029.35   47017.08  
   1200     4878.29     1.5993     1.6053     0.2135     0.0095    28529.5    46219.41  
   1300     5307.93     1.5384     1.5439     0.1044     0.0089    28195.78   45678.45  
   1400     5736.98     1.4905     1.4957     0.1181     0.0085    27957.15   45295.95  
   1500     6166.02     1.4546     1.4585     0.1343     0.0081    27784.49   45007.33  
   1600     6595.51     1.4268     1.4311     0.1542     0.0075    27656.55   44822.27  
   1700     7024.31     1.405      1.408      0.073      0.0072    27558.48   44646.46  
   1800     7452.94     1.3857     1.3895     0.0892     0.0067    27491.64   44522.35  
   1900     7882.22     1.3758     1.3783     0.1465     0.0063    27459.2    44460.84  
   2000     8311.24     1.3639     1.3671     0.1534     0.0059    27413.99   44395.03  
   2100     8740.64     1.3543     1.3567     0.1026     0.0056    27361.01   44314.57  
   2200     9171.39     1.3471     1.3497     0.1214     0.0053    27341.66   44269.24  
   2300     9600.94     1.3436     1.3436     0.0962     0.0051    27318.63   44234.53  
   2400     10030.82    1.3377     1.3377     0.1036     0.0047    27280.92   44182.45  
   2500     10460.24    1.3335     1.333      0.0662     0.0046    27267.51   44158.91  
   2600     10891.28    1.3301     1.3304     0.1424     0.0043    27258.52   44131.64  
   2700     11321.55    1.3269     1.3262     0.1297     0.0041    27248.18   44114.18  
   2800     11752.15    1.324      1.324      0.1114     0.004     27233.07   44094.73  
   2900     12182.26    1.3217     1.3217     0.1137     0.0037    27231.92   44078.62  
   3000     12612.28    1.319      1.3189     0.0941     0.0035    27210.29   44054.68  
   3100     13042.47    1.3172     1.3189     0.0952     0.0034    27215.56   44055.52  
   3200     13472.03    1.3163     1.3165     0.0819     0.0033    27211.37   44056.96  
   3300     13902.03    1.3143     1.3154     0.1257     0.0031    27198.66   44034.44  
   3400     14332.47    1.3138     1.3141     0.1202     0.003     27194.94   44024.64  
   3500     14762.53    1.3139     1.3126     0.066      0.0028    27204.87   44034.11  
   3600     15192.57    1.3118     1.3123     0.1656     0.0027    27224.23   44058.33  
   3700     15626.54    1.312      1.3119     0.0714     0.0025    27187.79   44031.27  
   3800     16061.11    1.3101      1.31      0.0811     0.0025    27168.61   44001.19  
   3900     16492.26    1.3091     1.3094     0.0812     0.0023    27178.82   44000.36  
   4000     16923.29    1.3089     1.3096     0.0984     0.0023    27174.31   43992.84  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       155.16     4.3431     4.3529     0.3127     0.2232   130330.72  314196.87  
   200       672.4      4.2981     4.3078     0.2991     0.0209   123160.12  296871.05  
   300      1188.84     4.1156     4.1249     0.2952     0.0188   101733.85  245208.24  
   400      1703.88     3.7235     3.732      0.2655     0.0117    73730.73  177784.79  
   500      2220.15     3.2213     3.2298     0.2892     0.0077    53063.49  128113.65  
   600      2736.97     2.7744     2.782      0.2055     0.0084    41943.95  101360.66  
   700      3251.61     2.4141     2.4219     0.2265     0.0095    36022.96   87128.02  
   800      3766.27     2.1409     2.1481     0.1937     0.0102    32763.36   79262.86  
   900      4280.55     1.9345     1.9425     0.2247     0.0102    30842.85   74648.34  
   1000     4795.29     1.7882     1.7943     0.1829      0.01     29689.3    71882.01  
   1100     5309.71     1.6736      1.68      0.1437     0.0097    28932.94   70041.98  
   1200      5823.4     1.5898     1.5951     0.1682     0.0093    28443.33   68838.88  
   1300     6339.84     1.5298     1.5353     0.153      0.0088    28109.78   68056.29  
   1400     6854.07     1.484      1.489      0.1343     0.0085    27869.19   67469.69  
   1500     7374.87     1.4495     1.4532     0.1127     0.0081    27716.48   67072.84  
   1600     7888.78     1.4237     1.4264     0.149      0.0076    27622.51   66826.69  
   1700     8404.39     1.4026     1.4058     0.1522     0.0073    27501.89   66576.86  
   1800      8918.5     1.3871     1.3896     0.1459     0.007     27446.35   66430.99  
   1900     9431.79     1.3764     1.3766     0.1316     0.0066    27388.53   66279.44  
   2000     9946.86     1.3643     1.3656     0.1492     0.0063    27339.23   66159.65  
   2100     10462.05    1.3552     1.3561     0.0677     0.0061    27300.26   66064.13  
   2200     10976.32    1.3474     1.3474     0.1206     0.0058    27273.87   65981.79  
   2300     11491.84    1.3418     1.3413     0.1542     0.0054    27252.43   65930.4   
   2400     12006.98    1.3355     1.3365     0.1096     0.0052    27224.41   65872.78  
   2500     12522.34    1.3323     1.3312      0.12      0.005     27210.12   65843.56  
   2600     13038.32    1.3272     1.3278     0.1269     0.0048    27193.63   65787.75  
   2700     13555.35    1.3256     1.3237     0.1284     0.0046    27175.26   65737.2   
   2800     14071.13    1.3216     1.3213     0.1621     0.0045    27161.63   65729.36  
   2900     14587.44    1.3186     1.3195     0.0997     0.0042    27158.18   65724.66  
   3000     15103.67    1.3175     1.3167     0.1021     0.0041    27147.71   65687.57  
   3100     15621.84    1.3174     1.3161     0.1659     0.004     27162.39   65680.58  
   3200     16139.02    1.314      1.3136     0.0839     0.0037    27141.28   65651.18  
   3300     16656.67    1.3122     1.3112     0.0923     0.0036    27126.84   65614.71  
   3400     17174.81    1.3113     1.3097     0.106      0.0034    27121.84   65613.94  
   3500     17694.09    1.3115     1.3089     0.0847     0.0033    27114.51   65582.42  
   3600     18213.37    1.3088     1.3075     0.0823     0.0032    27110.49   65568.86  
   3700     18731.24    1.3093     1.3072     0.0885     0.0031    27108.38   65566.2   
   3800     19250.24    1.3077     1.3061     0.0724     0.003     27110.73   65566.9   
   3900     19770.81    1.3069     1.3063     0.105      0.0029    27096.11   65548.75  
   4000     20289.47    1.3064     1.3061     0.0941     0.0028    27094.69   65539.99  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       154.72     4.3432     4.3515     0.3131     0.2437   130407.72   418971.6  
   200       755.01     4.2988     4.3068     0.3007     0.0178   123429.19  396473.44  
   300      1359.33     4.1169     4.1242     0.2917     0.0161   102081.59  327820.85  
   400      1965.62     3.7284     3.7348     0.2662     0.0113    74015.0   237788.32  
   500       2567.9     3.2271     3.2335     0.2167     0.0067    53169.32  171024.63  
   600       3168.6     2.7781     2.784      0.2166     0.0075    41970.09  135171.46  
   700      3770.72     2.4238     2.4297     0.2047     0.009     36107.89  116426.94  
   800      4372.87     2.1531     2.1591     0.1421     0.0098    32847.76  105980.16  
   900      4974.84     1.9503     1.9563     0.2001      0.01     30939.9    99838.84  
   1000     5577.42     1.7982     1.8033     0.1483     0.0099    29749.3    96012.23  
   1100     6181.35     1.6867     1.6916     0.1257     0.0095    28995.36   93600.56  
   1200     6784.07     1.6048     1.6095     0.1821     0.0092    28506.14   92033.22  
   1300     7390.56     1.5423     1.548      0.1834     0.0088    28153.6    90933.1   
   1400     7996.85     1.4947     1.5006     0.1032     0.0083    27905.19   90132.27  
   1500     8603.56     1.4585     1.4649     0.1498     0.008     27726.13   89575.25  
   1600     9212.27     1.4307     1.4362     0.1478     0.0077    27604.92   89161.6   
   1700      9819.3     1.4102     1.4165     0.0894     0.0073    27512.77   88868.68  
   1800     10428.9     1.3927     1.3989     0.1173     0.007     27426.99   88646.68  
   1900     11037.65    1.3788     1.3853     0.0783     0.0066    27361.43   88428.86  
   2000     11648.41    1.3682     1.3746     0.1133     0.0063    27326.69   88294.27  
   2100     12258.38    1.3615     1.3664     0.1432     0.0061    27319.96   88230.4   
   2200     12868.48    1.3521     1.3579     0.1882     0.0057    27264.22   88064.29  
   2300     13476.49    1.3445     1.3505     0.1444     0.0056    27222.62   87963.97  
   2400     14083.55    1.3398     1.3451     0.0816     0.0053    27199.01   87884.91  
   2500     14688.85    1.3353     1.3409     0.0853     0.0051    27185.21   87846.32  
   2600     15296.48    1.3302     1.3363     0.1294     0.005     27164.84   87780.16  
   2700     15904.14    1.3267     1.3325     0.0749     0.0047    27151.2    87729.51  
   2800     16510.35    1.3236     1.3295     0.0884     0.0046    27144.11   87683.67  
   2900     17116.43    1.3206     1.3265     0.1251     0.0044    27133.65   87654.32  
   3000     17724.08    1.318      1.3235     0.0607     0.0042    27111.41   87596.18  
   3100     18331.98    1.3177     1.3226     0.1234     0.0041    27154.91   87671.28  
   3200     18938.5     1.3132     1.3191     0.1424     0.0039    27092.95   87554.1   
   3300     19545.91    1.3117     1.3181     0.0982     0.0039    27096.79   87535.68  
   3400     20152.12    1.3115     1.3163     0.1502     0.0038    27103.33   87559.77  
   3500     20757.19    1.3112     1.3149     0.1069     0.0036    27079.89   87474.37  
   3600     21362.54    1.3091     1.3136     0.1556     0.0035    27084.67   87479.05  
   3700     21969.27    1.3074     1.3132     0.1105     0.0034    27074.34   87455.22  
   3800     22575.3     1.3072     1.3116     0.1212     0.0033    27059.53   87419.19  
   3900     23183.53    1.3061     1.3104     0.1078     0.0031    27055.46   87406.15  
   4000     23790.75    1.3058     1.3109     0.0985     0.0031    27061.72   87420.19  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       155.01     4.3436     4.3609     0.3136     0.2064   130488.98  525563.08  
   200       840.89     4.3015     4.3186     0.3004     0.0172   123929.11  498998.36  
   300       1527.8     4.1231     4.1392     0.2952     0.0175   102847.14  414024.21  
   400      2215.87     3.7398     3.7544     0.2686     0.0104    74703.52  300775.33  
   500      2906.82     3.2431     3.2572     0.213      0.007     53647.33  216283.44  
   600      3597.63     2.781      2.7944     0.2257     0.008     41992.31  169502.15  
   700      4283.72     2.4214     2.4344     0.2233     0.0091    36055.93  145648.97  
   800      4974.59     2.1527     2.1648     0.1529     0.0097    32834.82  132670.59  
   900      5663.58     1.9493     1.9617     0.1985     0.0098    30909.14  124928.22  
   1000     6349.58     1.7978     1.8105      0.21      0.0097    29724.16  120212.03  
   1100     7037.57     1.6855     1.6981     0.1568     0.0094    28963.79   117113.1  
   1200     7727.77     1.6019     1.6145     0.0928     0.009     28451.79  115109.97  
   1300      8414.5     1.5395     1.5525     0.1865     0.0087    28112.46  113739.17  
   1400     9103.73     1.4927     1.5063     0.1617     0.0082    27878.76  112820.08  
   1500     9794.65     1.4563     1.4697     0.2496     0.0079    27710.81  112093.74  
   1600     10486.1      1.43      1.4422     0.2164     0.0076    27603.02  111639.68  
   1700     11176.77    1.407      1.4197     0.1179     0.0072    27482.84  111156.18  
   1800     11867.72    1.3898     1.4017     0.1526     0.0069    27393.69   110845.8  
   1900     12557.06    1.3777     1.3889     0.1266     0.0066    27358.49  110635.76  
   2000     13248.28    1.3641     1.3757     0.1362     0.0063    27284.91  110391.49  
   2100     13939.51    1.3553     1.3667     0.0779     0.006     27253.82  110244.34  
   2200     14631.35    1.3467     1.3581     0.0962     0.0058    27218.81   110092.8  
   2300     15322.94    1.3401     1.3518     0.0745     0.0055    27200.82  110000.79  
   2400     16016.06    1.3348     1.3461     0.1213     0.0053    27175.27   109908.8  
   2500     16708.57    1.3313     1.3417     0.1894     0.0052    27169.83  109881.09  
   2600     17401.17    1.3261     1.3364     0.1056     0.005     27137.79  109756.36  
   2700     18094.69    1.3235     1.3331     0.0953     0.0049    27127.91   109697.0  
   2800     18787.35    1.3191     1.3291     0.1406     0.0046    27107.33  109622.62  
   2900     19481.69    1.3159     1.3267     0.0726     0.0045    27093.9   109598.94  
   3000     20177.64    1.3144     1.3244      0.13      0.0043    27077.8   109523.02  
   3100     20870.38    1.312      1.3219     0.1012     0.0042    27071.72  109477.09  
   3200     21563.35    1.3114     1.3208     0.0957     0.004     27081.12  109478.37  
   3300     22253.26    1.3095     1.3189     0.1539     0.004     27051.16  109402.65  
   3400     22946.2     1.3077     1.3172     0.0754     0.0038    27048.66  109393.22  
   3500     23637.42    1.3062     1.3152     0.0704     0.0037    27039.67   109351.4  
   3600     24327.92    1.3037     1.3146     0.1144     0.0036    27032.59  109334.68  
   3700     25018.61    1.3037     1.3134     0.0738     0.0035    27030.96  109317.06  
   3800     25709.33    1.3034     1.3139     0.094      0.0034    27032.93  109320.04  
   3900     26400.03    1.3027     1.3129     0.1015     0.0033    27008.54  109286.18  
   4000     27087.77    1.3028     1.3126     0.0674     0.0032    27016.06  109278.35  
