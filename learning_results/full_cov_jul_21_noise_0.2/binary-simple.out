Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  7.218216419219971
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       140.31     0.7594     0.7358     3.1331     0.2584     0.0001  
   200       423.35     0.6979     0.6447     2.6235     0.1027      0.0    
   300       704.74     0.6733     0.5946     2.0454     0.0945      0.0    
   400       986.49     0.6575     0.5627     2.1885     0.066       0.0    
   500      1268.99     0.6415     0.5431     2.1451     0.0624      0.0    
   600      1553.83     0.6172     0.495      2.085      0.0507      0.0    
   700      1839.54     0.6137     0.4944     1.8084     0.0446      0.0    
   800      2125.46     0.6043     0.4695     1.8742     0.0411      0.0    
   900      2410.97     0.5979     0.455      1.9921     0.0394      0.0    
   1000     2696.11     0.5909     0.4585     1.9696     0.0373      0.0    
   1100     2981.57     0.5946     0.4632     2.1572     0.0379      0.0    
   1200     3266.24     0.5898     0.4559     2.334      0.0377      0.0    
   1300     3549.58     0.6061     0.4815     2.4128     0.0379      0.0    
   1400      3832.0     0.5991     0.4825     2.5789     0.037       0.0    
   1500     4113.18     0.6035     0.4907     2.4554     0.0361      0.0    
   1600     4393.83     0.5976     0.4792     2.9242     0.0407      0.0    
   1700     4673.53     0.6087     0.5167     2.751      0.0463      0.0    
   1800      4949.8     0.5834     0.4823     2.529      0.0393      0.0    
   1900     5224.68     0.5816     0.4675     2.4427     0.0409      0.0    
   2000     5498.07     0.5751     0.4497     2.636      0.0444      0.0    
   2100     5769.89     0.5841     0.4718     2.5461     0.0531      0.0    
   2200     6041.31     0.5694     0.4497     2.6346     0.0532      0.0    
   2300     6312.08     0.5719     0.4441     2.3178     0.0648      0.0    
   2400     6582.27     0.5637     0.4508     2.3666     0.0504      0.0    
   2500     6851.71     0.572      0.4637     2.7215     0.0502      0.0    
   2600     7118.98     0.5535     0.4186     1.9607     0.0648      0.0    
   2700     7384.61     0.5672     0.4402     2.2658     0.0615      0.0    
   2800     7649.78     0.5613     0.435      1.792      0.0481      0.0    
   2900      7914.6     0.5647     0.4352     2.4703     0.0644      0.0    
   3000     8178.27     0.5596     0.4327     2.2407     0.0609      0.0    
   3100      8441.4     0.5547     0.4159     2.2871     0.0531      0.0    
   3200     8704.29     0.5528     0.4231     1.6705     0.0581      0.0    
   3300     8966.36     0.5476     0.4109     2.0895     0.0494      0.0    
   3400     9228.29     0.5551     0.4217     2.1409     0.0561      0.0    
   3500     9489.55     0.5617     0.4233     2.1956     0.0575      0.0    
   3600     9750.65     0.5531     0.4109     1.6063     0.0621      0.0    
   3700     10011.77    0.5535     0.4231     2.0746     0.0871      0.0    
   3800     10272.66    0.5531     0.4195     2.0331     0.0747      0.0    
   3900     10532.77    0.5527     0.4266     1.663      0.0556      0.0    
   4000     10793.05    0.5505     0.4152     2.118      0.0524      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       139.36     0.8054     0.7843     3.1198     0.1485     0.0001  
   200       482.31     0.7192     0.6967     2.4285     0.0762     0.0001  
   300       828.24     0.6632     0.6221     1.8542     0.0576      0.0    
   400      1174.13     0.6457     0.594      1.834      0.052       0.0    
   500      1519.98     0.6177     0.5618     2.1404     0.0443      0.0    
   600      1862.77     0.6188     0.5628     2.089      0.0481      0.0    
   700      2204.67     0.6052     0.545      1.7838     0.039       0.0    
   800      2545.65     0.5841     0.5201     1.7824     0.0323      0.0    
   900       2885.4     0.5884     0.5248     1.9775     0.0376      0.0    
   1000     3224.97     0.5886     0.5073     1.8735     0.0291      0.0    
   1100     3562.73     0.5936     0.505      1.9271     0.0353      0.0    
   1200     3900.83     0.5702     0.4814     1.3393     0.0337      0.0    
   1300     4237.94     0.5607     0.4695     1.4904     0.0274      0.0    
   1400     4574.47     0.5698     0.482      1.4594     0.0238      0.0    
   1500     4909.41     0.5503     0.4626     1.5427     0.0254      0.0    
   1600     5244.21     0.5509     0.466      1.4526     0.0267      0.0    
   1700      5577.9     0.5531     0.4613     1.4452     0.0269      0.0    
   1800     5910.81     0.5524     0.4461     1.3805     0.0226      0.0    
   1900     6243.77     0.5384     0.4415     1.5562     0.0244      0.0    
   2000     6577.07     0.5338     0.4412     1.4819     0.021       0.0    
   2100      6908.4     0.5288     0.4323     1.2814     0.0225      0.0    
   2200     7239.75     0.5335     0.4334     1.3638     0.0218      0.0    
   2300     7571.83     0.5263     0.4301     1.3385     0.021       0.0    
   2400     7902.62     0.5344     0.4375     1.7639     0.0207      0.0    
   2500     8233.61     0.5263     0.4306     1.3103     0.0229      0.0    
   2600      8564.2     0.5335     0.4398     1.6145     0.021       0.0    
   2700     8893.62     0.5332     0.4305     1.3379     0.0196      0.0    
   2800      9223.7     0.5254     0.4121     1.2149     0.0193      0.0    
   2900     9553.26     0.5288     0.4179     1.6617     0.0198      0.0    
   3000     9883.28     0.5464     0.442      1.5707     0.0205      0.0    
   3100     10212.71    0.5266     0.4317     1.7707     0.0208      0.0    
   3200     10542.25    0.5298     0.4254     1.5455     0.0208      0.0    
   3300     10870.73    0.5326     0.4257     1.4493     0.021       0.0    
   3400     11199.31    0.526      0.4266     1.8494     0.0205      0.0    
   3500     11527.72    0.5329     0.4266     1.5705     0.0197      0.0    
   3600     11856.64    0.5457     0.4463     1.8371     0.0218      0.0    
   3700     12185.7     0.5606     0.4667     2.0036     0.0206      0.0    
   3800     12514.52    0.5637     0.4821     2.2692     0.0254      0.0    
   3900     12843.33    0.5401      0.45      2.165      0.0258      0.0    
   4000     13172.11    0.5586     0.4815     2.5122     0.0295      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       139.4      0.8045     0.7933     3.1421     0.1296     0.0003  
   200       540.37     0.7044     0.6769     2.4306     0.0635     0.0001  
   300       944.27     0.6661     0.6316     1.8462     0.0563     0.0001  
   400      1347.31     0.6496     0.6061     1.9406     0.0507      0.0    
   500      1747.19     0.6237     0.5856     2.0745     0.0413      0.0    
   600      2143.08     0.6134     0.5707     1.7014     0.036       0.0    
   700      2537.73     0.5862      0.54      1.557      0.037       0.0    
   800      2930.74     0.5834     0.5373     1.9187     0.0433      0.0    
   900      3321.09     0.5696     0.5248     1.707      0.0354      0.0    
   1000      3711.0     0.5586     0.5171     1.6954     0.0316      0.0    
   1100     4100.57     0.5651     0.5173     1.6644     0.0257      0.0    
   1200     4490.41     0.5571     0.5029     1.3723     0.032       0.0    
   1300      4879.2     0.5452     0.4835     1.5059     0.0219      0.0    
   1400     5268.48     0.5399     0.4827     1.2975     0.022       0.0    
   1500     5656.32     0.5349     0.4741     1.3157     0.0213      0.0    
   1600     6046.58     0.5384     0.4824     1.6903     0.0193      0.0    
   1700     6436.82     0.5234     0.4665     1.217      0.0208      0.0    
   1800     6826.44     0.5264     0.4642     1.2686     0.0192      0.0    
   1900     7215.19     0.5279     0.4693     1.3152     0.0208      0.0    
   2000     7602.96     0.5311     0.4683     1.433      0.0211      0.0    
   2100     7991.87     0.5195     0.4539     1.1531     0.0172      0.0    
   2200     8379.27     0.5161     0.4545     1.3014     0.0218      0.0    
   2300     8764.99     0.5217     0.4465     1.4062     0.0184      0.0    
   2400     9150.32     0.5102     0.4352     1.1656     0.0164      0.0    
   2500     9534.63     0.5191     0.4557     1.1613     0.0224      0.0    
   2600     9921.82     0.5115     0.439      1.0932     0.0157      0.0    
   2700     10308.52    0.5088     0.4422     1.0773     0.0157      0.0    
   2800     10695.72    0.505      0.4298     1.2019     0.017       0.0    
   2900     11083.05    0.5045     0.4347     1.0448     0.0183      0.0    
   3000     11469.49    0.4996     0.4293     1.3614     0.0143      0.0    
   3100     11855.22    0.4998     0.4227     0.9687     0.0153      0.0    
   3200     12242.47    0.5042     0.4314     0.9418     0.014       0.0    
   3300     12629.78    0.4953     0.4231     1.0452     0.0147      0.0    
   3400     13015.23    0.4959     0.4304     1.1807     0.0144      0.0    
   3500     13398.8     0.4961     0.4183     1.0655     0.0144      0.0    
   3600     13782.74    0.4943     0.4286     1.366      0.0147      0.0    
   3700     14169.57    0.4812     0.4126     0.974      0.0128      0.0    
   3800     14554.58    0.497      0.4239     0.9281     0.0141      0.0    
   3900     14939.3     0.4938     0.4185     1.3387     0.0138      0.0    
   4000     15324.69    0.4982     0.4109     1.0788     0.0138      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       138.88     0.7756     0.7548     3.1276     0.0986     0.0003  
   200       600.23     0.7286     0.7031     2.1864     0.0507      0.0    
   300       1061.1     0.6846     0.6614     2.0429     0.047      0.0001  
   400      1518.52     0.6421     0.6109     2.1104     0.0441      0.0    
   500       1972.9     0.6115     0.5822     1.9765     0.0379      0.0    
   600      2426.77     0.5896      0.56      1.7814     0.0364      0.0    
   700      2879.34     0.5892     0.5557     2.3873     0.0291      0.0    
   800      3332.87     0.5856     0.5512     1.6817     0.0265      0.0    
   900      3785.02     0.5715     0.5257     1.6423     0.0247      0.0    
   1000     4236.92     0.5648     0.5228     1.7002     0.0256      0.0    
   1100     4688.24     0.5495     0.5053     1.4319     0.0226      0.0    
   1200     5138.45     0.5487     0.5038     1.3289     0.0254      0.0    
   1300      5586.6     0.5401     0.4911     1.7609     0.0248      0.0    
   1400     6033.83     0.5334     0.4898     1.3665     0.0216      0.0    
   1500     6481.24     0.5367     0.4931     1.6343     0.0178      0.0    
   1600     6927.58     0.5249     0.479      1.657      0.023       0.0    
   1700      7373.0     0.5188     0.4738     1.3101     0.0215      0.0    
   1800     7817.06     0.5296     0.4777     1.4306     0.0228      0.0    
   1900     8260.76     0.5212     0.4723     1.2069     0.0216      0.0    
   2000     8703.62     0.5164     0.4689     1.2349     0.0209      0.0    
   2100     9145.92     0.524      0.4673     1.154      0.0153      0.0    
   2200      9588.2     0.5254     0.4663     1.175      0.0166      0.0    
   2300     10030.46    0.5127     0.4621     1.1455     0.0212      0.0    
   2400     10473.55    0.5122     0.4524     1.3912     0.0161      0.0    
   2500     10914.79    0.5039     0.4487     1.0787     0.0185      0.0    
   2600     11356.09    0.4972     0.4484     1.1352     0.0126      0.0    
   2700     11797.27    0.4959     0.4375     0.9874     0.0137      0.0    
   2800     12238.4     0.5004     0.4392     1.0733     0.0144      0.0    
   2900     12679.39    0.5074     0.4444     1.0467     0.0117      0.0    
   3000     13119.12    0.4943     0.4355     1.056      0.0125      0.0    
   3100     13558.85    0.5006     0.442      1.065      0.0115      0.0    
   3200     13998.84    0.4886     0.4361     1.1454     0.0125      0.0    
   3300     14438.13    0.494      0.4339     1.117      0.0154      0.0    
   3400     14876.73    0.4888     0.4361     1.2236     0.0115      0.0    
   3500     15315.52    0.4972     0.4313     0.9413     0.0113      0.0    
   3600     15755.0     0.485      0.4242     1.0539     0.0103      0.0    
   3700     16194.42    0.4878     0.4294     0.9728     0.0114      0.0    
   3800     16634.12    0.4785     0.4223     0.8425      0.01       0.0    
   3900     17074.5     0.4978     0.4202     1.0051     0.0102      0.0    
   4000     17515.28    0.486      0.4178     0.9545     0.0102      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       143.97     0.9313     0.9148     3.1332     0.0681      0.0    
   200       659.35     0.7323     0.7111     2.2734     0.0431     0.0001  
   300      1181.06     0.6951     0.6752     1.8709     0.0352     0.0001  
   400      1699.65     0.6662     0.6426     1.7943     0.0311     0.0001  
   500      2214.02     0.6286     0.5982     2.0271     0.0274      0.0    
   600      2721.79     0.6118     0.5872     1.6956     0.0247      0.0    
   700      3225.15     0.5937     0.5663     1.9802     0.0296      0.0    
   800      3725.48     0.5864     0.5581     1.6034     0.023       0.0    
   900      4225.64     0.5737     0.5396     1.7139     0.0239      0.0    
   1000     4721.66     0.5719     0.5412     1.5379     0.0208      0.0    
   1100     5217.58     0.5547     0.5252     1.5421     0.0277      0.0    
   1200     5712.41     0.5471     0.5185     1.6032     0.0235      0.0    
   1300      6206.9     0.5349     0.5122     1.5552     0.0221      0.0    
   1400     6702.47     0.5295     0.5031     1.3081     0.0266      0.0    
   1500     7198.66     0.5346     0.5064     1.636      0.0213      0.0    
   1600     7693.99     0.5295     0.4896     1.431      0.0181      0.0    
   1700     8189.09     0.5269     0.4855     1.2594     0.019       0.0    
   1800     8683.72     0.518      0.4879     1.4377     0.024       0.0    
   1900      9179.6     0.5079     0.4812     1.6167     0.017       0.0    
   2000     9675.98     0.5063     0.4765     1.3259     0.0165      0.0    
   2100     10171.72    0.5104     0.4742     1.2394     0.0181      0.0    
   2200     10667.53    0.5072     0.4686     1.4012     0.0156      0.0    
   2300     11163.68    0.5049     0.4679     1.231      0.0227      0.0    
   2400     11661.65    0.5133      0.47      1.4895     0.0155      0.0    
   2500     12156.76    0.5141     0.4632     1.1169     0.0134      0.0    
   2600     12653.09    0.5014     0.4579     1.3492     0.0131      0.0    
   2700     13149.9     0.5052     0.461      1.0547     0.0102      0.0    
   2800     13646.76    0.491      0.4528     1.0195     0.0136      0.0    
   2900     14142.67    0.4881     0.4476     0.9783     0.0123      0.0    
   3000     14639.93    0.4795     0.4437     0.9769     0.014       0.0    
   3100     15137.12    0.4923     0.4429     1.406      0.0115      0.0    
   3200     15631.66    0.484      0.4426     1.056      0.0122      0.0    
   3300     16127.76    0.4797     0.4382     0.9437     0.0095      0.0    
   3400     16624.17    0.4787     0.4417     0.8509     0.0096      0.0    
   3500     17118.7     0.4841     0.439      1.0576     0.0118      0.0    
   3600     17612.85    0.4838     0.4381     1.2151     0.0091      0.0    
   3700     18108.08    0.478      0.435      0.9947     0.0099      0.0    
   3800     18607.51    0.4762     0.4313     0.942      0.0097      0.0    
   3900     19106.26    0.476      0.4278     0.8506     0.0105      0.0    
   4000     19604.82    0.4699     0.4229     0.9637     0.0082      0.0    
