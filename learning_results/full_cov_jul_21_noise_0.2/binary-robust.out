Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  7.3962321281433105
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       254.52     0.9413     0.9292     3.0625     0.4162     0.003   
   200       653.44     0.7753     0.7658     1.968      3.5913     0.0014  
   300      1048.43     0.7549     0.7313     1.7515    70.7141     0.0046  
   400      1440.15     0.7557     0.7409     1.8291    359.0335    0.0232  
   500      1832.79     0.7458     0.7214     1.6539    874.1454    0.0013  
   600      2223.98     0.7443     0.7192     1.632     39.9872     0.005   
   700      2613.64     0.7402     0.7283     1.5396     9.3451     0.0034  
   800      2999.62     0.7347     0.7206     1.8545     17.214     0.0046  
   900      3384.68     0.7417     0.7284     1.6227    50.3341     0.0002  
   1000     3770.81     0.7444     0.7279     1.8475    604.7859    0.0161  
   1100     4157.89     0.7452     0.7215     1.9279   3173.4633    0.0236  
   1200     4544.58     0.739      0.724      2.2062   10056.3739   0.0643  
   1300     4930.96     0.7238     0.7055     1.8668   1201.9845    0.0125  
   1400      5315.7     0.7136     0.687      2.5074   3460.6571    0.0062  
   1500     5698.74     0.699      0.6819     1.8169    940.0254    0.0005  
   1600     6081.21     0.6964     0.6657     1.7381   1128.2451    0.0035  
   1700     6464.43     0.6932     0.6598     2.1105   1829.5465    0.022   
   1800     6846.79     0.688      0.6615     1.7183    158.2629    0.0069  
   1900     7228.79     0.6884     0.6527     2.4621   3831.9377    0.0064  
   2000     7610.97     0.6828     0.6493     2.4765   2720.6136    0.0188  
   2100     7992.77     0.6766     0.6434     2.4334    725.5199    0.0169  
   2200     8375.42     0.6766     0.6394     2.5359   4361.3481    0.0398  
   2300     8758.95     0.6896     0.6621     2.4748   14048.7788   0.1906  
   2400     9142.89     0.672      0.6323     2.1128    912.9017    0.0297  
   2500      9526.6     0.662      0.6166     2.0871    662.7039    0.0244  
   2600     9910.27     0.6626     0.6081     2.1681    388.0207    0.0054  
   2700     10295.42    0.6505     0.597      2.451     4753.01     0.0351  
   2800     10678.89    0.6512     0.5958     2.5196   14691.9552   0.0562  
   2900     11063.31    0.6758     0.6312     2.5569   233548.6279   0.4172  
   3000     11448.58    0.6609     0.601      2.1469   10897.1417   0.0576  
   3100     11833.4     0.6729     0.6076     1.8116   6333.8516    0.0009  
   3200     12216.96    0.6746     0.6098     2.0405   2855.9503    0.012   
   3300     12602.01    0.6667     0.6027     1.3008    248.4539    0.0078  
   3400     12985.48    0.6892     0.6336     2.4127   2899.9327    0.0071  
   3500     13369.14    0.6813     0.619      1.7912   1277.4601    0.0063  
   3600     13751.67    0.6671     0.6155     1.2468    227.5605     0.0    
   3700     14134.89    0.6649     0.6155     1.9325   1372.6229    0.0038  
   3800     14517.95    0.6669     0.6056     0.953     169.3011    0.0062  
   3900     14900.51    0.6651     0.6134     2.2351   14269.9912   0.013   
   4000     15284.27    0.6654     0.6158     2.7853   12318.8557   0.0102  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       247.63     1.4139     1.4168     3.1619   13815.7696   0.0004  
   200       668.9      1.4105     1.4176     1.8441   37845.7444   0.0001  
   300      1091.52     1.4091     1.4112     1.0199   9704.5943    0.0001  
   400      1514.75     1.4162     1.4122     0.754    8429.8181    0.0001  
   500      1938.09     1.4174     1.4192     0.6095   1154.3226     0.0    
   600      2363.18     1.4143     1.4167     0.6886   1393.6534     0.0    
   700      2788.65     1.4142     1.4159     0.4895   4684.1292     0.0    
   800      3214.45     1.4165     1.4117     0.4617    544.3583     0.0    
   900      3640.98     1.4139     1.4165     0.3091    91.6069      0.0    
   1000     4067.23     1.4139     1.4171     0.2689    37.0661      0.0    
   1100     4492.71     1.4129     1.414      0.2814    214.7711     0.0    
   1200     4919.66     1.4146     1.4129     0.6421    5212.037     0.0    
   1300     5347.72     1.406      1.4061     0.8049   7224.4592     0.0    
   1400     5776.21     1.4175     1.4007     0.4986    978.7335     0.0    
   1500     6203.16     1.4046     1.4036     0.2366    158.6811     0.0    
   1600     6629.06     1.4046     1.396      0.2445    49.3385      0.0    
   1700     7054.63     1.3845     1.3726     0.2316     27.07       0.0    
   1800      7480.8     1.2546     1.2614     0.5061    13.1199      0.0    
   1900     7907.47     0.7753     0.7639     0.6373    15.2976      0.0    
   2000     8335.14     0.763      0.7558     0.7032    45.5083      0.0    
   2100     8762.68     0.7491     0.7391     3.0957    57.9671      0.0    
   2200     9189.61     0.7355     0.7244     2.937     386.5127     0.0    
   2300     9616.63     0.7298     0.7224     3.0256   2001.7375     0.0    
   2400     10042.2     0.7469     0.7334     0.749     85.5402      0.0    
   2500     10467.87    0.7389     0.7264     3.1231    31.0686      0.0    
   2600     10894.03    0.7366     0.716      0.5118    394.5282     0.0    
   2700     11320.27    0.7425     0.7293     2.6231   1536.7879     0.0    
   2800     11746.85    0.726      0.7123     1.2863   2429.0792     0.0    
   2900     12174.29    0.7468     0.7319     1.1511   76207.0115    0.0    
   3000     12601.88    0.7383     0.7206     0.8409   43625.5472    0.0    
   3100     13028.4     0.7283     0.7209     0.5767   3625.6748     0.0    
   3200     13455.94    0.7311     0.7211     0.5098   1183.7691     0.0    
   3300     13883.75    0.7286     0.7185     0.5434    566.3455     0.0    
   3400     14311.83    0.7368     0.7264     3.1621    296.0875     0.0    
   3500     14741.08    0.7364     0.7215     3.161     221.2396     0.0    
   3600     15169.28    0.7328     0.7194     1.5334    138.2777     0.0    
   3700     15597.9     0.7281     0.7194     3.1473    695.8712     0.0    
   3800     16026.19    0.7488     0.7341     3.1612    94.8289      0.0    
   3900     16454.6     0.7328     0.723      0.4316    182.4709     0.0    
   4000     16882.89    0.742      0.7186     0.6086   4603.1933     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       248.73     1.419      1.4154     3.1611   2903.2043    0.0001  
   200       730.19     1.4156     1.4121     1.6182    739.4776    0.0001  
   300      1214.52     1.4158     1.4132     0.9394   1095.2641    0.0001  
   400      1700.08     1.4117     1.412      0.7409    59.1662      0.0    
   500      2186.07     1.411      1.4164     0.4484    32.1636      0.0    
   600      2682.05     1.4175     1.4085     0.5631   1114.2381     0.0    
   700      3171.99     1.4148     1.4133     1.0004   30740.1687    0.0    
   800      3662.77     1.4149     1.4154     0.4986   1282.9875     0.0    
   900      4152.93     1.423      1.4133      0.59     100.1653     0.0    
   1000     4645.18     1.4188     1.4115     0.3849    161.2924     0.0    
   1100      5136.5     1.4082     1.4157     0.8896   22875.1877    0.0    
   1200     5628.91     1.3945     1.4052     0.7627   3450.4867     0.0    
   1300     6134.37     1.3679     1.366      0.396     261.0408     0.0    
   1400     6628.23     0.7592     0.7537     0.7895    69.0882      0.0    
   1500     7124.94     0.7478     0.742      0.8302    20.9027      0.0    
   1600     7621.75     0.7396     0.7352     0.9409    14.2938      0.0    
   1700     8113.43     0.7328     0.7212     3.1621    10.6469      0.0    
   1800     8607.71     0.7358     0.7279     0.7721    305.4789     0.0    
   1900     9100.78     0.727      0.7154     0.5516    55.5825      0.0    
   2000     9596.08     0.7356     0.725      3.1622    119.9528     0.0    
   2100     10090.46    0.7347     0.7197     0.7394     4.6724      0.0    
   2200     10583.64    0.7259     0.7132     2.1847    14.3714      0.0    
   2300     11075.68    0.734      0.722      2.5821     4.0111      0.0    
   2400     11567.39    0.7326     0.7164     1.3209     3.558       0.0    
   2500     12058.96    0.7917     0.7769     3.1599    826.4931     0.0    
   2600     12550.33    0.7333     0.724      3.1177    148.945      0.0    
   2700     13041.9     0.7405     0.7336     3.1513    176.2606     0.0    
   2800     13533.12    0.7509     0.7379     3.1356    50.2955      0.0    
   2900     14024.35    0.7328     0.7239     3.1264     9.3882      0.0    
   3000     14514.76    0.7615     0.7494     3.1595    39.8245      0.0    
   3100     15004.9     0.7428     0.7322     3.1467    38.8007      0.0    
   3200     15495.2     0.7299     0.7206     0.6768    413.0737     0.0    
   3300     15986.24    0.7327     0.721      1.2272    109.5284     0.0    
   3400     16476.56    0.7285     0.7183     3.161     680.8912     0.0    
   3500     16966.33    0.7302     0.7194     0.553     82.0237      0.0    
   3600     17456.94    0.7256     0.7132     0.5452    224.2501     0.0    
   3700     17947.17    0.7245     0.7157     3.0709    68.3435      0.0    
   3800     18436.7     0.7227      0.71      0.5125    430.8888     0.0    
   3900     18928.31    0.7285     0.7191     0.4085    84.8545      0.0    
   4000     19419.68    0.7297     0.716      0.4551    146.498      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       247.64     1.4089     1.4163      3.16    1160.2267    0.0001  
   200       784.94     1.4063     1.4141     1.793    3614.0928    0.0001  
   300      1329.07     1.4203     1.4194     1.2575   3086.3924     0.0    
   400      1876.78     1.418      1.416      0.7403   1348.7034     0.0    
   500      2425.55     1.4074     1.4107     0.9989   3277.8876     0.0    
   600      2974.57     1.4079     1.4131     0.9389   11728.269     0.0    
   700      3522.61     1.4116     1.4106     0.8049   1128.8592     0.0    
   800      4070.56     1.4059     1.4067     3.1614    310.0145     0.0    
   900      4620.23     1.4117     1.4148     1.1199   3434.2977     0.0    
   1000     5169.52     1.4135     1.4156     0.9628   4247.8293     0.0    
   1100      5720.6     1.411      1.4167     0.7224   3276.7845     0.0    
   1200      6267.1     1.4155     1.4125     0.6317    202.0044     0.0    
   1300     6813.26     1.3908     1.3877     0.4152    48.0169      0.0    
   1400     7359.07     0.7646     0.7515     0.7512    16.8918      0.0    
   1500     7908.36     0.7461     0.7346     0.6886     7.1413      0.0    
   1600     8459.48     0.7343     0.7212     0.6136     5.7801      0.0    
   1700     9012.89     0.7316     0.7215     0.5953     4.8159      0.0    
   1800     9563.23      0.72      0.7086     0.6294     5.7322      0.0    
   1900     10112.34    0.7284     0.7149     3.1531     11.875      0.0    
   2000     10659.01    0.7483     0.731      3.1622    550.6129     0.0    
   2100     11205.93    0.7335     0.7225     3.1619    467.0533     0.0    
   2200     11755.06    0.7217     0.7156     0.6794    696.6369     0.0    
   2300     12307.07    0.7295     0.7186     3.1405   18721.8892    0.0    
   2400     12859.4     0.7264     0.7127     3.1529   1424.5812     0.0    
   2500     13410.31    0.7305     0.7183     0.9472    263.1061     0.0    
   2600     13957.64    0.7321     0.7171     3.1541   1743.4948     0.0    
   2700     14504.78    0.7263     0.7084     2.722     301.0962     0.0    
   2800     15053.07    0.7289     0.7207     1.5873    120.8795     0.0    
   2900     15600.82    0.7249     0.7092     3.1002    50.2882      0.0    
   3000     16150.99    0.7323     0.712      2.9533    21.5467      0.0    
   3100     16702.42    0.7282     0.7098     1.0974    25.5932      0.0    
   3200     17249.32    0.7411     0.7218     1.7745    53.8255      0.0    
   3300     17799.98    0.7336     0.723      0.9174    521.8084     0.0    
   3400     18349.29    0.7296     0.7161     1.3035   9140.6111     0.0    
   3500     18896.55    0.7284     0.7184     3.0275    522.9447     0.0    
   3600     19446.95    0.7448     0.7288     1.8908   5233.1351     0.0    
   3700     19997.65    0.7269     0.7143     0.7428    521.7414     0.0    
   3800     20548.96    0.7352     0.7198     0.8326    272.011      0.0    
   3900     21098.26    0.7291     0.7114     0.4727    266.9221     0.0    
   4000     21649.53    0.7289     0.7149     0.5082    659.8219     0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.01  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       247.29     1.4187     1.4116     3.1352   11589.4631    0.0    
   200       829.61     1.419      1.4135     1.3514    585.5302     0.0    
   300      1419.34     1.408      1.4156     2.0801   37210.5252    0.0    
   400      2010.97     1.4101     1.4105     3.1509    732.3445     0.0    
   500      2604.38     1.4176     1.4047     3.0575    421.4097     0.0    
   600      3196.86     1.4071     1.4051     1.1041   1207.9753     0.0    
   700      3787.77     1.4199     1.4141     2.3092   133925.4096    0.0    
   800      4378.65     1.4044     1.4085     3.1617    191.3443     0.0    
   900      4971.51     1.4021     1.3953     1.3731   11777.735     0.0    
   1000     5564.16     1.3641     1.3565     0.9499    724.6783     0.0    
   1100     6158.29     0.7822     0.7671     0.9386     181.6       0.0    
   1200     6752.09     0.7832     0.773      0.9789    103.2983     0.0    
   1300     7344.93     0.7531     0.7408     0.8789    64.0889      0.0    
   1400     7938.74     0.7271     0.7193     0.8314    30.9851      0.0    
   1500     8534.02     0.7414     0.7272     3.1587    182.6468     0.0    
   1600     9130.53     0.738      0.7236     3.1243    6701.821     0.0    
   1700     9724.78     0.7449     0.7373     1.0701    600.3629     0.0    
   1800     10318.59    0.7368     0.7251     3.1467    94.6507      0.0    
   1900     10912.51    0.7335     0.7208     2.7384    81.8238      0.0    
   2000     11505.28    0.744      0.7329     3.1094   1296.2104     0.0    
   2100     12098.67    0.7513     0.7408     3.158     309.303      0.0    
   2200     12691.56    0.7252     0.7122     0.7897    330.4676     0.0    
   2300     13285.57    0.7298     0.7118     2.6525   4709.8624     0.0    
   2400     13879.1     0.7549     0.7458     3.0508    124.9967     0.0    
   2500     14472.11    0.7521     0.7429     2.3262   2097.7064     0.0    
   2600     15065.1     0.7785     0.7616     3.1607    324.0575     0.0    
   2700     15658.2     0.7489     0.7382     0.5547   2309.6032     0.0    
   2800     16251.21    0.7419     0.728      2.2219   26539.3187    0.0    
   2900     16843.36    0.7326     0.7221     3.0162   1273.0824     0.0    
   3000     17436.09    0.7357     0.7266     2.3029    214.5739     0.0    
   3100     18028.08    0.7433     0.7324     2.9176   2041.2814     0.0    
   3200     18621.93    0.732      0.7207     3.069    100306.1836    0.0    
   3300     19213.92    0.7383     0.7286     0.7355   1849.8398     0.0    
   3400     19807.0     0.7289     0.7173     0.5004    306.0387     0.0    
   3500     20402.15    0.7405     0.7336     3.1242    423.5902     0.0    
   3600     20996.96    0.7453     0.7326     0.6837    518.8529     0.0    
   3700     21589.26    0.7297     0.7204     2.9867   3359.0455     0.0    
   3800     22182.27    0.731      0.7211     3.1622   2017.2792     0.0    
   3900     22775.86    0.7315     0.7216     0.4791    248.3884     0.0    
   4000     23369.44    0.724      0.7163     2.8594    105.6206     0.0    
