Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  7.17829442024231
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       138.49     0.7812     0.7361     3.1419     0.2362   
   200       285.05     0.714      0.6787     2.4587     0.119    
   300       431.94     0.6792     0.6156     2.4564     0.0922   
   400       579.21     0.6524     0.5758     2.0728     0.0661   
   500       726.59     0.627      0.5244     1.9448     0.0611   
   600       873.7      0.629      0.5225     1.9274     0.0489   
   700      1020.57     0.6193     0.5018     2.0277     0.045    
   800      1167.53     0.6132     0.4804     1.6398     0.0384   
   900      1314.62     0.6195     0.4825     1.7238     0.036    
   1000     1461.83     0.6138     0.4615     1.6207     0.0318   
   1100      1609.4     0.6057     0.4645     1.5477     0.0352   
   1200     1756.71     0.6004     0.451      1.6359     0.0323   
   1300     1903.69     0.6025     0.4545     1.5073     0.0324   
   1400     2050.82     0.5925     0.4441     1.5835     0.0306   
   1500     2197.61     0.6091     0.4615     1.821      0.0332   
   1600     2344.05     0.5938     0.4366     1.7152     0.0297   
   1700     2490.32     0.5705     0.4079     1.4348     0.0292   
   1800     2636.26     0.5983     0.4391     1.6413     0.0306   
   1900     2782.01     0.634      0.5048     2.1371     0.0305   
   2000     2927.63     0.6278     0.5146     2.9023     0.0418   
   2100     3073.14     0.6531     0.5745     2.7266     0.0441   
   2200     3218.53     0.6364     0.5428     2.7229     0.0453   
   2300     3364.25     0.6346     0.5464     2.8922     0.0464   
   2400     3509.41     0.6225     0.5348     2.736      0.0442   
   2500     3653.65     0.6238     0.528      2.7695     0.0415   
   2600     3797.37     0.6031     0.5079     2.6433     0.0574   
   2700      3940.4     0.5922     0.4825     2.3769     0.0622   
   2800     4083.22     0.5849     0.4864     2.2854     0.0566   
   2900     4225.69     0.5847     0.4839     2.2679     0.0614   
   3000      4367.9     0.5748     0.4486     2.2508     0.0588   
   3100     4509.77     0.5798     0.4718     2.333      0.0493   
   3200     4651.43     0.5802     0.4821     2.2615     0.0588   
   3300     4792.88     0.5731     0.4615     2.4584     0.0545   
   3400     4934.07     0.5695     0.458      2.0038     0.0676   
   3500     5074.95     0.5647     0.4559     2.1735     0.0671   
   3600     5215.66     0.5691     0.4714     2.5945     0.0532   
   3700     5356.13     0.5601     0.4548     1.8876     0.0709   
   3800      5496.3     0.5509     0.4254     1.834      0.065    
   3900     5636.49     0.5499     0.4205     1.6241     0.0502   
   4000     5776.53     0.5623     0.4338     1.8868     0.0848   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       137.96     0.8151     0.7906     3.119      0.1299   
   200       284.39     0.7242     0.6935     2.0916     0.0848   
   300       430.64     0.6612     0.6248     2.0707     0.0679   
   400       577.02     0.6484     0.6049     1.8073     0.0594   
   500       723.14     0.6404     0.5869     2.1032     0.0505   
   600       869.01     0.6158     0.5659     1.7415     0.0409   
   700      1014.61     0.598      0.5391     1.6541     0.0389   
   800      1160.43     0.6009     0.5389     1.7825     0.0377   
   900      1305.98     0.591      0.5264     1.9032     0.0346   
   1000     1451.16     0.5803     0.5007     1.5869     0.0363   
   1100      1596.5     0.5831     0.5155     1.6125     0.0291   
   1200     1741.56     0.5624     0.4785     1.7617     0.0257   
   1300     1886.53     0.5724     0.4844     1.513      0.0275   
   1400     2031.53     0.5499     0.4716     1.6058     0.0258   
   1500     2176.31     0.549      0.4663     1.107      0.0221   
   1600     2321.06     0.541      0.4562     1.5539     0.0214   
   1700     2465.59     0.5468     0.4519     1.1848     0.0191   
   1800     2609.97     0.5441     0.4524     1.2339     0.0184   
   1900     2754.28     0.5319     0.4415     1.2253     0.0203   
   2000     2898.37     0.542      0.4409     1.2412     0.0185   
   2100     3042.47     0.5408     0.4361     1.1705     0.0202   
   2200      3186.5     0.5325     0.4268     1.2733     0.0168   
   2300     3330.48     0.5367     0.4226     1.1639     0.0167   
   2400     3474.16     0.5359     0.4274     1.2908     0.016    
   2500     3617.66     0.5287     0.4293     1.1903     0.0147   
   2600     3761.23     0.5267     0.4236     1.2934     0.0166   
   2700     3904.79     0.5209     0.4089     1.0004     0.0172   
   2800     4048.17     0.5296     0.4148     1.0613     0.0166   
   2900     4191.44     0.5232     0.4073     1.1071     0.0169   
   3000     4334.78     0.5195     0.4095     1.1102     0.0143   
   3100      4477.9     0.5214     0.4094     0.9643     0.0153   
   3200     4621.09     0.5266      0.42      1.1645     0.0153   
   3300     4764.31     0.5163     0.4076     0.965      0.0154   
   3400     4907.43     0.5121     0.395      1.0715     0.0144   
   3500     5050.49     0.5113     0.3996     1.0352     0.0143   
   3600     5193.93     0.5079     0.3937     1.2138     0.0144   
   3700     5337.13     0.5203     0.399      0.9173     0.0133   
   3800     5480.42     0.5177     0.4005     0.9184     0.0152   
   3900     5623.67     0.5205     0.402      1.0823     0.0143   
   4000     5767.04     0.5093     0.3906     0.9202     0.0138   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       136.79     0.8862     0.8749     3.1243     0.0932   
   200       283.3      0.7068     0.6873     2.0518     0.0579   
   300       429.96     0.6785     0.6498     1.9792     0.0488   
   400       576.81     0.6458     0.6082     1.6398     0.0416   
   500       724.38     0.6266     0.5863     2.0716     0.0538   
   600       871.41     0.6219     0.5763     1.7779     0.0332   
   700       1018.6     0.5896     0.5475     1.6188     0.0314   
   800       1165.9     0.5692     0.5319     1.5207     0.0277   
   900      1312.88     0.5717     0.5305     1.6443     0.0279   
   1000     1459.77     0.5644     0.5222     1.5376     0.0284   
   1100     1606.13     0.5457     0.5039     1.5606     0.0243   
   1200     1752.36     0.5553     0.506      1.4989     0.0316   
   1300     1898.49     0.5455     0.4984     1.5964     0.0209   
   1400     2044.51     0.5427     0.4844     1.4185     0.0245   
   1500     2190.58     0.5401     0.4866     1.4749     0.0189   
   1600     2336.47     0.5381     0.4777     1.3061     0.019    
   1700     2482.13     0.5317     0.4656     1.1597     0.0307   
   1800     2627.81     0.527      0.4657     1.1903     0.0223   
   1900     2773.31     0.5319     0.4635     1.1372     0.0156   
   2000     2918.85     0.5189     0.4582     1.1491     0.0246   
   2100     3064.18     0.5281     0.462      1.4473     0.0274   
   2200     3209.44     0.5146     0.4525     1.2753     0.0175   
   2300     3354.57     0.5175     0.4517     1.2787     0.0265   
   2400     3499.66     0.518      0.4517     1.1935     0.017    
   2500      3644.5     0.5168     0.4455     1.2295     0.0138   
   2600     3789.22     0.5052     0.4438     1.5252     0.0158   
   2700     3933.95     0.5056     0.4353     0.9713     0.0133   
   2800     4078.72     0.5085     0.4395     1.0924     0.0148   
   2900     4223.67     0.5039     0.4293     1.0852     0.019    
   3000     4369.08     0.5061     0.4348     1.0467     0.0232   
   3100     4513.77     0.5042     0.4326     1.2054     0.0121   
   3200     4658.37     0.5017     0.4267     1.0434     0.0111   
   3300     4802.79     0.4991     0.4219     0.9456     0.0213   
   3400     4947.11     0.4922     0.4239     0.9042     0.0119   
   3500     5091.36     0.4959     0.4175     0.8772     0.0111   
   3600     5235.78     0.4907     0.4219     1.0953     0.0104   
   3700     5379.96     0.4954     0.425      1.0101     0.0132   
   3800     5524.13     0.4946     0.4158     0.9304     0.0118   
   3900     5668.39     0.4957     0.4168     0.894      0.0113   
   4000     5812.68     0.4941     0.4217     1.0127     0.0131   
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       136.42     0.8384     0.8275     3.1264     0.0794   
   200       283.09     0.7321     0.7079     2.0989     0.0458   
   300       430.04     0.6722     0.6474     1.7648     0.0451   
   400       576.08     0.6462     0.6212     1.8262     0.0413   
   500       721.12     0.6255     0.5947     1.6876     0.0408   
   600       865.76     0.6164     0.5793     1.7761     0.0333   
   700      1009.98     0.593      0.5507     1.5377     0.0302   
   800      1153.92     0.5894     0.558      1.6556     0.0308   
   900      1297.58     0.5678     0.5339     1.4388     0.0257   
   1000     1441.09     0.5528     0.5222     1.6495     0.0241   
   1100     1584.63     0.5556     0.5122     1.5647     0.0264   
   1200     1727.87     0.5389     0.5008     1.3415     0.0262   
   1300     1871.13     0.5392     0.5046     1.665      0.0227   
   1400     2014.37     0.5489     0.5046     1.6908     0.0221   
   1500     2157.91     0.5319     0.4913     1.3396     0.032    
   1600     2300.89     0.5295     0.4901     1.2584     0.0196   
   1700     2443.87     0.5273     0.4831     1.4367     0.0185   
   1800     2586.89     0.5258     0.4715      1.18      0.0182   
   1900      2730.0     0.5364     0.4864     1.4021     0.0176   
   2000     2872.82     0.5177     0.4774     1.2696     0.018    
   2100     3015.67     0.5335     0.4732     1.2444     0.0164   
   2200     3158.56     0.5171     0.4713     1.6609     0.0306   
   2300     3301.59     0.5185     0.4734     1.0647     0.0232   
   2400     3444.44     0.5079     0.4569     1.278      0.0136   
   2500     3587.41     0.5099     0.4531     1.1045     0.0137   
   2600     3730.41     0.5023     0.4548     1.1604     0.013    
   2700     3873.35     0.5104     0.4555     1.3926     0.0162   
   2800     4016.19     0.4983     0.4549     1.0502     0.0209   
   2900     4159.13     0.4949     0.4546     1.019      0.0211   
   3000     4302.08     0.4975     0.447      1.0094     0.0184   
   3100     4445.06     0.4974     0.4482     1.0899     0.0133   
   3200     4588.02     0.4902     0.4356     0.8882     0.0143   
   3300     4730.81     0.4956     0.4434     0.9846     0.0137   
   3400      4873.5     0.4828     0.4354     1.0042     0.011    
   3500     5016.11     0.4871     0.4379     0.9054     0.0134   
   3600     5158.66     0.4946     0.4326     0.9897     0.0104   
   3700     5301.36     0.5009     0.4391     0.8736     0.0128   
   3800     5443.98     0.4881     0.4302     0.9986     0.0105   
   3900     5586.68     0.4874     0.4305     1.3271     0.0129   
   4000      5729.6     0.4922     0.4245     0.9292     0.0145   
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       136.41     0.8361     0.8287     3.1382     0.0765   
   200       285.43     0.7462     0.7269     1.9903     0.0501   
   300       434.94     0.6911     0.6747     1.8621     0.0415   
   400       584.88     0.6661     0.6453     1.8496     0.0365   
   500       735.02     0.6355     0.6107     1.8593     0.0364   
   600       884.94     0.621      0.5913     1.6325     0.0335   
   700      1034.53     0.611      0.5792     1.6758     0.0306   
   800      1183.99     0.5851     0.5512     1.8601     0.0287   
   900      1333.17     0.5724     0.5423     1.7315     0.0252   
   1000     1481.93     0.5677     0.5319     1.4529     0.0219   
   1100     1630.57     0.5603     0.5296     1.4581     0.0221   
   1200     1778.96     0.5587     0.5208     1.7024     0.0221   
   1300     1927.07     0.5547     0.5156     1.4268     0.0265   
   1400     2075.38     0.5384     0.5005     1.5453     0.0191   
   1500      2224.0     0.532      0.4927     1.4091     0.0267   
   1600     2372.47     0.5244     0.4868     1.4416     0.0169   
   1700     2521.34     0.5296     0.492      1.3996     0.0155   
   1800     2672.84     0.5172     0.4772     1.2378     0.0213   
   1900     2824.74     0.5105     0.4701     1.2466     0.0193   
   2000     2976.62     0.518      0.4717     1.1488     0.0256   
   2100     3128.32     0.5129     0.4664     1.157      0.0217   
   2200     3280.24     0.5147     0.468      1.2125     0.0155   
   2300     3431.73     0.5058     0.4667     1.138      0.0188   
   2400     3582.97     0.5041     0.4643     1.2765     0.0178   
   2500     3734.72     0.4988     0.4536     0.9979     0.0127   
   2600     3885.67     0.498      0.4538     1.2494     0.0109   
   2700     4036.23     0.5066     0.462      1.3711     0.0156   
   2800     4187.62     0.4977     0.4538     0.9721     0.0148   
   2900     4338.56     0.4988     0.4501     0.9909     0.0308   
   3000     4489.28     0.4928     0.451      1.0163     0.0142   
   3100     4640.48     0.4889     0.4444     0.8889     0.0318   
   3200     4791.74     0.4889     0.4442     0.9645     0.0141   
   3300     4943.08     0.4782     0.4351     0.9217     0.0121   
   3400      5094.4     0.4838     0.4374     0.9982     0.0138   
   3500     5245.12     0.4807     0.4345     0.9951     0.0109   
   3600     5396.33     0.4856     0.4366     0.975      0.0082   
   3700     5547.87     0.4775     0.4327     0.8963     0.0141   
   3800     5698.63     0.4843     0.4343     0.9339     0.0162   
   3900     5850.61     0.4706     0.4307     0.8544     0.0099   
   4000     6001.43     0.4785     0.4302     1.0049     0.0104   
