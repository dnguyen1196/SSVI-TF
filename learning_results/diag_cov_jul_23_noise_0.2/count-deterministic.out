Generating synthetic count valued data ... 
Generating synthetic  count valued data took:  11.235947132110596
max_count =  24  min count =  0
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       109.04     3.9846     4.0164     0.2744     0.2301    57886.05   46708.63  
   200       354.45     4.0274     4.0591     0.2355     0.1556    56377.85   45498.1   
   300       599.54     4.061      4.0886     0.2124     0.1168    55347.08   44668.78  
   400       844.54     4.0884     4.1241     0.1694     0.0941    54574.06   44045.26  
   500      1089.51     4.108      4.1422     0.1631     0.0791    53978.93   43564.13  
   600      1334.58     4.1257     4.1578     0.1447     0.0689    53500.55   43176.84  
   700      1579.82     4.142      4.1753     0.1427     0.0583    53108.44   42859.42  
   800      1824.95     4.1569     4.1873     0.1261     0.0532    52784.41   42596.49  
   900      2070.18     4.1682     4.2008     0.1289     0.0468    52510.37   42374.36  
   1000     2315.44     4.1777     4.2098     0.1149     0.0416    52279.4    42187.16  
   1100     2559.27     4.1852     4.2178     0.116      0.0372    52086.1    42030.77  
   1200     2803.44     4.1922     4.2267     0.1061     0.0349    51922.75   41898.68  
   1300     3048.09     4.2011     4.2311      0.1       0.0317    51784.1    41786.45  
   1400     3291.91     4.2058     4.2399     0.0956     0.0287    51670.23   41694.35  
   1500     3535.89     4.2087     4.2437     0.1032     0.0282    51572.26   41615.32  
   1600     3779.92     4.2164     4.2472     0.0965     0.0243    51491.04   41549.77  
   1700     4023.94     4.2155     4.2438     0.1089     0.0243    51414.7    41488.25  
   1800     4268.25     4.2052     4.2396     0.1192     0.0226    51325.17   41416.21  
   1900     4512.71     4.1795     4.2088     0.1304     0.0209    51163.05   41286.09  
   2000     4756.54     4.1142     4.1489     0.1474     0.0199    50871.52   41052.93  
   2100     5000.56     3.9913     4.0253     0.1715     0.0201    50432.63   40702.86  
   2200      5245.0     3.7914     3.8234     0.1768     0.0197    49966.7    40330.65  
   2300     5489.64     3.4981     3.5311     0.1863     0.0208    49646.26   40072.2   
   2400     5734.09     3.1273     3.1575     0.1901     0.0182    49482.77   39939.47  
   2500     5978.14     2.7364     2.7596     0.1749     0.0174    49427.33   39907.93  
   2600     6222.07     2.4011     2.4338     0.1761     0.0162    49597.35   40063.13  
   2700     6465.85     2.2788     2.2955     0.1649     0.015     50165.27   40517.61  
   2800     6709.55     2.4572     2.472      0.1594     0.014     51110.3    41270.69  
   2900     6952.93     2.8822     2.893      0.1603     0.0129    52261.82   42201.65  
   3000     7196.25     3.5083     3.522      0.1426     0.0125    53721.45   43397.09  
   3100     7439.47     4.2279     4.2648     0.1683     0.0122    55371.9    44758.36  
   3200     7683.11     5.0124     5.0449     0.1493     0.0122    57019.55   46117.97  
   3300     7927.24     5.7984     5.8438     0.2423     0.0119    58431.53   47258.37  
   3400     8171.67     6.5368     6.5858     0.186      0.011     59011.14   47694.05  
   3500     8415.91     7.279      7.3482     0.183      0.0113    59493.01   48049.52  
   3600     8659.66     8.0538     8.1048     0.1171     0.0103    59300.55   47885.43  
   3700      8903.2     8.7255     8.7874     0.1495     0.0102    58775.17   47456.71  
   3800     9146.36     9.3738     9.4481     0.1068     0.0098    58244.91   47026.91  
   3900     9388.87     9.9844     10.054     0.1918     0.0098    58032.18   46854.72  
   4000     9631.32    10.5976     10.672     0.1439     0.0096    57887.16   46732.66  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       108.13     3.9304     3.9454     0.2885     0.0809    59815.62   96632.8   
   200       410.71     3.9486     3.9624     0.2337     0.0841    59146.69   95547.04  
   300       713.11     3.9658     3.9773     0.2048     0.0721    58533.76   94553.09  
   400      1015.76     3.9829     3.9943     0.1781     0.0675    57972.96   93643.84  
   500      1318.58     3.997      4.0066     0.1626     0.0618    57461.36   92814.91  
   600      1621.29     4.0125     4.026      0.1535     0.0601    56990.75   92051.58  
   700      1923.88     4.0241     4.039      0.1371     0.0565    56557.82   91349.66  
   800      2226.24     4.041      4.0516     0.1382     0.051     56154.71   90695.81  
   900      2528.56     4.0496     4.0657     0.1282     0.0516    55779.91   90087.43  
   1000     2831.05     4.0588     4.0757     0.1196     0.0453    55431.54   89521.93  
   1100     3133.34     4.0723     4.0854     0.112      0.0442    55111.61   89002.23  
   1200     3435.85     4.078      4.0949     0.1082     0.0413    54813.57   88518.03  
   1300      3738.1     4.093      4.1061     0.1003     0.0389    54538.1    88070.66  
   1400     4040.25     4.0992     4.1159     0.1069     0.0368    54281.19   87652.88  
   1500     4342.36     4.1127     4.1233     0.0965     0.035     54040.58   87261.37  
   1600     4644.37     4.1198     4.1331     0.0914     0.0349    53816.38   86896.64  
   1700     4946.51     4.1271     4.1411     0.0936     0.0329    53607.19   86556.33  
   1800     5243.21     4.1331     4.1502     0.0834     0.0308    53412.17   86239.05  
   1900     5537.25     4.1442     4.1554     0.0868     0.029     53229.61   85941.63  
   2000     5828.23     4.1502     4.1654     0.0933     0.0275    53060.76   85666.36  
   2100     6119.83     4.1565     4.1706     0.0856     0.0274    52902.96   85409.05  
   2200     6401.22     4.1599     4.1756     0.087      0.0256    52754.18   85166.5   
   2300     6682.17     4.1679     4.1815     0.0813     0.0242    52615.77   84940.61  
   2400     6963.53     4.1715     4.1861     0.0733     0.0249    52488.32   84732.98  
   2500     7244.11     4.1772     4.1935     0.0742     0.0229    52369.53   84539.3   
   2600     7524.87     4.1811     4.197      0.0726     0.0221    52258.7    84358.51  
   2700     7805.94     4.1888     4.2022     0.0788     0.0215    52156.16   84191.16  
   2800     8088.18     4.1894     4.2074     0.0684     0.0197    52061.82   84037.18  
   2900     8370.11     4.1951     4.2123     0.0703     0.0179    51974.12   83893.95  
   3000     8654.89     4.2002     4.2167     0.0721     0.0183    51893.49   83762.23  
   3100     8935.54     4.2048     4.2211     0.067      0.0171    51819.3    83640.91  
   3200     9206.79     4.2074     4.2228     0.0638     0.0177    51751.7    83530.31  
   3300     9478.16     4.2116     4.2262     0.0642     0.0168    51688.75   83427.29  
   3400     9749.54     4.2149     4.2299     0.0605     0.016     51632.37   83334.91  
   3500     10021.01    4.2179     4.2331     0.0624     0.0164    51580.06   83249.31  
   3600     10296.8     4.221      4.2365     0.0661     0.0151    51533.6    83173.03  
   3700     10570.22    4.2235     4.2405     0.0605     0.0145    51491.85   83104.31  
   3800     10841.76    4.2264     4.2439     0.0647     0.0145    51454.36   83042.52  
   3900     11115.61    4.2271     4.2459     0.0566     0.0134    51421.42   82988.11  
   4000     11387.68    4.2333     4.2497     0.0598     0.0131    51392.74   82940.67  
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       97.02      3.9313     3.9351     0.2791     0.0603    60009.15   144844.3  
   200       433.75     3.946      3.9474     0.2286     0.056     59499.15  143608.06  
   300       763.55     3.9537     3.9645     0.2038     0.0536    59015.22  142435.28  
   400      1096.89     3.9666     3.9724     0.1941     0.0588    58553.22  141317.95  
   500      1433.22     3.981      3.985      0.1688     0.0507    58119.05  140268.05  
   600      1769.22     3.988      3.9953     0.1495     0.0479    57713.6   139288.13  
   700      2105.09     4.0011     4.0094     0.1468     0.0491    57324.61  138349.46  
   800      2441.84     4.0117     4.0167     0.1327     0.0532    56959.44  137467.35  
   900      2778.15     4.0222      4.03      0.1301     0.0437    56614.92  136634.74  
   1000      3114.1     4.0346     4.0392     0.1156     0.0437    56291.42  135853.31  
   1100      3441.1     4.0393     4.0489     0.1143     0.0385    55982.58  135106.87  
   1200     3773.99     4.0492     4.0577     0.1124     0.0391    55689.86  134399.49  
   1300     4103.21     4.0613     4.0666     0.1042     0.0375    55417.18  133740.65  
   1400     4435.65     4.0697     4.0771     0.1013     0.0372    55156.78  133110.81  
   1500     4769.87     4.0771     4.0866     0.1033     0.0338    54908.79  132511.17  
   1600      5103.7     4.0885     4.0943     0.0924     0.0331    54675.77  131947.53  
   1700     5434.97     4.0914     4.102      0.0968     0.0342    54453.11  131408.75  
   1800      5760.1     4.1001     4.1084     0.095      0.0299    54242.41  130898.71  
   1900     6085.56     4.1067     4.1169     0.0852     0.0294    54044.65  130419.81  
   2000     6416.63     4.116      4.1235     0.0897     0.0293    53854.59  129959.29  
   2100     6743.66     4.124      4.1298     0.0985     0.0276    53674.8   129523.86  
   2200     7072.56     4.1284     4.1376     0.0782     0.0274    53503.55   129108.9  
   2300     7401.61     4.1345     4.1449     0.0746     0.0248    53344.43  128723.21  
   2400     7730.12     4.1369     4.1486     0.0836     0.0247    53192.52  128354.89  
   2500     8055.54     4.1469     4.156      0.0818     0.0249    53048.62  128005.93  
   2600     8382.59     4.1497     4.1603     0.0801     0.0228    52912.77  127676.39  
   2700     8715.85     4.1561     4.1667     0.0725     0.0235    52783.0   127361.36  
   2800     9050.91     4.1625     4.1718     0.0702     0.0224    52660.81  127064.83  
   2900     9378.23     4.1683     4.1753     0.0703     0.0224    52545.68  126785.37  
   3000     9703.48     4.1698     4.1784     0.0658     0.0203    52436.47  126520.31  
   3100     10038.08    4.1782     4.1842     0.0676     0.0188    52333.08  126269.39  
   3200     10373.63    4.1791     4.1889     0.063      0.0194    52236.07  126033.78  
   3300     10706.8     4.1837     4.1934     0.0697     0.0177    52144.58  125811.48  
   3400     11038.71    4.1885     4.1974     0.0707     0.0178    52059.23  125604.03  
   3500     11373.75    4.193      4.2012     0.062      0.0177    51978.82  125408.75  
   3600     11710.21    4.1975     4.2041     0.0613     0.0162    51903.73  125226.22  
   3700     12046.69    4.1994     4.2082     0.0591     0.0152    51833.03  125054.56  
   3800     12382.0     4.2023     4.2122     0.0599     0.0163    51767.64  124895.47  
   3900     12711.22    4.2064     4.215      0.056      0.0152    51706.37  124746.43  
   4000     13036.83    4.2084     4.2178     0.0562     0.0153    51649.53  124608.08  
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       97.33      3.9244     3.9332     0.2757     0.0492    60096.18  193587.87  
   200       478.1      3.9377     3.9398     0.2269     0.0469    59666.31  192200.62  
   300       867.5      3.9468     3.9526     0.1958     0.0455    59257.43  190881.76  
   400       1258.6     3.958      3.9623     0.1915     0.0448    58861.63   189607.7  
   500      1649.08     3.9638     3.9748     0.1662     0.0436    58482.28  188385.75  
   600      2037.79     3.9778     3.983      0.1574     0.0442    58122.77   187227.3  
   700      2427.53     3.9849     3.9917     0.1578     0.0415    57778.33  186118.12  
   800      2818.11     4.0009     4.0005     0.1346     0.041     57449.88  185059.93  
   900      3208.51     4.0064     4.0106     0.1257     0.037     57132.72  184038.79  
   1000      3599.9     4.0139     4.023      0.1183     0.0417    56830.77  183065.88  
   1100     3988.87     4.0249     4.0303     0.1215     0.0358    56544.14  182141.71  
   1200     4379.61     4.0341     4.0387     0.1158     0.0362    56268.44  181252.83  
   1300     4768.96     4.0379     4.0473     0.1202     0.0338    56006.63  180408.55  
   1400     5142.05     4.0478     4.0556     0.1067     0.0317    55756.09  179600.17  
   1500     5508.42     4.057      4.0653     0.1191     0.0316    55516.52  178826.64  
   1600     5874.78     4.0614     4.0699     0.0905     0.0321    55287.36  178086.64  
   1700     6241.17     4.0688     4.0772     0.0933     0.0302    55067.96  177377.26  
   1800     6607.28     4.078      4.0834     0.0972     0.0313    54858.62  176700.79  
   1900     6973.55     4.0871     4.0902     0.0856     0.0269    54658.84  176054.62  
   2000     7340.01     4.0956     4.0992     0.0939     0.0283    54467.18   175434.7  
   2100      7706.3     4.0979     4.1055     0.0833     0.0292    54284.16  174842.65  
   2200     8072.46     4.1047     4.1132     0.0842     0.0258    54109.97  174278.91  
   2300     8438.68     4.1106     4.1169     0.0739     0.0254    53941.45  173733.61  
   2400     8804.87     4.117      4.1238     0.0836     0.0248    53781.19  173214.87  
   2500     9171.23     4.1254     4.1295     0.0799     0.0225    53627.88   172717.9  
   2600     9537.22     4.1288     4.137      0.0734     0.0232    53482.14  172245.87  
   2700     9903.11     4.1348     4.1396     0.074      0.0231    53340.92  171788.03  
   2800     10269.28    4.1366     4.1465     0.0733     0.0229    53207.2   171354.53  
   2900     10635.25    4.1427     4.1504     0.0725     0.0206    53078.76  170938.33  
   3000     11001.59    4.1505     4.1547     0.0724     0.0199    52956.62  170541.87  
   3100     11371.82    4.1546     4.161      0.074      0.0207    52838.62  170159.11  
   3200     11745.46    4.1579     4.1655     0.0653     0.0206    52726.96  169796.66  
   3300     12118.08    4.1625     4.1685     0.0632     0.019     52621.33  169453.77  
   3400     12484.86    4.1653     4.1739     0.0688     0.0181    52520.16  169125.32  
   3500     12851.33    4.1708     4.1772     0.0668     0.0179    52423.64  168812.14  
   3600     13217.78    4.1761     4.1818     0.0651     0.0169    52332.82   168517.2  
   3700     13584.07    4.179      4.1856     0.0653     0.0172    52245.68  168233.95  
   3800     13950.25    4.181      4.1888     0.0647     0.0159    52163.6   167966.91  
   3900     14316.46    4.1867     4.1928     0.0573     0.0162    52085.28  167712.29  
   4000     14682.98    4.1895     4.1961     0.0567     0.0158    52011.29  167471.54  
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | test_nll | train_nll  
   100       93.92      3.9236     3.937      0.2847     0.043     60156.04  242341.56  
   200       512.94     3.9359     3.9483     0.2238     0.0431    59778.82  240822.48  
   300       931.71     3.9439     3.9545     0.2125      0.04     59411.09  239341.74  
   400      1350.67     3.9512     3.9636     0.1804     0.042     59057.54  237917.34  
   500      1769.52     3.9637     3.976      0.1676     0.0387    58718.85  236554.39  
   600      2188.18     3.9686     3.9818     0.1527     0.039     58387.85  235221.87  
   700      2607.32     3.975      3.992      0.1461     0.0376    58071.51  233949.49  
   800      3027.05     3.9863     3.9992     0.1416     0.0355    57766.31   232720.8  
   900      3446.41     3.9981     4.0074     0.1266     0.0351    57475.93   231552.3  
   1000      3865.4     4.0049     4.0172     0.1249     0.037     57194.36   230418.5  
   1100     4284.43     4.0111     4.0266     0.1187     0.0348    56923.37  229326.63  
   1200     4723.33     4.0185     4.0325     0.1195     0.0344    56664.0   228282.44  
   1300     5160.91     4.0274     4.0423     0.1023     0.0304    56417.41  227288.29  
   1400     5599.91     4.0361     4.0496     0.1024     0.0303    56177.39  226319.77  
   1500     6033.96     4.0429     4.0585     0.1016     0.0316    55947.9    225393.3  
   1600     6471.01     4.0471     4.0644     0.0956     0.0294    55726.52  224499.75  
   1700     6912.03     4.0552     4.0697     0.0967     0.0287    55513.5   223639.78  
   1800     7344.58     4.0602     4.0778     0.0962     0.0271    55308.94  222813.72  
   1900     7763.47     4.073      4.0844     0.0927     0.0268    55112.78  222021.25  
   2000     8182.06     4.079      4.0918     0.0841     0.029     54922.7   221253.14  
   2100     8601.02     4.0852     4.0984     0.0856     0.0275    54739.59  220512.98  
   2200     9019.66     4.0882     4.105      0.0794     0.025     54563.18  219799.22  
   2300     9438.33     4.0942     4.1105     0.082      0.0247    54394.75  219117.94  
   2400     9857.31     4.1026     4.1157     0.0819     0.0256    54232.64  218461.65  
   2500     10276.4     4.1054     4.121      0.0788     0.023     54077.59  217834.04  
   2600     10695.31    4.1101     4.128      0.0759     0.0221    53927.87  217227.54  
   2700     11124.24    4.1141     4.1322     0.0717     0.0216    53783.49  216642.83  
   2800     11561.34    4.1219     4.1391     0.0777     0.0209    53644.32  216079.28  
   2900     11997.0     4.1266     4.1438     0.0713     0.0206    53510.67  215538.04  
   3000     12436.76    4.133      4.1467     0.0721     0.0209    53381.99  215016.46  
   3100     12873.75    4.138      4.1523     0.077      0.0197    53259.21  214518.92  
   3200     13322.86    4.1428     4.1581     0.0638     0.0198    53141.26  214040.69  
   3300     13780.88    4.147      4.1609     0.0686      0.02     53027.96  213581.08  
   3400     14231.71    4.151      4.1666     0.0645     0.0184    52918.43  213136.91  
   3500     14685.59    4.1538     4.1708     0.0653     0.0176    52814.64  212715.95  
   3600     15139.2     4.1574     4.1741     0.0625     0.0176    52714.74  212310.97  
   3700     15593.33    4.1626     4.1781     0.0638     0.0168    52618.05  211919.06  
   3800     16049.12    4.1665     4.1819     0.0619     0.0167    52526.79  211549.01  
   3900     16502.84    4.1696     4.1855     0.0609     0.016     52438.91  211192.71  
   4000     16956.26    4.1729     4.1886     0.0686     0.0159    52354.85  210851.53  
