Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  7.976619482040405
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       91.58      0.7491     0.7279     3.1254     0.7039      0.0    
   200       246.41     0.7172     0.6928     2.913      0.2231      0.0    
   300       401.43     0.7096     0.6893     2.8772     0.1516      0.0    
   400       557.53     0.7171     0.7067     3.0002     0.1079      0.0    
   500       712.29     0.7227     0.6991     2.9294     0.0867      0.0    
   600       866.32     0.7104     0.685      2.7163     0.0847      0.0    
   700      1019.88     0.7061     0.6859     2.6029     0.0701      0.0    
   800      1173.21     0.7097     0.693      2.5345     0.0696      0.0    
   900      1326.66     0.7165     0.7041     2.6589     0.0655      0.0    
   1000     1480.23     0.7176     0.705      2.6492     0.0845      0.0    
   1100     1633.72     0.7157     0.6917     2.6024     0.0583      0.0    
   1200      1787.2     0.7177     0.7043     2.3408     0.0729      0.0    
   1300     1940.51     0.7182     0.6927     2.5704     0.0932      0.0    
   1400     2094.05     0.7222     0.6958     2.5109     0.0867      0.0    
   1500     2247.54     0.7214     0.6973     2.1036     0.0743      0.0    
   1600     2400.86     0.7198     0.7004     2.2784     0.0444      0.0    
   1700     2553.12     0.718      0.7047     1.9262     0.0621      0.0    
   1800     2705.15     0.7174     0.7046     1.9685     0.064       0.0    
   1900     2858.79     0.7174     0.7067     2.332      0.0526      0.0    
   2000     3013.09     0.7171     0.7011     1.9128     0.0417      0.0    
   2100      3167.3     0.7163     0.7031     1.8955     0.0699      0.0    
   2200     3321.63     0.7155     0.7016     1.8869     0.0543      0.0    
   2300     3476.07     0.7177     0.7061     2.1406     0.0481      0.0    
   2400     3630.58     0.7164     0.7051     1.8068     0.0361      0.0    
   2500     3785.06     0.7162     0.7009     2.199      0.0284      0.0    
   2600      3938.5     0.7165     0.6977     1.6528     0.0386      0.0    
   2700     4090.81     0.7178     0.7011     1.532      0.0349      0.0    
   2800     4243.06     0.7171     0.7009     1.6911     0.0348      0.0    
   2900      4395.5     0.718      0.7011     1.6697     0.0227      0.0    
   3000     4548.27     0.7165     0.7044     1.8088     0.0404      0.0    
   3100     4701.14     0.7171     0.7023     1.7112     0.018       0.0    
   3200     4853.56     0.717      0.7006     1.5348     0.0194      0.0    
   3300     5006.06     0.7169     0.7029     1.5139     0.0321      0.0    
   3400     5158.66     0.7164     0.6966     1.3529     0.0239      0.0    
   3500      5311.4     0.718      0.7004     1.8411     0.0434      0.0    
   3600     5463.76     0.7163     0.6996     1.6195     0.0212      0.0    
   3700     5616.19     0.7155     0.699      1.4606     0.019       0.0    
   3800     5768.74     0.7165     0.6996     1.4211     0.0302      0.0    
   3900     5921.22     0.7171     0.701      1.5103     0.0212      0.0    
   4000     6073.72     0.7176     0.6994     1.6462     0.0194      0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       92.16      0.7691     0.7528     3.1063     0.1989      0.0    
   200       275.65      0.72      0.7099     2.9878     0.1218      0.0    
   300       460.68     0.7163     0.7075     2.9503     0.1122      0.0    
   400       644.8      0.7172     0.7084     2.981      0.0981      0.0    
   500       827.3      0.7172     0.7084     2.8216     0.1146      0.0    
   600      1009.27     0.7172     0.7084     2.8892     0.1215      0.0    
   700      1191.34     0.7172     0.7084     3.093      0.1335      0.0    
   800      1373.35     0.7172     0.7084     2.5269     0.1176      0.0    
   900      1555.25     0.7172     0.7084     2.3806     0.1253      0.0    
   1000     1737.15     0.7172     0.7084     2.4716     0.1333      0.0    
   1100     1919.32     0.7172     0.7084     2.4243     0.164       0.0    
   1200     2101.53     0.7172     0.7084     2.3508     0.1662      0.0    
   1300     2283.44     0.7172     0.7084     2.555      0.1408      0.0    
   1400      2464.7     0.7172     0.7084     2.3826     0.1518      0.0    
   1500     2646.08     0.7172     0.7084     2.057      0.1495      0.0    
   1600     2826.93     0.7172     0.7084     2.0772     0.1443      0.0    
   1700     3007.65     0.7172     0.7084     1.9025     0.1406      0.0    
   1800     3188.21     0.7172     0.7084     2.3401     0.157       0.0    
   1900      3368.6     0.7172     0.7084     1.6896     0.1479      0.0    
   2000     3549.03     0.7172     0.7084     1.722      0.1494      0.0    
   2100      3729.3     0.7172     0.7084     1.6848     0.1479      0.0    
   2200     3909.57     0.7172     0.7084     2.1284     0.1846      0.0    
   2300     4089.66     0.7172     0.7084     2.0095     0.1763      0.0    
   2400     4269.74     0.7172     0.7084     1.7191     0.1513      0.0    
   2500      4450.4     0.7172     0.7084     1.6416     0.1239      0.0    
   2600     4630.77     0.7172     0.7084     2.3816     0.1477      0.0    
   2700      4811.2     0.7172     0.7084     1.7926     0.1891      0.0    
   2800     4991.53     0.7172     0.7084     1.6688     0.1609      0.0    
   2900     5171.95     0.7172     0.7084     1.5662     0.2034      0.0    
   3000     5352.49     0.7172     0.7084     1.9467     0.1264      0.0    
   3100     5532.95     0.7172     0.7084     1.3802     0.1439      0.0    
   3200     5713.49     0.7172     0.7084     2.0026     0.1189      0.0    
   3300     5894.07     0.7172     0.7084     1.3204     0.1448      0.0    
   3400     6075.03     0.7172     0.7084     1.4178     0.1614      0.0    
   3500     6255.43     0.7172     0.7084     1.3815     0.1389      0.0    
   3600     6435.75     0.7172     0.7084     1.4683     0.1221      0.0    
   3700     6616.09     0.7172     0.7084     1.562      0.0991      0.0    
   3800     6796.44     0.7172     0.7084     1.2717     0.1127      0.0    
   3900     6976.95     0.7172     0.7084     1.2272     0.0954      0.0    
   4000     7157.41     0.7172     0.7084     1.1554     0.1042      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       91.59      0.8212     0.8025     3.0752     0.1313      0.0    
   200       300.08     0.7154     0.704      3.028      0.1426      0.0    
   300       510.44     0.7206     0.7084     3.0019     0.1253      0.0    
   400       720.35     0.7172     0.706      2.958      0.1118      0.0    
   500       928.43     0.7172     0.706      2.7658     0.1144      0.0    
   600      1136.43     0.7172     0.706      2.7788     0.1157      0.0    
   700      1343.86     0.7172     0.706      2.4988     0.1212      0.0    
   800      1547.32     0.7172     0.706      2.5912     0.1074      0.0    
   900      1750.11     0.7172     0.706      2.6189     0.118       0.0    
   1000     1951.21     0.7172     0.706      2.6308     0.1211      0.0    
   1100     2150.69     0.7172     0.706      2.532      0.1188      0.0    
   1200     2350.78     0.7172     0.706      2.7703     0.1199      0.0    
   1300     2549.49     0.7172     0.706      1.9541     0.1398      0.0    
   1400     2742.42     0.7172     0.706      2.476      0.1534      0.0    
   1500     2934.76     0.7172     0.706      2.7093     0.1949      0.0    
   1600     3127.59     0.7172     0.706      2.0494     0.115       0.0    
   1700     3321.66     0.7172     0.706      1.9392     0.1522      0.0    
   1800     3515.24     0.7172     0.706      2.0664     0.1493      0.0    
   1900     3708.85     0.7172     0.706      2.2439     0.1358      0.0    
   2000     3902.58     0.7172     0.706      2.0597     0.1773      0.0    
   2100     4095.66     0.7172     0.706      2.7933     0.1588      0.0    
   2200     4289.97     0.7172     0.706      2.3385     0.1509      0.0    
   2300      4482.9     0.7172     0.706      1.4664     0.1446      0.0    
   2400     4675.87     0.7172     0.706      2.006      0.1225      0.0    
   2500     4872.22     0.7172     0.706      1.7493     0.1349      0.0    
   2600     5067.23     0.7172     0.706      1.427      0.1512      0.0    
   2700     5261.39     0.7172     0.706      1.6468     0.1415      0.0    
   2800     5449.81     0.7172     0.706      1.3635     0.1818      0.0    
   2900     5635.49     0.7172     0.706      1.8778     0.1251      0.0    
   3000     5821.67     0.7172     0.706      1.4882     0.1269      0.0    
   3100     6008.95     0.7172     0.706      1.8166     0.1838      0.0    
   3200      6196.2     0.7172     0.706      1.4715     0.1044      0.0    
   3300     6383.39     0.7172     0.706      2.0648     0.0952      0.0    
   3400     6572.21     0.7172     0.706      1.6056     0.1191      0.0    
   3500     6759.37     0.7172     0.706      1.2714     0.1338      0.0    
   3600      6946.4     0.7172     0.706      1.5519     0.1053      0.0    
   3700     7132.01     0.7172     0.706      1.2909     0.0904      0.0    
   3800      7318.2     0.7172     0.706      2.3077     0.1149      0.0    
   3900     7506.03     0.7172     0.706      1.3818     0.1024      0.0    
   4000     7693.18     0.7172     0.706      1.3076     0.1015      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       82.13      0.7969     0.7852     3.0865     0.1193     0.0001  
   200       295.14     0.7208     0.7083     2.9079     0.1358      0.0    
   300       515.95     0.7159     0.7007     2.9152     0.1217      0.0    
   400       734.24     0.7127     0.6972     2.8203     0.1151      0.0    
   500       946.43     0.7172     0.7033     2.8507     0.1114      0.0    
   600      1162.41     0.7109     0.6964     2.6979     0.1261      0.0    
   700      1380.42     0.7172     0.7035     2.815      0.1122      0.0    
   800      1598.22     0.7172     0.7035     2.7096     0.1234      0.0    
   900      1815.75     0.7172     0.7035     2.7138     0.1084      0.0    
   1000      2033.3     0.7172     0.7035     2.6163     0.1509      0.0    
   1100     2250.93     0.7172     0.7035     2.4619     0.1214      0.0    
   1200      2468.4     0.7172     0.7035     2.7767     0.1534      0.0    
   1300      2686.4     0.7172     0.7035     2.2546     0.1388      0.0    
   1400     2904.18     0.7172     0.7035     2.4024     0.1418      0.0    
   1500     3121.97     0.7172     0.7035     2.4956     0.1524      0.0    
   1600     3338.75     0.7172     0.7035     2.2028     0.1653      0.0    
   1700     3549.46     0.7172     0.7035     2.1813     0.1548      0.0    
   1800     3764.37     0.7172     0.7035     1.877      0.1708      0.0    
   1900     3978.47     0.7172     0.7035     1.7704     0.1399      0.0    
   2000     4191.74     0.7172     0.7035     2.0273     0.2254      0.0    
   2100     4407.92     0.7172     0.7035     1.7556     0.123       0.0    
   2200     4622.88     0.7172     0.7035     2.4438     0.1534      0.0    
   2300     4839.74     0.7172     0.7035     2.1661     0.1956      0.0    
   2400     5056.12     0.7172     0.7035     1.4762     0.1446      0.0    
   2500     5272.76     0.7172     0.7035     1.7686     0.1541      0.0    
   2600     5488.72     0.7172     0.7035     1.7679     0.1332      0.0    
   2700     5699.37     0.7172     0.7035     2.017      0.1471      0.0    
   2800     5910.06     0.7172     0.7035     1.5357     0.1339      0.0    
   2900     6120.69     0.7172     0.7035      1.55      0.1204      0.0    
   3000      6332.7     0.7172     0.7035     1.9968     0.1245      0.0    
   3100     6547.96     0.7172     0.7035     2.0921     0.1226      0.0    
   3200     6758.53     0.7172     0.7035     1.6938     0.1302      0.0    
   3300     6973.46     0.7172     0.7035     1.3103     0.109       0.0    
   3400     7185.18     0.7172     0.7035     1.3922     0.0888      0.0    
   3500     7399.29     0.7172     0.7035     1.5171     0.1405      0.0    
   3600     7609.89     0.7172     0.7035     1.5392     0.0947      0.0    
   3700     7823.44     0.7172     0.7035     1.8268     0.1402      0.0    
   3800     8035.03     0.7172     0.7035     1.1606     0.1815      0.0    
   3900     8247.19     0.7172     0.7035     1.6849     0.1266      0.0    
   4000     8459.18     0.7172     0.7035     1.2445     0.0767      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       87.99      0.8885     0.8757     3.1195     0.1224     0.0001  
   200       357.41     0.7218     0.7129     2.935      0.1423      0.0    
   300       629.51     0.7179     0.7023     2.9487     0.1236      0.0    
   400       894.84     0.7172     0.7044     2.8887     0.115       0.0    
   500      1130.48     0.7172     0.7044     2.8809     0.1149      0.0    
   600      1369.61     0.7209     0.7068     3.0231     0.114       0.0    
   700      1611.75     0.7172     0.7044     2.7907     0.1093      0.0    
   800      1853.93     0.7172     0.7044     2.5312     0.1101      0.0    
   900      2097.02     0.7172     0.7044      2.36      0.1126      0.0    
   1000     2336.22     0.7172     0.7044     2.5312     0.1357      0.0    
   1100     2575.78     0.7172     0.7044     2.2106     0.1215      0.0    
   1200     2816.23     0.7172     0.7044     2.2109     0.1393      0.0    
   1300      3059.1     0.7172     0.7044     2.2252     0.1292      0.0    
   1400     3301.87     0.7172     0.7044     2.1262     0.1257      0.0    
   1500     3544.64     0.7172     0.7044     2.5998     0.158       0.0    
   1600     3787.43     0.7172     0.7044     2.0594     0.1325      0.0    
   1700     4027.12     0.7172     0.7044     1.9119     0.1603      0.0    
   1800     4264.47     0.7172     0.7044     1.7685     0.1364      0.0    
   1900     4499.27     0.7172     0.7044     2.304      0.1953      0.0    
   2000     4734.07     0.7172     0.7044     1.8808     0.1497      0.0    
   2100     4968.93     0.7172     0.7044     2.234      0.1612      0.0    
   2200     5204.59     0.7172     0.7044     2.089      0.1661      0.0    
   2300     5442.01     0.7172     0.7044     2.2208     0.1723      0.0    
   2400      5683.9     0.7172     0.7044     2.1025     0.1696      0.0    
   2500     5925.79     0.7172     0.7044     1.6815     0.1293      0.0    
   2600     6168.37     0.7172     0.7044     2.1656     0.153       0.0    
   2700     6409.79     0.7172     0.7044     1.8611     0.1586      0.0    
   2800     6650.99     0.7172     0.7044     1.8147     0.1154      0.0    
   2900     6891.65     0.7172     0.7044     1.5843     0.1337      0.0    
   3000     7133.05     0.7172     0.7044     1.5376     0.1481      0.0    
   3100     7374.57     0.7172     0.7044     1.7515     0.1288      0.0    
   3200     7616.64     0.7172     0.7044     1.4736     0.1074      0.0    
   3300     7858.05     0.7172     0.7044     1.4399     0.1715      0.0    
   3400     8100.13     0.7172     0.7044     1.3839     0.0984      0.0    
   3500     8342.82     0.7172     0.7044     1.4852     0.0979      0.0    
   3600     8585.06     0.7172     0.7044     1.441      0.1139      0.0    
   3700     8826.07     0.7172     0.7044     1.8571     0.1229      0.0    
   3800     9067.79     0.7172     0.7044     1.6335     0.099       0.0    
   3900      9310.6     0.7172     0.7044     1.3796     0.0901      0.0    
   4000     9552.04     0.7172     0.7044     1.6262     0.084       0.0    
