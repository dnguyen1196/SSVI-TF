Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.522478580474854
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.36      0.7779     0.738      3.1274     0.7064   
   200       184.3      0.7144     0.7014     2.9752     0.2353   
   300       277.81     0.7171     0.6964     3.0349     0.1498   
   400       372.19     0.7141     0.6948     3.0569     0.098    
   500       465.3      0.7227     0.7038     2.882      0.0788   
   600       557.84     0.7188     0.6984     2.9229     0.0652   
   700       649.98     0.7165     0.7054     2.6638     0.072    
   800       742.05     0.7194     0.7047     2.9592     0.0547   
   900       834.15     0.7161     0.697      2.766      0.068    
   1000      926.21     0.7209     0.7087     2.5475     0.0508   
   1100     1018.24     0.7136     0.6933     2.6816     0.0551   
   1200     1110.01     0.7153     0.6974     2.8198     0.0431   
   1300     1202.09     0.7243     0.7017     2.6186     0.0358   
   1400     1294.06     0.7145     0.6997     2.5787     0.0536   
   1500     1386.34     0.7193     0.7021     2.2466     0.0765   
   1600     1478.32     0.714      0.6924     2.2685     0.0509   
   1700     1570.48     0.7161     0.7053     2.1736     0.0339   
   1800      1662.1     0.7172     0.707      1.9602     0.0417   
   1900     1753.76     0.7171     0.7067     1.8806     0.0607   
   2000     1845.15     0.7161     0.7054     1.9366     0.0391   
   2100     1935.85     0.7172     0.707      2.3673     0.0254   
   2200     2026.63     0.7176     0.7067     2.3344     0.0303   
   2300     2117.42     0.718      0.7055     1.6596     0.0282   
   2400     2208.18     0.7183     0.7055     1.806      0.0373   
   2500     2298.91     0.7178     0.707      2.1138     0.031    
   2600     2389.57     0.7189     0.7046     2.0828     0.0327   
   2700      2480.2     0.7186     0.6976     2.1247     0.0603   
   2800     2569.96     0.7203     0.696      1.6098     0.0155   
   2900     2659.88     0.7259     0.7054     2.1505     0.0291   
   3000     2750.28     0.7262     0.7055     1.8193     0.049    
   3100     2841.18     0.7252     0.7013     1.7571     0.0236   
   3200      2932.3     0.7242     0.7006     1.5901     0.0234   
   3300     3023.38     0.7237     0.7046     1.5244     0.0139   
   3400     3114.46     0.7174     0.7011     2.3045     0.0106   
   3500     3205.56     0.7172     0.6967     2.1694     0.0245   
   3600     3296.68     0.7182     0.7007     2.1366     0.0299   
   3700     3387.81     0.7174     0.7027     1.582      0.0138   
   3800     3478.96     0.7209     0.7048      1.4       0.0184   
   3900     3570.08     0.7204     0.7051     1.7801     0.011    
   4000     3661.23     0.7177     0.7044     1.4168     0.0191   
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100        92.0      0.7526     0.7345     3.1283     0.1561   
   200       186.81     0.7231     0.7077     3.0132     0.1176   
   300       283.4      0.7172     0.7084     2.9685     0.112    
   400       378.61      0.71      0.702      3.0746     0.1069   
   500       473.21     0.7172     0.7084     2.8393     0.0973   
   600       567.53     0.7172     0.7084     2.8851     0.1073   
   700       661.85     0.7172     0.7084     2.537      0.1091   
   800       756.06     0.7172     0.7084     2.8845     0.1356   
   900       850.16     0.7172     0.7084     2.254      0.1201   
   1000      944.14     0.7172     0.7084     2.6051     0.1255   
   1100     1038.02     0.7172     0.7084     2.4573     0.1139   
   1200     1131.95     0.7172     0.7084     2.2021     0.1318   
   1300     1225.92     0.7172     0.7084     2.4432     0.1463   
   1400     1320.02     0.7172     0.7084     2.2573     0.1541   
   1500     1414.15     0.7172     0.7084     2.3265     0.1293   
   1600     1508.32     0.7172     0.7084     2.0185     0.1373   
   1700     1602.49     0.7172     0.7084     2.1711     0.1763   
   1800     1696.67     0.7172     0.7084     2.3263     0.1286   
   1900     1790.82     0.7172     0.7084     1.894      0.1882   
   2000     1884.89     0.7172     0.7084     1.9582     0.1626   
   2100     1979.01     0.7172     0.7084     1.7978     0.1408   
   2200     2073.38     0.7172     0.7084     1.503      0.1517   
   2300     2167.59     0.7172     0.7084     1.439      0.1392   
   2400     2261.75     0.7172     0.7084     1.4493     0.1905   
   2500     2355.76     0.7172     0.7084     2.2303     0.1229   
   2600     2449.85     0.7172     0.7084     2.074      0.1333   
   2700     2543.85     0.7172     0.7084     1.4844     0.1367   
   2800     2637.98     0.7172     0.7084     2.2106     0.1445   
   2900     2732.11     0.7172     0.7084      1.38      0.1541   
   3000     2826.25     0.7172     0.7084     1.5841     0.1265   
   3100     2920.37     0.7172     0.7084     1.5096     0.1448   
   3200     3014.43     0.7172     0.7084     1.5125     0.1297   
   3300     3108.63     0.7172     0.7084     1.1724     0.1352   
   3400     3202.52     0.7172     0.7084     1.3576     0.1053   
   3500     3296.44     0.7172     0.7084     1.5261     0.1677   
   3600     3390.36     0.7172     0.7084     1.6513     0.095    
   3700     3484.28     0.7172     0.7084     1.2413     0.1309   
   3800     3578.18     0.7172     0.7084     1.3109     0.1311   
   3900     3671.96     0.7172     0.7084     1.7017     0.1263   
   4000     3765.79     0.7172     0.7084     1.8938     0.1174   
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.84      0.8552     0.8342     3.0982     0.1371   
   200       187.59     0.726      0.7108     2.922      0.1516   
   300       284.62     0.7172     0.706      2.9426     0.1333   
   400       380.66     0.7172     0.706      2.9636     0.1181   
   500       475.97     0.7172     0.706      3.0145     0.1106   
   600       570.99     0.7172     0.7058     2.8314     0.1115   
   700       666.03     0.7172     0.706      2.8111     0.1052   
   800       761.1      0.7172     0.706      2.4759     0.1132   
   900       855.86     0.7172     0.706      2.6703     0.1064   
   1000      950.67     0.7172     0.706      2.7313     0.1138   
   1100     1045.27     0.7172     0.706      2.4321     0.1352   
   1200     1139.84     0.7172     0.706      2.4915     0.1415   
   1300     1234.31     0.7172     0.706      2.3785     0.1419   
   1400     1328.91     0.7172     0.706      2.4992     0.1438   
   1500      1423.3     0.7172     0.706      2.3081     0.1485   
   1600     1517.74     0.7172     0.706      2.413      0.1283   
   1700     1612.39     0.7172     0.706      1.9197     0.1344   
   1800      1706.6     0.7172     0.706      1.8871     0.1268   
   1900     1800.71     0.7172     0.706      2.0739     0.1835   
   2000     1894.76     0.7172     0.706      2.0474     0.1655   
   2100     1989.48     0.7172     0.706      1.9772     0.1356   
   2200     2083.56     0.7172     0.706      2.0184     0.1259   
   2300     2177.64     0.7172     0.706      1.6677     0.1443   
   2400     2271.69     0.7172     0.706      1.5601     0.1388   
   2500     2365.68     0.7172     0.706      1.8136     0.1099   
   2600     2459.77     0.7172     0.706      1.5417     0.111    
   2700     2553.74     0.7172     0.706      2.2008     0.1787   
   2800     2647.73     0.7172     0.706      1.4186     0.1431   
   2900     2741.82     0.7172     0.706      1.4651     0.1428   
   3000     2835.92     0.7172     0.706      1.5393     0.1191   
   3100     2929.99     0.7172     0.706      1.549      0.1236   
   3200     3024.23     0.7172     0.706      1.2605     0.1253   
   3300     3118.34     0.7172     0.706      1.7514     0.1175   
   3400     3212.48     0.7172     0.706      1.8665     0.1411   
   3500     3306.68     0.7172     0.706      1.6055     0.1344   
   3600      3400.9     0.7172     0.706      1.5796      0.11    
   3700     3494.95     0.7172     0.706      1.6373     0.1306   
   3800     3589.21     0.7172     0.706      1.0517     0.097    
   3900     3683.25     0.7172     0.706      1.3843     0.1148   
   4000     3777.54     0.7172     0.706      1.0941     0.093    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       91.47      0.7906     0.7727     3.1213     0.1335   
   200       186.91     0.7171     0.7031     3.015      0.1425   
   300       284.3      0.7172     0.7035     2.987      0.1259   
   400       380.4      0.7172     0.7035     2.9177     0.1142   
   500       475.79     0.7172     0.7035     2.9319     0.1125   
   600       570.92     0.7172     0.7035     2.8958     0.109    
   700       665.85     0.7172     0.7035     2.6773     0.1296   
   800       760.75     0.7172     0.7035     2.8695     0.1324   
   900       855.58     0.7172     0.7035     2.8725     0.1384   
   1000      950.34     0.7172     0.7035     2.7972     0.1285   
   1100      1045.4     0.7172     0.7035     2.8745     0.1309   
   1200     1140.25     0.7172     0.7035     2.1902     0.1282   
   1300     1235.11     0.7172     0.7035     2.3795     0.1595   
   1400      1330.0     0.7172     0.7035     2.227      0.1365   
   1500     1424.86     0.7172     0.7035     1.9519     0.1314   
   1600     1519.63     0.7172     0.7035     2.2373     0.1581   
   1700      1614.4     0.7172     0.7035     1.7943     0.1406   
   1800     1709.24     0.7172     0.7035     2.3163     0.164    
   1900     1804.56     0.7172     0.7035     1.795      0.1386   
   2000     1899.35     0.7172     0.7035     2.0354     0.1618   
   2100     1994.11     0.7172     0.7035     1.7092     0.1664   
   2200     2088.82     0.7172     0.7035     2.0368     0.159    
   2300     2183.52     0.7172     0.7035     1.9897     0.1695   
   2400     2278.41     0.7172     0.7035     1.7336     0.1617   
   2500     2373.37     0.7172     0.7035     1.5164     0.1316   
   2600     2468.01     0.7172     0.7035     1.9481     0.1239   
   2700     2562.65     0.7172     0.7035     1.4089     0.1223   
   2800     2657.28     0.7172     0.7035     1.8081     0.1623   
   2900     2751.79     0.7172     0.7035     1.3809     0.1828   
   3000      2846.3     0.7172     0.7035     1.6029     0.1284   
   3100     2940.82     0.7172     0.7035     1.237      0.1874   
   3200     3035.33     0.7172     0.7035     1.8861     0.0904   
   3300     3129.81     0.7172     0.7035     1.6033     0.1268   
   3400     3224.31     0.7172     0.7035     2.0123     0.1015   
   3500     3318.91     0.7172     0.7035     1.1742     0.1471   
   3600     3413.47     0.7172     0.7035     1.2233     0.0814   
   3700     3508.03     0.7172     0.7035     1.5092     0.0829   
   3800     3601.97     0.7172     0.7035     1.2159     0.1672   
   3900     3693.71     0.7172     0.7035     1.5278     0.088    
   4000     3786.07     0.7172     0.7035     1.8133     0.125    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  | 
   100       89.65      0.9504     0.9358     3.0834     0.1153   
   200       183.42     0.7157     0.7029     2.9357     0.1306   
   300       277.79     0.7172     0.7044     3.0426     0.1165   
   400       371.55     0.7172     0.7044     2.9774     0.1043   
   500       464.27     0.7174     0.7051     2.9504     0.1029   
   600       556.68     0.7172     0.7044     3.0541     0.1009   
   700       648.89     0.7172     0.7044     3.0182     0.0976   
   800       741.64     0.715      0.7013     2.5327     0.106    
   900       834.1      0.7172     0.7044     3.0097     0.1173   
   1000      924.89     0.7172     0.7044     2.3532     0.1227   
   1100     1013.86     0.7172     0.7044     2.1936     0.1389   
   1200     1102.62     0.7172     0.7044     2.5935     0.1257   
   1300      1191.4     0.7172     0.7044     2.1864     0.1242   
   1400     1280.16     0.7172     0.7044     2.7417     0.1298   
   1500     1368.93     0.7172     0.7044     2.1975     0.1488   
   1600     1457.74     0.7172     0.7044     2.1915     0.125    
   1700     1547.19     0.7172     0.7044     2.0704     0.1545   
   1800      1640.5     0.7172     0.7044     2.011      0.149    
   1900     1733.85     0.7172     0.7044     2.3244     0.1316   
   2000     1827.19     0.7172     0.7044     2.2394     0.1548   
   2100     1920.51     0.7172     0.7044     2.4694     0.1793   
   2200     2013.85     0.7172     0.7044     1.7356     0.1527   
   2300     2107.16     0.7172     0.7044     1.6375     0.158    
   2400     2200.45     0.7172     0.7044     1.8645     0.1464   
   2500     2293.82     0.7172     0.7044     1.5329     0.1258   
   2600     2387.09     0.7172     0.7044     1.9087     0.1182   
   2700     2480.41     0.7172     0.7044     1.6617     0.1375   
   2800     2574.04     0.7172     0.7044     1.7639     0.1463   
   2900     2668.72     0.7172     0.7044     2.0648     0.1335   
   3000     2762.05     0.7172     0.7044     1.3832     0.1218   
   3100     2855.47     0.7172     0.7044     2.2583     0.1635   
   3200      2949.0     0.7172     0.7044     1.3498     0.1532   
   3300     3042.43     0.7172     0.7044     1.7212     0.1176   
   3400     3136.71     0.7172     0.7044     1.9623     0.1376   
   3500     3232.17     0.7172     0.7044     1.3243     0.1448   
   3600     3327.05     0.7172     0.7044     1.3494     0.0992   
   3700     3421.17     0.7172     0.7044     1.5323     0.0992   
   3800     3514.45     0.7172     0.7044     1.168      0.1265   
   3900     3607.67     0.7172     0.7044     1.2365     0.1138   
   4000     3700.92     0.7172     0.7044     1.3242     0.124    
