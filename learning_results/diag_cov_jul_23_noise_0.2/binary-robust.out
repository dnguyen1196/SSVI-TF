Generating synthetic binary valued data ... 
Generating synthetic  binary valued data took:  8.283449172973633
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       173.11     1.4094     1.4156     3.1431   39168.1636   0.1377  
   200       413.19      1.41      1.4146     3.0108   173159.9202   0.4007  
   300       652.73     1.4158     1.4135     1.894    20055.1915   0.8983  
   400       892.04     1.4112     1.4061     3.0938   7542.3477    0.0186  
   500      1130.85     1.4227     1.422      1.6298    5800.312    0.0867  
   600      1369.29     1.4093     1.419      3.1598   5176.3467    0.0366  
   700      1607.68     1.4157     1.4147     3.1588   70985.0837   0.0453  
   800      1845.83     1.4196     1.4072     2.7006   16037.6764   0.0277  
   900      2083.82     1.4102     1.4184     1.0795   3479.3847    0.0187  
   1000     2320.99     1.417      1.4052     1.8349   6015.1542    0.0292  
   1100     2557.02     1.4144     1.4115     0.931     3093.99     0.0141  
   1200     2793.15     1.4259     1.4151     2.8791   3387.8217    0.0113  
   1300     3030.85     1.4172     1.4071     3.0616   3196.5255    0.0135  
   1400     3266.04     1.4104     1.4079     0.967     1199.197    0.013   
   1500     3500.27     1.406      1.4071     2.8301    373.711     0.0078  
   1600     3734.01     1.3998     1.4085     2.3127   2202.1756    0.0063  
   1700     3968.35     1.4221     1.4159     3.0898   5365.8018    0.0124  
   1800     4202.99     1.4096     1.4119     1.1957    834.0867    0.0091  
   1900     4437.35     1.4117     1.4117     0.9407   4934.3352     0.01   
   2000     4671.59     1.4222      1.41      3.1276   13712.2719   0.0067  
   2100     4905.51     1.4139     1.416      3.1405   59634.3008   0.0017  
   2200     5140.14     1.4167     1.4224     2.7227   8388.1986    0.0055  
   2300      5374.4      1.42      1.4099     2.8703    868.4393    0.0063  
   2400     5608.31     1.4152     1.4155     0.6966   1457.9224    0.0051  
   2500     5841.97     1.4152     1.407      2.2902    930.756     0.0031  
   2600      6075.3     1.4146     1.4125     2.852     383.4329    0.0032  
   2700     6308.34     1.4135     1.4149     2.1749   1259.9193    0.0028  
   2800     6541.94     1.416       1.41      3.1621   3459.4029    0.0024  
   2900     6775.86     1.4162      1.41      2.9019   1274.3519    0.0056  
   3000     7009.95     1.4178     1.4075     1.3828   55587.0291   0.0003  
   3100     7244.54     1.4139     1.4062     0.8999   143723.6246   0.0066  
   3200      7479.9     1.4139     1.4155     0.6474   14352.6732   0.0049  
   3300     7714.16     1.4056     1.4121     3.0938   2276.5495    0.0032  
   3400     7948.18     1.4131     1.4184     1.3102   1789.0042    0.0036  
   3500     8181.63     1.4227     1.4032     3.0872    803.9398    0.0029  
   3600      8414.4     1.4134     1.4134     1.3077   1182.2274    0.0011  
   3700     8647.26     1.4117     1.4127     0.5643   6338.8337    0.002   
   3800     8879.92     1.4181     1.4105     0.7976   2855.2704    0.0013  
   3900     9112.14     1.4135     1.4076     0.615    2520.1372    0.002   
   4000     9344.08     1.4181     1.4014     0.9793    692.9439    0.0018  
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       168.05     1.3444     1.3497     3.1572   4067.1902    0.0008  
   200       426.69     1.2041     1.1987     2.5159   9751.2981    0.0003  
   300       684.75     0.9648     0.956      3.1623   13747.3482   0.0002  
   400       941.95     0.7833     0.7779     1.9455   44631.5451   0.0001  
   500      1199.15     0.7567     0.7504     1.4006   2995.5893    0.0001  
   600      1456.33     0.7492     0.732      1.3928   7323.9716     0.0    
   700      1714.48     0.7363     0.7257     1.2104   361529.4268    0.0    
   800      1973.63     0.7378     0.7251     1.0574   1105.8123     0.0    
   900      2232.78     0.7599     0.7489     3.1405   29056.973     0.0    
   1000     2491.99     0.7399     0.7189     1.0767   35858.0291    0.0    
   1100     2751.08     0.731      0.7232     0.9965   11937.4983    0.0    
   1200     3010.21     0.7253     0.717      0.9363   79216.6951    0.0    
   1300     3269.18     0.7377     0.7298     0.808    2417.8004     0.0    
   1400     3528.15     0.7689     0.7566     1.0829   4385.5015     0.0    
   1500     3787.29     0.7478     0.7239     0.7771     4390.8      0.0    
   1600     4046.47     0.7543     0.7338     0.7609   5191.7062     0.0    
   1700     4305.42     0.7331     0.7248     0.9487   51189.1731    0.0    
   1800      4563.9     0.7488     0.7275     2.3519   3695.0601     0.0    
   1900      4822.0     0.7463     0.7218     3.0505    519.8527     0.0    
   2000     5078.63     0.739      0.7302     2.8007    792.2113     0.0    
   2100      5335.3     0.7439     0.7331     2.9004    8961.409     0.0    
   2200     5591.89     0.7402     0.7268     0.6578   6985.5901     0.0    
   2300     5846.92     0.7307     0.7198     0.6518   3126.3786     0.0    
   2400     6092.94     0.7625     0.7508     3.1378    1244.988     0.0    
   2500     6329.03     0.7343     0.723      0.6905   13699.6563    0.0    
   2600     6564.04     0.7295     0.7193     0.8366   33278.0815    0.0    
   2700      6800.1     0.734      0.7235     0.6966   5412.1624     0.0    
   2800     7034.51     0.7358     0.7192     0.7591   6163.2957     0.0    
   2900     7269.67     0.7402     0.7253     3.1598   2868.7848     0.0    
   3000     7504.15     0.7359     0.7244     0.5042   2951.4874     0.0    
   3100     7738.54     0.7345     0.7172     2.5151    2370.816     0.0    
   3200     7971.95     0.7357     0.7192     0.6384   18634.6467    0.0    
   3300     8206.48     0.732      0.7193     0.4883   1192.2569     0.0    
   3400     8441.66     0.7362     0.7185     0.6475    775.1777     0.0    
   3500     8674.91     0.733      0.7199     0.6947   22510.9469    0.0    
   3600     8909.33     0.7238     0.7181     0.8149   6739.8881     0.0    
   3700     9144.02     0.7274     0.7218     0.8404   1468.9198     0.0    
   3800     9378.44     0.738      0.729      0.7203   24830.8678    0.0    
   3900     9609.41     0.7239     0.7204     0.6865   6043.4346     0.0    
   4000     9841.32     0.734      0.7224     0.689    4612.9598     0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       150.17     0.9287     0.9142     3.1569     0.8244     0.0001  
   200       403.92     0.7641     0.7549     1.4362    195.8037    0.0001  
   300       657.26     0.8073     0.7997     1.2137   2254.2578    0.0001  
   400       911.05     1.1726     1.1712     1.6535   6293.0771    0.0001  
   500      1165.62     1.3462     1.3473     2.5443   7201.7437    0.0001  
   600      1422.41     1.3924     1.3885     1.6028   17203.7809    0.0    
   700      1681.72     1.4007     1.4047     1.6673   6037.1154     0.0    
   800      1936.87     1.4021     1.4044     3.1341   83745.118     0.0    
   900      2192.74     1.4116     1.4125     1.7829   24596.8531    0.0    
   1000     2451.75     1.4215     1.4134     1.6631   6821.8735     0.0    
   1100     2712.17     1.4149     1.4127     1.6073   5905.6902     0.0    
   1200     2973.64     1.4037     1.4118     1.4932   11306.2496    0.0    
   1300     3233.94     1.4148     1.4128     1.3179   43223.4518    0.0    
   1400     3494.22     1.4128     1.4138     1.2037   22405.0266    0.0    
   1500      3754.6     1.4127     1.4088     1.1636   399183.0754    0.0    
   1600     4015.54     1.4125     1.4132     1.204    18579.124     0.0    
   1700     4275.72     1.4134     1.4159     0.9577   9958.8823     0.0    
   1800     4535.96     1.4147     1.4154     1.0553   19481.8636    0.0    
   1900      4797.6     1.4168     1.4097     0.9588   67820.0907    0.0    
   2000     5058.44     1.4112     1.4131     1.1701   85514.4933    0.0    
   2100     5320.52     1.4155     1.4203     1.1168   216869.5213    0.0    
   2200     5582.51     1.4158     1.414      0.7946   22951.0515    0.0    
   2300     5845.86     1.4156     1.4179     0.8281   78468.8625    0.0    
   2400     6109.69     1.4099     1.4119     0.8547   10199.0211    0.0    
   2500     6372.75     1.4142     1.4093     0.9246   49760.9167    0.0    
   2600     6636.11     1.4181     1.4136     0.825    79300.6364    0.0    
   2700     6898.59     1.403      1.4174     0.841    31533.227     0.0    
   2800     7160.63     1.4163     1.4093      0.93    144580.1016    0.0    
   2900     7422.71     1.4143     1.4149     0.7353    3440.961     0.0    
   3000     7684.66     1.4187     1.4127     0.723    33651.4231    0.0    
   3100     7946.49     1.421      1.4121     0.8538   155506.8967    0.0    
   3200     8208.59     1.4154     1.412      0.8424   163062.8812    0.0    
   3300     8469.98     1.4112     1.414       0.84    229978.8285    0.0    
   3400     8731.43     1.4198     1.4139     0.6788   3624.7882     0.0    
   3500     8992.67     1.4117     1.4163     0.7041   62231.4962    0.0    
   3600      9254.1     1.4123     1.4189     0.8281   249935.3311    0.0    
   3700     9517.03     1.4112     1.4154     0.5995    1199.967     0.0    
   3800     9779.97     1.4019     1.4139     0.6834    1198.052     0.0    
   3900     10042.32    1.4063     1.4121     1.9181   2007.8948     0.0    
   4000     10304.91    1.4151     1.4115     0.6075   9744.5883     0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       153.84     1.2145     1.2041     3.156      0.236      0.0001  
   200       439.45     0.7767     0.763      1.2803     0.6089      0.0    
   300       725.91     0.7511     0.7427     1.001     15.7392      0.0    
   400      1005.05     1.0171     1.0073     1.0813    176.0983     0.0    
   500      1282.67     1.4128     1.4076     1.2885    689.3406     0.0    
   600      1561.51     1.4166     1.4107     1.2382    657.5462     0.0    
   700      1841.38     1.4112     1.413      1.3085   3196.4888     0.0    
   800      2121.69     1.409      1.418      1.5249    19608.64     0.0    
   900      2402.38     1.4085     1.4125     3.1006   12968.8757    0.0    
   1000     2686.25     1.4182     1.4183     1.4453   5997.2986     0.0    
   1100     2972.69     1.413      1.4131     3.1623   10103.5885    0.0    
   1200     3271.42     1.4093     1.416      1.3685    3614.456     0.0    
   1300     3561.81     1.4215     1.4116     2.7635   5152.5133     0.0    
   1400     3848.67     1.4147     1.4125     1.4978   15262.623     0.0    
   1500     4132.11     1.4099     1.4157     1.5602   21267.9623    0.0    
   1600     4415.45     1.4241     1.4156     1.3643   621299.8452    0.0    
   1700     4696.51     1.4153     1.414      1.2248   32096.2439    0.0    
   1800     4977.47     1.4066     1.413      1.3581   15543.6949    0.0    
   1900     5258.41     1.4202     1.4161     1.1438   7700.1597     0.0    
   2000     5539.34     1.4195     1.4132     1.8446   88240.9698    0.0    
   2100     5820.43     1.4105     1.4148     1.4787   144416.0502    0.0    
   2200     6102.57     1.4203     1.4169     1.6433   29451.4734    0.0    
   2300     6384.97     1.4118     1.4153     1.4999   3918.8663     0.0    
   2400     6667.72     1.419      1.4151     1.6115   31104.8403    0.0    
   2500     6950.52     1.4131     1.4084     1.226    41428.9172    0.0    
   2600     7232.71     1.4236     1.4167     1.2781   21824.5074    0.0    
   2700     7513.79     1.4138     1.4142     1.1435   90867.1254    0.0    
   2800      7794.8     1.4135     1.4134     1.2876   1365087.974    0.0    
   2900     8076.32     1.4096     1.4146     1.1853   3045.7385     0.0    
   3000     8357.85     1.4153     1.4191     1.5008   44040.1876    0.0    
   3100     8639.53     1.4071     1.4144     1.053    14873.9936    0.0    
   3200     8921.22     1.4089     1.4143     1.1253   150801.9432    0.0    
   3300     9202.78     1.4111     1.4132     1.4759   28144.805     0.0    
   3400     9484.38     1.4131     1.4191     1.319    316622.8163    0.0    
   3500     9765.62     1.4142     1.4125     1.2521   189954.9474    0.0    
   3600     10043.11    1.417      1.4149     1.1478   95105.2083    0.0    
   3700     10314.55    1.4062     1.4143     1.2286   58021.0342    0.0    
   3800     10585.8     1.4166     1.4164     0.9588   91024.5449    0.0    
   3900     10856.9     1.4106     1.4126     0.9999   39453.0066    0.0    
   4000     11128.08    1.4177     1.4172     0.9365   23509.1331    0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  0.001  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       144.12     0.9701     0.9661     3.158      0.1004     0.0001  
   200       435.75     0.7541     0.7452     1.2872     0.1311      0.0    
   300       727.39     0.7271     0.7247     1.2552     0.1485      0.0    
   400       1027.9     0.7409     0.7343     1.188      0.1937      0.0    
   500      1328.58     0.7353     0.7261     1.0035     0.3296      0.0    
   600      1623.91     0.7418     0.7311     1.007      0.5165      0.0    
   700      1918.53     0.7383     0.7246     0.9223     1.1226      0.0    
   800      2216.26     0.7354     0.7178     0.9724     3.297       0.0    
   900      2512.48     0.7352     0.7223     0.8901    18.5801      0.0    
   1000      2808.5     0.7572     0.7423     0.9854    494.2917     0.0    
   1100     3103.12     0.7559     0.7408     0.8619    72.1659      0.0    
   1200     3396.83     0.8179     0.7951     1.0027    515.912      0.0    
   1300     3691.05     1.0156     1.0137     1.1781    1719.55      0.0    
   1400     3985.72     1.3638     1.3643     1.3633   10279.0416    0.0    
   1500     4286.34     1.4116     1.4104     1.577    3714.8874     0.0    
   1600      4588.8     1.4208     1.4178     1.3955   9301.3589     0.0    
   1700     4888.94     1.4136     1.4129     1.0776   13278.2189    0.0    
   1800     5183.74     1.4148     1.4145     1.1951   11939.7162    0.0    
   1900     5487.34     1.4164     1.4143     3.1481   8813.9832     0.0    
   2000     5792.01     1.4091     1.4148     1.2635   6346.9327     0.0    
   2100     6096.72     1.4188     1.4103     1.0836   8056.2595     0.0    
   2200     6401.45     1.411      1.4154     1.3099   19042.2348    0.0    
   2300     6706.11     1.4149     1.4147     0.9907   14707.2728    0.0    
   2400     7008.37     1.4207     1.4185     1.0906   64410.9246    0.0    
   2500      7308.3     1.4092     1.4136     1.1058   11217.4978    0.0    
   2600     7605.02     1.4178     1.4124     1.1476   2119.1949     0.0    
   2700     7899.99     1.4114     1.4118     1.0751   1876.9096     0.0    
   2800     8194.98     1.415      1.4111     1.1402   3342.9321     0.0    
   2900     8489.99     1.4161     1.4162     2.9152   1805.6967     0.0    
   3000      8785.0     1.4122     1.4185     1.2769   3411.0527     0.0    
   3100     9080.16     1.4179     1.4153     2.1303   11994.4331    0.0    
   3200     9378.08     1.4165     1.4193     1.4252   217355.7309    0.0    
   3300     9676.15     1.4111     1.4141     1.5641   14778.2073    0.0    
   3400     9972.21     1.4117     1.4152     1.5332   152754.7693    0.0    
   3500     10269.4     1.4196     1.4113     1.4991   291384.603    0.0    
   3600     10567.7     1.4194     1.4151     1.2148   788183.2535    0.0    
   3700     10865.92    1.4152     1.415      1.123    20194.1624    0.0    
   3800     11168.28    1.4089     1.4158     1.3028   23242.7476    0.0    
   3900     11472.99    1.4156     1.4176     1.0202   37209.2727    0.0    
   4000     11777.4     1.4038     1.4141     1.0547   185313.6015    0.0    
