Generating synthetic real valued data ... 
Generating synthetic  real valued data took:  3.8184070587158203
Using  0.2  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       41.55     443.8421   444.5028    3.1612     0.6932      0.0    
   200       83.44     429.5019    428.44     2.8154     0.2332      0.0    
   300       125.37    423.5084   424.6623    2.4755     0.1281      0.0    
   400       167.3     422.7422   423.7971    1.9413     0.0793      0.0    
   500       209.22    421.7747   423.0267    2.4481     0.057       0.0    
   600       251.15    421.9599   422.3267    2.0974     0.0417      0.0    
   700       292.93    422.4351   421.9509    1.9036     0.0327      0.0    
   800       334.61    421.962    421.2214    1.6905     0.0259      0.0    
   900       376.3     422.0223   422.6017    1.7407     0.0216      0.0    
   1000      418.01    420.7305   420.7054    1.4224     0.0183      0.0    
   1100      459.78    420.0213   420.2176    1.6693     0.0158      0.0    
   1200      501.57    419.3497   420.3446    1.4197     0.0137      0.0    
   1300      543.4     420.6671   420.2243    1.4744     0.0121      0.0    
   1400      585.21    419.1857   419.9761    1.4055     0.0109      0.0    
   1500      627.06    419.2184   420.4646    1.3871     0.0095      0.0    
   1600      668.9     418.629    419.991     1.3031     0.0086      0.0    
   1700      710.8     418.7321   419.3954    1.3008     0.0078      0.0    
   1800      752.69    418.1647   418.7738    1.3325     0.0073      0.0    
   1900      794.52    420.7432   420.4851    1.4413     0.0065      0.0    
   2000      836.3     418.6443   418.9051    0.9857     0.0062      0.0    
   2100      878.1     419.6379   421.0879    1.6013     0.0055      0.0    
   2200      919.91    418.8894   419.1356    1.0099     0.0053      0.0    
   2300      961.72    419.2521   420.0832    1.2794     0.0048      0.0    
   2400     1003.49    418.6359   419.2481    0.9917     0.0046      0.0    
   2500     1045.26    418.1561   418.8567    1.3238     0.0042      0.0    
   2600     1087.05    417.8176   420.1809    1.1819     0.004       0.0    
   2700     1128.85    418.374    419.5843    1.5533     0.0038      0.0    
   2800     1170.62    418.3773   419.8441    1.2958     0.0036      0.0    
   2900     1212.41    419.6956   419.574     1.3089     0.0034      0.0    
   3000     1254.16    418.6586   419.4835    1.1612     0.0032      0.0    
   3100     1295.92    419.0774   419.1837    0.9178     0.003       0.0    
   3200     1337.71    419.0562   419.0893    1.2297     0.0028      0.0    
   3300     1379.47    418.0134   419.4201     1.07      0.0027      0.0    
   3400     1421.22    418.9248   419.1194    1.0397     0.0026      0.0    
   3500     1462.96    417.0997   418.5941    1.0687     0.0024      0.0    
   3600     1504.78    418.7048   418.6959    1.0349     0.0024      0.0    
   3700     1546.63    419.1082   419.1474    1.055      0.0023      0.0    
   3800     1588.47    418.8467   418.634     0.9652     0.0022      0.0    
   3900     1630.27    419.0513   418.7029    1.0437     0.0021      0.0    
   4000     1672.03    418.7543   419.3464    1.1573     0.002       0.0    
Using  0.4  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       41.65     435.7607   437.8306    3.162      0.6382      0.0    
   200       83.83     426.6521   428.8219    2.6149     0.3228      0.0    
   300       126.0     423.1836   425.5561    2.0889     0.203       0.0    
   400       168.2     421.8545   423.3527    2.1977     0.1392      0.0    
   500       210.38    421.9928   424.0339    2.1376     0.1026      0.0    
   600       252.58    420.5853    422.28     1.7005     0.0814      0.0    
   700       294.81    420.3059   422.8636    1.9258     0.0645      0.0    
   800       337.03    418.7957   421.256     1.8265     0.0534      0.0    
   900       378.82    419.818    420.9604    1.3853     0.0437      0.0    
   1000      420.61    420.0404   421.1826    1.3125     0.0359      0.0    
   1100      462.41    421.6154   422.8692    1.9705     0.0312      0.0    
   1200      504.16    419.8377   421.3973    1.6169     0.0269      0.0    
   1300      545.88    420.2837   422.4202    1.3956     0.024       0.0    
   1400      587.62    419.9045   421.7196    1.7058     0.0218      0.0    
   1500      629.4     418.2865    420.29     1.6667     0.0187      0.0    
   1600      671.13    419.5898   421.0307    1.2898     0.0168      0.0    
   1700      712.83    420.4565   422.2833    1.3215     0.0153      0.0    
   1800      754.55    418.6312   420.2806    1.5649     0.0143      0.0    
   1900      796.27    420.2517   421.0885    1.328      0.0125      0.0    
   2000      837.93    419.3153   420.8046    1.6146     0.0117      0.0    
   2100      879.52    418.483    420.3399    1.0348     0.0107      0.0    
   2200      921.1     418.0388   420.0782    1.2674      0.01       0.0    
   2300      962.72    418.3744   419.8949    1.2983     0.0093      0.0    
   2400     1004.33    417.7074   419.6302    1.3925     0.0085      0.0    
   2500     1046.07    418.0757   419.277     1.1146     0.0079      0.0    
   2600     1087.87    418.0857   419.2723    1.2366     0.0074      0.0    
   2700     1129.62    419.1609   420.1729    1.0902     0.007       0.0    
   2800      1171.4    419.0021   419.9003    1.1402     0.0066      0.0    
   2900     1213.19    418.9575   420.2889    1.2241     0.0061      0.0    
   3000     1254.96    418.106    419.5156    0.9728     0.0058      0.0    
   3100     1296.75    417.5881   419.4264    1.2866     0.0057      0.0    
   3200     1338.48    417.5676   419.2832    1.056      0.0053      0.0    
   3300     1380.32    419.3268   420.2571    1.1548     0.0048      0.0    
   3400     1422.13    417.9294   419.4802    1.0085     0.0047      0.0    
   3500     1463.92    417.5647   419.1154    1.0652     0.0046      0.0    
   3600     1505.71    417.5458   419.1625    0.8828     0.0042      0.0    
   3700     1547.46    418.4237   419.8668    0.9185     0.0041      0.0    
   3800     1589.25    417.6888   418.9121    1.133      0.0038      0.0    
   3900     1631.02    418.5646   419.4908    0.8933     0.0038      0.0    
   4000     1672.82    418.8249   419.7901    0.9487     0.0035      0.0    
Using  0.6  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       41.39     440.4131   440.8184    3.1621     0.5237     0.0001  
   200       83.57     426.3961   427.2998    3.1354     0.3098      0.0    
   300       125.8     425.7454   426.4345    2.2821     0.2206      0.0    
   400       168.02    422.6589   424.4575    2.0965     0.1629      0.0    
   500       210.27    421.1676   423.2265    2.3877     0.1135      0.0    
   600       252.53    422.3514   423.4758    2.2214     0.0907      0.0    
   700       294.78    420.1507   422.2181    1.754      0.0734      0.0    
   800       337.05    419.5155   421.2346    1.7712     0.0606      0.0    
   900       379.32    420.2523   422.6684    1.6198     0.051       0.0    
   1000      421.56    420.1491   421.1866    1.6899     0.0436      0.0    
   1100      463.78    420.9057   422.2068    1.5403     0.0381      0.0    
   1200      506.01    420.7426   422.4733    1.6945     0.0339      0.0    
   1300      548.25    419.1326   420.1544    1.5421     0.0297      0.0    
   1400      590.48    418.5758   420.2357    1.781      0.0266      0.0    
   1500      632.8     418.8538   420.2133    1.3755     0.0236      0.0    
   1600      675.12    418.5906   420.8154    1.7065     0.0213      0.0    
   1700      717.43    419.6736   420.6577    1.5665     0.0194      0.0    
   1800      759.78    419.354    420.6315    1.2375     0.0179      0.0    
   1900      802.09    418.3036   419.4942    1.3905     0.0161      0.0    
   2000      844.44    419.8501   421.1939    1.3681     0.0153      0.0    
   2100      886.83    418.2346   420.9303    1.481      0.0136      0.0    
   2200      929.16    418.7502   420.1329    1.3433     0.0128      0.0    
   2300      971.46    417.9254   419.8285    1.2132     0.0118      0.0    
   2400     1013.78    418.501    419.8391    1.1149     0.011       0.0    
   2500     1056.06    417.9291   419.3659    1.2133     0.0099      0.0    
   2600     1098.17    419.0555   420.2228    1.2461     0.0092      0.0    
   2700     1140.01    418.5143   419.9896    1.0753     0.0089      0.0    
   2800     1182.17    418.1562   419.1113    0.9935     0.0082      0.0    
   2900     1224.26    418.173    419.4905    1.2133     0.0079      0.0    
   3000     1266.43    417.4121   419.5359    1.422      0.0073      0.0    
   3100     1308.77    417.7984   419.6479    0.8789     0.007       0.0    
   3200     1351.08    419.0978   420.1168    1.2872     0.0065      0.0    
   3300     1393.38    417.0035   419.391     1.2806     0.0062      0.0    
   3400     1435.69    418.3466   420.5705    1.0348     0.006       0.0    
   3500     1477.97    417.4489   419.3403    0.937      0.0057      0.0    
   3600     1520.25    417.2091   419.5031    0.9595     0.0054      0.0    
   3700     1562.57    417.0926   419.2005    1.1353     0.0051      0.0    
   3800     1605.05    417.7311   419.0359    1.1264     0.0048      0.0    
   3900     1647.55    417.5534   419.0566    1.1179     0.0047      0.0    
   4000     1690.04    417.4846   419.6118    1.0889     0.0044      0.0    
Using  0.8  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       41.55     441.9261   443.4001    3.1619     0.4528     0.0001  
   200       84.26     428.4729   429.8914    2.8941     0.2933      0.0    
   300       126.99    424.7629   426.3346    2.5321     0.2042      0.0    
   400       169.71    421.9153   423.8991    2.0353     0.1578      0.0    
   500       212.46    420.517    422.9438    1.9025     0.1212      0.0    
   600       255.2     419.9163   422.3404    1.7482     0.0968      0.0    
   700       297.92    420.6058   422.8882    1.7405     0.0788      0.0    
   800       340.65    419.9854   422.6471    1.9355     0.0676      0.0    
   900       383.32    419.8281   421.6235    1.7309     0.0588      0.0    
   1000      425.99    420.8373   422.3429    1.7005     0.0503      0.0    
   1100      468.65    420.3036   422.6515    1.4789     0.0429      0.0    
   1200      511.29    418.8725   421.5831    1.7619     0.0383      0.0    
   1300      553.96    418.422    420.643     1.7674     0.0333      0.0    
   1400      596.62    420.3988   421.8888    1.754      0.0305      0.0    
   1500      639.28    419.8689   421.0908    1.7932     0.0269      0.0    
   1600      681.91    419.2995   421.5312    1.3674     0.0242      0.0    
   1700      724.6     419.1072   421.2615    1.2987     0.0216      0.0    
   1800      767.26    417.6009   420.1183    1.1582     0.0198      0.0    
   1900      810.01    418.7654   420.9356    1.3398     0.0185      0.0    
   2000      852.73    417.6958   420.0264    1.0922     0.0168      0.0    
   2100      895.47    418.3033   421.1692    1.2257     0.0159      0.0    
   2200      938.22    419.0169   420.8394    1.6669     0.0143      0.0    
   2300      980.95    418.0057   420.1721    1.631      0.0134      0.0    
   2400     1023.68    419.7861   421.6645    1.3368     0.0126      0.0    
   2500     1066.49    418.4833   420.1639    1.369      0.0115      0.0    
   2600     1109.31    419.1545   421.7584    1.3942     0.0109      0.0    
   2700     1152.17    417.7928   420.3541    1.2814     0.0102      0.0    
   2800     1195.02    417.7043   420.3183    1.6146     0.0095      0.0    
   2900      1237.9    420.6216   421.5107    1.0957     0.0091      0.0    
   3000     1280.64    417.6606   419.8976    0.9462     0.0086      0.0    
   3100     1323.38    417.8066   419.9031    1.0703     0.0082      0.0    
   3200     1366.15    418.4903   420.1483    1.1004     0.0078      0.0    
   3300     1408.93    417.3589   419.756     1.0465     0.0075      0.0    
   3400     1451.82    417.1405   420.2507    1.112      0.0071      0.0    
   3500     1494.72    416.6678   419.3369    1.2062     0.0066      0.0    
   3600     1537.49    417.3306   419.6986    0.9839     0.0063      0.0    
   3700     1580.27    416.4034   419.531     0.9431     0.006       0.0    
   3800     1623.01    417.0813   419.4112    1.2931     0.0056      0.0    
   3900     1665.72    417.0185   419.5945    1.0934     0.0056      0.0    
   4000     1708.38    417.3314   419.7025    1.0838     0.0053      0.0    
Using  1  of training data 
Tensor dimensions:  [50, 50, 50]
Optimization metrics: 
Using Ada Grad
Mean update scheme:  S
Covariance update :  N
k1 samples =  64  k2 samples =  64
eta =  1  cov eta =  1  sigma eta =  1
iteration |   time   |test_rsme |train_rsme|  d_mean  |   d_cov  |    dw   
   100       41.73     440.4331   441.3719    3.1609     0.3927     0.0001  
   200       84.74     425.6643   426.5145    2.9578     0.265       0.0    
   300       127.74    426.6584   427.9438    2.8082     0.2001      0.0    
   400       170.83    424.854    424.9593    2.4139     0.1583      0.0    
   500       213.82    420.5206   422.7121    2.3016     0.1206      0.0    
   600       256.77    422.6267   424.8273    2.2752     0.1036      0.0    
   700       299.76    420.0826   422.2858    1.9034     0.0838      0.0    
   800       342.7     419.2355   421.455      1.54      0.0687      0.0    
   900       385.63    420.1289   422.4887    1.7326     0.0636      0.0    
   1000      428.63    418.391    421.3199    1.8008     0.0535      0.0    
   1100      471.66    419.215    421.916      1.39      0.0472      0.0    
   1200      514.66    418.6519   421.3599    1.581      0.0399      0.0    
   1300      557.69    420.4853   422.1082    1.5115     0.036       0.0    
   1400      600.66    419.8373   420.8458    1.4184     0.0324      0.0    
   1500      643.54    418.2496   421.5363    1.6079     0.0302      0.0    
   1600      686.42    419.1698   420.2433    1.6119     0.0275      0.0    
   1700      729.29    419.2233   421.1115    1.6095     0.0254      0.0    
   1800      772.24    418.6208   420.2841    1.3573     0.0231      0.0    
   1900      815.32    417.6132   420.1003    1.7478     0.0207      0.0    
   2000      858.31    419.1809   420.9071    1.343      0.019       0.0    
   2100      901.45    418.7877   420.3472    1.5318     0.0172      0.0    
   2200      944.46    420.6852   421.8037    1.3488     0.0162      0.0    
   2300      987.53    419.0167   420.3071    1.3764     0.0151      0.0    
   2400     1030.73    419.3035   420.9627    1.3129     0.014       0.0    
   2500      1073.8    417.7557   419.791     1.0865     0.0129      0.0    
   2600     1116.84    418.7537   420.7592    0.8812     0.0127      0.0    
   2700     1160.01    418.1948   419.9859    1.213      0.0115      0.0    
   2800     1203.27    418.5118   420.3097    1.1604     0.0105      0.0    
   2900     1246.52    417.8754   420.1893    1.1407     0.0101      0.0    
   3000     1289.93    418.1774   419.8685    1.0728     0.0095      0.0    
   3100     1333.28    418.7297   420.0675    1.1168     0.0091      0.0    
   3200     1376.55    417.7377   419.3287    1.0347     0.0085      0.0    
   3300     1419.82    418.8605   419.7966    1.1741     0.0083      0.0    
   3400     1463.05    417.5715   419.3856    1.1713     0.008       0.0    
   3500     1506.45    417.0678   419.527     1.4477     0.0075      0.0    
   3600     1549.65    417.9736   419.4798    1.0325     0.0071      0.0    
   3700     1593.04    417.1982   419.6222    1.1053     0.0067      0.0    
   3800     1636.16    417.8508   420.1504    1.1207     0.0065      0.0    
   3900     1679.26    417.6109   419.1228    1.0435     0.0061      0.0    
   4000     1722.31    417.0544   418.9046    0.9645     0.006       0.0    
